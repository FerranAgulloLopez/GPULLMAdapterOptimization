INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 8640, 8640, 17280, 34560, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 336960 . Total input tokens: 75294651 . Total output tokens: 65936992
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 7.608463103882968,
    "estimated_duration": 3600.0748630134494,
    "input_throughput": 7703.093423114851,
    "output_throughput": 6656.589629900281,
    "total_throughput": 14359.683053015131,
    "itl": 74.20510188431611,
    "ttft": 20182.334767881013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 112207,
    "finished_requests": 111582,
    "scheduler_time": 88.6810376197302
}
#Debug simulation 
Total elapsed time: 7.608653716742992. Arrivals time: 0.28027717769145966 Scheduler time: 7.148934605065733 Scheduler overhead time: 0.06691929884254932 Adapter cache time: 0.011154945939779282 Engine time: 0.07035364955663681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 8640, 8640, 17280, 34560, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 336960 . Total input tokens: 75294651 . Total output tokens: 65936992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 7.58964279294014,
    "estimated_duration": 3600.0362785138364,
    "input_throughput": 7703.001818483874,
    "output_throughput": 6656.50180889084,
    "total_throughput": 14359.503627374714,
    "itl": 74.2043485239554,
    "ttft": 20250.049779933965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 112207,
    "finished_requests": 111580,
    "scheduler_time": 88.68109422970399
}
#Debug simulation 
Total elapsed time: 7.589765828102827. Arrivals time: 0.27440456161275506 Scheduler time: 7.136877798940986 Scheduler overhead time: 0.06689154263585806 Adapter cache time: 0.011141880881041288 Engine time: 0.06970014236867428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 8640, 8640, 17280, 34560, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 336960 . Total input tokens: 75294651 . Total output tokens: 65936992
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 7.546425886917859,
    "estimated_duration": 3600.033719811302,
    "input_throughput": 7702.957293814885,
    "output_throughput": 6656.457373753727,
    "total_throughput": 14359.414667568612,
    "itl": 74.20244888924961,
    "ttft": 20303.68462789007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 112207,
    "finished_requests": 111579,
    "scheduler_time": 88.68432164715009
}
#Debug simulation 
Total elapsed time: 7.546533769927919. Arrivals time: 0.26899430667981505 Scheduler time: 7.099393573589623 Scheduler overhead time: 0.0666301716119051 Adapter cache time: 0.011114687193185091 Engine time: 0.06953398976475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 8640, 8640, 17280, 34560, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 336960 . Total input tokens: 75294651 . Total output tokens: 65936992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 7.5747804450802505,
    "estimated_duration": 3600.006697116829,
    "input_throughput": 7702.948170126716,
    "output_throughput": 6656.37150597288,
    "total_throughput": 14359.319676099596,
    "itl": 74.20403016992044,
    "ttft": 20314.33228215538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 112207,
    "finished_requests": 111577,
    "scheduler_time": 88.68029295967067
}
#Debug simulation 
Total elapsed time: 7.57487825024873. Arrivals time: 0.2685663620941341 Scheduler time: 7.1264659175649285 Scheduler overhead time: 0.06747441552579403 Adapter cache time: 0.011138061992824078 Engine time: 0.070385389495641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 8640, 8640, 17280, 34560, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 336960 . Total input tokens: 75294651 . Total output tokens: 65936992
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 7.588976562023163,
    "estimated_duration": 3600.009970737483,
    "input_throughput": 7702.941165554386,
    "output_throughput": 6656.365453090966,
    "total_throughput": 14359.306618645353,
    "itl": 74.20174681161038,
    "ttft": 20335.96602608795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 112207,
    "finished_requests": 111577,
    "scheduler_time": 88.6834602552281
}
#Debug simulation 
Total elapsed time: 7.5891202739439905. Arrivals time: 0.2674800679087639 Scheduler time: 7.140512186102569 Scheduler overhead time: 0.06812297739088535 Adapter cache time: 0.011164999101310968 Engine time: 0.07071157917380333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 8640, 8640, 17280, 34560, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 336960 . Total input tokens: 75294651 . Total output tokens: 65936992
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 7.5863370769657195,
    "estimated_duration": 3600.0669015732296,
    "input_throughput": 7703.110458275439,
    "output_throughput": 6656.604350749047,
    "total_throughput": 14359.714809024485,
    "itl": 74.20452734354348,
    "ttft": 20185.78328067726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 112207,
    "finished_requests": 111582,
    "scheduler_time": 88.68183882837874
}
#Debug simulation 
Total elapsed time: 7.586517101153731. Arrivals time: 0.2750948038883507 Scheduler time: 7.1322901248931885 Scheduler overhead time: 0.06709098257124424 Adapter cache time: 0.011162362527102232 Engine time: 0.06982908770442009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 8640, 8640, 17280, 34560, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 336960 . Total input tokens: 75294651 . Total output tokens: 65936992
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 7.590442867018282,
    "estimated_duration": 3600.0089238675846,
    "input_throughput": 7702.94340554251,
    "output_throughput": 6656.367388738563,
    "total_throughput": 14359.310794281073,
    "itl": 74.2019312882541,
    "ttft": 20335.93771107539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 112207,
    "finished_requests": 111577,
    "scheduler_time": 88.68347785687412
}
#Debug simulation 
Total elapsed time: 7.590545909013599. Arrivals time: 0.2770843617618084 Scheduler time: 7.133903351612389 Scheduler overhead time: 0.06704048719257116 Adapter cache time: 0.011127293109893799 Engine time: 0.0704090571962297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 4320, 4320, 17280, 34560, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 315360 . Total input tokens: 70416790 . Total output tokens: 61745315
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 7.205496804788709,
    "estimated_duration": 3600.0090207759495,
    "input_throughput": 7191.52697967955,
    "output_throughput": 6262.630973946981,
    "total_throughput": 13454.15795362653,
    "itl": 64.86416704238673,
    "ttft": 16013.443341555892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 105029,
    "finished_requests": 104562,
    "scheduler_time": 81.59738906337009
}
#Debug simulation 
Total elapsed time: 7.205606327857822. Arrivals time: 0.27659966982901096 Scheduler time: 6.729430697858334 Scheduler overhead time: 0.07463255384936929 Adapter cache time: 0.012404018081724644 Engine time: 0.07806503772735596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 4320, 4320, 17280, 34560, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 315360 . Total input tokens: 70416790 . Total output tokens: 61745315
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 7.241482831072062,
    "estimated_duration": 3600.0657920417416,
    "input_throughput": 7191.485793740687,
    "output_throughput": 6262.532770884036,
    "total_throughput": 13454.018564624723,
    "itl": 64.86425964692324,
    "ttft": 16081.542319818296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 105029,
    "finished_requests": 104563,
    "scheduler_time": 81.5990552109718
}
#Debug simulation 
Total elapsed time: 7.241617646999657. Arrivals time: 0.2627574233338237 Scheduler time: 6.780294467695057 Scheduler overhead time: 0.07437247829511762 Adapter cache time: 0.012381386943161488 Engine time: 0.07760693179443479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 4320, 4320, 17280, 34560, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 315360 . Total input tokens: 70416790 . Total output tokens: 61745315
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 7.166781653650105,
    "estimated_duration": 3600.0078562251692,
    "input_throughput": 7191.529306035128,
    "output_throughput": 6262.632999818056,
    "total_throughput": 13454.162305853184,
    "itl": 64.86446773265601,
    "ttft": 16013.386379232841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 105029,
    "finished_requests": 104562,
    "scheduler_time": 81.59777278746031
}
#Debug simulation 
Total elapsed time: 7.166882146615535. Arrivals time: 0.2597541557624936 Scheduler time: 6.708224109373987 Scheduler overhead time: 0.07526901504024863 Adapter cache time: 0.012252685148268938 Engine time: 0.07714928407222033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 4320, 4320, 17280, 34560, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 315360 . Total input tokens: 70416790 . Total output tokens: 61745315
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 7.193601805251092,
    "estimated_duration": 3600.029414682535,
    "input_throughput": 7191.558461830809,
    "output_throughput": 6262.596052145912,
    "total_throughput": 13454.15451397672,
    "itl": 64.86391371047092,
    "ttft": 16013.410330726607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 105029,
    "finished_requests": 104563,
    "scheduler_time": 81.59795494123595
}
#Debug simulation 
Total elapsed time: 7.193724723998457. Arrivals time: 0.2569384463131428 Scheduler time: 6.738946139346808 Scheduler overhead time: 0.07411860534921288 Adapter cache time: 0.012336979620158672 Engine time: 0.07712256629019976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 4320, 4320, 17280, 34560, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 315360 . Total input tokens: 70416790 . Total output tokens: 61745315
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 7.2083918559364974,
    "estimated_duration": 3600.0065242727014,
    "input_throughput": 7191.531966801196,
    "output_throughput": 6262.635316905378,
    "total_throughput": 13454.167283706574,
    "itl": 64.86449438425831,
    "ttft": 16013.4072300567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 105029,
    "finished_requests": 104562,
    "scheduler_time": 81.59776882460213
}
#Debug simulation 
Total elapsed time: 7.208510355092585. Arrivals time: 0.266590541228652 Scheduler time: 6.7450806512497365 Scheduler overhead time: 0.07379369623959064 Adapter cache time: 0.012287538964301348 Engine time: 0.07653381023555994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 4320, 4320, 17280, 34560, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 315360 . Total input tokens: 70416790 . Total output tokens: 61745315
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 7.198519965168089,
    "estimated_duration": 3600.0086375151627,
    "input_throughput": 7191.527745297238,
    "output_throughput": 6262.631640673401,
    "total_throughput": 13454.159385970639,
    "itl": 64.86435160045576,
    "ttft": 16013.458616405429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 105029,
    "finished_requests": 104562,
    "scheduler_time": 81.59757189923062
}
#Debug simulation 
Total elapsed time: 7.198700906243175. Arrivals time: 0.27004532516002655 Scheduler time: 6.730409542564303 Scheduler overhead time: 0.07418581005185843 Adapter cache time: 0.01224780734628439 Engine time: 0.07764783222228289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 4320, 4320, 17280, 34560, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 315360 . Total input tokens: 70416790 . Total output tokens: 61745315
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 7.240335400216281,
    "estimated_duration": 3600.068761496528,
    "input_throughput": 7191.479861967344,
    "output_throughput": 6262.527605341614,
    "total_throughput": 13454.007467308957,
    "itl": 64.86413699895829,
    "ttft": 16081.562896354257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 105029,
    "finished_requests": 104563,
    "scheduler_time": 81.59912740289272
}
#Debug simulation 
Total elapsed time: 7.240442066919059. Arrivals time: 0.2685575485229492 Scheduler time: 6.771989406552166 Scheduler overhead time: 0.07489613816142082 Adapter cache time: 0.012406317051500082 Engine time: 0.07816954981535673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 1080, 1080, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 299160 . Total input tokens: 66820400 . Total output tokens: 58543101
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.862219651695341,
    "estimated_duration": 3600.057874622931,
    "input_throughput": 6870.667878524235,
    "output_throughput": 5923.516716306561,
    "total_throughput": 12794.184594830795,
    "itl": 57.0254883050358,
    "ttft": 15251.444005275878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 99759,
    "finished_requests": 99339,
    "scheduler_time": 75.22311480035718
}
#Debug simulation 
Total elapsed time: 6.862386793829501. Arrivals time: 0.24246363574638963 Scheduler time: 6.401277996599674 Scheduler overhead time: 0.08144261781126261 Adapter cache time: 0.014946337789297104 Engine time: 0.08483091462403536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 1080, 1080, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 299160 . Total input tokens: 66820400 . Total output tokens: 58543101
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.831360177136958,
    "estimated_duration": 3600.0320252113747,
    "input_throughput": 6870.646935022118,
    "output_throughput": 5923.519805007268,
    "total_throughput": 12794.166740029385,
    "itl": 57.02623560932238,
    "ttft": 15323.758966713538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 99759,
    "finished_requests": 99337,
    "scheduler_time": 75.22266441902909
}
#Debug simulation 
Total elapsed time: 6.831481974106282. Arrivals time: 0.25543596688658 Scheduler time: 6.357772353570908 Scheduler overhead time: 0.08098169416189194 Adapter cache time: 0.015008762013167143 Engine time: 0.08477050298824906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 1080, 1080, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 299160 . Total input tokens: 66820400 . Total output tokens: 58543101
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.869119454175234,
    "estimated_duration": 3600.041976938751,
    "input_throughput": 6870.698219200466,
    "output_throughput": 5923.542874389882,
    "total_throughput": 12794.241093590348,
    "itl": 57.026069753624306,
    "ttft": 15251.52086571415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 99759,
    "finished_requests": 99339,
    "scheduler_time": 75.22295664309434
}
#Debug simulation 
Total elapsed time: 6.869229717180133. Arrivals time: 0.251820610370487 Scheduler time: 6.399001667276025 Scheduler overhead time: 0.08147850027307868 Adapter cache time: 0.015021839644759893 Engine time: 0.08437842968851328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 1080, 1080, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 299160 . Total input tokens: 66820400 . Total output tokens: 58543101
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 6.852682656142861,
    "estimated_duration": 3600.0566598334976,
    "input_throughput": 6870.670196936285,
    "output_throughput": 5923.518715115522,
    "total_throughput": 12794.188912051808,
    "itl": 57.025474143797695,
    "ttft": 15251.413200532495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 99759,
    "finished_requests": 99339,
    "scheduler_time": 75.22308276969203
}
#Debug simulation 
Total elapsed time: 6.852823039051145. Arrivals time: 0.2541640573181212 Scheduler time: 6.37806755816564 Scheduler overhead time: 0.0818848698399961 Adapter cache time: 0.015003375243395567 Engine time: 0.08585667703300714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 1080, 1080, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 299160 . Total input tokens: 66820400 . Total output tokens: 58543101
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 6.855712434276938,
    "estimated_duration": 3600.0372411956687,
    "input_throughput": 6870.637258125973,
    "output_throughput": 5923.511778149674,
    "total_throughput": 12794.149036275647,
    "itl": 57.02592118165301,
    "ttft": 15287.660527400296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 99759,
    "finished_requests": 99338,
    "scheduler_time": 75.22279076498965
}
#Debug simulation 
Total elapsed time: 6.8558448241092265. Arrivals time: 0.25633161421865225 Scheduler time: 6.379557510372251 Scheduler overhead time: 0.08155749691650271 Adapter cache time: 0.015080243814736605 Engine time: 0.08571203704923391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 1080, 1080, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 299160 . Total input tokens: 66820400 . Total output tokens: 58543101
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.828349170740694,
    "estimated_duration": 3600.045884675885,
    "input_throughput": 6870.690761272586,
    "output_throughput": 5923.536444569485,
    "total_throughput": 12794.227205842071,
    "itl": 57.02539609476846,
    "ttft": 15251.58046667444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 99759,
    "finished_requests": 99339,
    "scheduler_time": 75.22283137696981
}
#Debug simulation 
Total elapsed time: 6.8284492189995944. Arrivals time: 0.25907813804224133 Scheduler time: 6.351886649150401 Scheduler overhead time: 0.08116806671023369 Adapter cache time: 0.014918839558959007 Engine time: 0.08405182184651494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [5 5 6]
Adapter prompts. [17280, 1080, 1080, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 299160 . Total input tokens: 66820400 . Total output tokens: 58543101
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.894651958718896,
    "estimated_duration": 3600.0330177906953,
    "input_throughput": 6870.645040688918,
    "output_throughput": 5923.5181718102285,
    "total_throughput": 12794.163212499147,
    "itl": 57.02612464743284,
    "ttft": 15323.753788358388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 99759,
    "finished_requests": 99337,
    "scheduler_time": 75.22265671267468
}
#Debug simulation 
Total elapsed time: 6.894755501765758. Arrivals time: 0.2548106340691447 Scheduler time: 6.42242033733055 Scheduler overhead time: 0.08134415093809366 Adapter cache time: 0.014975627418607473 Engine time: 0.08366740355268121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [5 5 6]
Adapter prompts. [17280, 540, 540, 17280, 34560, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 296460 . Total input tokens: 66204646 . Total output tokens: 58009872
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.814333318732679,
    "estimated_duration": 3600.0350151552752,
    "input_throughput": 6710.663340300316,
    "output_throughput": 5903.378414524491,
    "total_throughput": 12614.041754824808,
    "itl": 55.12518852871244,
    "ttft": 11494.488513835237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 98818,
    "finished_requests": 98505,
    "scheduler_time": 74.45563459443187
}
#Debug simulation 
Total elapsed time: 6.814468025695533. Arrivals time: 0.24817268596962094 Scheduler time: 6.341567426919937 Scheduler overhead time: 0.08341531315818429 Adapter cache time: 0.01538624381646514 Engine time: 0.08730262843891978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [5 5 6]
Adapter prompts. [17280, 540, 540, 17280, 34560, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 296460 . Total input tokens: 66204646 . Total output tokens: 58009872
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.791309278924018,
    "estimated_duration": 3600.0053141887543,
    "input_throughput": 6710.717316105974,
    "output_throughput": 5903.359896786451,
    "total_throughput": 12614.077212892425,
    "itl": 55.12518827046664,
    "ttft": 11494.593288754002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 98818,
    "finished_requests": 98504,
    "scheduler_time": 74.45525065097907
}
#Debug simulation 
Total elapsed time: 6.791414998006076. Arrivals time: 0.24047690443694592 Scheduler time: 6.325930833816528 Scheduler overhead time: 0.08408870315179229 Adapter cache time: 0.015387400984764099 Engine time: 0.08699933113530278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [5 5 6]
Adapter prompts. [17280, 540, 540, 17280, 34560, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 296460 . Total input tokens: 66204646 . Total output tokens: 58009872
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.821124017704278,
    "estimated_duration": 3600.0168887446407,
    "input_throughput": 6710.697129097174,
    "output_throughput": 5903.408138568732,
    "total_throughput": 12614.105267665907,
    "itl": 55.12520531778723,
    "ttft": 11458.185515680238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 98818,
    "finished_requests": 98505,
    "scheduler_time": 74.45535660899985
}
#Debug simulation 
Total elapsed time: 6.8212920618243515. Arrivals time: 0.24935607239603996 Scheduler time: 6.34761926997453 Scheduler overhead time: 0.08316774107515812 Adapter cache time: 0.015438783448189497 Engine time: 0.08722339104861021 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [5 5 6]
Adapter prompts. [17280, 540, 540, 17280, 34560, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 296460 . Total input tokens: 66204646 . Total output tokens: 58009872
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 6.8350400612689555,
    "estimated_duration": 3600.0350469986497,
    "input_throughput": 6710.6632809425155,
    "output_throughput": 5903.378362307363,
    "total_throughput": 12614.041643249879,
    "itl": 55.124969228228565,
    "ttft": 11494.447323448356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 98818,
    "finished_requests": 98505,
    "scheduler_time": 74.45553380202965
}
#Debug simulation 
Total elapsed time: 6.835198947228491. Arrivals time: 0.2538055586628616 Scheduler time: 6.357644324190915 Scheduler overhead time: 0.08337320480495691 Adapter cache time: 0.015420631039887667 Engine time: 0.08640583883970976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [5 5 6]
Adapter prompts. [17280, 540, 540, 17280, 34560, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 296460 . Total input tokens: 66204646 . Total output tokens: 58009872
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 6.762149385176599,
    "estimated_duration": 3600.0168979511423,
    "input_throughput": 6710.697111935576,
    "output_throughput": 5903.408123471655,
    "total_throughput": 12614.10523540723,
    "itl": 55.12523752206138,
    "ttft": 11458.19884594756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 98818,
    "finished_requests": 98505,
    "scheduler_time": 74.45539685220565
}
#Debug simulation 
Total elapsed time: 6.762287525925785. Arrivals time: 0.25259319646283984 Scheduler time: 6.285177657846361 Scheduler overhead time: 0.08359846705570817 Adapter cache time: 0.015222322195768356 Engine time: 0.08705897256731987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [5 5 6]
Adapter prompts. [17280, 540, 540, 17280, 34560, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 296460 . Total input tokens: 66204646 . Total output tokens: 58009872
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.823669895995408,
    "estimated_duration": 3600.0271523289393,
    "input_throughput": 6710.677997073227,
    "output_throughput": 5903.391308104818,
    "total_throughput": 12614.069305178045,
    "itl": 55.12512041286981,
    "ttft": 11494.484509954093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 98818,
    "finished_requests": 98505,
    "scheduler_time": 74.45547824006516
}
#Debug simulation 
Total elapsed time: 6.823773310985416. Arrivals time: 0.25540061481297016 Scheduler time: 6.343835818581283 Scheduler overhead time: 0.08375833602622151 Adapter cache time: 0.015394204296171665 Engine time: 0.08682227088138461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [5 5 6]
Adapter prompts. [17280, 540, 540, 17280, 34560, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 296460 . Total input tokens: 66204646 . Total output tokens: 58009872
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.8167712721042335,
    "estimated_duration": 3600.0045055342725,
    "input_throughput": 6710.718823507319,
    "output_throughput": 5903.361222834357,
    "total_throughput": 12614.080046341676,
    "itl": 55.1251575595651,
    "ttft": 11494.623640492542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 98818,
    "finished_requests": 98504,
    "scheduler_time": 74.45517330792713
}
#Debug simulation 
Total elapsed time: 6.816950817126781. Arrivals time: 0.2526352424174547 Scheduler time: 6.340915679931641 Scheduler overhead time: 0.08344881841912866 Adapter cache time: 0.015321367885917425 Engine time: 0.08611743664368987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [5 5 6]
Adapter prompts. [17280, 270, 270, 17280, 34560, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 295110 . Total input tokens: 65907731 . Total output tokens: 57755455
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.770376289263368,
    "estimated_duration": 3600.00846119898,
    "input_throughput": 6740.758045862472,
    "output_throughput": 5834.736564203593,
    "total_throughput": 12575.494610066064,
    "itl": 52.94171507955854,
    "ttft": 12784.859465611667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 98387,
    "finished_requests": 98039,
    "scheduler_time": 73.04491413064252
}
#Debug simulation 
Total elapsed time: 6.770513586234301. Arrivals time: 0.2736671045422554 Scheduler time: 6.265912679024041 Scheduler overhead time: 0.08600678248330951 Adapter cache time: 0.015415099449455738 Engine time: 0.08976647490635514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [5 5 6]
Adapter prompts. [17280, 270, 270, 17280, 34560, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 295110 . Total input tokens: 65907731 . Total output tokens: 57755455
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.733641024213284,
    "estimated_duration": 3600.0476532954435,
    "input_throughput": 6740.684662267305,
    "output_throughput": 5834.673044056004,
    "total_throughput": 12575.35770632331,
    "itl": 52.94183735543371,
    "ttft": 12821.550754804837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 98387,
    "finished_requests": 98039,
    "scheduler_time": 73.0459288857613
}
#Debug simulation 
Total elapsed time: 6.73374551627785. Arrivals time: 0.2419195561669767 Scheduler time: 6.2615109900943935 Scheduler overhead time: 0.08594600576907396 Adapter cache time: 0.015427825041115284 Engine time: 0.08901857025921345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [5 5 6]
Adapter prompts. [17280, 270, 270, 17280, 34560, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 295110 . Total input tokens: 65907731 . Total output tokens: 57755455
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.709737056866288,
    "estimated_duration": 3600.05292456249,
    "input_throughput": 6740.67479242659,
    "output_throughput": 5834.664500815005,
    "total_throughput": 12575.339293241595,
    "itl": 52.941954666530904,
    "ttft": 12821.479989853611,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 98387,
    "finished_requests": 98039,
    "scheduler_time": 73.04603834142834
}
#Debug simulation 
Total elapsed time: 6.709852370899171. Arrivals time: 0.2425428032875061 Scheduler time: 6.236943040974438 Scheduler overhead time: 0.08550822827965021 Adapter cache time: 0.015342587605118752 Engine time: 0.0901714377105236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [5 5 6]
Adapter prompts. [17280, 270, 270, 17280, 34560, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 295110 . Total input tokens: 65907731 . Total output tokens: 57755455
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 6.720622429158539,
    "estimated_duration": 3600.030502398334,
    "input_throughput": 6740.716775547738,
    "output_throughput": 5834.700841008552,
    "total_throughput": 12575.41761655629,
    "itl": 52.94174141637775,
    "ttft": 12821.396996356232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 98387,
    "finished_requests": 98039,
    "scheduler_time": 73.04547635895968
}
#Debug simulation 
Total elapsed time: 6.720754771027714. Arrivals time: 0.25028858706355095 Scheduler time: 6.24159484449774 Scheduler overhead time: 0.08552004164084792 Adapter cache time: 0.015335030388087034 Engine time: 0.08835229184478521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [5 5 6]
Adapter prompts. [17280, 270, 270, 17280, 34560, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 295110 . Total input tokens: 65907731 . Total output tokens: 57755455
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 6.803389871027321,
    "estimated_duration": 3600.0499752194664,
    "input_throughput": 6740.680314728311,
    "output_throughput": 5834.669280867271,
    "total_throughput": 12575.349595595582,
    "itl": 52.94194833360331,
    "ttft": 12821.473396415537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 98387,
    "finished_requests": 98039,
    "scheduler_time": 73.04594956052604
}
#Debug simulation 
Total elapsed time: 6.80352795869112. Arrivals time: 0.2529046298004687 Scheduler time: 6.3183062872849405 Scheduler overhead time: 0.08646726422011852 Adapter cache time: 0.0154497642070055 Engine time: 0.09043097542598844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [5 5 6]
Adapter prompts. [17280, 270, 270, 17280, 34560, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 295110 . Total input tokens: 65907731 . Total output tokens: 57755455
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.713634755928069,
    "estimated_duration": 3600.0553122953256,
    "input_throughput": 6740.670321681243,
    "output_throughput": 5834.660630980015,
    "total_throughput": 12575.330952661257,
    "itl": 52.94157236711941,
    "ttft": 12821.4182654757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 98387,
    "finished_requests": 98039,
    "scheduler_time": 73.04587352867154
}
#Debug simulation 
Total elapsed time: 6.713826071936637. Arrivals time: 0.2805112851783633 Scheduler time: 6.200990195386112 Scheduler overhead time: 0.08747947355732322 Adapter cache time: 0.015530148986727 Engine time: 0.08959557767957449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [5 5 6]
Adapter prompts. [17280, 270, 270, 17280, 34560, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 295110 . Total input tokens: 65907731 . Total output tokens: 57755455
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.955675575882196,
    "estimated_duration": 3600.0499135625064,
    "input_throughput": 6740.680430173892,
    "output_throughput": 5834.669380795877,
    "total_throughput": 12575.34981096977,
    "itl": 52.94190489071774,
    "ttft": 12821.479272590352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 98387,
    "finished_requests": 98039,
    "scheduler_time": 73.04596937481708
}
#Debug simulation 
Total elapsed time: 6.955798465758562. Arrivals time: 0.2541378550231457 Scheduler time: 6.469137910287827 Scheduler overhead time: 0.08685170952230692 Adapter cache time: 0.015650964342057705 Engine time: 0.0899239000864327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [5 5 6]
Adapter prompts. [17280, 135, 135, 17280, 34560, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294435 . Total input tokens: 65753374 . Total output tokens: 57628668
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.772722657304257,
    "estimated_duration": 3600.0229606578573,
    "input_throughput": 6760.41854898409,
    "output_throughput": 5829.59254131117,
    "total_throughput": 12590.01109029526,
    "itl": 52.18874112878305,
    "ttft": 10500.955304103136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 98173,
    "finished_requests": 97889,
    "scheduler_time": 72.71006334140863
}
#Debug simulation 
Total elapsed time: 6.772835039999336. Arrivals time: 0.25335265742614865 Scheduler time: 6.286066255066544 Scheduler overhead time: 0.08729345072060823 Adapter cache time: 0.015352415852248669 Engine time: 0.09042566223070025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [5 5 6]
Adapter prompts. [17280, 135, 135, 17280, 34560, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294435 . Total input tokens: 65753374 . Total output tokens: 57628668
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.7087169690057635,
    "estimated_duration": 3600.003854583824,
    "input_throughput": 6760.418317055809,
    "output_throughput": 5829.486536043194,
    "total_throughput": 12589.904853099002,
    "itl": 52.1888042510251,
    "ttft": 10428.077400572862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 98173,
    "finished_requests": 97888,
    "scheduler_time": 72.70984195709255
}
#Debug simulation 
Total elapsed time: 6.708830181043595. Arrivals time: 0.24543889053165913 Scheduler time: 6.230595807079226 Scheduler overhead time: 0.08685448439791799 Adapter cache time: 0.015197063330560923 Engine time: 0.09061489906162024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [5 5 6]
Adapter prompts. [17280, 135, 135, 17280, 34560, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294435 . Total input tokens: 65753374 . Total output tokens: 57628668
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.9002070291899145,
    "estimated_duration": 3600.004683203613,
    "input_throughput": 6760.416760997722,
    "output_throughput": 5829.4851942594105,
    "total_throughput": 12589.901955257134,
    "itl": 52.18866700588145,
    "ttft": 10428.079428815294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 98173,
    "finished_requests": 97888,
    "scheduler_time": 72.7097387733675
}
#Debug simulation 
Total elapsed time: 6.900302715133876. Arrivals time: 0.2462920704856515 Scheduler time: 6.4209482707083225 Scheduler overhead time: 0.0871328511275351 Adapter cache time: 0.015316270291805267 Engine time: 0.09029479697346687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [5 5 6]
Adapter prompts. [17280, 135, 135, 17280, 34560, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294435 . Total input tokens: 65753374 . Total output tokens: 57628668
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 6.722377059049904,
    "estimated_duration": 3600.0370151652482,
    "input_throughput": 6760.392156379775,
    "output_throughput": 5829.569782641992,
    "total_throughput": 12589.961939021767,
    "itl": 52.18882867165892,
    "ttft": 10500.924751599247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900571,
    "arrivals": 98173,
    "finished_requests": 97889,
    "scheduler_time": 72.71035955481896
}
#Debug simulation 
Total elapsed time: 6.722487954888493. Arrivals time: 0.2585563766770065 Scheduler time: 6.230163182597607 Scheduler overhead time: 0.08726026304066181 Adapter cache time: 0.015503672882914543 Engine time: 0.090785872656852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [5 5 6]
Adapter prompts. [17280, 135, 135, 17280, 34560, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294435 . Total input tokens: 65753374 . Total output tokens: 57628668
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 6.716217270120978,
    "estimated_duration": 3600.0041758769153,
    "input_throughput": 6760.417713702148,
    "output_throughput": 5829.486015773311,
    "total_throughput": 12589.903729475458,
    "itl": 52.18874734497817,
    "ttft": 10428.08632119106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 98173,
    "finished_requests": 97888,
    "scheduler_time": 72.70974633030956
}
#Debug simulation 
Total elapsed time: 6.716366816312075. Arrivals time: 0.2453668648377061 Scheduler time: 6.23747832654044 Scheduler overhead time: 0.08710569934919477 Adapter cache time: 0.015219549182802439 Engine time: 0.09082272602245212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [5 5 6]
Adapter prompts. [17280, 135, 135, 17280, 34560, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294435 . Total input tokens: 65753374 . Total output tokens: 57628668
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.726634496822953,
    "estimated_duration": 3600.0211346014526,
    "input_throughput": 6760.385867205287,
    "output_throughput": 5829.4585546435455,
    "total_throughput": 12589.844421848831,
    "itl": 52.1888220155736,
    "ttft": 10537.649708228004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 98173,
    "finished_requests": 97888,
    "scheduler_time": 72.71011951799713
}
#Debug simulation 
Total elapsed time: 6.72674022288993. Arrivals time: 0.2513410411775112 Scheduler time: 6.241628437303007 Scheduler overhead time: 0.08695209678262472 Adapter cache time: 0.015320450998842716 Engine time: 0.09129770426079631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [5 5 6]
Adapter prompts. [17280, 135, 135, 17280, 34560, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294435 . Total input tokens: 65753374 . Total output tokens: 57628668
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.712539381813258,
    "estimated_duration": 3600.0039112241775,
    "input_throughput": 6760.418210691346,
    "output_throughput": 5829.4864443254655,
    "total_throughput": 12589.904655016811,
    "itl": 52.18878093919138,
    "ttft": 10428.062465371762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 98173,
    "finished_requests": 97888,
    "scheduler_time": 72.70979470410643
}
#Debug simulation 
Total elapsed time: 6.712650785688311. Arrivals time: 0.2508959136903286 Scheduler time: 6.227521668653935 Scheduler overhead time: 0.0875238087028265 Adapter cache time: 0.015256463550031185 Engine time: 0.09107585996389389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [5 5 6]
Adapter prompts. [17280, 66, 66, 17280, 34560, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294090 . Total input tokens: 65678489 . Total output tokens: 57560941
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.764617668930441,
    "estimated_duration": 3600.0207522057863,
    "input_throughput": 6721.337921503415,
    "output_throughput": 5839.00967435268,
    "total_throughput": 12560.347595856096,
    "itl": 51.840717464875915,
    "ttft": 12344.2388174275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 98091,
    "finished_requests": 97756,
    "scheduler_time": 72.82715215000789
}
#Debug simulation 
Total elapsed time: 6.764751822222024. Arrivals time: 0.25522559182718396 Scheduler time: 6.276157363317907 Scheduler overhead time: 0.08740409649908543 Adapter cache time: 0.015091671608388424 Engine time: 0.09053783630952239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [5 5 6]
Adapter prompts. [17280, 66, 66, 17280, 34560, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294090 . Total input tokens: 65678489 . Total output tokens: 57560941
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.784577357117087,
    "estimated_duration": 3600.007690464283,
    "input_throughput": 6721.3623082231215,
    "output_throughput": 5839.03085976159,
    "total_throughput": 12560.393167984712,
    "itl": 51.84100460842389,
    "ttft": 12344.312336535697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 98091,
    "finished_requests": 97756,
    "scheduler_time": 72.82712169116816
}
#Debug simulation 
Total elapsed time: 6.784746062010527. Arrivals time: 0.24701653933152556 Scheduler time: 6.2994529022835195 Scheduler overhead time: 0.09005283890292048 Adapter cache time: 0.015337471384555101 Engine time: 0.0920525542460382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [5 5 6]
Adapter prompts. [17280, 66, 66, 17280, 34560, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294090 . Total input tokens: 65678489 . Total output tokens: 57560941
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.953526826109737,
    "estimated_duration": 3600.009632646828,
    "input_throughput": 6721.3586820904475,
    "output_throughput": 5839.027709641182,
    "total_throughput": 12560.38639173163,
    "itl": 51.84091982623447,
    "ttft": 12344.275167800251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 98091,
    "finished_requests": 97756,
    "scheduler_time": 72.82713043638329
}
#Debug simulation 
Total elapsed time: 6.953625809401274. Arrivals time: 0.2463408405892551 Scheduler time: 6.4713989878073335 Scheduler overhead time: 0.08860062016174197 Adapter cache time: 0.015103811398148537 Engine time: 0.09142587799578905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [5 5 6]
Adapter prompts. [17280, 66, 66, 17280, 34560, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294090 . Total input tokens: 65678489 . Total output tokens: 57560941
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 6.759869093075395,
    "estimated_duration": 3600.0324332127097,
    "input_throughput": 6721.316112812451,
    "output_throughput": 5838.990728547692,
    "total_throughput": 12560.306841360143,
    "itl": 51.84072710126794,
    "ttft": 12380.830225615477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 98091,
    "finished_requests": 97756,
    "scheduler_time": 72.82741003432453
}
#Debug simulation 
Total elapsed time: 6.7599910837598145. Arrivals time: 0.24221169808879495 Scheduler time: 6.28395559405908 Scheduler overhead time: 0.08760682819411159 Adapter cache time: 0.015068613458424807 Engine time: 0.09032595064491034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [5 5 6]
Adapter prompts. [17280, 66, 66, 17280, 34560, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294090 . Total input tokens: 65678489 . Total output tokens: 57560941
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 6.945606385357678,
    "estimated_duration": 3600.0065490684506,
    "input_throughput": 6721.364439256724,
    "output_throughput": 5839.032711048636,
    "total_throughput": 12560.39715030536,
    "itl": 51.84091822208303,
    "ttft": 12344.33011921591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 98091,
    "finished_requests": 97756,
    "scheduler_time": 72.82704557736362
}
#Debug simulation 
Total elapsed time: 6.94575018202886. Arrivals time: 0.24974602973088622 Scheduler time: 6.460844774264842 Scheduler overhead time: 0.08791153598576784 Adapter cache time: 0.015125090256333351 Engine time: 0.0914449212141335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [5 5 6]
Adapter prompts. [17280, 66, 66, 17280, 34560, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294090 . Total input tokens: 65678489 . Total output tokens: 57560941
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.73499730695039,
    "estimated_duration": 3600.0201726587516,
    "input_throughput": 6721.339003533869,
    "output_throughput": 5839.0106143420635,
    "total_throughput": 12560.349617875932,
    "itl": 51.84087235076213,
    "ttft": 12344.27599445522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 98091,
    "finished_requests": 97756,
    "scheduler_time": 72.82732084367326
}
#Debug simulation 
Total elapsed time: 6.735094740986824. Arrivals time: 0.2524543865583837 Scheduler time: 6.247466160915792 Scheduler overhead time: 0.08938174275681376 Adapter cache time: 0.014968940988183022 Engine time: 0.09034335287287831 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [5 5 6]
Adapter prompts. [17280, 66, 66, 17280, 34560, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 294090 . Total input tokens: 65678489 . Total output tokens: 57560941
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.777252939995378,
    "estimated_duration": 3600.006653249427,
    "input_throughput": 6721.364244746442,
    "output_throughput": 5839.032542072246,
    "total_throughput": 12560.396786818688,
    "itl": 51.840954060388796,
    "ttft": 12344.318399300997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 98091,
    "finished_requests": 97756,
    "scheduler_time": 72.82710142615272
}
#Debug simulation 
Total elapsed time: 6.777355263009667. Arrivals time: 0.2477713106200099 Scheduler time: 6.293675225228071 Scheduler overhead time: 0.08917307248339057 Adapter cache time: 0.015231492463499308 Engine time: 0.0906056403182447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [17280, 33, 33, 17280, 34560, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 293925 . Total input tokens: 65637799 . Total output tokens: 57526135
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.746352476067841,
    "estimated_duration": 3600.022079045804,
    "input_throughput": 6671.864081002044,
    "output_throughput": 5840.769455939903,
    "total_throughput": 12512.633536941948,
    "itl": 51.610091439492734,
    "ttft": 12975.539509914805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 98030,
    "finished_requests": 97679,
    "scheduler_time": 72.80489979616577
}
#Debug simulation 
Total elapsed time: 6.746475710999221. Arrivals time: 0.2453478155657649 Scheduler time: 6.263023655861616 Scheduler overhead time: 0.08980185305699706 Adapter cache time: 0.015023528132587671 Engine time: 0.0925112976692617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [17280, 33, 33, 17280, 34560, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 293925 . Total input tokens: 65637799 . Total output tokens: 57526135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.773922620341182,
    "estimated_duration": 3599.998681784403,
    "input_throughput": 6671.907165281135,
    "output_throughput": 5840.771305387731,
    "total_throughput": 12512.678470668867,
    "itl": 51.610152013921734,
    "ttft": 13012.26586279021,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 98030,
    "finished_requests": 97678,
    "scheduler_time": 72.80451131620607
}
#Debug simulation 
Total elapsed time: 6.774048608262092. Arrivals time: 0.244857894256711 Scheduler time: 6.294700735248625 Scheduler overhead time: 0.08776435721665621 Adapter cache time: 0.014838170725852251 Engine time: 0.09133173618465662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [17280, 33, 33, 17280, 34560, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 293925 . Total input tokens: 65637799 . Total output tokens: 57526135
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.765317712910473,
    "estimated_duration": 3600.004033238904,
    "input_throughput": 6671.897247401239,
    "output_throughput": 5840.762623002489,
    "total_throughput": 12512.659870403728,
    "itl": 51.610242999620354,
    "ttft": 13012.308038189745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 98030,
    "finished_requests": 97678,
    "scheduler_time": 72.80453196448352
}
#Debug simulation 
Total elapsed time: 6.7654281668365. Arrivals time: 0.24070524610579014 Scheduler time: 6.288861189968884 Scheduler overhead time: 0.08842974388971925 Adapter cache time: 0.014773852657526731 Engine time: 0.09195129806175828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [17280, 33, 33, 17280, 34560, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 293925 . Total input tokens: 65637799 . Total output tokens: 57526135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 6.770450661890209,
    "estimated_duration": 3600.0348078893,
    "input_throughput": 6671.894934838802,
    "output_throughput": 5840.864358844439,
    "total_throughput": 12512.759293683243,
    "itl": 51.610146446324215,
    "ttft": 12938.877289287791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 98030,
    "finished_requests": 97680,
    "scheduler_time": 72.80513116972232
}
#Debug simulation 
Total elapsed time: 6.770572608802468. Arrivals time: 0.24550879560410976 Scheduler time: 6.288565033115447 Scheduler overhead time: 0.08909349143505096 Adapter cache time: 0.01496143639087677 Engine time: 0.09143793396651745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [17280, 33, 33, 17280, 34560, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 293925 . Total input tokens: 65637799 . Total output tokens: 57526135
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 6.743966157082468,
    "estimated_duration": 3600.0020507146455,
    "input_throughput": 6671.9009216208515,
    "output_throughput": 5840.765839515542,
    "total_throughput": 12512.666761136394,
    "itl": 51.61021808210707,
    "ttft": 13012.265169529363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 98030,
    "finished_requests": 97678,
    "scheduler_time": 72.80454879931426
}
#Debug simulation 
Total elapsed time: 6.744113822933286. Arrivals time: 0.25071969302371144 Scheduler time: 6.257492974866182 Scheduler overhead time: 0.08850708371028304 Adapter cache time: 0.0148949408903718 Engine time: 0.0918561271391809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [17280, 33, 33, 17280, 34560, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 293925 . Total input tokens: 65637799 . Total output tokens: 57526135
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.737362464889884,
    "estimated_duration": 3600.0204710742537,
    "input_throughput": 6671.867061031662,
    "output_throughput": 5840.7720647559345,
    "total_throughput": 12512.639125787597,
    "itl": 51.61008710067383,
    "ttft": 12975.553120341041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 98030,
    "finished_requests": 97679,
    "scheduler_time": 72.8049884131687
}
#Debug simulation 
Total elapsed time: 6.737517423927784. Arrivals time: 0.25272230338305235 Scheduler time: 6.249410862568766 Scheduler overhead time: 0.0880206935107708 Adapter cache time: 0.014748671557754278 Engine time: 0.09200833179056644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [17280, 33, 33, 17280, 34560, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 34560]
Prompts retrieved: 293925 . Total input tokens: 65637799 . Total output tokens: 57526135
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.752868141978979,
    "estimated_duration": 3600.0007615022682,
    "input_throughput": 6671.903310925138,
    "output_throughput": 5840.767931178326,
    "total_throughput": 12512.671242103463,
    "itl": 51.610234922007194,
    "ttft": 13012.316861818646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 98030,
    "finished_requests": 97678,
    "scheduler_time": 72.80454847151468
}
#Debug simulation 
Total elapsed time: 6.752967028878629. Arrivals time: 0.24372255383059382 Scheduler time: 6.2747252197004855 Scheduler overhead time: 0.08777642389759421 Adapter cache time: 0.014760468155145645 Engine time: 0.09148216294124722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 272160 . Total input tokens: 60761959 . Total output tokens: 53307290
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.336217024363577,
    "estimated_duration": 3600.0534448685985,
    "input_throughput": 6190.6494837643895,
    "output_throughput": 5456.212053739929,
    "total_throughput": 11646.861537504317,
    "itl": 51.72015497866662,
    "ttft": 11063.581136199062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 90838,
    "finished_requests": 90561,
    "scheduler_time": 67.16253039343391
}
#Debug simulation 
Total elapsed time: 6.336331386119127. Arrivals time: 0.236231857445091 Scheduler time: 5.867653265595436 Scheduler overhead time: 0.08723020972684026 Adapter cache time: 0.014664336573332548 Engine time: 0.0903897825628519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 272160 . Total input tokens: 60761959 . Total output tokens: 53307290
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.33957967441529,
    "estimated_duration": 3600.037525810451,
    "input_throughput": 6190.649080798896,
    "output_throughput": 5456.078404509996,
    "total_throughput": 11646.727485308893,
    "itl": 51.720440377450664,
    "ttft": 11103.110985661659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 90838,
    "finished_requests": 90560,
    "scheduler_time": 67.16238943977666
}
#Debug simulation 
Total elapsed time: 6.33971490431577. Arrivals time: 0.22606897680088878 Scheduler time: 5.881743788253516 Scheduler overhead time: 0.08632241329178214 Adapter cache time: 0.014511462301015854 Engine time: 0.0907116262242198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 272160 . Total input tokens: 60761959 . Total output tokens: 53307290
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.303622815292329,
    "estimated_duration": 3600.041783035947,
    "input_throughput": 6190.669537508938,
    "output_throughput": 5456.2297283769785,
    "total_throughput": 11646.899265885917,
    "itl": 51.720457130447215,
    "ttft": 11063.460753790989,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 90838,
    "finished_requests": 90561,
    "scheduler_time": 67.16240549608386
}
#Debug simulation 
Total elapsed time: 6.303723311051726. Arrivals time: 0.23355045029893517 Scheduler time: 5.839631222654134 Scheduler overhead time: 0.08607919607311487 Adapter cache time: 0.01442780066281557 Engine time: 0.09008340630680323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 272160 . Total input tokens: 60761959 . Total output tokens: 53307290
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 6.281872977968305,
    "estimated_duration": 3600.0170695316733,
    "input_throughput": 6190.6342579923485,
    "output_throughput": 5455.998019074724,
    "total_throughput": 11646.632277067072,
    "itl": 51.72045597357921,
    "ttft": 11142.93454295482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 90838,
    "finished_requests": 90558,
    "scheduler_time": 67.16205751966716
}
#Debug simulation 
Total elapsed time: 6.281978155020624. Arrivals time: 0.2300364845432341 Scheduler time: 5.822955412790179 Scheduler overhead time: 0.08618436986580491 Adapter cache time: 0.014182102400809526 Engine time: 0.08875269442796707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 272160 . Total input tokens: 60761959 . Total output tokens: 53307290
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 6.288829576224089,
    "estimated_duration": 3600.041766053482,
    "input_throughput": 6190.669566712163,
    "output_throughput": 5456.229754115633,
    "total_throughput": 11646.899320827795,
    "itl": 51.72048856511215,
    "ttft": 11063.484789974525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 90838,
    "finished_requests": 90561,
    "scheduler_time": 67.1624376906486
}
#Debug simulation 
Total elapsed time: 6.288944568950683. Arrivals time: 0.2251964332535863 Scheduler time: 5.832540860399604 Scheduler overhead time: 0.08643318992108107 Adapter cache time: 0.014358848799020052 Engine time: 0.09041160950437188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 272160 . Total input tokens: 60761959 . Total output tokens: 53307290
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 6.331523912027478,
    "estimated_duration": 3600.051765750135,
    "input_throughput": 6190.652371176717,
    "output_throughput": 5456.214598599558,
    "total_throughput": 11646.866969776274,
    "itl": 51.720213612057506,
    "ttft": 11063.497284751427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 90838,
    "finished_requests": 90561,
    "scheduler_time": 67.16257100541391
}
#Debug simulation 
Total elapsed time: 6.33162589604035. Arrivals time: 0.2407876313664019 Scheduler time: 5.859853673260659 Scheduler overhead time: 0.08707402879372239 Adapter cache time: 0.014342783484607935 Engine time: 0.08958146162331104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 272160 . Total input tokens: 60761959 . Total output tokens: 53307290
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 6.28611317416653,
    "estimated_duration": 3600.0417633684415,
    "input_throughput": 6190.669571329387,
    "output_throughput": 5456.229758185085,
    "total_throughput": 11646.899329514472,
    "itl": 51.720520551025594,
    "ttft": 11063.499782773066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 90838,
    "finished_requests": 90561,
    "scheduler_time": 67.16247413489592
}
#Debug simulation 
Total elapsed time: 6.286268188152462. Arrivals time: 0.23230081982910633 Scheduler time: 5.824244967661798 Scheduler overhead time: 0.08601238671690226 Adapter cache time: 0.014311919454485178 Engine time: 0.08950498839840293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 255960 . Total input tokens: 57164337 . Total output tokens: 50162217
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.827698250766844,
    "estimated_duration": 3600.028595702736,
    "input_throughput": 5897.280378645373,
    "output_throughput": 5093.525929735177,
    "total_throughput": 10990.80630838055,
    "itl": 45.93113955005239,
    "ttft": 10172.53282610657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 85575,
    "finished_requests": 85335,
    "scheduler_time": 60.31064636043666
}
#Debug simulation 
Total elapsed time: 5.827800556086004. Arrivals time: 0.22553716972470284 Scheduler time: 5.35994042083621 Scheduler overhead time: 0.09093852946534753 Adapter cache time: 0.0167101938277483 Engine time: 0.09244114020839334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 255960 . Total input tokens: 57164337 . Total output tokens: 50162217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.8252192158252,
    "estimated_duration": 3600.03802883758,
    "input_throughput": 5897.264926074989,
    "output_throughput": 5093.512583232572,
    "total_throughput": 10990.777509307562,
    "itl": 45.930954179802235,
    "ttft": 10172.513916243402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 85575,
    "finished_requests": 85335,
    "scheduler_time": 60.3107393090719
}
#Debug simulation 
Total elapsed time: 5.8253893330693245. Arrivals time: 0.21915308199822903 Scheduler time: 5.365113356616348 Scheduler overhead time: 0.09074057172983885 Adapter cache time: 0.01670967461541295 Engine time: 0.09189140517264605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 255960 . Total input tokens: 57164337 . Total output tokens: 50162217
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.790846812073141,
    "estimated_duration": 3600.001116115214,
    "input_throughput": 5897.304560535738,
    "output_throughput": 5093.417865321897,
    "total_throughput": 10990.722425857635,
    "itl": 45.9309941485183,
    "ttft": 10214.729048464997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 85575,
    "finished_requests": 85333,
    "scheduler_time": 60.31017220196047
}
#Debug simulation 
Total elapsed time: 5.790951354894787. Arrivals time: 0.2155431667342782 Scheduler time: 5.334894372150302 Scheduler overhead time: 0.09065658133476973 Adapter cache time: 0.01661686645820737 Engine time: 0.09125391812995076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 255960 . Total input tokens: 57164337 . Total output tokens: 50162217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.8123711831867695,
    "estimated_duration": 3600.0343913920183,
    "input_throughput": 5897.270884623658,
    "output_throughput": 5093.51772967639,
    "total_throughput": 10990.788614300049,
    "itl": 45.93115541271904,
    "ttft": 10172.59288984555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 85575,
    "finished_requests": 85335,
    "scheduler_time": 60.31068268175887
}
#Debug simulation 
Total elapsed time: 5.8125016540288925. Arrivals time: 0.20903831953182817 Scheduler time: 5.357040803413838 Scheduler overhead time: 0.09333378355950117 Adapter cache time: 0.016826997976750135 Engine time: 0.09368246328085661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 255960 . Total input tokens: 57164337 . Total output tokens: 50162217
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.766013916116208,
    "estimated_duration": 3600.036517103501,
    "input_throughput": 5897.267402465525,
    "output_throughput": 5093.51472210992,
    "total_throughput": 10990.782124575444,
    "itl": 45.9308010258409,
    "ttft": 10172.567399095662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 85575,
    "finished_requests": 85335,
    "scheduler_time": 60.31058985604833
}
#Debug simulation 
Total elapsed time: 5.766134538222104. Arrivals time: 0.2119970414787531 Scheduler time: 5.31462264386937 Scheduler overhead time: 0.09004105906933546 Adapter cache time: 0.016415098682045937 Engine time: 0.09116209531202912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 255960 . Total input tokens: 57164337 . Total output tokens: 50162217
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.82724992511794,
    "estimated_duration": 3600.023244298869,
    "input_throughput": 5897.2888671264045,
    "output_throughput": 5093.392668794041,
    "total_throughput": 10990.681535920447,
    "itl": 45.931022327054336,
    "ttft": 10214.614979921405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 85575,
    "finished_requests": 85334,
    "scheduler_time": 60.310593695983
}
#Debug simulation 
Total elapsed time: 5.827440568245947. Arrivals time: 0.2125689247623086 Scheduler time: 5.37378142029047 Scheduler overhead time: 0.09024605574086308 Adapter cache time: 0.016564016696065664 Engine time: 0.09249268192797899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 255960 . Total input tokens: 57164337 . Total output tokens: 50162217
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.786554146092385,
    "estimated_duration": 3600.038045512017,
    "input_throughput": 5897.264898760396,
    "output_throughput": 5093.512559640751,
    "total_throughput": 10990.777458401148,
    "itl": 45.930920348760424,
    "ttft": 10172.507128374147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 85575,
    "finished_requests": 85335,
    "scheduler_time": 60.31073126043075
}
#Debug simulation 
Total elapsed time: 5.786677303723991. Arrivals time: 0.2204509382136166 Scheduler time: 5.325339775998145 Scheduler overhead time: 0.09083731984719634 Adapter cache time: 0.016628149431198835 Engine time: 0.09140658052638173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 34560, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 253260 . Total input tokens: 56574834 . Total output tokens: 49625007
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.753816281910986,
    "estimated_duration": 3599.983385246585,
    "input_throughput": 5804.449010969174,
    "output_throughput": 5071.254793791084,
    "total_throughput": 10875.703804760258,
    "itl": 44.784406217293444,
    "ttft": 10408.006682471725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 84654,
    "finished_requests": 84411,
    "scheduler_time": 59.59910929082379
}
#Debug simulation 
Total elapsed time: 5.7539444216527045. Arrivals time: 0.21533515164628625 Scheduler time: 5.296677879989147 Scheduler overhead time: 0.09062537038698792 Adapter cache time: 0.016223674174398184 Engine time: 0.09295936999842525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 34560, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 253260 . Total input tokens: 56574834 . Total output tokens: 49625007
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.728719817940146,
    "estimated_duration": 3599.9633748320675,
    "input_throughput": 5804.390718551337,
    "output_throughput": 5071.282426825143,
    "total_throughput": 10875.67314537648,
    "itl": 44.784457180578094,
    "ttft": 10450.40420713509,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 84654,
    "finished_requests": 84410,
    "scheduler_time": 59.59884995789714
}
#Debug simulation 
Total elapsed time: 5.728850389830768. Arrivals time: 0.21071977308019996 Scheduler time: 5.273543250281364 Scheduler overhead time: 0.09319902304559946 Adapter cache time: 0.01622839504852891 Engine time: 0.09248026553541422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 34560, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 253260 . Total input tokens: 56574834 . Total output tokens: 49625007
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.787787736859173,
    "estimated_duration": 3599.972063820743,
    "input_throughput": 5804.467265177225,
    "output_throughput": 5071.270742202365,
    "total_throughput": 10875.73800737959,
    "itl": 44.7843446845958,
    "ttft": 10407.976991520401,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 84654,
    "finished_requests": 84411,
    "scheduler_time": 59.59903985900029
}
#Debug simulation 
Total elapsed time: 5.787924550008029. Arrivals time: 0.20578279439359903 Scheduler time: 5.336968335788697 Scheduler overhead time: 0.09312641434371471 Adapter cache time: 0.016782489139586687 Engine time: 0.09264257038012147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 34560, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 253260 . Total input tokens: 56574834 . Total output tokens: 49625007
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.749088520184159,
    "estimated_duration": 3599.9960457418706,
    "input_throughput": 5804.428597835825,
    "output_throughput": 5071.236959161103,
    "total_throughput": 10875.665556996928,
    "itl": 44.78440109299693,
    "ttft": 10407.93596890733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 84654,
    "finished_requests": 84411,
    "scheduler_time": 59.599384214845664
}
#Debug simulation 
Total elapsed time: 5.74921003729105. Arrivals time: 0.20973743870854378 Scheduler time: 5.298678193241358 Scheduler overhead time: 0.09105033706873655 Adapter cache time: 0.0161132775247097 Engine time: 0.09109916491433978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 34560, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 253260 . Total input tokens: 56574834 . Total output tokens: 49625007
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.756154287606478,
    "estimated_duration": 3599.972264120277,
    "input_throughput": 5804.466942221379,
    "output_throughput": 5071.270460040979,
    "total_throughput": 10875.737402262357,
    "itl": 44.78440191038597,
    "ttft": 10407.964230612133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 84654,
    "finished_requests": 84411,
    "scheduler_time": 59.599043985758215
}
#Debug simulation 
Total elapsed time: 5.756247642915696. Arrivals time: 0.2098237737081945 Scheduler time: 5.3058125814422965 Scheduler overhead time: 0.09093206049874425 Adapter cache time: 0.016114401165395975 Engine time: 0.09103791834786534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 34560, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 253260 . Total input tokens: 56574834 . Total output tokens: 49625007
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.78815621137619,
    "estimated_duration": 3599.976243669074,
    "input_throughput": 5804.460525745861,
    "output_throughput": 5071.264854068358,
    "total_throughput": 10875.725379814217,
    "itl": 44.784172886286534,
    "ttft": 10407.961941163341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 84654,
    "finished_requests": 84411,
    "scheduler_time": 59.598987276496075
}
#Debug simulation 
Total elapsed time: 5.788276549428701. Arrivals time: 0.2179385949857533 Scheduler time: 5.323384799063206 Scheduler overhead time: 0.09438351355493069 Adapter cache time: 0.016645044554024935 Engine time: 0.09274250734597445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 34560, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 253260 . Total input tokens: 56574834 . Total output tokens: 49625007
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.766133031342179,
    "estimated_duration": 3599.964366168402,
    "input_throughput": 5804.479676625364,
    "output_throughput": 5071.281585887227,
    "total_throughput": 10875.761262512591,
    "itl": 44.78419957441741,
    "ttft": 10407.826217508662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 84654,
    "finished_requests": 84411,
    "scheduler_time": 59.59882564807415
}
#Debug simulation 
Total elapsed time: 5.766226448118687. Arrivals time: 0.21340432949364185 Scheduler time: 5.308252779766917 Scheduler overhead time: 0.09418913396075368 Adapter cache time: 0.016546906903386116 Engine time: 0.09095392283052206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 34560, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251910 . Total input tokens: 56274204 . Total output tokens: 49372919
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.693297366145998,
    "estimated_duration": 3599.9481897202636,
    "input_throughput": 5768.564130811471,
    "output_throughput": 5030.34767325559,
    "total_throughput": 10798.91180406706,
    "itl": 43.69916589842539,
    "ttft": 9949.514390861817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 84194,
    "finished_requests": 83963,
    "scheduler_time": 58.61838379175241
}
#Debug simulation 
Total elapsed time: 5.693452104926109. Arrivals time: 0.210868910420686 Scheduler time: 5.240690778475255 Scheduler overhead time: 0.09194749779999256 Adapter cache time: 0.015420966781675816 Engine time: 0.09166903560981154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 34560, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251910 . Total input tokens: 56274204 . Total output tokens: 49372919
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.721869580913335,
    "estimated_duration": 3599.921936029924,
    "input_throughput": 5768.606200083829,
    "output_throughput": 5030.384358826127,
    "total_throughput": 10798.990558909956,
    "itl": 43.69907424779397,
    "ttft": 9949.45280840742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 84194,
    "finished_requests": 83963,
    "scheduler_time": 58.61802713758423
}
#Debug simulation 
Total elapsed time: 5.7219696207903326. Arrivals time: 0.20350504899397492 Scheduler time: 5.275734739378095 Scheduler overhead time: 0.09227158734574914 Adapter cache time: 0.015679868403822184 Engine time: 0.09144018590450287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 34560, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251910 . Total input tokens: 56274204 . Total output tokens: 49372919
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.6984713957645,
    "estimated_duration": 3599.9339662576735,
    "input_throughput": 5768.586922606232,
    "output_throughput": 5030.367548331804,
    "total_throughput": 10798.954470938035,
    "itl": 43.69926346339372,
    "ttft": 9949.441991699543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 84194,
    "finished_requests": 83963,
    "scheduler_time": 58.61826533053455
}
#Debug simulation 
Total elapsed time: 5.698589693754911. Arrivals time: 0.20605723187327385 Scheduler time: 5.25122374901548 Scheduler overhead time: 0.09171377634629607 Adapter cache time: 0.015573865734040737 Engine time: 0.09149080235511065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 34560, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251910 . Total input tokens: 56274204 . Total output tokens: 49372919
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.713222899939865,
    "estimated_duration": 3599.952261369973,
    "input_throughput": 5768.557606399268,
    "output_throughput": 5030.341983787465,
    "total_throughput": 10798.899590186733,
    "itl": 43.69905243707605,
    "ttft": 9949.56671280201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 84194,
    "finished_requests": 83963,
    "scheduler_time": 58.61840409774271
}
#Debug simulation 
Total elapsed time: 5.713305079843849. Arrivals time: 0.2071078191511333 Scheduler time: 5.262492811307311 Scheduler overhead time: 0.09170235227793455 Adapter cache time: 0.015532075427472591 Engine time: 0.09369212482124567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 34560, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251910 . Total input tokens: 56274204 . Total output tokens: 49372919
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.708764767739922,
    "estimated_duration": 3599.9301889668295,
    "input_throughput": 5768.592975398765,
    "output_throughput": 5030.372826534514,
    "total_throughput": 10798.96580193328,
    "itl": 43.69916833809757,
    "ttft": 9949.516922029316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 84194,
    "finished_requests": 83963,
    "scheduler_time": 58.61822484147944
}
#Debug simulation 
Total elapsed time: 5.7088473457843065. Arrivals time: 0.21021470427513123 Scheduler time: 5.2525840708985925 Scheduler overhead time: 0.09446432162076235 Adapter cache time: 0.01617168541997671 Engine time: 0.09247983573004603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 34560, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251910 . Total input tokens: 56274204 . Total output tokens: 49372919
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.726117939222604,
    "estimated_duration": 3599.942396103282,
    "input_throughput": 5768.573414529772,
    "output_throughput": 5030.355768915047,
    "total_throughput": 10798.929183444818,
    "itl": 43.69907719175462,
    "ttft": 9949.490887457237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 84194,
    "finished_requests": 83963,
    "scheduler_time": 58.61829460110164
}
#Debug simulation 
Total elapsed time: 5.726197982206941. Arrivals time: 0.2141459253616631 Scheduler time: 5.269768381956965 Scheduler overhead time: 0.09149579051882029 Adapter cache time: 0.015720501076430082 Engine time: 0.09248916525393724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 34560, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251910 . Total input tokens: 56274204 . Total output tokens: 49372919
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.7104944959282875,
    "estimated_duration": 3599.9232766165555,
    "input_throughput": 5768.604051894615,
    "output_throughput": 5030.382485545642,
    "total_throughput": 10798.986537440256,
    "itl": 43.69908879598429,
    "ttft": 9949.505940383453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 84194,
    "finished_requests": 83963,
    "scheduler_time": 58.61804736162445
}
#Debug simulation 
Total elapsed time: 5.710608548019081. Arrivals time: 0.2093650526367128 Scheduler time: 5.259413089137524 Scheduler overhead time: 0.0914253331720829 Adapter cache time: 0.015532263554632664 Engine time: 0.09201116347685456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 34560, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251235 . Total input tokens: 56122533 . Total output tokens: 49236202
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.635407310910523,
    "estimated_duration": 3600.013770242132,
    "input_throughput": 5796.160884850327,
    "output_throughput": 4984.614822401933,
    "total_throughput": 10780.775707252262,
    "itl": 42.84158344569928,
    "ttft": 9587.492416242825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 83981,
    "finished_requests": 83758,
    "scheduler_time": 57.691706395160644
}
#Debug simulation 
Total elapsed time: 5.635495665017515. Arrivals time: 0.19764646422117949 Scheduler time: 5.195700496900827 Scheduler overhead time: 0.09173671342432499 Adapter cache time: 0.014995642006397247 Engine time: 0.0927049582824111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 34560, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251235 . Total input tokens: 56122533 . Total output tokens: 49236202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.656843685079366,
    "estimated_duration": 3600.036480113072,
    "input_throughput": 5796.124321313717,
    "output_throughput": 4984.583378287429,
    "total_throughput": 10780.707699601146,
    "itl": 42.84171432763175,
    "ttft": 9630.279828965513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 83981,
    "finished_requests": 83758,
    "scheduler_time": 57.69207963831093
}
#Debug simulation 
Total elapsed time: 5.656925703398883. Arrivals time: 0.2047026790678501 Scheduler time: 5.2092195064760745 Scheduler overhead time: 0.09218149725347757 Adapter cache time: 0.015237792860716581 Engine time: 0.09279859298840165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 34560, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251235 . Total input tokens: 56122533 . Total output tokens: 49236202
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.680881116073579,
    "estimated_duration": 3600.04076919119,
    "input_throughput": 5796.117415828032,
    "output_throughput": 4984.577439669267,
    "total_throughput": 10780.694855497299,
    "itl": 42.84170034216108,
    "ttft": 9630.168655882502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 83981,
    "finished_requests": 83758,
    "scheduler_time": 57.69213138682898
}
#Debug simulation 
Total elapsed time: 5.6809684401378036. Arrivals time: 0.19956030789762735 Scheduler time: 5.236934676300734 Scheduler overhead time: 0.09154229704290628 Adapter cache time: 0.01536117447540164 Engine time: 0.09463456505909562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 34560, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251235 . Total input tokens: 56122533 . Total output tokens: 49236202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.656623738817871,
    "estimated_duration": 3600.0253121951077,
    "input_throughput": 5796.142301920884,
    "output_throughput": 4984.598841350443,
    "total_throughput": 10780.741143271327,
    "itl": 42.841575653224304,
    "ttft": 9630.383522936945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 83981,
    "finished_requests": 83758,
    "scheduler_time": 57.69191367825651
}
#Debug simulation 
Total elapsed time: 5.656703487969935. Arrivals time: 0.20239551225677133 Scheduler time: 5.207988610491157 Scheduler overhead time: 0.09507481474429369 Adapter cache time: 0.01575337629765272 Engine time: 0.09179386030882597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 34560, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251235 . Total input tokens: 56122533 . Total output tokens: 49236202
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.707233575172722,
    "estimated_duration": 3600.0394241444656,
    "input_throughput": 5796.119581373412,
    "output_throughput": 4984.579302007082,
    "total_throughput": 10780.698883380495,
    "itl": 42.841690018304305,
    "ttft": 9630.169154244415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 83981,
    "finished_requests": 83758,
    "scheduler_time": 57.6921031960975
}
#Debug simulation 
Total elapsed time: 5.707320488058031. Arrivals time: 0.20552879385650158 Scheduler time: 5.254618102684617 Scheduler overhead time: 0.09520772658288479 Adapter cache time: 0.015955721028149128 Engine time: 0.09250969253480434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 34560, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251235 . Total input tokens: 56122533 . Total output tokens: 49236202
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.702175145037472,
    "estimated_duration": 3600.006164275087,
    "input_throughput": 5796.173130776214,
    "output_throughput": 4984.625353721698,
    "total_throughput": 10780.798484497913,
    "itl": 42.84154891917085,
    "ttft": 9587.429482351921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 83981,
    "finished_requests": 83758,
    "scheduler_time": 57.69154893447164
}
#Debug simulation 
Total elapsed time: 5.702275701798499. Arrivals time: 0.21298386389389634 Scheduler time: 5.244890460278839 Scheduler overhead time: 0.09423071518540382 Adapter cache time: 0.015491500962525606 Engine time: 0.09143658727407455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 34560, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 251235 . Total input tokens: 56122533 . Total output tokens: 49236202
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.684962139930576,
    "estimated_duration": 3600.036913422784,
    "input_throughput": 5796.123623677269,
    "output_throughput": 4984.582778330139,
    "total_throughput": 10780.706402007409,
    "itl": 42.84165407633164,
    "ttft": 9630.273185831951,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 83981,
    "finished_requests": 83758,
    "scheduler_time": 57.69205974206996
}
#Debug simulation 
Total elapsed time: 5.685052880086005. Arrivals time: 0.20915714465081692 Scheduler time: 5.232664267066866 Scheduler overhead time: 0.0921624549664557 Adapter cache time: 0.01524275541305542 Engine time: 0.09295844100415707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 34560, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250890 . Total input tokens: 56044293 . Total output tokens: 49166237
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.6544807786121964,
    "estimated_duration": 3599.977900537739,
    "input_throughput": 5752.601702612285,
    "output_throughput": 5017.914692560104,
    "total_throughput": 10770.51639517239,
    "itl": 42.97840965364274,
    "ttft": 11018.89152557833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 83852,
    "finished_requests": 83597,
    "scheduler_time": 58.22627947120846
}
#Debug simulation 
Total elapsed time: 5.65455575985834. Arrivals time: 0.19539149338379502 Scheduler time: 5.214364632498473 Scheduler overhead time: 0.09529702365398407 Adapter cache time: 0.0149114984087646 Engine time: 0.09115827921777964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 34560, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250890 . Total input tokens: 56044293 . Total output tokens: 49166237
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.655471953097731,
    "estimated_duration": 3600.0048222412493,
    "input_throughput": 5752.752016344981,
    "output_throughput": 5017.9016117966285,
    "total_throughput": 10770.653628141608,
    "itl": 42.978722448955686,
    "ttft": 10975.987690089434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 83852,
    "finished_requests": 83598,
    "scheduler_time": 58.22680809732656
}
#Debug simulation 
Total elapsed time: 5.655549285933375. Arrivals time: 0.20092286309227347 Scheduler time: 5.213834576308727 Scheduler overhead time: 0.09262085612863302 Adapter cache time: 0.01466868445277214 Engine time: 0.09068238083273172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 34560, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250890 . Total input tokens: 56044293 . Total output tokens: 49166237
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.702830057125539,
    "estimated_duration": 3600.0111803225273,
    "input_throughput": 5752.741856247398,
    "output_throughput": 5017.89274953907,
    "total_throughput": 10770.634605786468,
    "itl": 42.97867031817665,
    "ttft": 10975.958849002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 83852,
    "finished_requests": 83598,
    "scheduler_time": 58.22693781800792
}
#Debug simulation 
Total elapsed time: 5.702931753825396. Arrivals time: 0.20560230035334826 Scheduler time: 5.2523501720279455 Scheduler overhead time: 0.09359292266890407 Adapter cache time: 0.015102940611541271 Engine time: 0.09310612501576543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 34560, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250890 . Total input tokens: 56044293 . Total output tokens: 49166237
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.700751468073577,
    "estimated_duration": 3599.99201322163,
    "input_throughput": 5752.579151270761,
    "output_throughput": 5017.8950213376165,
    "total_throughput": 10770.474172608378,
    "itl": 42.97861686969723,
    "ttft": 11018.942697566765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 83852,
    "finished_requests": 83597,
    "scheduler_time": 58.22658167002675
}
#Debug simulation 
Total elapsed time: 5.700840387959033. Arrivals time: 0.20990197965875268 Scheduler time: 5.2449764451012015 Scheduler overhead time: 0.09466674737632275 Adapter cache time: 0.015545175410807133 Engine time: 0.09252430591732264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 34560, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250890 . Total input tokens: 56044293 . Total output tokens: 49166237
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.67988086072728,
    "estimated_duration": 3600.009640391188,
    "input_throughput": 5752.744317026217,
    "output_throughput": 5017.8948959806285,
    "total_throughput": 10770.639213006845,
    "itl": 42.978698207340926,
    "ttft": 10975.91759173891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 83852,
    "finished_requests": 83598,
    "scheduler_time": 58.226897247002555
}
#Debug simulation 
Total elapsed time: 5.679964535869658. Arrivals time: 0.21083846036344767 Scheduler time: 5.2255762023851275 Scheduler overhead time: 0.09407955454662442 Adapter cache time: 0.015093506779521704 Engine time: 0.09133623773232102 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 34560, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250890 . Total input tokens: 56044293 . Total output tokens: 49166237
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.709774738643318,
    "estimated_duration": 3600.019550242514,
    "input_throughput": 5752.728481323075,
    "output_throughput": 5017.8810831133105,
    "total_throughput": 10770.609564436387,
    "itl": 42.97859576376917,
    "ttft": 10976.065098434552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 83852,
    "finished_requests": 83598,
    "scheduler_time": 58.22700800186683
}
#Debug simulation 
Total elapsed time: 5.709878462832421. Arrivals time: 0.20778621127828956 Scheduler time: 5.260418506804854 Scheduler overhead time: 0.09201741823926568 Adapter cache time: 0.015118810348212719 Engine time: 0.09172666678205132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 34560, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250890 . Total input tokens: 56044293 . Total output tokens: 49166237
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.692113853991032,
    "estimated_duration": 3600.0079921855786,
    "input_throughput": 5752.74695082744,
    "output_throughput": 5017.897193342894,
    "total_throughput": 10770.644144170334,
    "itl": 42.97876425181191,
    "ttft": 10975.924409896967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 83852,
    "finished_requests": 83598,
    "scheduler_time": 58.22689320219466
}
#Debug simulation 
Total elapsed time: 5.692212081048638. Arrivals time: 0.20189917087554932 Scheduler time: 5.247685352340341 Scheduler overhead time: 0.0933855646289885 Adapter cache time: 0.015337114222347736 Engine time: 0.09098107647150755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 34560, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250725 . Total input tokens: 56009826 . Total output tokens: 49137310
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.671775094233453,
    "estimated_duration": 3600.046671536599,
    "input_throughput": 5798.20149695184,
    "output_throughput": 4977.851021123301,
    "total_throughput": 10776.052518075141,
    "itl": 42.43625991172651,
    "ttft": 10896.198899312843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 83799,
    "finished_requests": 83547,
    "scheduler_time": 57.48450943336335
}
#Debug simulation 
Total elapsed time: 5.6718729780986905. Arrivals time: 0.20040906686335802 Scheduler time: 5.225054951384664 Scheduler overhead time: 0.0945323621854186 Adapter cache time: 0.015258223749697208 Engine time: 0.09297260828316212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 34560, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250725 . Total input tokens: 56009826 . Total output tokens: 49137310
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.6005903971381485,
    "estimated_duration": 3600.027388433201,
    "input_throughput": 5798.146999399505,
    "output_throughput": 4977.793796118646,
    "total_throughput": 10775.94079551815,
    "itl": 42.436309013540985,
    "ttft": 10896.208717286314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 83799,
    "finished_requests": 83546,
    "scheduler_time": 57.48431089548207
}
#Debug simulation 
Total elapsed time: 5.6007054089568555. Arrivals time: 0.2037345776334405 Scheduler time: 5.156664280220866 Scheduler overhead time: 0.0914959660731256 Adapter cache time: 0.01460975967347622 Engine time: 0.09143372997641563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 34560, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250725 . Total input tokens: 56009826 . Total output tokens: 49137310
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.683853900060058,
    "estimated_duration": 3600.036653914598,
    "input_throughput": 5798.217631285034,
    "output_throughput": 4977.864872712798,
    "total_throughput": 10776.082503997834,
    "itl": 42.43643009421618,
    "ttft": 10853.223780383629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 83799,
    "finished_requests": 83547,
    "scheduler_time": 57.48453658523393
}
#Debug simulation 
Total elapsed time: 5.683940149843693. Arrivals time: 0.20762525545433164 Scheduler time: 5.2288309298455715 Scheduler overhead time: 0.09522796561941504 Adapter cache time: 0.01542673446238041 Engine time: 0.09301542397588491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 34560, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250725 . Total input tokens: 56009826 . Total output tokens: 49137310
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.650937492027879,
    "estimated_duration": 3600.009204362036,
    "input_throughput": 5798.176286523975,
    "output_throughput": 4977.818939542314,
    "total_throughput": 10775.995226066289,
    "itl": 42.436162705764914,
    "ttft": 10896.302033958344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 83799,
    "finished_requests": 83546,
    "scheduler_time": 57.483930314752214
}
#Debug simulation 
Total elapsed time: 5.651036740280688. Arrivals time: 0.21376707404851913 Scheduler time: 5.195324731990695 Scheduler overhead time: 0.09226609533652663 Adapter cache time: 0.014755822718143463 Engine time: 0.0919911595992744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 34560, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250725 . Total input tokens: 56009826 . Total output tokens: 49137310
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.645903890952468,
    "estimated_duration": 3600.033882940312,
    "input_throughput": 5798.136539468253,
    "output_throughput": 4977.784816115052,
    "total_throughput": 10775.921355583305,
    "itl": 42.4363277062828,
    "ttft": 10896.195730002624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 83799,
    "finished_requests": 83546,
    "scheduler_time": 57.48445180816481
}
#Debug simulation 
Total elapsed time: 5.646060538943857. Arrivals time: 0.20863728690892458 Scheduler time: 5.195486351847649 Scheduler overhead time: 0.0922273057512939 Adapter cache time: 0.014845720492303371 Engine time: 0.09173273108899593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 34560, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250725 . Total input tokens: 56009826 . Total output tokens: 49137310
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.618831699248403,
    "estimated_duration": 3600.039831158827,
    "input_throughput": 5798.212514021234,
    "output_throughput": 4977.860479457951,
    "total_throughput": 10776.072993479185,
    "itl": 42.43626406957453,
    "ttft": 10896.139400383932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 83799,
    "finished_requests": 83547,
    "scheduler_time": 57.48444434766018
}
#Debug simulation 
Total elapsed time: 5.61892335023731. Arrivals time: 0.20592812355607748 Scheduler time: 5.172715479042381 Scheduler overhead time: 0.09198241448029876 Adapter cache time: 0.014672956429421902 Engine time: 0.09070414397865534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 34560, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 34560]
Prompts retrieved: 250725 . Total input tokens: 56009826 . Total output tokens: 49137310
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.635993420146406,
    "estimated_duration": 3600.0293587940887,
    "input_throughput": 5798.143825969255,
    "output_throughput": 4977.791071682475,
    "total_throughput": 10775.93489765173,
    "itl": 42.43630951772385,
    "ttft": 10896.220499459872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 83799,
    "finished_requests": 83546,
    "scheduler_time": 57.4843472168046
}
#Debug simulation 
Total elapsed time: 5.636154405307025. Arrivals time: 0.19960326701402664 Scheduler time: 5.195490741636604 Scheduler overhead time: 0.09167956793680787 Adapter cache time: 0.01469565974548459 Engine time: 0.09216574439778924 
