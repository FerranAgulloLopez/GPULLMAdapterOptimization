INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 92.99673252098728,
    "estimated_duration": 3600.0740858081285,
    "input_throughput": 7699.0223921400775,
    "output_throughput": 6680.877789380546,
    "total_throughput": 14379.900181520623,
    "itl": 89.23688158905905,
    "ttft": 1742759.8470626601,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8001014928659416,
    "arrivals": 435333,
    "finished_requests": 111423,
    "scheduler_time": 237.31925300950883
}
#Debug simulation 
Total elapsed time: 92.99694560992066. Arrivals time: 0.5062605359125882 Scheduler time: 92.27821404975839 Scheduler overhead time: 0.08326349454000592 Adapter cache time: 0.01426547986920923 Engine time: 0.08228631387464702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 89.98696981591638,
    "estimated_duration": 3600.02390958264,
    "input_throughput": 7653.210837478617,
    "output_throughput": 6650.251387573576,
    "total_throughput": 14303.462225052192,
    "itl": 88.36193478545901,
    "ttft": 1745737.1901233802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9209846002701675,
    "arrivals": 435333,
    "finished_requests": 110712,
    "scheduler_time": 238.2245523557684
}
#Debug simulation 
Total elapsed time: 89.98716875200626. Arrivals time: 0.48900961596518755 Scheduler time: 89.28221312060487 Scheduler overhead time: 0.08554504171479493 Adapter cache time: 0.014316707733087242 Engine time: 0.0827931931708008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.3291080519557,
    "estimated_duration": 3600.0902739830312,
    "input_throughput": 7605.802331647048,
    "output_throughput": 6605.839351269582,
    "total_throughput": 14211.64168291663,
    "itl": 86.33850242342054,
    "ttft": 1758784.3664513524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9215871563879774,
    "arrivals": 435333,
    "finished_requests": 110014,
    "scheduler_time": 239.62319719625185
}
#Debug simulation 
Total elapsed time: 81.32928656204604. Arrivals time: 0.49462917167693377 Scheduler time: 80.6202526568668 Scheduler overhead time: 0.0847243539756164 Adapter cache time: 0.014287250000052154 Engine time: 0.08218701393343508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 90.00166704389267,
    "estimated_duration": 3600.0537152414613,
    "input_throughput": 7676.652957425771,
    "output_throughput": 6671.405178849981,
    "total_throughput": 14348.058136275753,
    "itl": 88.31623303425894,
    "ttft": 1744676.1833959639,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7988401099853222,
    "arrivals": 435333,
    "finished_requests": 111012,
    "scheduler_time": 237.32557751968238
}
#Debug simulation 
Total elapsed time: 90.00184459390584. Arrivals time: 0.48773398238699883 Scheduler time: 89.3017627953086 Scheduler overhead time: 0.08377490087877959 Adapter cache time: 0.01407448889221996 Engine time: 0.08214532653801143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 89.13185669993982,
    "estimated_duration": 3600.0369098457495,
    "input_throughput": 6554.960571504559,
    "output_throughput": 5693.915232907512,
    "total_throughput": 12248.875804412071,
    "itl": 66.57519583643588,
    "ttft": 1824377.437129903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6918883102200927,
    "arrivals": 435333,
    "finished_requests": 94818,
    "scheduler_time": 274.08243854992463
}
#Debug simulation 
Total elapsed time: 89.13203170092311. Arrivals time: 0.46188824775163084 Scheduler time: 88.41719659266528 Scheduler overhead time: 0.10051658400334418 Adapter cache time: 0.016541365534067154 Engine time: 0.09605529310647398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 90.75648009194992,
    "estimated_duration": 3600.079665461261,
    "input_throughput": 7684.30800723837,
    "output_throughput": 6673.473154078602,
    "total_throughput": 14357.781161316972,
    "itl": 88.52314220238881,
    "ttft": 1745976.9871075233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7213830662844697,
    "arrivals": 435333,
    "finished_requests": 111163,
    "scheduler_time": 237.30693819883965
}
#Debug simulation 
Total elapsed time: 90.75665743800346. Arrivals time: 0.49599358008708805 Scheduler time: 90.04628846491687 Scheduler overhead time: 0.08531602926086634 Adapter cache time: 0.013849429320544004 Engine time: 0.08236029371619225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 84.04922695399728,
    "estimated_duration": 3600.0394497758507,
    "input_throughput": 7585.794928358452,
    "output_throughput": 6593.271915805777,
    "total_throughput": 14179.06684416423,
    "itl": 86.18337004546942,
    "ttft": 1750426.4669657291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9186863781511784,
    "arrivals": 435333,
    "finished_requests": 109723,
    "scheduler_time": 240.18833633479784
}
#Debug simulation 
Total elapsed time: 84.04939857497811. Arrivals time: 0.5023699968587607 Scheduler time: 83.33253346488345 Scheduler overhead time: 0.08485595742240548 Adapter cache time: 0.013938277610577643 Engine time: 0.08263570431154221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.30561329994816,
    "estimated_duration": 3600.0476754421707,
    "input_throughput": 7741.119427419,
    "output_throughput": 6725.335379628342,
    "total_throughput": 14466.454807047343,
    "itl": 89.37479699768579,
    "ttft": 1717114.2032886322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8860628102812906,
    "arrivals": 404972,
    "finished_requests": 112371,
    "scheduler_time": 235.17011694826084
}
#Debug simulation 
Total elapsed time: 82.30578566202894. Arrivals time: 0.4795689405873418 Scheduler time: 81.61955814040266 Scheduler overhead time: 0.08105531171895564 Adapter cache time: 0.013770697405561805 Engine time: 0.07957776566036046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.96855435008183,
    "estimated_duration": 3600.0253933290974,
    "input_throughput": 7703.437606687131,
    "output_throughput": 6691.511688955979,
    "total_throughput": 14394.94929564311,
    "itl": 88.21087142512599,
    "ttft": 1721820.6032131063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9712249307474128,
    "arrivals": 404972,
    "finished_requests": 111809,
    "scheduler_time": 236.64031651415158
}
#Debug simulation 
Total elapsed time: 81.96872281900141. Arrivals time: 0.4803112780209631 Scheduler time: 81.2804236401571 Scheduler overhead time: 0.0820403021061793 Adapter cache time: 0.01407795341219753 Engine time: 0.07953728886786848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.81451720499899,
    "estimated_duration": 3600.0381708644522,
    "input_throughput": 7626.891909706312,
    "output_throughput": 6626.916679128251,
    "total_throughput": 14253.808588834563,
    "itl": 85.96418808408609,
    "ttft": 1729630.4137463677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9972214709408582,
    "arrivals": 404972,
    "finished_requests": 110710,
    "scheduler_time": 238.98606243238785
}
#Debug simulation 
Total elapsed time: 79.81482536101248. Arrivals time: 0.46531636954750866 Scheduler time: 79.13870769366622 Scheduler overhead time: 0.0829893680056557 Adapter cache time: 0.013885581050999463 Engine time: 0.08083200047258288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 81.9885020739166,
    "estimated_duration": 3600.0893725130027,
    "input_throughput": 7715.155688096652,
    "output_throughput": 6697.767889903381,
    "total_throughput": 14412.923578000033,
    "itl": 88.21585358126056,
    "ttft": 1720899.770618998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9179153498355295,
    "arrivals": 404972,
    "finished_requests": 111926,
    "scheduler_time": 236.33557142299426
}
#Debug simulation 
Total elapsed time: 81.98867244599387. Arrivals time: 0.4588831003056839 Scheduler time: 81.3197346504312 Scheduler overhead time: 0.08264082192908973 Adapter cache time: 0.013987881131470203 Engine time: 0.0806157753104344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 80.09304769698065,
    "estimated_duration": 3600.0636054613587,
    "input_throughput": 7626.838025402412,
    "output_throughput": 6626.869859690336,
    "total_throughput": 14253.707885092748,
    "itl": 85.96410558035646,
    "ttft": 1729650.9354546566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9881083151698113,
    "arrivals": 404972,
    "finished_requests": 110710,
    "scheduler_time": 238.98969614205444
}
#Debug simulation 
Total elapsed time: 80.09322275000159. Arrivals time: 0.45555046445224434 Scheduler time: 79.4245919904206 Scheduler overhead time: 0.08409497211687267 Adapter cache time: 0.014115480473265052 Engine time: 0.08156357880216092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.00896134704817,
    "estimated_duration": 3600.0639172332303,
    "input_throughput": 7715.341349090983,
    "output_throughput": 6697.996356279089,
    "total_throughput": 14413.337705370071,
    "itl": 88.21397963101029,
    "ttft": 1720930.2127969882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8554454060364501,
    "arrivals": 404972,
    "finished_requests": 111924,
    "scheduler_time": 236.3237531261486
}
#Debug simulation 
Total elapsed time: 82.00912655703723. Arrivals time: 0.4684044247260317 Scheduler time: 81.33249591465574 Scheduler overhead time: 0.0819943908136338 Adapter cache time: 0.013791509554721415 Engine time: 0.08015627146232873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.93440145696513,
    "estimated_duration": 3600.095213483516,
    "input_throughput": 7626.833839606093,
    "output_throughput": 6627.030282600396,
    "total_throughput": 14253.864122206489,
    "itl": 85.96359845117186,
    "ttft": 1729582.2856313917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9789951593987646,
    "arrivals": 404972,
    "finished_requests": 110713,
    "scheduler_time": 238.99077388105727
}
#Debug simulation 
Total elapsed time: 79.93457067897543. Arrivals time: 0.4620309545425698 Scheduler time: 79.26156042981893 Scheduler overhead time: 0.08336589112877846 Adapter cache time: 0.014033667859621346 Engine time: 0.08046538301277906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 96.67894704290666,
    "estimated_duration": 3600.093924443351,
    "input_throughput": 7728.675580124526,
    "output_throughput": 6724.711773663878,
    "total_throughput": 14453.387353788405,
    "itl": 89.64119949879506,
    "ttft": 1676032.926025583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7009153573866926,
    "arrivals": 382341,
    "finished_requests": 112502,
    "scheduler_time": 234.63944528881422
}
#Debug simulation 
Total elapsed time: 96.67911988298874. Arrivals time: 0.4681428528856486 Scheduler time: 95.99450432264712 Scheduler overhead time: 0.08596991316881031 Adapter cache time: 0.014258597744628787 Engine time: 0.08325601811520755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 89.38470748602413,
    "estimated_duration": 3600.0173749838677,
    "input_throughput": 7717.082476615684,
    "output_throughput": 6711.25955332598,
    "total_throughput": 14428.342029941665,
    "itl": 88.57145473465646,
    "ttft": 1682666.685548721,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7985757345473397,
    "arrivals": 382341,
    "finished_requests": 112228,
    "scheduler_time": 235.8441485932483
}
#Debug simulation 
Total elapsed time: 89.38487343501765. Arrivals time: 0.4758231147425249 Scheduler time: 88.69621093384922 Scheduler overhead time: 0.08439599792473018 Adapter cache time: 0.013960981625132263 Engine time: 0.08172386710066348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.10573083290365,
    "estimated_duration": 3600.0277688829233,
    "input_throughput": 7618.333179832738,
    "output_throughput": 6641.5434921545375,
    "total_throughput": 14259.876671987276,
    "itl": 86.41430176719584,
    "ttft": 1700086.0086692409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9458153765089812,
    "arrivals": 382341,
    "finished_requests": 110923,
    "scheduler_time": 238.16962372074636
}
#Debug simulation 
Total elapsed time: 80.10590290196706. Arrivals time: 0.46166439552325755 Scheduler time: 79.43322883860674 Scheduler overhead time: 0.08319952944293618 Adapter cache time: 0.014087631599977612 Engine time: 0.0807646163739264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 77.37277765606996,
    "estimated_duration": 3600.019506731512,
    "input_throughput": 7690.987492769977,
    "output_throughput": 6692.73762404558,
    "total_throughput": 14383.725116815556,
    "itl": 88.84908849621775,
    "ttft": 1694009.9491943736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7599794004159045,
    "arrivals": 382341,
    "finished_requests": 111971,
    "scheduler_time": 235.3535562356259
}
#Debug simulation 
Total elapsed time: 77.37294890999328. Arrivals time: 0.49374622327741235 Scheduler time: 76.67220094159711 Scheduler overhead time: 0.0815212766174227 Adapter cache time: 0.013489790027961135 Engine time: 0.07977899594698101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 79.10666326305363,
    "estimated_duration": 3600.0709819624744,
    "input_throughput": 7618.3361765409445,
    "output_throughput": 6636.199708200372,
    "total_throughput": 14254.535884741317,
    "itl": 86.5003562936274,
    "ttft": 1699801.8622679913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.924796155486256,
    "arrivals": 382341,
    "finished_requests": 110985,
    "scheduler_time": 237.83580803932878
}
#Debug simulation 
Total elapsed time: 79.10682908608578. Arrivals time: 0.46853665239177644 Scheduler time: 78.42533623741474 Scheduler overhead time: 0.08329273120034486 Adapter cache time: 0.01393037661910057 Engine time: 0.08236072410363704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 94.2479937520111,
    "estimated_duration": 3600.0806581549,
    "input_throughput": 7658.9036797112885,
    "output_throughput": 6659.147745952668,
    "total_throughput": 14318.051425663956,
    "itl": 88.15473444029504,
    "ttft": 1687251.6939110595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6639277778193352,
    "arrivals": 382341,
    "finished_requests": 111454,
    "scheduler_time": 237.01662763747638
}
#Debug simulation 
Total elapsed time: 94.24816273909528. Arrivals time: 0.48608330078423023 Scheduler time: 93.54555954399984 Scheduler overhead time: 0.08634901500772685 Adapter cache time: 0.014153730473481119 Engine time: 0.08302910381462425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.06018583092373,
    "estimated_duration": 3600.0107055641065,
    "input_throughput": 7619.049009384,
    "output_throughput": 6641.249139355998,
    "total_throughput": 14260.298148739997,
    "itl": 86.4192514349677,
    "ttft": 1700224.9188488128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9441554460674526,
    "arrivals": 382341,
    "finished_requests": 110934,
    "scheduler_time": 238.16126840791097
}
#Debug simulation 
Total elapsed time: 80.06036649597809. Arrivals time: 0.4658091322053224 Scheduler time: 79.38184700685088 Scheduler overhead time: 0.08372509328182787 Adapter cache time: 0.013904843130148947 Engine time: 0.08154719660524279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 84.31665581301786,
    "estimated_duration": 3600.0660588324035,
    "input_throughput": 7765.653613885949,
    "output_throughput": 6752.031102419172,
    "total_throughput": 14517.684716305122,
    "itl": 89.56957322679625,
    "ttft": 1675150.649446086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7538146296422921,
    "arrivals": 378554,
    "finished_requests": 112536,
    "scheduler_time": 234.19706456560112
}
#Debug simulation 
Total elapsed time: 84.3168286740547. Arrivals time: 0.4855694167781621 Scheduler time: 83.62310619745404 Scheduler overhead time: 0.08208335028029978 Adapter cache time: 0.013578102109022439 Engine time: 0.08046660455875099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 85.62327906198334,
    "estimated_duration": 3600.0794987244867,
    "input_throughput": 7740.046854485445,
    "output_throughput": 6723.786796534988,
    "total_throughput": 14463.833651020432,
    "itl": 88.48543629728071,
    "ttft": 1677112.6305777254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8035714345145974,
    "arrivals": 378554,
    "finished_requests": 112135,
    "scheduler_time": 235.26271395183858
}
#Debug simulation 
Total elapsed time: 85.62358090898488. Arrivals time: 0.47004707087762654 Scheduler time: 84.94122843886726 Scheduler overhead time: 0.0840513544389978 Adapter cache time: 0.013970104278996587 Engine time: 0.081348130479455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.70912658201996,
    "estimated_duration": 3600.0945469452313,
    "input_throughput": 7641.491255651324,
    "output_throughput": 6642.027504608138,
    "total_throughput": 14283.518760259461,
    "itl": 86.39702136118974,
    "ttft": 1696319.4348271827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2645047814678412,
    "arrivals": 378554,
    "finished_requests": 110742,
    "scheduler_time": 237.42596793997367
}
#Debug simulation 
Total elapsed time: 74.70929632906336. Arrivals time: 0.4561748133273795 Scheduler time: 74.04358036862686 Scheduler overhead time: 0.0820303859654814 Adapter cache time: 0.014267442747950554 Engine time: 0.08058110228739679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 83.23756179597694,
    "estimated_duration": 3600.0862998359776,
    "input_throughput": 7739.256973164619,
    "output_throughput": 6723.17855299823,
    "total_throughput": 14462.43552616285,
    "itl": 88.48685713223574,
    "ttft": 1677139.4673699432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7627558423625304,
    "arrivals": 378554,
    "finished_requests": 112125,
    "scheduler_time": 235.26186981645031
}
#Debug simulation 
Total elapsed time: 83.2377353879856. Arrivals time: 0.47967888740822673 Scheduler time: 82.54632923193276 Scheduler overhead time: 0.08294332062359899 Adapter cache time: 0.013973385095596313 Engine time: 0.08234734262805432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 82.73576142301317,
    "estimated_duration": 3600.0836170625435,
    "input_throughput": 7628.1880981440845,
    "output_throughput": 6628.015773552923,
    "total_throughput": 14256.203871697007,
    "itl": 86.22078207055405,
    "ttft": 1690764.1638445065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2329240809520718,
    "arrivals": 378554,
    "finished_requests": 110459,
    "scheduler_time": 237.9418365959144
}
#Debug simulation 
Total elapsed time: 82.73593163397163. Arrivals time: 0.4521524327574298 Scheduler time: 82.06952594523318 Scheduler overhead time: 0.08374305721372366 Adapter cache time: 0.01465176057536155 Engine time: 0.08252380311023444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 85.16494442499243,
    "estimated_duration": 3600.078084905405,
    "input_throughput": 7739.767678048057,
    "output_throughput": 6723.694439154374,
    "total_throughput": 14463.46211720243,
    "itl": 88.48026102924513,
    "ttft": 1677129.4781544986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7086152244033287,
    "arrivals": 378554,
    "finished_requests": 112133,
    "scheduler_time": 235.2768957447749
}
#Debug simulation 
Total elapsed time: 85.16511657997034. Arrivals time: 0.4692718512378633 Scheduler time: 84.48410720098764 Scheduler overhead time: 0.08282351109664887 Adapter cache time: 0.013766221585683525 Engine time: 0.08171677787322551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 71.92013783298898,
    "estimated_duration": 3600.054677150373,
    "input_throughput": 7646.349422056345,
    "output_throughput": 6646.555995905097,
    "total_throughput": 14292.905417961441,
    "itl": 86.35354926352595,
    "ttft": 1695829.3012985988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.427105503734204,
    "arrivals": 378554,
    "finished_requests": 110688,
    "scheduler_time": 237.3352647053941
}
#Debug simulation 
Total elapsed time: 71.92030641599558. Arrivals time: 0.45483946066815406 Scheduler time: 71.2573819742538 Scheduler overhead time: 0.08143143751658499 Adapter cache time: 0.01417637150734663 Engine time: 0.0798481289530173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.13256829394959,
    "estimated_duration": 3600.068964434334,
    "input_throughput": 7700.477483590624,
    "output_throughput": 6739.9769948053145,
    "total_throughput": 14440.454478395937,
    "itl": 90.20615602499082,
    "ttft": 1673357.8912186485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2100708528468371,
    "arrivals": 376669,
    "finished_requests": 112638,
    "scheduler_time": 233.37802094589375
}
#Debug simulation 
Total elapsed time: 83.13274052296765. Arrivals time: 0.4619926711311564 Scheduler time: 82.46261369681451 Scheduler overhead time: 0.08150731341447681 Adapter cache time: 0.014255894697271287 Engine time: 0.08015140669886023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.79367091599852,
    "estimated_duration": 3600.038890706032,
    "input_throughput": 7625.934283897652,
    "output_throughput": 6688.697464398105,
    "total_throughput": 14314.631748295757,
    "itl": 88.92010100535605,
    "ttft": 1680579.4795262227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2110469622258104,
    "arrivals": 376669,
    "finished_requests": 111609,
    "scheduler_time": 235.27314887766542
}
#Debug simulation 
Total elapsed time: 80.7938414249802. Arrivals time: 0.45995241310447454 Scheduler time: 80.1251123509137 Scheduler overhead time: 0.08236824383493513 Adapter cache time: 0.014003011165186763 Engine time: 0.0798193976515904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.23081662307959,
    "estimated_duration": 3600.0027048842476,
    "input_throughput": 7561.401263134676,
    "output_throughput": 6630.231129442294,
    "total_throughput": 14191.63239257697,
    "itl": 86.82321002464448,
    "ttft": 1684383.1450527022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6600693487748537,
    "arrivals": 376669,
    "finished_requests": 110680,
    "scheduler_time": 237.30240224876775
}
#Debug simulation 
Total elapsed time: 76.23097768810112. Arrivals time: 0.4568517990410328 Scheduler time: 75.56388293078635 Scheduler overhead time: 0.08238094300031662 Adapter cache time: 0.014362277928739786 Engine time: 0.08086719072889537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 81.32971509604249,
    "estimated_duration": 3600.0089449300563,
    "input_throughput": 7620.626620563304,
    "output_throughput": 6676.741465837385,
    "total_throughput": 14297.36808640069,
    "itl": 89.00860640523824,
    "ttft": 1677190.9506521237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2457240535086014,
    "arrivals": 376669,
    "finished_requests": 111494,
    "scheduler_time": 235.6728004924633
}
#Debug simulation 
Total elapsed time: 81.32988814008422. Arrivals time: 0.46793976065237075 Scheduler time: 80.65181587892585 Scheduler overhead time: 0.08296882302965969 Adapter cache time: 0.014215199858881533 Engine time: 0.08060532319359481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 81.09728831495158,
    "estimated_duration": 3600.0534133343176,
    "input_throughput": 7531.439922411537,
    "output_throughput": 6599.602081457683,
    "total_throughput": 14131.04200386922,
    "itl": 86.5663102067649,
    "ttft": 1680419.8635737062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1924899898376324,
    "arrivals": 376669,
    "finished_requests": 110170,
    "scheduler_time": 238.54620926372465
}
#Debug simulation 
Total elapsed time: 81.09745690005366. Arrivals time: 0.4535600708331913 Scheduler time: 80.42990491061937 Scheduler overhead time: 0.08362279995344579 Adapter cache time: 0.014391185017302632 Engine time: 0.0825211483752355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 79.75440723903012,
    "estimated_duration": 3600.038275413161,
    "input_throughput": 7652.97557755496,
    "output_throughput": 6699.211829138719,
    "total_throughput": 14352.18740669368,
    "itl": 88.98490466321933,
    "ttft": 1683108.4653028916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1427218483621226,
    "arrivals": 376669,
    "finished_requests": 111984,
    "scheduler_time": 234.87642727022975
}
#Debug simulation 
Total elapsed time: 79.75458285503555. Arrivals time: 0.4667481695069 Scheduler time: 79.07949231017847 Scheduler overhead time: 0.08133949828334153 Adapter cache time: 0.014490899862721562 Engine time: 0.0800163900712505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.64714867004659,
    "estimated_duration": 3600.0540204366816,
    "input_throughput": 7533.471677380628,
    "output_throughput": 6606.918358718095,
    "total_throughput": 14140.390036098723,
    "itl": 86.63815943155731,
    "ttft": 1683843.7956645382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 163,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.197043574694545,
    "arrivals": 376669,
    "finished_requests": 110288,
    "scheduler_time": 238.31671512399456
}
#Debug simulation 
Total elapsed time: 79.64730671502184. Arrivals time: 0.4618034412851557 Scheduler time: 78.97297413484193 Scheduler overhead time: 0.08360170701052994 Adapter cache time: 0.014429293223656714 Engine time: 0.0814236473524943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 81.32782810600474,
    "estimated_duration": 3600.0193957491306,
    "input_throughput": 7714.14677176235,
    "output_throughput": 6749.038082597354,
    "total_throughput": 14463.184854359704,
    "itl": 89.92796532008103,
    "ttft": 1678046.2131640103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2960321702621862,
    "arrivals": 375735,
    "finished_requests": 112593,
    "scheduler_time": 233.02525947854355
}
#Debug simulation 
Total elapsed time: 81.32812215201557. Arrivals time: 0.4627490215934813 Scheduler time: 80.65817566565238 Scheduler overhead time: 0.08156327030155808 Adapter cache time: 0.014078485197387636 Engine time: 0.07987969683017582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 83.61452602699865,
    "estimated_duration": 3600.0734092895477,
    "input_throughput": 7651.09926062195,
    "output_throughput": 6691.970485333611,
    "total_throughput": 14343.06974595556,
    "itl": 88.7434477745745,
    "ttft": 1669410.1397549934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9337524421513087,
    "arrivals": 375735,
    "finished_requests": 111640,
    "scheduler_time": 235.1544791407726
}
#Debug simulation 
Total elapsed time: 83.61469778500032. Arrivals time: 0.46822695597074926 Scheduler time: 82.93303689092863 Scheduler overhead time: 0.0845496222609654 Adapter cache time: 0.014310316764749587 Engine time: 0.08190656139049679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.99449258099776,
    "estimated_duration": 3600.009785263993,
    "input_throughput": 7580.689950263219,
    "output_throughput": 6632.059473207566,
    "total_throughput": 14212.749423470785,
    "itl": 86.44719718162462,
    "ttft": 1691336.6436604888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.148490100046621,
    "arrivals": 375735,
    "finished_requests": 110640,
    "scheduler_time": 237.25038280669747
}
#Debug simulation 
Total elapsed time: 76.99466228601523. Arrivals time: 0.4530862629180774 Scheduler time: 76.33236298523843 Scheduler overhead time: 0.0819606666918844 Adapter cache time: 0.014178896090015769 Engine time: 0.08023133559618145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 82.49650503695011,
    "estimated_duration": 3600.019093481084,
    "input_throughput": 7665.68656537738,
    "output_throughput": 6702.958338108823,
    "total_throughput": 14368.644903486203,
    "itl": 88.74619819315222,
    "ttft": 1671125.4812011626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8576836194237689,
    "arrivals": 375735,
    "finished_requests": 111871,
    "scheduler_time": 234.74526309507155
}
#Debug simulation 
Total elapsed time: 82.49666811898351. Arrivals time: 0.4581194843631238 Scheduler time: 81.82749243208673 Scheduler overhead time: 0.0827495789853856 Adapter cache time: 0.014193973620422184 Engine time: 0.08135009231045842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 80.69846128707286,
    "estimated_duration": 3600.0108088858024,
    "input_throughput": 7591.483317923263,
    "output_throughput": 6643.535052996746,
    "total_throughput": 14235.01837092001,
    "itl": 86.55884975686068,
    "ttft": 1685182.897896717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.643190974770118,
    "arrivals": 375735,
    "finished_requests": 110823,
    "scheduler_time": 236.8841089125634
}
#Debug simulation 
Total elapsed time: 80.6986185060814. Arrivals time: 0.45765572728123516 Scheduler time: 80.02998425601982 Scheduler overhead time: 0.08252028364222497 Adapter cache time: 0.01465073530562222 Engine time: 0.08105846552643925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 79.84018257702701,
    "estimated_duration": 3600.0667324360907,
    "input_throughput": 7672.243336808849,
    "output_throughput": 6717.558811371093,
    "total_throughput": 14389.802148179942,
    "itl": 88.8715843611312,
    "ttft": 1680360.14287624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4938375000935,
    "arrivals": 375735,
    "finished_requests": 112010,
    "scheduler_time": 234.4049221031693
}
#Debug simulation 
Total elapsed time: 79.84035662305541. Arrivals time: 0.44670551165472716 Scheduler time: 79.18665496085305 Scheduler overhead time: 0.08105606609024107 Adapter cache time: 0.014506292063742876 Engine time: 0.07897756830789149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.19247770495713,
    "estimated_duration": 3600.0401062460514,
    "input_throughput": 7559.811612315381,
    "output_throughput": 6608.118048108785,
    "total_throughput": 14167.929660424166,
    "itl": 86.10205850421366,
    "ttft": 1690928.368347037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8354917485266925,
    "arrivals": 375735,
    "finished_requests": 110386,
    "scheduler_time": 238.17645035568782
}
#Debug simulation 
Total elapsed time: 79.19264821498655. Arrivals time: 0.46077496418729424 Scheduler time: 78.52029793011025 Scheduler overhead time: 0.08313608483877033 Adapter cache time: 0.013947039493359625 Engine time: 0.08183653152082115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 80.52023435092997,
    "estimated_duration": 3600.014113867904,
    "input_throughput": 7698.837872116461,
    "output_throughput": 6737.61219617541,
    "total_throughput": 14436.45006829187,
    "itl": 90.06142176889739,
    "ttft": 1678468.675909659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.038148218016139,
    "arrivals": 375251,
    "finished_requests": 112579,
    "scheduler_time": 233.54222845987618
}
#Debug simulation 
Total elapsed time: 80.52039438195061. Arrivals time: 0.4724874022649601 Scheduler time: 79.84378687280696 Scheduler overhead time: 0.08017295959871262 Adapter cache time: 0.013745642849244177 Engine time: 0.07818160590250045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.18243043601979,
    "estimated_duration": 3600.0591409543545,
    "input_throughput": 7659.626111778264,
    "output_throughput": 6703.066270628793,
    "total_throughput": 14362.692382407056,
    "itl": 88.84549774193232,
    "ttft": 1682007.829064465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1147309641912588,
    "arrivals": 375251,
    "finished_requests": 111981,
    "scheduler_time": 234.81631932345798
}
#Debug simulation 
Total elapsed time: 81.18259674997535. Arrivals time: 0.45204886281862855 Scheduler time: 80.52364817354828 Scheduler overhead time: 0.08137000678107142 Adapter cache time: 0.013926642946898937 Engine time: 0.07959603751078248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.46356668195222,
    "estimated_duration": 3600.0932654252274,
    "input_throughput": 7581.601082986414,
    "output_throughput": 6634.775334682534,
    "total_throughput": 14216.376417668947,
    "itl": 86.53651217737047,
    "ttft": 1690156.9587788421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2211747604096315,
    "arrivals": 375251,
    "finished_requests": 110832,
    "scheduler_time": 237.31897160711367
}
#Debug simulation 
Total elapsed time: 78.46372880099807. Arrivals time: 0.45183947950135916 Scheduler time: 77.8030958888121 Scheduler overhead time: 0.08210686384700239 Adapter cache time: 0.013793198857456446 Engine time: 0.08036247862037271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 81.27476779895369,
    "estimated_duration": 3600.087701036183,
    "input_throughput": 7667.2804365447255,
    "output_throughput": 6707.181881444203,
    "total_throughput": 14374.462317988928,
    "itl": 88.82760971997537,
    "ttft": 1682322.4033664537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.055868499386124,
    "arrivals": 375251,
    "finished_requests": 112073,
    "scheduler_time": 234.7405995762851
}
#Debug simulation 
Total elapsed time: 81.2749372490216. Arrivals time: 0.4725602479884401 Scheduler time: 80.59561543725431 Scheduler overhead time: 0.08070882980246097 Adapter cache time: 0.013998645707033575 Engine time: 0.0798557250527665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 79.94360296800733,
    "estimated_duration": 3600.0428679570196,
    "input_throughput": 7584.768293466332,
    "output_throughput": 6639.691769438501,
    "total_throughput": 14224.460062904833,
    "itl": 86.52718307467484,
    "ttft": 1686042.3822485064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3071104305330679,
    "arrivals": 375251,
    "finished_requests": 110873,
    "scheduler_time": 237.55482680078845
}
#Debug simulation 
Total elapsed time: 79.94376819906756. Arrivals time: 0.4487205856712535 Scheduler time: 79.28211114101578 Scheduler overhead time: 0.08366349956486374 Adapter cache time: 0.014301156159490347 Engine time: 0.0817046653246507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 78.60379126504995,
    "estimated_duration": 3600.0987874088632,
    "input_throughput": 7648.4253977425205,
    "output_throughput": 6702.004701755924,
    "total_throughput": 14350.430099498444,
    "itl": 88.90169274554036,
    "ttft": 1683237.6473734935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1044183227186997,
    "arrivals": 375251,
    "finished_requests": 111918,
    "scheduler_time": 234.85873213182595
}
#Debug simulation 
Total elapsed time: 78.60395027406048. Arrivals time: 0.4567283005453646 Scheduler time: 77.94115749397315 Scheduler overhead time: 0.08092538674827665 Adapter cache time: 0.01387066429015249 Engine time: 0.07910041580908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.70529406296555,
    "estimated_duration": 3600.028250871027,
    "input_throughput": 7575.814715732095,
    "output_throughput": 6633.86488542742,
    "total_throughput": 14209.679601159514,
    "itl": 86.50892308183863,
    "ttft": 1688272.4882562547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.193319176044316,
    "arrivals": 375251,
    "finished_requests": 110730,
    "scheduler_time": 237.38948756609486
}
#Debug simulation 
Total elapsed time: 79.70559859299101. Arrivals time: 0.45247534522786736 Scheduler time: 79.04299680236727 Scheduler overhead time: 0.08234372560400516 Adapter cache time: 0.014113305718638003 Engine time: 0.08121499430853873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.79311021603644,
    "estimated_duration": 3600.0422702143296,
    "input_throughput": 7719.628802676657,
    "output_throughput": 6776.305156702407,
    "total_throughput": 14495.933959379063,
    "itl": 89.96787684605073,
    "ttft": 1669587.1854963044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8926752193132406,
    "arrivals": 375015,
    "finished_requests": 113065,
    "scheduler_time": 234.23198577078625
}
#Debug simulation 
Total elapsed time: 83.79327740997542. Arrivals time: 0.4612275002291426 Scheduler time: 83.12482276943047 Scheduler overhead time: 0.08159297972451895 Adapter cache time: 0.013578009558841586 Engine time: 0.08001794712617993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 83.40885812893976,
    "estimated_duration": 3600.011314439986,
    "input_throughput": 7664.9425765186725,
    "output_throughput": 6733.411337561533,
    "total_throughput": 14398.353914080206,
    "itl": 88.76362163211301,
    "ttft": 1673421.5032552562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9776088516879831,
    "arrivals": 375015,
    "finished_requests": 112281,
    "scheduler_time": 235.62175008050093
}
#Debug simulation 
Total elapsed time: 83.40902354801074. Arrivals time: 0.47363287361804396 Scheduler time: 82.72574052354321 Scheduler overhead time: 0.08258750638924539 Adapter cache time: 0.01372289506252855 Engine time: 0.08060241281054914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.87843013403472,
    "estimated_duration": 3600.023429437211,
    "input_throughput": 7593.848355668545,
    "output_throughput": 6668.042714308863,
    "total_throughput": 14261.891069977408,
    "itl": 86.49779258665191,
    "ttft": 1678162.5682107843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9568411040725189,
    "arrivals": 375015,
    "finished_requests": 111194,
    "scheduler_time": 238.07462025763948
}
#Debug simulation 
Total elapsed time: 82.87859376205597. Arrivals time: 0.47166097711306065 Scheduler time: 82.19518815819174 Scheduler overhead time: 0.08360937796533108 Adapter cache time: 0.013898104196414351 Engine time: 0.08143556246068329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 86.10047389601823,
    "estimated_duration": 3600.0112113314663,
    "input_throughput": 7680.551914107402,
    "output_throughput": 6734.490971497073,
    "total_throughput": 14415.042885604475,
    "itl": 88.79308189706859,
    "ttft": 1669290.7850362607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8832193031860509,
    "arrivals": 375015,
    "finished_requests": 112355,
    "scheduler_time": 235.4595705538208
}
#Debug simulation 
Total elapsed time: 86.10063915501814. Arrivals time: 0.4669341907138005 Scheduler time: 85.42359367699828 Scheduler overhead time: 0.08240523771382868 Adapter cache time: 0.014088953146710992 Engine time: 0.08102576492819935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 83.21882934996393,
    "estimated_duration": 3600.0813933956815,
    "input_throughput": 7594.043304174589,
    "output_throughput": 6668.276457315499,
    "total_throughput": 14262.319761490087,
    "itl": 86.49241677935933,
    "ttft": 1678167.2208829594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9485564170079308,
    "arrivals": 375015,
    "finished_requests": 111199,
    "scheduler_time": 238.08888539523582
}
#Debug simulation 
Total elapsed time: 83.21899327693973. Arrivals time: 0.46991318615619093 Scheduler time: 82.53531847719569 Scheduler overhead time: 0.08429849648382515 Adapter cache time: 0.014056544983759522 Engine time: 0.08204000920522958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.48573628300801,
    "estimated_duration": 3600.0780737617724,
    "input_throughput": 7678.1840375801685,
    "output_throughput": 6736.8416748411055,
    "total_throughput": 14415.025712421273,
    "itl": 88.77387462089409,
    "ttft": 1675151.412877096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8682132479175911,
    "arrivals": 375015,
    "finished_requests": 112399,
    "scheduler_time": 235.57839498743135
}
#Debug simulation 
Total elapsed time: 82.48590828303713. Arrivals time: 0.46966237341985106 Scheduler time: 81.80777187156491 Scheduler overhead time: 0.08211142185609788 Adapter cache time: 0.01380372908897698 Engine time: 0.08019536454230547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.9045312240487,
    "estimated_duration": 3600.0340978326003,
    "input_throughput": 7594.109460368575,
    "output_throughput": 6668.092675692273,
    "total_throughput": 14262.202136060849,
    "itl": 86.4960696361863,
    "ttft": 1678172.0686365834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9396503784134985,
    "arrivals": 375015,
    "finished_requests": 111197,
    "scheduler_time": 238.07937877334544
}
#Debug simulation 
Total elapsed time: 82.90470001404174. Arrivals time: 0.45863990392535925 Scheduler time: 82.23389900755137 Scheduler overhead time: 0.08293418039102107 Adapter cache time: 0.013999058865010738 Engine time: 0.08203082042746246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 87.10815216891933,
    "estimated_duration": 3600.0103167942702,
    "input_throughput": 7741.183648833579,
    "output_throughput": 6757.912577779511,
    "total_throughput": 14499.09622661309,
    "itl": 89.8673687807288,
    "ttft": 1625101.2586211753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7207525844825424,
    "arrivals": 344340,
    "finished_requests": 112857,
    "scheduler_time": 232.60887007388388
}
#Debug simulation 
Total elapsed time: 87.1083246609196. Arrivals time: 0.48339861806016415 Scheduler time: 86.41635722410865 Scheduler overhead time: 0.08185760874766856 Adapter cache time: 0.013589388807304204 Engine time: 0.080877072410658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 95.24519133707508,
    "estimated_duration": 3600.0850428377576,
    "input_throughput": 7575.133274769422,
    "output_throughput": 6595.606691913941,
    "total_throughput": 14170.739966683364,
    "itl": 85.93240628795242,
    "ttft": 1647353.100332299,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7053100315807388,
    "arrivals": 344340,
    "finished_requests": 110487,
    "scheduler_time": 238.106037174154
}
#Debug simulation 
Total elapsed time: 95.24537216906901. Arrivals time: 0.4682858888991177 Scheduler time: 94.55825475999154 Scheduler overhead time: 0.0866445095743984 Adapter cache time: 0.014452529605478048 Engine time: 0.08410727721638978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 83.50504240498412,
    "estimated_duration": 3600.0060757270016,
    "input_throughput": 7625.599074706055,
    "output_throughput": 6658.050152084698,
    "total_throughput": 14283.649226790752,
    "itl": 86.08935702460765,
    "ttft": 1647988.9929870267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8459528418350967,
    "arrivals": 344340,
    "finished_requests": 111348,
    "scheduler_time": 236.92653673250405
}
#Debug simulation 
Total elapsed time: 83.50520836899523. Arrivals time: 0.4761126630473882 Scheduler time: 82.81579945597332 Scheduler overhead time: 0.08428479940630496 Adapter cache time: 0.01410284114535898 Engine time: 0.08162339206319302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 92.36080700298771,
    "estimated_duration": 3600.0490614011246,
    "input_throughput": 7672.135720631091,
    "output_throughput": 6688.282462080915,
    "total_throughput": 14360.418182712006,
    "itl": 88.29254524037125,
    "ttft": 1640818.112562215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6828151652030642,
    "arrivals": 344340,
    "finished_requests": 111950,
    "scheduler_time": 234.82063235104943
}
#Debug simulation 
Total elapsed time: 92.36097990395501. Arrivals time: 0.4769688087981194 Scheduler time: 91.66698515205644 Scheduler overhead time: 0.08589512528851628 Adapter cache time: 0.01401563489343971 Engine time: 0.08346997061744332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 86.06516801391263,
    "estimated_duration": 3600.0875151215446,
    "input_throughput": 7623.022463961814,
    "output_throughput": 6634.808431648245,
    "total_throughput": 14257.830895610057,
    "itl": 86.13849783378242,
    "ttft": 1649366.5984191042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7267280235514045,
    "arrivals": 344340,
    "finished_requests": 111153,
    "scheduler_time": 236.7044946981523
}
#Debug simulation 
Total elapsed time: 86.06534458894748. Arrivals time: 0.47125541465356946 Scheduler time: 85.37880036444403 Scheduler overhead time: 0.08477113442495465 Adapter cache time: 0.014082789653912187 Engine time: 0.08311327861156315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 84.41703841800336,
    "estimated_duration": 3600.029410189666,
    "input_throughput": 7721.853305230477,
    "output_throughput": 6727.49031756494,
    "total_throughput": 14449.343622795417,
    "itl": 88.75833964467937,
    "ttft": 1639011.4881343145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6958473825221877,
    "arrivals": 344340,
    "finished_requests": 112591,
    "scheduler_time": 233.4860538600245
}
#Debug simulation 
Total elapsed time: 84.41733491397463. Arrivals time: 0.4743519254261628 Scheduler time: 83.73180461884476 Scheduler overhead time: 0.083062325255014 Adapter cache time: 0.013895465759560466 Engine time: 0.08129874104633927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.76025630405638,
    "estimated_duration": 3600.036813406865,
    "input_throughput": 7629.339205008815,
    "output_throughput": 6641.130699264868,
    "total_throughput": 14270.469904273683,
    "itl": 86.32186122279514,
    "ttft": 1647152.9871501708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7358381862938406,
    "arrivals": 344340,
    "finished_requests": 111252,
    "scheduler_time": 236.37862179755663
}
#Debug simulation 
Total elapsed time: 82.76043004996609. Arrivals time: 0.4629181114723906 Scheduler time: 82.08352293760981 Scheduler overhead time: 0.08415481995325536 Adapter cache time: 0.01401705527678132 Engine time: 0.08233929704874754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 88.15317409997806,
    "estimated_duration": 3600.0289834904465,
    "input_throughput": 7735.104391576601,
    "output_throughput": 6711.290412049571,
    "total_throughput": 14446.394803626174,
    "itl": 89.20846769959378,
    "ttft": 1592948.3142421965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6546284941630431,
    "arrivals": 321710,
    "finished_requests": 112303,
    "scheduler_time": 233.02167480181788
}
#Debug simulation 
Total elapsed time: 88.15333887492307. Arrivals time: 0.47304843447636813 Scheduler time: 87.47071252798196 Scheduler overhead time: 0.08260603423696011 Adapter cache time: 0.013848709990270436 Engine time: 0.08067737228702754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 92.40588412806392,
    "estimated_duration": 3600.0756698122746,
    "input_throughput": 7737.295144535806,
    "output_throughput": 6718.895717339549,
    "total_throughput": 14456.190861875355,
    "itl": 88.72161341871015,
    "ttft": 1588279.0925235187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.744170741150156,
    "arrivals": 321710,
    "finished_requests": 112361,
    "scheduler_time": 232.6501830067291
}
#Debug simulation 
Total elapsed time: 92.40604982897639. Arrivals time: 0.47115091839805245 Scheduler time: 91.71923455118667 Scheduler overhead time: 0.08569348021410406 Adapter cache time: 0.01424161868635565 Engine time: 0.08214262395631522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.85092084296048,
    "estimated_duration": 3600.021440479664,
    "input_throughput": 7459.5647398205265,
    "output_throughput": 6480.712236228081,
    "total_throughput": 13940.276976048608,
    "itl": 81.57516464900165,
    "ttft": 1613670.635816909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9012445076555015,
    "arrivals": 321710,
    "finished_requests": 108347,
    "scheduler_time": 241.67501304298509
}
#Debug simulation 
Total elapsed time: 82.85107327601872. Arrivals time: 0.4589250677963719 Scheduler time: 82.17097196134273 Scheduler overhead time: 0.08706479740794748 Adapter cache time: 0.014627897529862821 Engine time: 0.08468768699094653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 86.9079645529855,
    "estimated_duration": 3600.090115116468,
    "input_throughput": 7731.893955409913,
    "output_throughput": 6709.682598936426,
    "total_throughput": 14441.57655434634,
    "itl": 88.65436963071934,
    "ttft": 1591602.0159993437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 115,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7882915261248125,
    "arrivals": 321710,
    "finished_requests": 112186,
    "scheduler_time": 233.1953286795741
}
#Debug simulation 
Total elapsed time: 86.90811648603994. Arrivals time: 0.45174692338332534 Scheduler time: 86.24774451297708 Scheduler overhead time: 0.08199433307163417 Adapter cache time: 0.013827841728925705 Engine time: 0.08058702736161649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 86.6959544250276,
    "estimated_duration": 3600.094969872307,
    "input_throughput": 7629.496785462366,
    "output_throughput": 6619.0059427363385,
    "total_throughput": 14248.502728198704,
    "itl": 85.90242629270527,
    "ttft": 1604554.2208305527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7701117688557134,
    "arrivals": 321710,
    "finished_requests": 110751,
    "scheduler_time": 236.35960221978632
}
#Debug simulation 
Total elapsed time: 86.69611955503933. Arrivals time: 0.4496477213688195 Scheduler time: 86.03440672508441 Scheduler overhead time: 0.083567681373097 Adapter cache time: 0.014188135974109173 Engine time: 0.08137239678762853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 90.05068564496469,
    "estimated_duration": 3600.0104915217744,
    "input_throughput": 7718.559172380104,
    "output_throughput": 6705.159347965195,
    "total_throughput": 14423.7185203453,
    "itl": 88.39574058415194,
    "ttft": 1587185.3347357356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6894634615816172,
    "arrivals": 321710,
    "finished_requests": 112087,
    "scheduler_time": 233.1797308458276
}
#Debug simulation 
Total elapsed time: 90.05084125406574. Arrivals time: 0.46597433218266815 Scheduler time: 89.37157278414816 Scheduler overhead time: 0.08438132144510746 Adapter cache time: 0.01411405997350812 Engine time: 0.08214438008144498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 83.42501305695623,
    "estimated_duration": 3600.0265559402383,
    "input_throughput": 7632.557030631008,
    "output_throughput": 6639.8051871472935,
    "total_throughput": 14272.362217778302,
    "itl": 86.18714746334182,
    "ttft": 1604648.705192249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7594450548850001,
    "arrivals": 321710,
    "finished_requests": 110857,
    "scheduler_time": 235.77178718551102
}
#Debug simulation 
Total elapsed time: 83.42516625800636. Arrivals time: 0.4073620430426672 Scheduler time: 82.81742741784547 Scheduler overhead time: 0.07893064094241709 Adapter cache time: 0.013260925887152553 Engine time: 0.0763087848899886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 86.14970954600722,
    "estimated_duration": 3600.038994452836,
    "input_throughput": 7683.75816001523,
    "output_throughput": 6731.742083166898,
    "total_throughput": 14415.500243182127,
    "itl": 89.74207323731888,
    "ttft": 1595637.8404249647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6480160851310932,
    "arrivals": 317887,
    "finished_requests": 112186,
    "scheduler_time": 232.29025027044617
}
#Debug simulation 
Total elapsed time: 86.14986872603185. Arrivals time: 0.4123509341152385 Scheduler time: 85.54076553613413 Scheduler overhead time: 0.07745199394412339 Adapter cache time: 0.013029369292780757 Engine time: 0.07515105523634702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 91.27191417000722,
    "estimated_duration": 3600.0774775544933,
    "input_throughput": 7440.798195875641,
    "output_throughput": 6523.867929629715,
    "total_throughput": 13964.666125505355,
    "itl": 84.208839549354,
    "ttft": 1611035.0081274142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6606225849967451,
    "arrivals": 317887,
    "finished_requests": 108662,
    "scheduler_time": 240.0867911921187
}
#Debug simulation 
Total elapsed time: 91.27206539607141. Arrivals time: 0.4127425071783364 Scheduler time: 90.6477046571672 Scheduler overhead time: 0.08327099785674363 Adapter cache time: 0.013859882717952132 Engine time: 0.08179320232011378 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.05577926896513,
    "estimated_duration": 3600.0652747157137,
    "input_throughput": 7585.075801759271,
    "output_throughput": 6636.500778971756,
    "total_throughput": 14221.576580731025,
    "itl": 86.5010730049416,
    "ttft": 1614591.2025201137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7737361400714146,
    "arrivals": 317887,
    "finished_requests": 110626,
    "scheduler_time": 235.60635824490265
}
#Debug simulation 
Total elapsed time: 80.05593659298029. Arrivals time: 0.4154925914481282 Scheduler time: 79.44009504967835 Scheduler overhead time: 0.07861063885502517 Adapter cache time: 0.013131056213751435 Engine time: 0.07703270833007991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 80.87115080107469,
    "estimated_duration": 3600.092968659026,
    "input_throughput": 7679.328906413708,
    "output_throughput": 6715.499630279079,
    "total_throughput": 14394.828536692787,
    "itl": 88.80584240904915,
    "ttft": 1606960.5792144511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6955830070842056,
    "arrivals": 317887,
    "finished_requests": 112016,
    "scheduler_time": 232.9833660706465
}
#Debug simulation 
Total elapsed time: 80.87130858900491. Arrivals time: 0.4176670798333362 Scheduler time: 80.25219502288383 Scheduler overhead time: 0.07957159262150526 Adapter cache time: 0.01323716330807656 Engine time: 0.07723070110660046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 83.78132202499546,
    "estimated_duration": 3600.0514153802173,
    "input_throughput": 7521.083694617277,
    "output_throughput": 6585.6664987369395,
    "total_throughput": 14106.750193354215,
    "itl": 85.3792621371437,
    "ttft": 1617578.8861845364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7044694512477145,
    "arrivals": 317887,
    "finished_requests": 109716,
    "scheduler_time": 237.55553490794816
}
#Debug simulation 
Total elapsed time: 83.78154965594877. Arrivals time: 0.40701962157618254 Scheduler time: 83.16964328382164 Scheduler overhead time: 0.08149143820628524 Adapter cache time: 0.013666628510691226 Engine time: 0.07713978923857212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.90891466697212,
    "estimated_duration": 3600.0793382116713,
    "input_throughput": 7600.219725599629,
    "output_throughput": 6657.647720593309,
    "total_throughput": 14257.867446192937,
    "itl": 88.08799314895374,
    "ttft": 1605531.3363175492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6256242521759122,
    "arrivals": 317887,
    "finished_requests": 110898,
    "scheduler_time": 234.99814707566284
}
#Debug simulation 
Total elapsed time: 83.90905880997889. Arrivals time: 0.4043984168674797 Scheduler time: 83.30756866000593 Scheduler overhead time: 0.0783607525518164 Adapter cache time: 0.012972772936336696 Engine time: 0.07432841835543513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.69866417895537,
    "estimated_duration": 3600.042403899915,
    "input_throughput": 7582.083469469837,
    "output_throughput": 6635.258233103882,
    "total_throughput": 14217.341702573718,
    "itl": 86.41517393861777,
    "ttft": 1613916.4253984208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8905744419060648,
    "arrivals": 317887,
    "finished_requests": 110587,
    "scheduler_time": 235.42372484019765
}
#Debug simulation 
Total elapsed time: 82.69881729793269. Arrivals time: 0.40607088792603463 Scheduler time: 82.09027103881817 Scheduler overhead time: 0.080652667558752 Adapter cache time: 0.01363263395614922 Engine time: 0.07636977336369455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 79.92644139402546,
    "estimated_duration": 3600.013941012903,
    "input_throughput": 7722.982592722229,
    "output_throughput": 6727.478392260257,
    "total_throughput": 14450.460984982486,
    "itl": 89.88172734747164,
    "ttft": 1589619.0992886347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8397759470576411,
    "arrivals": 315978,
    "finished_requests": 112455,
    "scheduler_time": 232.05109796684337
}
#Debug simulation 
Total elapsed time: 79.92659094103146. Arrivals time: 0.3944662919966504 Scheduler time: 79.33570388820954 Scheduler overhead time: 0.07863360154442489 Adapter cache time: 0.013408684288151562 Engine time: 0.07333413057494909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.16068817698397,
    "estimated_duration": 3600.028921825428,
    "input_throughput": 7662.129832560716,
    "output_throughput": 6676.856081426113,
    "total_throughput": 14338.985913986828,
    "itl": 88.64995816489727,
    "ttft": 1592586.1646244898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9173771212762227,
    "arrivals": 315978,
    "finished_requests": 111519,
    "scheduler_time": 233.9156876560223
}
#Debug simulation 
Total elapsed time: 80.16083625191823. Arrivals time: 0.391302541247569 Scheduler time: 79.57367901527323 Scheduler overhead time: 0.07806646882090718 Adapter cache time: 0.012867351179011166 Engine time: 0.07371116545982659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.50149210495874,
    "estimated_duration": 3600.0460015731705,
    "input_throughput": 7624.662292649901,
    "output_throughput": 6645.291473927228,
    "total_throughput": 14269.95376657713,
    "itl": 86.51217032893982,
    "ttft": 1605760.449698464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.132805911898614,
    "arrivals": 315978,
    "finished_requests": 110965,
    "scheduler_time": 235.06744577776948
}
#Debug simulation 
Total elapsed time: 78.50162808399182. Arrivals time: 0.3899156139232218 Scheduler time: 77.91310452553444 Scheduler overhead time: 0.07892871578224003 Adapter cache time: 0.013577562174759805 Engine time: 0.07464802998583764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 79.29482951003592,
    "estimated_duration": 3600.0161560063398,
    "input_throughput": 7662.408668354716,
    "output_throughput": 6676.9230909967255,
    "total_throughput": 14339.33175935144,
    "itl": 88.64865717897298,
    "ttft": 1592576.788191789,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8668439823109653,
    "arrivals": 315978,
    "finished_requests": 111522,
    "scheduler_time": 233.91893382708517
}
#Debug simulation 
Total elapsed time: 79.29495884303469. Arrivals time: 0.3935850744601339 Scheduler time: 78.70395552692935 Scheduler overhead time: 0.07778526237234473 Adapter cache time: 0.013048120774328709 Engine time: 0.07535650907084346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 78.08425672305748,
    "estimated_duration": 3600.090294574862,
    "input_throughput": 7625.7723428134195,
    "output_throughput": 6646.536626055846,
    "total_throughput": 14272.308968869265,
    "itl": 86.50203829786584,
    "ttft": 1605989.3595077405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.140518123386429,
    "arrivals": 315978,
    "finished_requests": 110982,
    "scheduler_time": 235.08674957015225
}
#Debug simulation 
Total elapsed time: 78.0843813699903. Arrivals time: 0.4047589342808351 Scheduler time: 77.47893346636556 Scheduler overhead time: 0.07972010842058808 Adapter cache time: 0.013808398973196745 Engine time: 0.0754325354937464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 80.25829309597611,
    "estimated_duration": 3600.004639206763,
    "input_throughput": 7662.510958896483,
    "output_throughput": 6676.916673445281,
    "total_throughput": 14339.427632341763,
    "itl": 88.64659080851901,
    "ttft": 1592695.416099872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8043740385118862,
    "arrivals": 315978,
    "finished_requests": 111523,
    "scheduler_time": 233.92276425479693
}
#Debug simulation 
Total elapsed time: 80.25842120393645. Arrivals time: 0.4021399321500212 Scheduler time: 79.65832620405126 Scheduler overhead time: 0.07824061112478375 Adapter cache time: 0.013154443120583892 Engine time: 0.07479349116329104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.38271348609123,
    "estimated_duration": 3600.047194457867,
    "input_throughput": 7624.68921025731,
    "output_throughput": 6645.335104725663,
    "total_throughput": 14270.024314982973,
    "itl": 86.51313251353861,
    "ttft": 1605669.7251557189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1270036379247916,
    "arrivals": 315978,
    "finished_requests": 110968,
    "scheduler_time": 235.06437678231563
}
#Debug simulation 
Total elapsed time: 78.38283820904326. Arrivals time: 0.3976030523190275 Scheduler time: 77.78702476539183 Scheduler overhead time: 0.07785542716737837 Adapter cache time: 0.013383146142587066 Engine time: 0.07506188610568643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 92.45509144396055,
    "estimated_duration": 3600.0189763555195,
    "input_throughput": 7760.511037162096,
    "output_throughput": 6759.6532573379445,
    "total_throughput": 14520.16429450004,
    "itl": 89.94297270194026,
    "ttft": 1574813.6661757873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7670394477061919,
    "arrivals": 314998,
    "finished_requests": 112912,
    "scheduler_time": 230.77160578614493
}
#Debug simulation 
Total elapsed time: 92.45521801500581. Arrivals time: 0.4195835051359609 Scheduler time: 91.83456271851901 Scheduler overhead time: 0.08086823986377567 Adapter cache time: 0.013730780803598464 Engine time: 0.07524276454932988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 90.90417868201621,
    "estimated_duration": 3600.0608795447633,
    "input_throughput": 7722.408295360695,
    "output_throughput": 6725.11099397339,
    "total_throughput": 14447.519289334085,
    "itl": 88.4761412909154,
    "ttft": 1584610.3644438065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8182846812950455,
    "arrivals": 314998,
    "finished_requests": 112326,
    "scheduler_time": 232.1448295512561
}
#Debug simulation 
Total elapsed time: 90.90431921300478. Arrivals time: 0.41278010059613734 Scheduler time: 90.28886579698883 Scheduler overhead time: 0.080901546520181 Adapter cache time: 0.013680775766260922 Engine time: 0.07644037844147533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.37577610800508,
    "estimated_duration": 3600.010248291602,
    "input_throughput": 7641.7099126468,
    "output_throughput": 6642.404146307048,
    "total_throughput": 14284.114058953848,
    "itl": 86.48837305470303,
    "ttft": 1605629.8621465887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4013300702208684,
    "arrivals": 314998,
    "finished_requests": 110987,
    "scheduler_time": 234.93668731054623
}
#Debug simulation 
Total elapsed time: 74.37591107294429. Arrivals time: 0.39418629894498736 Scheduler time: 73.78144582058303 Scheduler overhead time: 0.07895520410966128 Adapter cache time: 0.014051209785975516 Engine time: 0.07529054640326649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 87.53036620700732,
    "estimated_duration": 3600.0869751871032,
    "input_throughput": 7718.798237799738,
    "output_throughput": 6727.295247843645,
    "total_throughput": 14446.093485643381,
    "itl": 88.67434210390496,
    "ttft": 1589216.697476047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9767588592739769,
    "arrivals": 314998,
    "finished_requests": 112328,
    "scheduler_time": 231.91642798911556
}
#Debug simulation 
Total elapsed time: 87.53055762697477. Arrivals time: 0.41377763089258224 Scheduler time: 86.91306613455527 Scheduler overhead time: 0.08132271619979292 Adapter cache time: 0.014172072522342205 Engine time: 0.07672406209167093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 80.24353854090441,
    "estimated_duration": 3600.0777624647603,
    "input_throughput": 7625.442229671481,
    "output_throughput": 6642.242911894343,
    "total_throughput": 14267.685141565824,
    "itl": 86.30509715629414,
    "ttft": 1600068.5937275053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0614124717982492,
    "arrivals": 314998,
    "finished_requests": 110875,
    "scheduler_time": 234.99772961143844
}
#Debug simulation 
Total elapsed time: 80.24368128995411. Arrivals time: 0.40292670018970966 Scheduler time: 79.63654554542154 Scheduler overhead time: 0.08189405279699713 Adapter cache time: 0.013885697815567255 Engine time: 0.07639250752981752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 88.95019141805824,
    "estimated_duration": 3600.0478780037556,
    "input_throughput": 7691.032991303766,
    "output_throughput": 6713.775988280006,
    "total_throughput": 14404.808979583771,
    "itl": 88.39706373507063,
    "ttft": 1579173.1858917526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8299097222741681,
    "arrivals": 314998,
    "finished_requests": 111928,
    "scheduler_time": 233.41058695249166
}
#Debug simulation 
Total elapsed time: 88.95032081298996. Arrivals time: 0.41122315789107233 Scheduler time: 88.33403209131211 Scheduler overhead time: 0.08214838663116097 Adapter cache time: 0.014057166525162756 Engine time: 0.07674844795837998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.09930567094125,
    "estimated_duration": 3600.0185524211165,
    "input_throughput": 7654.986661517726,
    "output_throughput": 6659.492902856457,
    "total_throughput": 14314.479564374184,
    "itl": 86.38084797995464,
    "ttft": 1612056.7239101185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1382364826649443,
    "arrivals": 314998,
    "finished_requests": 111288,
    "scheduler_time": 234.63281054385897
}
#Debug simulation 
Total elapsed time: 74.09943595994264. Arrivals time: 0.39554932503961027 Scheduler time: 73.50535961450078 Scheduler overhead time: 0.0783651958918199 Adapter cache time: 0.01386973774060607 Engine time: 0.07468683447223157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 87.6494431559695,
    "estimated_duration": 3600.0429759534554,
    "input_throughput": 7817.576953382421,
    "output_throughput": 6751.313571072835,
    "total_throughput": 14568.890524455257,
    "itl": 89.83925284898405,
    "ttft": 1572256.7714993232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0579854451119888,
    "arrivals": 314533,
    "finished_requests": 113225,
    "scheduler_time": 231.18934239747287
}
#Debug simulation 
Total elapsed time: 87.6495682670502. Arrivals time: 0.41637845581863075 Scheduler time: 87.03152310918085 Scheduler overhead time: 0.08007586724124849 Adapter cache time: 0.014043681556358933 Engine time: 0.07585537806153297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 86.58985615591519,
    "estimated_duration": 3600.087738611424,
    "input_throughput": 7804.060356272465,
    "output_throughput": 6735.584452548057,
    "total_throughput": 14539.644808820522,
    "itl": 88.73584892896585,
    "ttft": 1577922.1640782622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0814231385150936,
    "arrivals": 314533,
    "finished_requests": 112988,
    "scheduler_time": 232.0648041112919
}
#Debug simulation 
Total elapsed time: 86.59000176691916. Arrivals time: 0.4188894310500473 Scheduler time: 85.96868445177097 Scheduler overhead time: 0.08104766684118658 Adapter cache time: 0.013869019225239754 Engine time: 0.07561313838232309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 77.68537707999349,
    "estimated_duration": 3600.00223258419,
    "input_throughput": 7683.313568432111,
    "output_throughput": 6643.453935536048,
    "total_throughput": 14326.76750396816,
    "itl": 86.41286807381118,
    "ttft": 1597845.0209158156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.203781765867026,
    "arrivals": 314533,
    "finished_requests": 111385,
    "scheduler_time": 234.77699701783902
}
#Debug simulation 
Total elapsed time: 77.6855189029593. Arrivals time: 0.4093833238584921 Scheduler time: 77.07430833263788 Scheduler overhead time: 0.08019573905039579 Adapter cache time: 0.013846856076270342 Engine time: 0.076017314218916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 86.59157613897696,
    "estimated_duration": 3600.051386085492,
    "input_throughput": 7801.183924359979,
    "output_throughput": 6734.185543490539,
    "total_throughput": 14535.369467850518,
    "itl": 88.74986375343909,
    "ttft": 1578058.7641461038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0217296366626396,
    "arrivals": 314533,
    "finished_requests": 112952,
    "scheduler_time": 231.98588276263575
}
#Debug simulation 
Total elapsed time: 86.59171076701023. Arrivals time: 0.4415550831472501 Scheduler time: 85.94901438709348 Scheduler overhead time: 0.08053067058790475 Adapter cache time: 0.01366325409617275 Engine time: 0.07553754840046167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 79.72601519396994,
    "estimated_duration": 3600.0511690167177,
    "input_throughput": 7684.348833173914,
    "output_throughput": 6646.269976916789,
    "total_throughput": 14330.618810090704,
    "itl": 86.45078485705983,
    "ttft": 1595099.2260063074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1826072966493681,
    "arrivals": 314533,
    "finished_requests": 111459,
    "scheduler_time": 234.73176488901578
}
#Debug simulation 
Total elapsed time: 79.72615394496825. Arrivals time: 0.41932366078253835 Scheduler time: 79.1025520360563 Scheduler overhead time: 0.08160016324836761 Adapter cache time: 0.013644306338392198 Engine time: 0.07676836696919054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 86.76988436700776,
    "estimated_duration": 3600.086541685241,
    "input_throughput": 7780.349909835288,
    "output_throughput": 6717.928783089384,
    "total_throughput": 14498.278692924672,
    "itl": 88.62367224322993,
    "ttft": 1575565.95356619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9958916667290011,
    "arrivals": 314533,
    "finished_requests": 112683,
    "scheduler_time": 232.50420628710788
}
#Debug simulation 
Total elapsed time: 86.77001221396495. Arrivals time: 0.43721899506635964 Scheduler time: 86.12798909121193 Scheduler overhead time: 0.08219978353008628 Adapter cache time: 0.013919893302954733 Engine time: 0.07610522059258074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 85.29496432794258,
    "estimated_duration": 3600.0337201961793,
    "input_throughput": 7713.798024782588,
    "output_throughput": 6682.310464216726,
    "total_throughput": 14396.108488999314,
    "itl": 86.42032076017702,
    "ttft": 1573915.6837725667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1097640363126995,
    "arrivals": 314533,
    "finished_requests": 111681,
    "scheduler_time": 234.99867659430316
}
#Debug simulation 
Total elapsed time: 85.29508215200622. Arrivals time: 0.4253157560015097 Scheduler time: 84.66527652204968 Scheduler overhead time: 0.08184457430616021 Adapter cache time: 0.013913314905948937 Engine time: 0.07679386425297707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 85.72406402998604,
    "estimated_duration": 3600.08660012297,
    "input_throughput": 7810.34433978326,
    "output_throughput": 6751.615085917587,
    "total_throughput": 14561.959425700847,
    "itl": 89.86043584908649,
    "ttft": 1577554.4937376687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7868766748020417,
    "arrivals": 314275,
    "finished_requests": 113193,
    "scheduler_time": 230.9846019840387
}
#Debug simulation 
Total elapsed time: 85.72418763395399. Arrivals time: 0.4321390764089301 Scheduler time: 85.08831992722116 Scheduler overhead time: 0.08152976678684354 Adapter cache time: 0.013789737597107887 Engine time: 0.07647874939721078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 84.87635197106283,
    "estimated_duration": 3600.014855918859,
    "input_throughput": 7799.495314258464,
    "output_throughput": 6745.9221619799255,
    "total_throughput": 14545.417476238388,
    "itl": 88.69416161461837,
    "ttft": 1579882.9597299201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9484656889317564,
    "arrivals": 314275,
    "finished_requests": 113037,
    "scheduler_time": 232.18820448392972
}
#Debug simulation 
Total elapsed time: 84.87647720507812. Arrivals time: 0.42454870080109686 Scheduler time: 84.24860230705235 Scheduler overhead time: 0.0815041275927797 Adapter cache time: 0.013887147186324 Engine time: 0.07621620828285813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.43794292304665,
    "estimated_duration": 3600.064058545913,
    "input_throughput": 7712.531374015241,
    "output_throughput": 6668.807446081119,
    "total_throughput": 14381.338820096362,
    "itl": 86.36043620590569,
    "ttft": 1594957.3292613374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9428657223191114,
    "arrivals": 314275,
    "finished_requests": 111808,
    "scheduler_time": 234.94156582968625
}
#Debug simulation 
Total elapsed time: 79.43812917603645. Arrivals time: 0.4172272109426558 Scheduler time: 78.81772508053109 Scheduler overhead time: 0.08117090736050159 Adapter cache time: 0.013723864336498082 Engine time: 0.07627053477335721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 81.67977868102025,
    "estimated_duration": 3600.044132799863,
    "input_throughput": 7806.293468447969,
    "output_throughput": 6749.142539289741,
    "total_throughput": 14555.43600773771,
    "itl": 88.69472884765895,
    "ttft": 1583222.41613871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8823882661387321,
    "arrivals": 314275,
    "finished_requests": 113114,
    "scheduler_time": 232.19120639339047
}
#Debug simulation 
Total elapsed time: 81.67991824494675. Arrivals time: 0.41849039238877594 Scheduler time: 81.06100495369174 Scheduler overhead time: 0.08001605595927685 Adapter cache time: 0.013388092629611492 Engine time: 0.07598711969330907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 79.35591007699259,
    "estimated_duration": 3600.094480341872,
    "input_throughput": 7712.538976855768,
    "output_throughput": 6668.933310250257,
    "total_throughput": 14381.472287106026,
    "itl": 86.35972113696454,
    "ttft": 1594913.4227942382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9349952696077526,
    "arrivals": 314275,
    "finished_requests": 111811,
    "scheduler_time": 234.94345287579856
}
#Debug simulation 
Total elapsed time: 79.35604285798036. Arrivals time: 0.4184804280521348 Scheduler time: 78.73590587533545 Scheduler overhead time: 0.08069005608558655 Adapter cache time: 0.013473071507178247 Engine time: 0.07589709770400077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 95.63482556096278,
    "estimated_duration": 3600.082232845785,
    "input_throughput": 7758.532220504554,
    "output_throughput": 6719.654839905506,
    "total_throughput": 14478.18706041006,
    "itl": 88.40808500902081,
    "ttft": 1573310.836683388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6766956197004762,
    "arrivals": 314275,
    "finished_requests": 112576,
    "scheduler_time": 232.3552413553487
}
#Debug simulation 
Total elapsed time: 95.63494983199053. Arrivals time: 0.4297608274500817 Scheduler time: 94.9849281872157 Scheduler overhead time: 0.08301634876988828 Adapter cache time: 0.014003925258293748 Engine time: 0.09110835881438106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.38087727909442,
    "estimated_duration": 3600.031462919644,
    "input_throughput": 7712.673704657497,
    "output_throughput": 6668.9878261588665,
    "total_throughput": 14381.661530816364,
    "itl": 86.36349088856231,
    "ttft": 1594892.4575972857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9250536451302469,
    "arrivals": 314275,
    "finished_requests": 111810,
    "scheduler_time": 234.93481913848177
}
#Debug simulation 
Total elapsed time: 79.38099992799107. Arrivals time: 0.3960164497839287 Scheduler time: 78.7821123964386 Scheduler overhead time: 0.08134989626705647 Adapter cache time: 0.01360605820082128 Engine time: 0.07580023445188999 
