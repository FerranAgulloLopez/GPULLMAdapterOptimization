INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 29.437854785937816,
    "estimated_duration": 3600.012078037567,
    "input_throughput": 4581.443518098077,
    "output_throughput": 3978.5324853153284,
    "total_throughput": 8559.976003413405,
    "itl": 47.75189731580203,
    "ttft": 262596.70250789647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8448621199140303,
    "arrivals": 69367,
    "finished_requests": 66386,
    "scheduler_time": 72.45289615095777
}
#Debug simulation 
Total elapsed time: 29.438048060052097. Arrivals time: 0.2430790546350181 Scheduler time: 28.890864954330027 Scheduler overhead time: 0.12097907857969403 Adapter cache time: 0.020577564369887114 Engine time: 0.11200680676847696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 29.612576789222658,
    "estimated_duration": 3600.0130157624844,
    "input_throughput": 4581.442324731907,
    "output_throughput": 3978.53144899434,
    "total_throughput": 8559.973773726248,
    "itl": 47.727943285288745,
    "ttft": 262609.91299512115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.051817032215188,
    "arrivals": 69367,
    "finished_requests": 66386,
    "scheduler_time": 72.45377333405122
}
#Debug simulation 
Total elapsed time: 29.61271173832938. Arrivals time: 0.24165835324674845 Scheduler time: 29.064109175466 Scheduler overhead time: 0.12478647148236632 Adapter cache time: 0.020707743242383003 Engine time: 0.110245565418154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 26.236314471811056,
    "estimated_duration": 3600.034979824117,
    "input_throughput": 4642.314336850279,
    "output_throughput": 4023.525071611296,
    "total_throughput": 8665.839408461574,
    "itl": 49.24262611958517,
    "ttft": 215943.6129108349,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.729202590882795,
    "arrivals": 69367,
    "finished_requests": 67257,
    "scheduler_time": 70.72274350097193
}
#Debug simulation 
Total elapsed time: 26.236491580028087. Arrivals time: 0.2524107778444886 Scheduler time: 25.68291875999421 Scheduler overhead time: 0.12100152112543583 Adapter cache time: 0.020915388595312834 Engine time: 0.10870745917782187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 29.31767984619364,
    "estimated_duration": 3600.003858848841,
    "input_throughput": 4581.453978017119,
    "output_throughput": 3978.541568724855,
    "total_throughput": 8559.995546741973,
    "itl": 47.724878570223964,
    "ttft": 262608.93515192665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9185478187771496,
    "arrivals": 69367,
    "finished_requests": 66386,
    "scheduler_time": 72.45439590924121
}
#Debug simulation 
Total elapsed time: 29.317817179951817. Arrivals time: 0.23812497360631824 Scheduler time: 28.777823101729155 Scheduler overhead time: 0.12113232119008899 Adapter cache time: 0.020166559610515833 Engine time: 0.11011217860504985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 29.169101675972342,
    "estimated_duration": 3600.0193735816792,
    "input_throughput": 4581.434233669352,
    "output_throughput": 3978.5244227033704,
    "total_throughput": 8559.958656372723,
    "itl": 47.653704183809836,
    "ttft": 262636.32426779007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0898014851519906,
    "arrivals": 69367,
    "finished_requests": 66386,
    "scheduler_time": 72.45685577562915
}
#Debug simulation 
Total elapsed time: 29.169231990817934. Arrivals time: 0.23331111622974277 Scheduler time: 28.634931615088135 Scheduler overhead time: 0.12142590852454305 Adapter cache time: 0.02007437963038683 Engine time: 0.10929357865825295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 29.278439448215067,
    "estimated_duration": 3600.0101186809793,
    "input_throughput": 4581.446011613718,
    "output_throughput": 3978.5346506880846,
    "total_throughput": 8559.980662301803,
    "itl": 47.722240573738134,
    "ttft": 262606.39171614195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7811139424191724,
    "arrivals": 69367,
    "finished_requests": 66386,
    "scheduler_time": 72.45459289940648
}
#Debug simulation 
Total elapsed time: 29.27860314399004. Arrivals time: 0.23951276810839772 Scheduler time: 28.739174556918442 Scheduler overhead time: 0.12089172517880797 Adapter cache time: 0.02013843785971403 Engine time: 0.10867032967507839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 29.120563718955964,
    "estimated_duration": 3600.0295916075825,
    "input_throughput": 4581.509562713024,
    "output_throughput": 3978.587574221603,
    "total_throughput": 8560.097136934626,
    "itl": 47.655290928478486,
    "ttft": 262632.4088784316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.069296884667135,
    "arrivals": 69367,
    "finished_requests": 66387,
    "scheduler_time": 72.45734493578057
}
#Debug simulation 
Total elapsed time: 29.120726868044585. Arrivals time: 0.23014738410711288 Scheduler time: 28.587763387244195 Scheduler overhead time: 0.12203082116320729 Adapter cache time: 0.0201964327134192 Engine time: 0.11019091308116913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 31.064981148112565,
    "estimated_duration": 3600.017301175737,
    "input_throughput": 4539.644294115635,
    "output_throughput": 3938.943292124968,
    "total_throughput": 8478.587586240603,
    "itl": 48.886710191976334,
    "ttft": 273427.9033219558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4679548050928843,
    "arrivals": 68405,
    "finished_requests": 65449,
    "scheduler_time": 74.54598997924417
}
#Debug simulation 
Total elapsed time: 31.065116077195853. Arrivals time: 0.23346790112555027 Scheduler time: 30.528275730088353 Scheduler overhead time: 0.1229706397280097 Adapter cache time: 0.019759700167924166 Engine time: 0.11041298974305391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 33.71243650326505,
    "estimated_duration": 3600.0525501362285,
    "input_throughput": 4498.535167045595,
    "output_throughput": 3894.667592968729,
    "total_throughput": 8393.202760014325,
    "itl": 48.53847494755803,
    "ttft": 311246.96006373654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4947254032408832,
    "arrivals": 68405,
    "finished_requests": 64870,
    "scheduler_time": 76.82512033571051
}
#Debug simulation 
Total elapsed time: 33.712603552266955. Arrivals time: 0.23458706447854638 Scheduler time: 33.17668901011348 Scheduler overhead time: 0.12268967973068357 Adapter cache time: 0.02001996012404561 Engine time: 0.10842379741370678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.630882842000574,
    "estimated_duration": 3600.0000042051042,
    "input_throughput": 4526.834994712274,
    "output_throughput": 3927.264717634845,
    "total_throughput": 8454.099712347119,
    "itl": 48.37021695785772,
    "ttft": 280480.68581587356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6630190029647236,
    "arrivals": 68405,
    "finished_requests": 65246,
    "scheduler_time": 73.72250821247533
}
#Debug simulation 
Total elapsed time: 30.631057895254344. Arrivals time: 0.23102340986952186 Scheduler time: 30.099700934253633 Scheduler overhead time: 0.12166528729721904 Adapter cache time: 0.019946672022342682 Engine time: 0.10839516017585993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 34.06540586706251,
    "estimated_duration": 3600.031594673848,
    "input_throughput": 4498.561352617022,
    "output_throughput": 3894.6902634809408,
    "total_throughput": 8393.251616097963,
    "itl": 48.53612728187945,
    "ttft": 311198.67774217745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3989381560822933,
    "arrivals": 68405,
    "finished_requests": 64870,
    "scheduler_time": 76.82578966186563
}
#Debug simulation 
Total elapsed time: 34.0655668140389. Arrivals time: 0.24556946568191051 Scheduler time: 33.5126012256369 Scheduler overhead time: 0.12487779138609767 Adapter cache time: 0.02011859556660056 Engine time: 0.11180837219581008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 30.75537812896073,
    "estimated_duration": 3600.006513063679,
    "input_throughput": 4526.826810135756,
    "output_throughput": 3927.257617089182,
    "total_throughput": 8454.084427224938,
    "itl": 48.372000683226034,
    "ttft": 280479.02828365343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6478994490718504,
    "arrivals": 68405,
    "finished_requests": 65246,
    "scheduler_time": 73.72231953713408
}
#Debug simulation 
Total elapsed time: 30.75548710906878. Arrivals time: 0.24123218655586243 Scheduler time: 30.20936113037169 Scheduler overhead time: 0.12419928004965186 Adapter cache time: 0.020331496838480234 Engine time: 0.1095197182148695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.745931269135326,
    "estimated_duration": 3600.0225229307534,
    "input_throughput": 4526.806678624068,
    "output_throughput": 3927.2401519561126,
    "total_throughput": 8454.04683058018,
    "itl": 48.455090947625,
    "ttft": 280443.29276690446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.417230448806654,
    "arrivals": 68405,
    "finished_requests": 65246,
    "scheduler_time": 73.71985669279877
}
#Debug simulation 
Total elapsed time: 30.7461203802377. Arrivals time: 0.24322707066312432 Scheduler time: 30.198451590258628 Scheduler overhead time: 0.12314105965197086 Adapter cache time: 0.020377812907099724 Engine time: 0.11031271377578378 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.578258539084345,
    "estimated_duration": 3600.030810469054,
    "input_throughput": 4526.796257578887,
    "output_throughput": 3927.231111157606,
    "total_throughput": 8454.027368736493,
    "itl": 48.37045989123702,
    "ttft": 280529.7325501758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6327798951789774,
    "arrivals": 68405,
    "finished_requests": 65246,
    "scheduler_time": 73.72364186915524
}
#Debug simulation 
Total elapsed time: 30.578364451881498. Arrivals time: 0.23458128329366446 Scheduler time: 30.040821118745953 Scheduler overhead time: 0.12319021625444293 Adapter cache time: 0.019800382666289806 Engine time: 0.10914155002683401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 28.503942522220314,
    "estimated_duration": 3600.0220652272687,
    "input_throughput": 4408.444646296382,
    "output_throughput": 3874.1101435775604,
    "total_throughput": 8282.554789873942,
    "itl": 47.34181441554546,
    "ttft": 297760.97980523686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5803657586360331,
    "arrivals": 67954,
    "finished_requests": 64171,
    "scheduler_time": 70.11640786322467
}
#Debug simulation 
Total elapsed time: 28.50412647612393. Arrivals time: 0.22802307177335024 Scheduler time: 27.973841646220535 Scheduler overhead time: 0.12230242975056171 Adapter cache time: 0.020094614941626787 Engine time: 0.10895185871049762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 28.183855328243226,
    "estimated_duration": 3600.079146000461,
    "input_throughput": 4373.108579429543,
    "output_throughput": 3836.29149246438,
    "total_throughput": 8209.400071893924,
    "itl": 46.97125099781523,
    "ttft": 320012.5267536189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8963741939375198,
    "arrivals": 67954,
    "finished_requests": 63710,
    "scheduler_time": 69.7240487582026
}
#Debug simulation 
Total elapsed time: 28.18396469205618. Arrivals time: 0.22587815904989839 Scheduler time: 27.655394938774407 Scheduler overhead time: 0.12248626630753279 Adapter cache time: 0.020097289234399796 Engine time: 0.10920811584219337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 28.061990432906896,
    "estimated_duration": 3600.005437147677,
    "input_throughput": 4543.978414921755,
    "output_throughput": 3986.9367562322436,
    "total_throughput": 8530.915171153998,
    "itl": 48.0931142132777,
    "ttft": 211324.6491104336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.994910900481051,
    "arrivals": 67954,
    "finished_requests": 66074,
    "scheduler_time": 71.11523090454013
}
#Debug simulation 
Total elapsed time: 28.062134988140315. Arrivals time: 0.23077775118872523 Scheduler time: 27.535805047489703 Scheduler overhead time: 0.11935564875602722 Adapter cache time: 0.01991112530231476 Engine time: 0.1061925683170557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 28.292040403932333,
    "estimated_duration": 3600.041153780513,
    "input_throughput": 4532.264577827316,
    "output_throughput": 3967.498256235438,
    "total_throughput": 8499.762834062754,
    "itl": 48.131811953098044,
    "ttft": 219879.8138785129,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7842021482205,
    "arrivals": 67954,
    "finished_requests": 65908,
    "scheduler_time": 71.14032465252723
}
#Debug simulation 
Total elapsed time: 28.29218597896397. Arrivals time: 0.23491939529776573 Scheduler time: 27.756662503350526 Scheduler overhead time: 0.12175293313339353 Adapter cache time: 0.02007986418902874 Engine time: 0.10836036689579487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 28.239166710991412,
    "estimated_duration": 3600.057568562644,
    "input_throughput": 4543.912614856106,
    "output_throughput": 3986.8790225292328,
    "total_throughput": 8530.791637385339,
    "itl": 48.09254259679194,
    "ttft": 211375.29759830516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9754418858792688,
    "arrivals": 67954,
    "finished_requests": 66074,
    "scheduler_time": 71.11699510237908
}
#Debug simulation 
Total elapsed time: 28.239292792975903. Arrivals time: 0.2341076717711985 Scheduler time: 27.704860914032906 Scheduler overhead time: 0.12165024364367127 Adapter cache time: 0.02030637301504612 Engine time: 0.10788370203226805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 28.13671794673428,
    "estimated_duration": 3600.035482667235,
    "input_throughput": 4532.271717475231,
    "output_throughput": 3967.5045062105146,
    "total_throughput": 8499.776223685745,
    "itl": 48.129180033388714,
    "ttft": 219877.4579397781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6662033654889035,
    "arrivals": 67954,
    "finished_requests": 65908,
    "scheduler_time": 71.140224019155
}
#Debug simulation 
Total elapsed time: 28.13680957769975. Arrivals time: 0.22775005688890815 Scheduler time: 27.61020218441263 Scheduler overhead time: 0.1213010884821415 Adapter cache time: 0.019993241410702467 Engine time: 0.10746905393898487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 28.164260109420866,
    "estimated_duration": 3600.0761611165594,
    "input_throughput": 4373.112205247669,
    "output_throughput": 3836.2946731983993,
    "total_throughput": 8209.406878446069,
    "itl": 46.82058477423883,
    "ttft": 320105.46489100956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.91191776072607,
    "arrivals": 67954,
    "finished_requests": 63710,
    "scheduler_time": 69.74006982166263
}
#Debug simulation 
Total elapsed time: 28.164404875133187. Arrivals time: 0.23399579850956798 Scheduler time: 27.629077184945345 Scheduler overhead time: 0.12209250219166279 Adapter cache time: 0.02019127458333969 Engine time: 0.10830180859193206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 23.554075140040368,
    "estimated_duration": 3600.023027183057,
    "input_throughput": 4545.344537088691,
    "output_throughput": 3924.790728645953,
    "total_throughput": 8470.135265734643,
    "itl": 47.82229490142842,
    "ttft": 201046.37198380104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7456759844347813,
    "arrivals": 67703,
    "finished_requests": 65543,
    "scheduler_time": 64.54316232786073
}
#Debug simulation 
Total elapsed time: 23.554194482974708. Arrivals time: 0.22022919030860066 Scheduler time: 23.031240326352417 Scheduler overhead time: 0.12072610994800925 Adapter cache time: 0.02002898370847106 Engine time: 0.11126569798216224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 22.496423231903464,
    "estimated_duration": 3600.009859534256,
    "input_throughput": 4582.643282575436,
    "output_throughput": 3959.7122108560593,
    "total_throughput": 8542.355493431496,
    "itl": 47.62636103266648,
    "ttft": 169137.57675749427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.101500178766439,
    "arrivals": 67703,
    "finished_requests": 66128,
    "scheduler_time": 63.89476637575844
}
#Debug simulation 
Total elapsed time: 22.49654965987429. Arrivals time: 0.21695527248084545 Scheduler time: 21.985242480412126 Scheduler overhead time: 0.11823854828253388 Adapter cache time: 0.0199175332672894 Engine time: 0.10622473387047648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 22.436018510721624,
    "estimated_duration": 3600.017757035882,
    "input_throughput": 4580.9437933379695,
    "output_throughput": 3959.1299159966725,
    "total_throughput": 8540.073709334642,
    "itl": 47.42292113058493,
    "ttft": 171082.2155867012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1508379901712833,
    "arrivals": 67703,
    "finished_requests": 66091,
    "scheduler_time": 63.868755812011926
}
#Debug simulation 
Total elapsed time: 22.43612483283505. Arrivals time: 0.2159303082153201 Scheduler time: 21.92404046608135 Scheduler overhead time: 0.11939272796735168 Adapter cache time: 0.01992773823440075 Engine time: 0.10659273201599717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 22.642458319664,
    "estimated_duration": 3600.031001333645,
    "input_throughput": 4580.950551228764,
    "output_throughput": 3959.2461828022515,
    "total_throughput": 8540.196734031015,
    "itl": 47.504601607168254,
    "ttft": 170935.68955575392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9557369765685841,
    "arrivals": 67703,
    "finished_requests": 66093,
    "scheduler_time": 63.86368667780106
}
#Debug simulation 
Total elapsed time: 22.642680169083178. Arrivals time: 0.21867451118305326 Scheduler time: 22.128187873400748 Scheduler overhead time: 0.11835742136463523 Adapter cache time: 0.019839861430227757 Engine time: 0.10736156301572919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 22.7515741661191,
    "estimated_duration": 3600.008603458387,
    "input_throughput": 4582.6448815015165,
    "output_throughput": 3959.713592435801,
    "total_throughput": 8542.358473937318,
    "itl": 47.5403728797908,
    "ttft": 169170.50825637166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1371667600004076,
    "arrivals": 67703,
    "finished_requests": 66128,
    "scheduler_time": 63.898865706225145
}
#Debug simulation 
Total elapsed time: 22.751676458399743. Arrivals time: 0.21701647341251373 Scheduler time: 22.240627460181713 Scheduler overhead time: 0.11699671763926744 Adapter cache time: 0.01960751600563526 Engine time: 0.10725169815123081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 22.53521557478234,
    "estimated_duration": 3600.022536555472,
    "input_throughput": 4581.139932468274,
    "output_throughput": 3959.7388225351033,
    "total_throughput": 8540.878755003378,
    "itl": 47.55627881405586,
    "ttft": 170707.03245478013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8321853099437364,
    "arrivals": 67703,
    "finished_requests": 66096,
    "scheduler_time": 63.84427489634305
}
#Debug simulation 
Total elapsed time: 22.53531662467867. Arrivals time: 0.217215939424932 Scheduler time: 22.023659769445658 Scheduler overhead time: 0.11752078076824546 Adapter cache time: 0.019807602744549513 Engine time: 0.10695115616545081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 22.538748460356146,
    "estimated_duration": 3600.004174143773,
    "input_throughput": 4582.65051982163,
    "output_throughput": 3959.7184643238415,
    "total_throughput": 8542.36898414547,
    "itl": 47.53932753888498,
    "ttft": 169164.33374593483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.118733331281699,
    "arrivals": 67703,
    "finished_requests": 66128,
    "scheduler_time": 63.89702976197391
}
#Debug simulation 
Total elapsed time: 22.538867299910635. Arrivals time: 0.21770350961014628 Scheduler time: 22.027277222834527 Scheduler overhead time: 0.11701621301472187 Adapter cache time: 0.019531124737113714 Engine time: 0.10701470682397485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 24.35202111583203,
    "estimated_duration": 3600.0203927984567,
    "input_throughput": 4481.55350238407,
    "output_throughput": 3918.91030068113,
    "total_throughput": 8400.4638030652,
    "itl": 47.68104880043221,
    "ttft": 177871.37659094064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.963885482489129,
    "arrivals": 66533,
    "finished_requests": 65030,
    "scheduler_time": 65.66812458805612
}
#Debug simulation 
Total elapsed time: 24.35212571406737. Arrivals time: 0.21888588834553957 Scheduler time: 23.83758182823658 Scheduler overhead time: 0.11830997746437788 Adapter cache time: 0.019910171627998352 Engine time: 0.10706218937411904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.628087447956204,
    "estimated_duration": 3600.020002729507,
    "input_throughput": 4412.1271515039,
    "output_throughput": 3849.4283335906352,
    "total_throughput": 8261.555485094535,
    "itl": 49.84124250051066,
    "ttft": 240170.0471678518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1156562416208935,
    "arrivals": 66533,
    "finished_requests": 64044,
    "scheduler_time": 68.30576805206624
}
#Debug simulation 
Total elapsed time: 25.628259271848947. Arrivals time: 0.22142346808686852 Scheduler time: 25.10817062947899 Scheduler overhead time: 0.12067749816924334 Adapter cache time: 0.02003869926556945 Engine time: 0.10744659742340446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 27.37996585527435,
    "estimated_duration": 3600.0072136864587,
    "input_throughput": 4363.559034070357,
    "output_throughput": 3812.0165281411087,
    "total_throughput": 8175.575562211465,
    "itl": 48.549732269751175,
    "ttft": 284887.2598783044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.906542051970033,
    "arrivals": 66533,
    "finished_requests": 63239,
    "scheduler_time": 69.73975631104811
}
#Debug simulation 
Total elapsed time: 27.380099565256387. Arrivals time: 0.22459349082782865 Scheduler time: 26.85330748092383 Scheduler overhead time: 0.12232477404177189 Adapter cache time: 0.020014349836856127 Engine time: 0.10898691741749644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 24.455876019783318,
    "estimated_duration": 3600.023437489394,
    "input_throughput": 4461.003457021886,
    "output_throughput": 3906.0373478586353,
    "total_throughput": 8367.040804880522,
    "itl": 47.83247351001785,
    "ttft": 193048.16395807298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9909902071440568,
    "arrivals": 66533,
    "finished_requests": 64747,
    "scheduler_time": 65.67793347979932
}
#Debug simulation 
Total elapsed time: 24.4559843740426. Arrivals time: 0.22131288889795542 Scheduler time: 23.93625757517293 Scheduler overhead time: 0.11977962870150805 Adapter cache time: 0.020213625393807888 Engine time: 0.10801160940900445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 27.52762798918411,
    "estimated_duration": 3600.0017604516565,
    "input_throughput": 4363.565643931565,
    "output_throughput": 3812.022302533062,
    "total_throughput": 8175.587946464627,
    "itl": 48.55195045130616,
    "ttft": 284886.4723948663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8881086232513247,
    "arrivals": 66533,
    "finished_requests": 63239,
    "scheduler_time": 69.73912156640675
}
#Debug simulation 
Total elapsed time: 27.52775534009561. Arrivals time: 0.2296400754712522 Scheduler time: 26.994752026628703 Scheduler overhead time: 0.12232623994350433 Adapter cache time: 0.020060990937054157 Engine time: 0.11019954876974225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 24.408004489261657,
    "estimated_duration": 3600.02441745649,
    "input_throughput": 4461.002242686622,
    "output_throughput": 3906.0362845913814,
    "total_throughput": 8367.038527278002,
    "itl": 47.830127560033304,
    "ttft": 193046.1359474076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8577209937060184,
    "arrivals": 66533,
    "finished_requests": 64747,
    "scheduler_time": 65.67852745404595
}
#Debug simulation 
Total elapsed time: 24.408136097248644. Arrivals time: 0.21975741535425186 Scheduler time: 23.88941061636433 Scheduler overhead time: 0.12037101713940501 Adapter cache time: 0.019976555835455656 Engine time: 0.10794743755832314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 27.552744211163372,
    "estimated_duration": 3600.0317005085726,
    "input_throughput": 4363.529353861198,
    "output_throughput": 3811.9905994331457,
    "total_throughput": 8175.519953294343,
    "itl": 48.51257261614065,
    "ttft": 284952.67041880597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8707107804156895,
    "arrivals": 66533,
    "finished_requests": 63239,
    "scheduler_time": 69.74155141234004
}
#Debug simulation 
Total elapsed time: 27.55286852410063. Arrivals time: 0.22875940753147006 Scheduler time: 27.021861824207008 Scheduler overhead time: 0.1218976522795856 Adapter cache time: 0.020119356457144022 Engine time: 0.10940289637073874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 25.204752502031624,
    "estimated_duration": 3600.0104612576574,
    "input_throughput": 4480.960312088784,
    "output_throughput": 3892.9389652673444,
    "total_throughput": 8373.899277356128,
    "itl": 46.963115447872156,
    "ttft": 174715.49404339743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7126139392750317,
    "arrivals": 66085,
    "finished_requests": 64695,
    "scheduler_time": 65.69801243629036
}
#Debug simulation 
Total elapsed time: 25.204901767894626. Arrivals time: 0.21930133691057563 Scheduler time: 24.68446687469259 Scheduler overhead time: 0.12138161435723305 Adapter cache time: 0.01978782843798399 Engine time: 0.10883985739201307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.737471686210483,
    "estimated_duration": 3600.043372138856,
    "input_throughput": 4492.843371048181,
    "output_throughput": 3893.213095283621,
    "total_throughput": 8386.056466331802,
    "itl": 48.70427688828663,
    "ttft": 171697.92440147788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7903406490897777,
    "arrivals": 66085,
    "finished_requests": 64842,
    "scheduler_time": 67.08040603035982
}
#Debug simulation 
Total elapsed time: 25.73762947227806. Arrivals time: 0.2257884657010436 Scheduler time: 25.21199136832729 Scheduler overhead time: 0.1209670314565301 Adapter cache time: 0.020007355138659477 Engine time: 0.10843841452151537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.569398468360305,
    "estimated_duration": 3600.0056542608895,
    "input_throughput": 4492.705443630923,
    "output_throughput": 3893.1347186668395,
    "total_throughput": 8385.840162297762,
    "itl": 48.53515740304651,
    "ttft": 171838.30807813964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.83851585219149,
    "arrivals": 66085,
    "finished_requests": 64840,
    "scheduler_time": 67.0917118700418
}
#Debug simulation 
Total elapsed time: 25.569496987387538. Arrivals time: 0.2217286927625537 Scheduler time: 25.04938301164657 Scheduler overhead time: 0.12051670718938112 Adapter cache time: 0.01987021043896675 Engine time: 0.10771381249651313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 25.234992294106632,
    "estimated_duration": 3600.0143047491556,
    "input_throughput": 4480.955528070887,
    "output_throughput": 3892.9348090400217,
    "total_throughput": 8373.890337110908,
    "itl": 46.924095779340654,
    "ttft": 174730.3965321161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.76726964341942,
    "arrivals": 66085,
    "finished_requests": 64695,
    "scheduler_time": 65.70017351596682
}
#Debug simulation 
Total elapsed time: 25.235125321894884. Arrivals time: 0.222709309309721 Scheduler time: 24.712737551890314 Scheduler overhead time: 0.12032314902171493 Adapter cache time: 0.020031656604260206 Engine time: 0.10853336052969098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 25.70821719802916,
    "estimated_duration": 3600.0247430022846,
    "input_throughput": 4492.681899322639,
    "output_throughput": 3893.2093528644687,
    "total_throughput": 8385.891252187108,
    "itl": 48.53384656359777,
    "ttft": 171835.15414553584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.821118009355855,
    "arrivals": 66085,
    "finished_requests": 64841,
    "scheduler_time": 67.09206532065049
}
#Debug simulation 
Total elapsed time: 25.708317348733544. Arrivals time: 0.22107028868049383 Scheduler time: 25.189099279697984 Scheduler overhead time: 0.12006549397483468 Adapter cache time: 0.019981812685728073 Engine time: 0.10772488079965115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 25.17971977731213,
    "estimated_duration": 3600.0297950092427,
    "input_throughput": 4480.93624735086,
    "output_throughput": 3892.9180584640185,
    "total_throughput": 8373.854305814879,
    "itl": 46.92182125695241,
    "ttft": 174729.3022635202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6534355236077625,
    "arrivals": 66085,
    "finished_requests": 64695,
    "scheduler_time": 65.69998532509842
}
#Debug simulation 
Total elapsed time: 25.17982103303075. Arrivals time: 0.2218824466690421 Scheduler time: 24.658951281569898 Scheduler overhead time: 0.11992957070469856 Adapter cache time: 0.019878384191542864 Engine time: 0.10857961047440767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.59239562600851,
    "estimated_duration": 3600.0336299336745,
    "input_throughput": 4492.670808827411,
    "output_throughput": 3893.19974220858,
    "total_throughput": 8385.87055103599,
    "itl": 48.53611229213152,
    "ttft": 171834.43378687082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8047557524032936,
    "arrivals": 66085,
    "finished_requests": 64841,
    "scheduler_time": 67.0921184497075
}
#Debug simulation 
Total elapsed time: 25.592568075284362. Arrivals time: 0.2054263292811811 Scheduler time: 25.089561705477536 Scheduler overhead time: 0.12042036140337586 Adapter cache time: 0.019238504581153393 Engine time: 0.10742129944264889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 22.99306905036792,
    "estimated_duration": 3600.00424809629,
    "input_throughput": 4459.661681924376,
    "output_throughput": 3854.566284842018,
    "total_throughput": 8314.227966766393,
    "itl": 46.67721908016487,
    "ttft": 174861.5700202121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.778738029594531,
    "arrivals": 65845,
    "finished_requests": 64220,
    "scheduler_time": 62.302684146417576
}
#Debug simulation 
Total elapsed time: 22.99322414631024. Arrivals time: 0.2122610006481409 Scheduler time: 22.483762732241303 Scheduler overhead time: 0.11888078041374683 Adapter cache time: 0.01988007826730609 Engine time: 0.10794733371585608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.1388771943748,
    "estimated_duration": 3600.0202490776655,
    "input_throughput": 4460.568243779778,
    "output_throughput": 3864.720484159656,
    "total_throughput": 8325.288727939433,
    "itl": 47.900900644739174,
    "ttft": 184356.9575288656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.761197486333551,
    "arrivals": 65845,
    "finished_requests": 64259,
    "scheduler_time": 65.25358030632479
}
#Debug simulation 
Total elapsed time: 25.138977714348584. Arrivals time: 0.21521779242902994 Scheduler time: 24.625597859267145 Scheduler overhead time: 0.12008728878572583 Adapter cache time: 0.01982097141444683 Engine time: 0.10765421483665705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.125647895038128,
    "estimated_duration": 3600.0242633180383,
    "input_throughput": 4460.18049478393,
    "output_throughput": 3864.1936782888397,
    "total_throughput": 8324.374173072769,
    "itl": 47.76243016719216,
    "ttft": 184917.34277304696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8020210567116783,
    "arrivals": 65845,
    "finished_requests": 64249,
    "scheduler_time": 65.25627864799078
}
#Debug simulation 
Total elapsed time: 25.125767721328884. Arrivals time: 0.22183752106502652 Scheduler time: 24.602483240887523 Scheduler overhead time: 0.12186396354809403 Adapter cache time: 0.020009436178952456 Engine time: 0.10904638422653079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 22.298031326383352,
    "estimated_duration": 3600.035868404046,
    "input_throughput": 4474.158199746091,
    "output_throughput": 3868.6928433783232,
    "total_throughput": 8342.851043124414,
    "itl": 46.332373731157396,
    "ttft": 161157.85242797981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.900227093002757,
    "arrivals": 65845,
    "finished_requests": 64447,
    "scheduler_time": 61.8311291046651
}
#Debug simulation 
Total elapsed time: 22.29816589318216. Arrivals time: 0.21394779160618782 Scheduler time: 21.78579521458596 Scheduler overhead time: 0.12008469179272652 Adapter cache time: 0.019992211367934942 Engine time: 0.10778398253023624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 25.15413932967931,
    "estimated_duration": 3600.023094896998,
    "input_throughput": 4460.181942377069,
    "output_throughput": 3864.194932448904,
    "total_throughput": 8324.376874825974,
    "itl": 47.762121505555285,
    "ttft": 184917.5483809714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7846232138760434,
    "arrivals": 65845,
    "finished_requests": 64249,
    "scheduler_time": 65.25615692268701
}
#Debug simulation 
Total elapsed time: 25.154221365693957. Arrivals time: 0.21676084818318486 Scheduler time: 24.637058186344802 Scheduler overhead time: 0.12129695527255535 Adapter cache time: 0.019955985713750124 Engine time: 0.10825706413015723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 25.129939449951053,
    "estimated_duration": 3600.008580481967,
    "input_throughput": 4460.5827016807125,
    "output_throughput": 3864.733010757804,
    "total_throughput": 8325.315712438516,
    "itl": 47.89836040846201,
    "ttft": 184352.25212639585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.532141025736923,
    "arrivals": 65845,
    "finished_requests": 64259,
    "scheduler_time": 65.25343838095301
}
#Debug simulation 
Total elapsed time: 25.13005579682067. Arrivals time: 0.21763109834864736 Scheduler time: 24.6133368415758 Scheduler overhead time: 0.12082777824252844 Adapter cache time: 0.01989746792241931 Engine time: 0.1079646241851151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.098731877747923,
    "estimated_duration": 3600.000923490242,
    "input_throughput": 4460.2094114000365,
    "output_throughput": 3864.218730953253,
    "total_throughput": 8324.42814235329,
    "itl": 47.76031641910512,
    "ttft": 184917.51017326917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7680538397468673,
    "arrivals": 65845,
    "finished_requests": 64249,
    "scheduler_time": 65.25616149703204
}
#Debug simulation 
Total elapsed time: 25.098908097017556. Arrivals time: 0.2187049137428403 Scheduler time: 24.581744659226388 Scheduler overhead time: 0.11983326217159629 Adapter cache time: 0.019765112083405256 Engine time: 0.10802669916301966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 22.83176654484123,
    "estimated_duration": 3599.9533664524693,
    "input_throughput": 4380.967583350008,
    "output_throughput": 3847.862344297646,
    "total_throughput": 8228.829927647654,
    "itl": 48.122416418462414,
    "ttft": 170565.79960670508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.785350438626481,
    "arrivals": 65172,
    "finished_requests": 63705,
    "scheduler_time": 62.584876377857334
}
#Debug simulation 
Total elapsed time: 22.831894391682. Arrivals time: 0.21211998304352164 Scheduler time: 22.319478473160416 Scheduler overhead time: 0.1204580101184547 Adapter cache time: 0.01968148024752736 Engine time: 0.10973808029666543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.078138899989426,
    "estimated_duration": 3599.9492232551893,
    "input_throughput": 4380.972625424729,
    "output_throughput": 3847.866772819219,
    "total_throughput": 8228.839398243948,
    "itl": 48.05793385796079,
    "ttft": 170598.2011793436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9777030920702987,
    "arrivals": 65172,
    "finished_requests": 63705,
    "scheduler_time": 62.589379072029494
}
#Debug simulation 
Total elapsed time: 23.078282684087753. Arrivals time: 0.21787142287939787 Scheduler time: 22.562024963088334 Scheduler overhead time: 0.11992161720991135 Adapter cache time: 0.01996098691597581 Engine time: 0.10799284093081951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.191323070786893,
    "estimated_duration": 3599.9523280945514,
    "input_throughput": 4380.968846981291,
    "output_throughput": 3847.8634541618794,
    "total_throughput": 8228.83230114317,
    "itl": 47.908563702708776,
    "ttft": 170667.6369338239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.031873654560192,
    "arrivals": 65172,
    "finished_requests": 63705,
    "scheduler_time": 62.599261097204554
}
#Debug simulation 
Total elapsed time: 23.191428727004677. Arrivals time: 0.2142100245691836 Scheduler time: 22.679648086428642 Scheduler overhead time: 0.11923313792794943 Adapter cache time: 0.01980135915800929 Engine time: 0.10789948655292392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 22.867434454150498,
    "estimated_duration": 3599.93169796412,
    "input_throughput": 4380.993953001714,
    "output_throughput": 3847.8855051149535,
    "total_throughput": 8228.879458116668,
    "itl": 48.05495654997804,
    "ttft": 170596.03462449255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8444338786322603,
    "arrivals": 65172,
    "finished_requests": 63705,
    "scheduler_time": 62.58919033863729
}
#Debug simulation 
Total elapsed time: 22.867587426211685. Arrivals time: 0.21290226001292467 Scheduler time: 22.360015249811113 Scheduler overhead time: 0.11777891684323549 Adapter cache time: 0.019780898466706276 Engine time: 0.10681765107437968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 22.788273326586932,
    "estimated_duration": 3599.940136435513,
    "input_throughput": 4380.98368369424,
    "output_throughput": 3847.8764854450346,
    "total_throughput": 8228.860169139274,
    "itl": 47.90902344271653,
    "ttft": 170665.8003032025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0119904056051805,
    "arrivals": 65172,
    "finished_requests": 63705,
    "scheduler_time": 62.59926671427108
}
#Debug simulation 
Total elapsed time: 22.788399365730584. Arrivals time: 0.21129317209124565 Scheduler time: 22.280872521921992 Scheduler overhead time: 0.11877643084153533 Adapter cache time: 0.019783438183367252 Engine time: 0.10720504727214575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 23.156758989673108,
    "estimated_duration": 3599.927768533768,
    "input_throughput": 4380.998734989497,
    "output_throughput": 3847.889705198696,
    "total_throughput": 8228.888440188193,
    "itl": 48.05009895735686,
    "ttft": 170593.48856189766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.723658653954038,
    "arrivals": 65172,
    "finished_requests": 63705,
    "scheduler_time": 62.588852645991615
}
#Debug simulation 
Total elapsed time: 23.156862375792116. Arrivals time: 0.21141139464452863 Scheduler time: 22.64886441687122 Scheduler overhead time: 0.1188433114439249 Adapter cache time: 0.019753717817366123 Engine time: 0.1072842231951654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 22.873069940600544,
    "estimated_duration": 3599.935514442812,
    "input_throughput": 4380.989308482387,
    "output_throughput": 3847.881425771593,
    "total_throughput": 8228.87073425398,
    "itl": 47.9120868375934,
    "ttft": 170664.5329635146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9939712112397014,
    "arrivals": 65172,
    "finished_requests": 63705,
    "scheduler_time": 62.59873991820369
}
#Debug simulation 
Total elapsed time: 22.87321617268026. Arrivals time: 0.21237559290602803 Scheduler time: 22.361004812642932 Scheduler overhead time: 0.12078403308987617 Adapter cache time: 0.01997541869059205 Engine time: 0.10826319316402078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 23.410189537797123,
    "estimated_duration": 3600.00206693426,
    "input_throughput": 4338.352786919442,
    "output_throughput": 3798.039486028352,
    "total_throughput": 8136.3922729477945,
    "itl": 46.18796175856259,
    "ttft": 194019.25987561827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4745672141248343,
    "arrivals": 64890,
    "finished_requests": 62986,
    "scheduler_time": 62.1414342624776
}
#Debug simulation 
Total elapsed time: 23.410320546012372. Arrivals time: 0.20895646326243877 Scheduler time: 22.900492761749774 Scheduler overhead time: 0.12150725955143571 Adapter cache time: 0.01968098571524024 Engine time: 0.10852792719379067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.513016756623983,
    "estimated_duration": 3599.9758349173203,
    "input_throughput": 4338.384399282696,
    "output_throughput": 3798.0671612797155,
    "total_throughput": 8136.451560562411,
    "itl": 46.16192329153603,
    "ttft": 194031.9714505338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6360121786640975,
    "arrivals": 64890,
    "finished_requests": 62986,
    "scheduler_time": 62.14248482382007
}
#Debug simulation 
Total elapsed time: 23.513111028820276. Arrivals time: 0.21458369120955467 Scheduler time: 22.99525049328804 Scheduler overhead time: 0.12279161252081394 Adapter cache time: 0.019959354307502508 Engine time: 0.109466802328825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.63524314807728,
    "estimated_duration": 3599.9921571800414,
    "input_throughput": 4338.364729170413,
    "output_throughput": 3798.0499409505223,
    "total_throughput": 8136.414670120935,
    "itl": 46.098065130409815,
    "ttft": 194055.19550146622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6813479147059889,
    "arrivals": 64890,
    "finished_requests": 62986,
    "scheduler_time": 62.14544761285638
}
#Debug simulation 
Total elapsed time: 23.635352358222008. Arrivals time: 0.2169613908044994 Scheduler time: 23.11396611155942 Scheduler overhead time: 0.12264292221516371 Adapter cache time: 0.019992550369352102 Engine time: 0.10997156891971827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 23.446049883030355,
    "estimated_duration": 3599.9896022597927,
    "input_throughput": 4338.367808117054,
    "output_throughput": 3798.0526364346133,
    "total_throughput": 8136.420444551666,
    "itl": 46.158500226593155,
    "ttft": 194029.85301598615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5291191637190045,
    "arrivals": 64890,
    "finished_requests": 62986,
    "scheduler_time": 62.14238769037627
}
#Debug simulation 
Total elapsed time: 23.446181538980454. Arrivals time: 0.21085498295724392 Scheduler time: 22.93449262017384 Scheduler overhead time: 0.12129929289221764 Adapter cache time: 0.0196039704605937 Engine time: 0.10877349134534597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 23.500527125783265,
    "estimated_duration": 3600.0008931212315,
    "input_throughput": 4338.35420147882,
    "output_throughput": 3798.040724413664,
    "total_throughput": 8136.394925892485,
    "itl": 46.0992356707702,
    "ttft": 194055.0731891169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6653998921066566,
    "arrivals": 64890,
    "finished_requests": 62986,
    "scheduler_time": 62.145506628011894
}
#Debug simulation 
Total elapsed time: 23.500625254120678. Arrivals time: 0.21444535162299871 Scheduler time: 22.982412141747773 Scheduler overhead time: 0.12345489207655191 Adapter cache time: 0.019970199558883905 Engine time: 0.10910798935219646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 23.52005851501599,
    "estimated_duration": 3599.979814565423,
    "input_throughput": 4338.379603354903,
    "output_throughput": 3798.062962653181,
    "total_throughput": 8136.442566008084,
    "itl": 46.15737674225748,
    "ttft": 194028.36671023248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4236143697472246,
    "arrivals": 64890,
    "finished_requests": 62986,
    "scheduler_time": 62.14245459351535
}
#Debug simulation 
Total elapsed time: 23.520207287278026. Arrivals time: 0.21233419002965093 Scheduler time: 23.00931068882346 Scheduler overhead time: 0.11860644863918424 Adapter cache time: 0.01956145651638508 Engine time: 0.1091310940682888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.48458256619051,
    "estimated_duration": 3599.976621239415,
    "input_throughput": 4338.383451674456,
    "output_throughput": 3798.066331689848,
    "total_throughput": 8136.4497833643045,
    "itl": 46.0995162500879,
    "ttft": 194055.5642527066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6496589866839393,
    "arrivals": 64890,
    "finished_requests": 62986,
    "scheduler_time": 62.145368697663116
}
#Debug simulation 
Total elapsed time: 23.48469293024391. Arrivals time: 0.21409768238663673 Scheduler time: 22.97051152959466 Scheduler overhead time: 0.12074337806552649 Adapter cache time: 0.019676991272717714 Engine time: 0.10880704503506422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 23.525529108941555,
    "estimated_duration": 3600.0274234329327,
    "input_throughput": 4293.329795043975,
    "output_throughput": 3776.321511194529,
    "total_throughput": 8069.651306238504,
    "itl": 46.2999060474612,
    "ttft": 191429.2063282948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5671409405721333,
    "arrivals": 64419,
    "finished_requests": 62656,
    "scheduler_time": 62.40076815712103
}
#Debug simulation 
Total elapsed time: 23.525637664832175. Arrivals time: 0.21163307456299663 Scheduler time: 23.0131916818209 Scheduler overhead time: 0.1205996717326343 Adapter cache time: 0.019856452010571957 Engine time: 0.10886906832456589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.687600325793028,
    "estimated_duration": 3600.0427086483683,
    "input_throughput": 4293.311566240551,
    "output_throughput": 3776.305477527008,
    "total_throughput": 8069.617043767558,
    "itl": 46.02813112914544,
    "ttft": 191479.80670135553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7556446024402994,
    "arrivals": 64419,
    "finished_requests": 62656,
    "scheduler_time": 62.39453696593505
}
#Debug simulation 
Total elapsed time: 23.68772687483579. Arrivals time: 0.21283471351489425 Scheduler time: 23.173069634009153 Scheduler overhead time: 0.12166269542649388 Adapter cache time: 0.019746172707527876 Engine time: 0.10918748751282692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.64057466108352,
    "estimated_duration": 3600.0237549117373,
    "input_throughput": 4291.070018336238,
    "output_throughput": 3777.437852027009,
    "total_throughput": 8068.507870363246,
    "itl": 45.98547472773256,
    "ttft": 193320.0959828587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8113379778806156,
    "arrivals": 64419,
    "finished_requests": 62623,
    "scheduler_time": 62.39350178689348
}
#Debug simulation 
Total elapsed time: 23.640667791943997. Arrivals time: 0.2142744460143149 Scheduler time: 23.126317015849054 Scheduler overhead time: 0.12025930592790246 Adapter cache time: 0.019672987051308155 Engine time: 0.10914585087448359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 23.724165886174887,
    "estimated_duration": 3600.0015408431223,
    "input_throughput": 4293.360662390209,
    "output_throughput": 3776.3486614553162,
    "total_throughput": 8069.7093238455245,
    "itl": 46.26145729578732,
    "ttft": 191424.18126997026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6404222616553283,
    "arrivals": 64419,
    "finished_requests": 62656,
    "scheduler_time": 62.4098883729518
}
#Debug simulation 
Total elapsed time: 23.72431514924392. Arrivals time: 0.21258065151050687 Scheduler time: 23.207541280891746 Scheduler overhead time: 0.12286070408299565 Adapter cache time: 0.020134107675403357 Engine time: 0.110076071228832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 23.66576715419069,
    "estimated_duration": 3600.0362125832044,
    "input_throughput": 4291.055169391012,
    "output_throughput": 3777.4247804696774,
    "total_throughput": 8068.47994986069,
    "itl": 45.98811901892272,
    "ttft": 193321.60468014094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7860711788479273,
    "arrivals": 64419,
    "finished_requests": 62623,
    "scheduler_time": 62.394477876752276
}
#Debug simulation 
Total elapsed time: 23.665865208022296. Arrivals time: 0.20947940228506923 Scheduler time: 23.154007166158408 Scheduler overhead time: 0.12130415486171842 Adapter cache time: 0.01979480916634202 Engine time: 0.10986068146303296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 23.792175028007478,
    "estimated_duration": 3600.019118138877,
    "input_throughput": 4293.339699815381,
    "output_throughput": 3776.330223220652,
    "total_throughput": 8069.669923036034,
    "itl": 46.2588489517331,
    "ttft": 191475.04022046507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.532141025736923,
    "arrivals": 64419,
    "finished_requests": 62656,
    "scheduler_time": 62.41011156924037
}
#Debug simulation 
Total elapsed time: 23.792301188223064. Arrivals time: 0.21894333558157086 Scheduler time: 23.266880896873772 Scheduler overhead time: 0.12363170320168138 Adapter cache time: 0.02003078442066908 Engine time: 0.11103373672813177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.7993328939192,
    "estimated_duration": 3600.032420413069,
    "input_throughput": 4291.059689464546,
    "output_throughput": 3777.4287594997995,
    "total_throughput": 8068.4884489643455,
    "itl": 45.98556304651638,
    "ttft": 193319.34566166188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7777849952690339,
    "arrivals": 64419,
    "finished_requests": 62623,
    "scheduler_time": 62.39359494612875
}
#Debug simulation 
Total elapsed time: 23.79944324400276. Arrivals time: 0.2151694972999394 Scheduler time: 23.28055803431198 Scheduler overhead time: 0.12295943778008223 Adapter cache time: 0.020051983650773764 Engine time: 0.10947669623419642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.254191014915705,
    "estimated_duration": 3600.019089656249,
    "input_throughput": 2971.928685250834,
    "output_throughput": 2552.404243188997,
    "total_throughput": 5524.3329284398305,
    "itl": 31.047233588372514,
    "ttft": 64023.22063816396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.498979306561244,
    "arrivals": 43251,
    "finished_requests": 42842,
    "scheduler_time": 22.251871192225313
}
#Debug simulation 
Total elapsed time: 5.25433666119352. Arrivals time: 0.12906189542263746 Scheduler time: 4.801361170597374 Scheduler overhead time: 0.1261414336040616 Adapter cache time: 0.027210228610783815 Engine time: 0.1156381731852889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.28635242395103,
    "estimated_duration": 3600.007849067168,
    "input_throughput": 2971.7657428919656,
    "output_throughput": 2553.2310998660005,
    "total_throughput": 5524.996842757966,
    "itl": 31.044236897261715,
    "ttft": 64628.73171478942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1728,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.654245703108437,
    "arrivals": 43251,
    "finished_requests": 42833,
    "scheduler_time": 22.250019577824
}
#Debug simulation 
Total elapsed time: 5.286445207893848. Arrivals time: 0.13473291415721178 Scheduler time: 4.823735901620239 Scheduler overhead time: 0.12596336472779512 Adapter cache time: 0.027170011308044195 Engine time: 0.11998009728267789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.337166714016348,
    "estimated_duration": 3600.0270713746463,
    "input_throughput": 2973.291252477381,
    "output_throughput": 2553.731351939396,
    "total_throughput": 5527.022604416777,
    "itl": 31.032534159404623,
    "ttft": 63300.859261087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1747,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.150259796427342,
    "arrivals": 43251,
    "finished_requests": 42847,
    "scheduler_time": 22.19476466640195
}
#Debug simulation 
Total elapsed time: 5.3372763763181865. Arrivals time: 0.1442520278505981 Scheduler time: 4.8665661052800715 Scheduler overhead time: 0.12711913278326392 Adapter cache time: 0.02761202771216631 Engine time: 0.11627236008644104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.379039044957608,
    "estimated_duration": 3600.0323369671332,
    "input_throughput": 2970.425818177207,
    "output_throughput": 2552.459850330475,
    "total_throughput": 5522.885668507683,
    "itl": 31.04617519736355,
    "ttft": 65070.32432843628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1665,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.42745542570495,
    "arrivals": 43251,
    "finished_requests": 42843,
    "scheduler_time": 22.497174356026346
}
#Debug simulation 
Total elapsed time: 5.379124057944864. Arrivals time: 0.13795152120292187 Scheduler time: 4.917473892681301 Scheduler overhead time: 0.1259048073552549 Adapter cache time: 0.027237073983997107 Engine time: 0.11571152228862047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.286260122898966,
    "estimated_duration": 3600.0153448266633,
    "input_throughput": 2971.7595552402054,
    "output_throughput": 2553.225783664699,
    "total_throughput": 5524.985338904905,
    "itl": 31.047984759491754,
    "ttft": 64711.53541419196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1728,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.87844711526742,
    "arrivals": 43251,
    "finished_requests": 42833,
    "scheduler_time": 22.251466315485594
}
#Debug simulation 
Total elapsed time: 5.286363400053233. Arrivals time: 0.13155128993093967 Scheduler time: 4.8308840449899435 Scheduler overhead time: 0.12596372980624437 Adapter cache time: 0.027265028562396765 Engine time: 0.11579899955540895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.288902170024812,
    "estimated_duration": 3600.0132826579024,
    "input_throughput": 2969.0857118472795,
    "output_throughput": 2553.615300333764,
    "total_throughput": 5522.701012181044,
    "itl": 31.048723509923406,
    "ttft": 65754.89352243609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1719,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.973960096841102,
    "arrivals": 43251,
    "finished_requests": 42823,
    "scheduler_time": 22.28190172583924
}
#Debug simulation 
Total elapsed time: 5.28898049518466. Arrivals time: 0.13763810135424137 Scheduler time: 4.8285840675234795 Scheduler overhead time: 0.12582367612048984 Adapter cache time: 0.027184078004211187 Engine time: 0.11480120662599802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.303920170292258,
    "estimated_duration": 3600.024297950637,
    "input_throughput": 2972.5607702403163,
    "output_throughput": 2555.617195483205,
    "total_throughput": 5528.177965723521,
    "itl": 31.05820157676352,
    "ttft": 63690.452271931616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1709,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.62211707087214,
    "arrivals": 43251,
    "finished_requests": 42849,
    "scheduler_time": 22.329231736267484
}
#Debug simulation 
Total elapsed time: 5.304007259197533. Arrivals time: 0.13937212992459536 Scheduler time: 4.834826488047838 Scheduler overhead time: 0.1319723608903587 Adapter cache time: 0.02739924006164074 Engine time: 0.11526311887428164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.839342812076211,
    "estimated_duration": 3599.9700460387576,
    "input_throughput": 2833.413853324652,
    "output_throughput": 2446.9661934251444,
    "total_throughput": 5280.380046749796,
    "itl": 30.273371127028124,
    "ttft": 56294.51909980809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.849436985254602,
    "arrivals": 41341,
    "finished_requests": 40993,
    "scheduler_time": 19.34317139112648
}
#Debug simulation 
Total elapsed time: 4.839427601080388. Arrivals time: 0.11708665080368519 Scheduler time: 4.400137312244624 Scheduler overhead time: 0.12421605736017227 Adapter cache time: 0.027212458197027445 Engine time: 0.11568975495174527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.836254990193993,
    "estimated_duration": 3599.9908912321566,
    "input_throughput": 2833.103557245161,
    "output_throughput": 2445.9672999300797,
    "total_throughput": 5279.070857175241,
    "itl": 30.337298308566766,
    "ttft": 56367.57455249568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1806,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.21743792221862,
    "arrivals": 41341,
    "finished_requests": 40988,
    "scheduler_time": 19.263496656384206
}
#Debug simulation 
Total elapsed time: 4.836369352880865. Arrivals time: 0.13584335939958692 Scheduler time: 4.376431579235941 Scheduler overhead time: 0.12505272636190057 Adapter cache time: 0.027781197801232338 Engine time: 0.11575015773996711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.8380578849464655,
    "estimated_duration": 3599.988241019216,
    "input_throughput": 2833.1056428985594,
    "output_throughput": 2445.969100584348,
    "total_throughput": 5279.074743482907,
    "itl": 30.33996048578019,
    "ttft": 56370.011578041915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1806,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.577192740030348,
    "arrivals": 41341,
    "finished_requests": 40988,
    "scheduler_time": 19.266179165264575
}
#Debug simulation 
Total elapsed time: 4.838151718955487. Arrivals time: 0.12779284082353115 Scheduler time: 4.387483305763453 Scheduler overhead time: 0.12436625082045794 Adapter cache time: 0.027523028664290905 Engine time: 0.11584592889994383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.871984247118235,
    "estimated_duration": 3599.980324453716,
    "input_throughput": 2833.587153437552,
    "output_throughput": 2447.132819076401,
    "total_throughput": 5280.7199725139535,
    "itl": 30.339966219983026,
    "ttft": 55274.90575469341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.20796533223258,
    "arrivals": 41341,
    "finished_requests": 41005,
    "scheduler_time": 19.357079089640035
}
#Debug simulation 
Total elapsed time: 4.87206512875855. Arrivals time: 0.12421334395185113 Scheduler time: 4.425083674956113 Scheduler overhead time: 0.1251869359984994 Adapter cache time: 0.02742375945672393 Engine time: 0.11488064890727401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.823384975083172,
    "estimated_duration": 3599.9746699589828,
    "input_throughput": 2833.116323042408,
    "output_throughput": 2445.978321314224,
    "total_throughput": 5279.094644356632,
    "itl": 30.337249599650928,
    "ttft": 56370.45494469037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1806,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.448987207705848,
    "arrivals": 41341,
    "finished_requests": 40988,
    "scheduler_time": 19.265280073972566
}
#Debug simulation 
Total elapsed time: 4.823472787160426. Arrivals time: 0.1272230022586882 Scheduler time: 4.373256015125662 Scheduler overhead time: 0.12444037338718772 Adapter cache time: 0.027654970530420542 Engine time: 0.11578558059409261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.834092658013105,
    "estimated_duration": 3599.971426785275,
    "input_throughput": 2831.8583098043214,
    "output_throughput": 2445.7844122008837,
    "total_throughput": 5277.642722005205,
    "itl": 30.29687547361381,
    "ttft": 57641.10800322614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1805,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.522977297730241,
    "arrivals": 41341,
    "finished_requests": 40977,
    "scheduler_time": 19.312400996788483
}
#Debug simulation 
Total elapsed time: 4.834183997940272. Arrivals time: 0.13387297466397285 Scheduler time: 4.377656474709511 Scheduler overhead time: 0.12468771589919925 Adapter cache time: 0.027500228490680456 Engine time: 0.11531039653345942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.8275736081413925,
    "estimated_duration": 3599.9676343674414,
    "input_throughput": 2831.6001795919346,
    "output_throughput": 2445.2130946857083,
    "total_throughput": 5276.813274277643,
    "itl": 30.35940837099571,
    "ttft": 56998.79488359423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1799,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.268753870669526,
    "arrivals": 41341,
    "finished_requests": 40984,
    "scheduler_time": 19.314450414882042
}
#Debug simulation 
Total elapsed time: 4.8276671641506255. Arrivals time: 0.12462611217051744 Scheduler time: 4.380705708172172 Scheduler overhead time: 0.125984915997833 Adapter cache time: 0.027149620931595564 Engine time: 0.1139229703694582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.622724806889892,
    "estimated_duration": 3600.012484790904,
    "input_throughput": 2761.8306997558893,
    "output_throughput": 2398.5900150292214,
    "total_throughput": 5160.42071478511,
    "itl": 30.143969337654827,
    "ttft": 49288.912282604935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1735,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.472529670433444,
    "arrivals": 40385,
    "finished_requests": 40092,
    "scheduler_time": 17.869658205431374
}
#Debug simulation 
Total elapsed time: 4.622813499066979. Arrivals time: 0.12063374649733305 Scheduler time: 4.18452290026471 Scheduler overhead time: 0.12225360050797462 Adapter cache time: 0.026911166962236166 Engine time: 0.11336768977344036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.5644996128976345,
    "estimated_duration": 3600.0083671577654,
    "input_throughput": 2762.120246917813,
    "output_throughput": 2398.403592271877,
    "total_throughput": 5160.52383918969,
    "itl": 30.080205051445496,
    "ttft": 48721.8608606908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1810,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.237420722087643,
    "arrivals": 40385,
    "finished_requests": 40086,
    "scheduler_time": 17.6532325095756
}
#Debug simulation 
Total elapsed time: 4.564580574631691. Arrivals time: 0.13031526980921626 Scheduler time: 4.115391264669597 Scheduler overhead time: 0.12154820095747709 Adapter cache time: 0.027320175897330046 Engine time: 0.11495546763762832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.575555399060249,
    "estimated_duration": 3600.0192353722264,
    "input_throughput": 2762.3424625873654,
    "output_throughput": 2398.547184181142,
    "total_throughput": 5160.889646768507,
    "itl": 30.122368725279017,
    "ttft": 48889.14497055465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.39951912580966,
    "arrivals": 40385,
    "finished_requests": 40087,
    "scheduler_time": 17.713864650558172
}
#Debug simulation 
Total elapsed time: 4.5756524237804115. Arrivals time: 0.12385133374482393 Scheduler time: 4.135949457529932 Scheduler overhead time: 0.11998146306723356 Adapter cache time: 0.027054046280682087 Engine time: 0.11376262828707695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.554671201854944,
    "estimated_duration": 3600.00764274219,
    "input_throughput": 2762.516912998733,
    "output_throughput": 2399.1407399960685,
    "total_throughput": 5161.6576529948015,
    "itl": 30.05271874730044,
    "ttft": 47575.957264432225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.35200959423575,
    "arrivals": 40385,
    "finished_requests": 40100,
    "scheduler_time": 17.66732502130591
}
#Debug simulation 
Total elapsed time: 4.554788877721876. Arrivals time: 0.1215394176542759 Scheduler time: 4.112614413257688 Scheduler overhead time: 0.12174431839957833 Adapter cache time: 0.027217054273933172 Engine time: 0.11633610213175416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.5846831989474595,
    "estimated_duration": 3600.0018331052684,
    "input_throughput": 2763.5977594539963,
    "output_throughput": 2398.3421121073443,
    "total_throughput": 5161.939871561341,
    "itl": 30.08412959493129,
    "ttft": 46570.867504237234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1794,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.34710351498309,
    "arrivals": 40385,
    "finished_requests": 40112,
    "scheduler_time": 17.706693302667095
}
#Debug simulation 
Total elapsed time: 4.584798148833215. Arrivals time: 0.1299334247596562 Scheduler time: 4.1356382560916245 Scheduler overhead time: 0.1217261366546154 Adapter cache time: 0.02718307450413704 Engine time: 0.11481531336903572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.650484499055892,
    "estimated_duration": 3600.01110318518,
    "input_throughput": 2762.0325924001813,
    "output_throughput": 2398.3320474653206,
    "total_throughput": 5160.364639865502,
    "itl": 30.162710539254125,
    "ttft": 50646.54901772537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.999495780603388,
    "arrivals": 40385,
    "finished_requests": 40078,
    "scheduler_time": 17.87623498148584
}
#Debug simulation 
Total elapsed time: 4.6505976328626275. Arrivals time: 0.12670739134773612 Scheduler time: 4.204172928351909 Scheduler overhead time: 0.122563979588449 Adapter cache time: 0.027025924995541573 Engine time: 0.11465940624475479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.537850625813007,
    "estimated_duration": 3600.006899810316,
    "input_throughput": 2763.442203548049,
    "output_throughput": 2398.9442910387315,
    "total_throughput": 5162.38649458678,
    "itl": 30.09250536354334,
    "ttft": 47455.61408234798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.387148723732446,
    "arrivals": 40385,
    "finished_requests": 40101,
    "scheduler_time": 17.67054145240351
}
#Debug simulation 
Total elapsed time: 4.537928584031761. Arrivals time: 0.11896893661469221 Scheduler time: 4.10137732187286 Scheduler overhead time: 0.12060208711773157 Adapter cache time: 0.027149207424372435 Engine time: 0.1146253258921206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.458365468773991,
    "estimated_duration": 3600.0231568273157,
    "input_throughput": 2709.3031280931436,
    "output_throughput": 2391.617393813613,
    "total_throughput": 5100.920521906756,
    "itl": 30.03048143797781,
    "ttft": 49244.67175017896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1695,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.208033309155438,
    "arrivals": 39938,
    "finished_requests": 39621,
    "scheduler_time": 17.15979498921259
}
#Debug simulation 
Total elapsed time: 4.458445969969034. Arrivals time: 0.12384644709527493 Scheduler time: 4.0186961204744875 Scheduler overhead time: 0.11950101424008608 Adapter cache time: 0.026799719780683517 Engine time: 0.11459094425663352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.437924037221819,
    "estimated_duration": 3600.031505379231,
    "input_throughput": 2710.4507239505706,
    "output_throughput": 2392.33100797343,
    "total_throughput": 5102.781731924001,
    "itl": 30.04577928035302,
    "ttft": 47484.111811969444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1716,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.545709569435402,
    "arrivals": 39938,
    "finished_requests": 39638,
    "scheduler_time": 17.129888924814896
}
#Debug simulation 
Total elapsed time: 4.438010604120791. Arrivals time: 0.12427678983658552 Scheduler time: 3.9984484231099486 Scheduler overhead time: 0.11952948290854692 Adapter cache time: 0.026862510479986668 Engine time: 0.11377755971625447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.515970750711858,
    "estimated_duration": 3600.0024732569223,
    "input_throughput": 2709.6064717908434,
    "output_throughput": 2392.0491899577173,
    "total_throughput": 5101.655661748561,
    "itl": 30.060038197470522,
    "ttft": 48885.73138319759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1697,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.736203289376377,
    "arrivals": 39938,
    "finished_requests": 39623,
    "scheduler_time": 17.136867930331523
}
#Debug simulation 
Total elapsed time: 4.516053081024438. Arrivals time: 0.11693716514855623 Scheduler time: 4.084047789685428 Scheduler overhead time: 0.11836688546463847 Adapter cache time: 0.026380972005426884 Engine time: 0.11516301101073623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.475389609113336,
    "estimated_duration": 3600.0151961871843,
    "input_throughput": 2710.5649471521356,
    "output_throughput": 2391.8246259403536,
    "total_throughput": 5102.389573092489,
    "itl": 30.018583321752665,
    "ttft": 48099.1184919995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.66808006118069,
    "arrivals": 39938,
    "finished_requests": 39632,
    "scheduler_time": 17.13488894377515
}
#Debug simulation 
Total elapsed time: 4.475474669132382. Arrivals time: 0.12114063324406743 Scheduler time: 4.038212004583329 Scheduler overhead time: 0.11939080338925123 Adapter cache time: 0.026733269914984703 Engine time: 0.11503506638109684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.458678147289902,
    "estimated_duration": 3600.0053094575446,
    "input_throughput": 2711.0548904914326,
    "output_throughput": 2393.012304000769,
    "total_throughput": 5104.067194492201,
    "itl": 30.04093192060819,
    "ttft": 46865.81161265261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1700,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.631552756661357,
    "arrivals": 39938,
    "finished_requests": 39646,
    "scheduler_time": 17.155866621878204
}
#Debug simulation 
Total elapsed time: 4.458796862047166. Arrivals time: 0.12853015074506402 Scheduler time: 4.013900704216212 Scheduler overhead time: 0.11951176635921001 Adapter cache time: 0.026491586584597826 Engine time: 0.11499621532857418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.47317612497136,
    "estimated_duration": 3600.0137369479553,
    "input_throughput": 2711.70326374261,
    "output_throughput": 2392.9889243432467,
    "total_throughput": 5104.692188085857,
    "itl": 30.000781942705483,
    "ttft": 46766.08179320764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.756906784861675,
    "arrivals": 39938,
    "finished_requests": 39650,
    "scheduler_time": 17.190012579243266
}
#Debug simulation 
Total elapsed time: 4.47327498998493. Arrivals time: 0.11685682833194733 Scheduler time: 4.0411503235809505 Scheduler overhead time: 0.11905746581032872 Adapter cache time: 0.02661523036658764 Engine time: 0.11394129926338792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.461301392875612,
    "estimated_duration": 3600.0309811005345,
    "input_throughput": 2709.5775150851664,
    "output_throughput": 2391.726639354593,
    "total_throughput": 5101.304154439759,
    "itl": 30.0546885361379,
    "ttft": 49084.43706543653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1699,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.51940048141386,
    "arrivals": 39938,
    "finished_requests": 39622,
    "scheduler_time": 17.160274465302532
}
#Debug simulation 
Total elapsed time: 4.461384130176157. Arrivals time: 0.12012335658073425 Scheduler time: 4.0238578747957945 Scheduler overhead time: 0.12027552863582969 Adapter cache time: 0.026604069862514734 Engine time: 0.11507148947566748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.289545595180243,
    "estimated_duration": 3599.7949506190034,
    "input_throughput": 2727.0139368110313,
    "output_throughput": 2341.125846223779,
    "total_throughput": 5068.13978303481,
    "itl": 29.66699077682361,
    "ttft": 44947.19974933835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.91047490271768,
    "arrivals": 39719,
    "finished_requests": 39423,
    "scheduler_time": 15.764360828955091
}
#Debug simulation 
Total elapsed time: 4.289639552123845. Arrivals time: 0.12750636925920844 Scheduler time: 3.847419979516417 Scheduler overhead time: 0.11826538108289242 Adapter cache time: 0.026594568509608507 Engine time: 0.11456032004207373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.232034285087138,
    "estimated_duration": 3599.796395799783,
    "input_throughput": 2726.1175136044594,
    "output_throughput": 2339.872335509856,
    "total_throughput": 5065.989849114316,
    "itl": 29.673536680124705,
    "ttft": 46498.23499436662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.019980361237998,
    "arrivals": 39719,
    "finished_requests": 39406,
    "scheduler_time": 15.748095170948536
}
#Debug simulation 
Total elapsed time: 4.232139662373811. Arrivals time: 0.11549796489998698 Scheduler time: 3.8034594976343215 Scheduler overhead time: 0.11833070823922753 Adapter cache time: 0.02632768265902996 Engine time: 0.11310263676568866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.248891352675855,
    "estimated_duration": 3599.774739657242,
    "input_throughput": 2726.562829577095,
    "output_throughput": 2340.2556018830614,
    "total_throughput": 5066.818431460156,
    "itl": 29.685240881756357,
    "ttft": 45699.47870852579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.303512939987543,
    "arrivals": 39719,
    "finished_requests": 39415,
    "scheduler_time": 15.758605531331812
}
#Debug simulation 
Total elapsed time: 4.249127916991711. Arrivals time: 0.11785596469417214 Scheduler time: 3.8195924228057265 Scheduler overhead time: 0.11700178822502494 Adapter cache time: 0.026158802676945925 Engine time: 0.11306896060705185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.269602737855166,
    "estimated_duration": 3599.795166227481,
    "input_throughput": 2726.118444757048,
    "output_throughput": 2339.8731347337234,
    "total_throughput": 5065.991579490771,
    "itl": 29.66576675485951,
    "ttft": 46497.299674090376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.149565810971142,
    "arrivals": 39719,
    "finished_requests": 39406,
    "scheduler_time": 15.742074506098573
}
#Debug simulation 
Total elapsed time: 4.2697056867182255. Arrivals time: 0.12608172046020627 Scheduler time: 3.8306581964716315 Scheduler overhead time: 0.11741389986127615 Adapter cache time: 0.026363196782767773 Engine time: 0.11398076312616467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.28253680607304,
    "estimated_duration": 3599.778841460955,
    "input_throughput": 2724.5334871793343,
    "output_throughput": 2339.0656400943594,
    "total_throughput": 5063.599127273694,
    "itl": 29.671867895447953,
    "ttft": 47690.70700170873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1632,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.08564213184625,
    "arrivals": 39719,
    "finished_requests": 39396,
    "scheduler_time": 15.797222921598598
}
#Debug simulation 
Total elapsed time: 4.282636791002005. Arrivals time: 0.1156312208622694 Scheduler time: 3.8491057041101158 Scheduler overhead time: 0.12014725804328918 Adapter cache time: 0.026421838905662298 Engine time: 0.11503331456333399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.261098484043032,
    "estimated_duration": 3599.7981180727634,
    "input_throughput": 2726.1162093317253,
    "output_throughput": 2339.871216030716,
    "total_throughput": 5065.987425362441,
    "itl": 29.657994483407066,
    "ttft": 46492.648630987635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.520701710060534,
    "arrivals": 39719,
    "finished_requests": 39406,
    "scheduler_time": 15.737218739215628
}
#Debug simulation 
Total elapsed time: 4.261247407179326. Arrivals time: 0.11841040896251798 Scheduler time: 3.830138224642724 Scheduler overhead time: 0.11742535792291164 Adapter cache time: 0.02628149650990963 Engine time: 0.11364523228257895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.304764974862337,
    "estimated_duration": 3599.7951899496766,
    "input_throughput": 2726.1184267922717,
    "output_throughput": 2339.873119314255,
    "total_throughput": 5065.991546106527,
    "itl": 29.67474631386787,
    "ttft": 46498.89949565915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.115080336928218,
    "arrivals": 39719,
    "finished_requests": 39406,
    "scheduler_time": 15.748848005951968
}
#Debug simulation 
Total elapsed time: 4.304849594831467. Arrivals time: 0.12781023513525724 Scheduler time: 3.8618348664604127 Scheduler overhead time: 0.11906201532110572 Adapter cache time: 0.026315368711948395 Engine time: 0.11433105496689677 
