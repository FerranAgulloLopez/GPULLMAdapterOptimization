INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 13.854749807855114,
    "estimated_duration": 3600.055191911191,
    "input_throughput": 6925.61715609805,
    "output_throughput": 6051.8641627918305,
    "total_throughput": 12977.48131888988,
    "itl": 99.58774868262277,
    "ttft": 1364520.0520489914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24465913418214755,
    "arrivals": 186986,
    "finished_requests": 100962,
    "scheduler_time": 178.75937567765382
}
#Debug simulation 
Total elapsed time: 13.854953277856112. Arrivals time: 0.42166404286399484 Scheduler time: 13.279927480500191 Scheduler overhead time: 0.05854600598104298 Adapter cache time: 0.009635842172428966 Engine time: 0.05861364468000829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 13.567840083036572,
    "estimated_duration": 3600.0488976961387,
    "input_throughput": 6853.919405314322,
    "output_throughput": 5988.910876682153,
    "total_throughput": 12842.830281996476,
    "itl": 96.90134627052485,
    "ttft": 1378494.4435497173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26674593621399256,
    "arrivals": 186986,
    "finished_requests": 99889,
    "scheduler_time": 180.56027608370087
}
#Debug simulation 
Total elapsed time: 13.567974888952449. Arrivals time: 0.405264726607129 Scheduler time: 13.006042564054951 Scheduler overhead time: 0.06013084412552416 Adapter cache time: 0.009880098281428218 Engine time: 0.05952873034402728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 13.038473821943626,
    "estimated_duration": 3600.072565535333,
    "input_throughput": 6664.478719037233,
    "output_throughput": 5823.747887949129,
    "total_throughput": 12488.226606986362,
    "itl": 90.59697952187095,
    "ttft": 1412304.1168666861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27318261890672146,
    "arrivals": 186986,
    "finished_requests": 97069,
    "scheduler_time": 185.26183410289192
}
#Debug simulation 
Total elapsed time: 13.03862681495957. Arrivals time: 0.4021144558209926 Scheduler time: 12.471093992702663 Scheduler overhead time: 0.06348700961098075 Adapter cache time: 0.010384266264736652 Engine time: 0.06283897976391017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 13.647081694100052,
    "estimated_duration": 3600.0797076276076,
    "input_throughput": 6854.061855275011,
    "output_throughput": 5989.120450393734,
    "total_throughput": 12843.182305668744,
    "itl": 96.90404457170962,
    "ttft": 1378483.5700124386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2514755055075511,
    "arrivals": 186986,
    "finished_requests": 99893,
    "scheduler_time": 180.56061063223123
}
#Debug simulation 
Total elapsed time: 13.647204954177141. Arrivals time: 0.4364251692313701 Scheduler time: 13.053660130361095 Scheduler overhead time: 0.060031915083527565 Adapter cache time: 0.009897074429318309 Engine time: 0.060072166845202446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 12.999799231998622,
    "estimated_duration": 3600.0122628407184,
    "input_throughput": 6664.483131806912,
    "output_throughput": 5823.676551439459,
    "total_throughput": 12488.159683246371,
    "itl": 90.59486511394317,
    "ttft": 1412324.9281725872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27090432996395974,
    "arrivals": 186986,
    "finished_requests": 97067,
    "scheduler_time": 185.26056506092667
}
#Debug simulation 
Total elapsed time: 12.999914621002972. Arrivals time: 0.40816736803390086 Scheduler time: 12.426381981465966 Scheduler overhead time: 0.06352482852526009 Adapter cache time: 0.010267703095450997 Engine time: 0.06291671749204397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 13.629287136020139,
    "estimated_duration": 3600.0747245796088,
    "input_throughput": 6854.053564924651,
    "output_throughput": 5989.087629984614,
    "total_throughput": 12843.141194909265,
    "itl": 96.90183208861986,
    "ttft": 1378512.8571518068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23620507480110967,
    "arrivals": 186986,
    "finished_requests": 99892,
    "scheduler_time": 180.56154595525086
}
#Debug simulation 
Total elapsed time: 13.629407760920003. Arrivals time: 0.43441806244663894 Scheduler time: 13.037873550551012 Scheduler overhead time: 0.05995857063680887 Adapter cache time: 0.009977129055187106 Engine time: 0.05998528120107949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 13.052312792045996,
    "estimated_duration": 3600.020435851159,
    "input_throughput": 6664.334669069063,
    "output_throughput": 5823.595830517333,
    "total_throughput": 12487.930499586395,
    "itl": 90.59526406958241,
    "ttft": 1412393.5093857823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.268626041021198,
    "arrivals": 186986,
    "finished_requests": 97065,
    "scheduler_time": 185.26106169986934
}
#Debug simulation 
Total elapsed time: 13.05244222492911. Arrivals time: 0.40152011439204216 Scheduler time: 12.48531540716067 Scheduler overhead time: 0.0634171711280942 Adapter cache time: 0.010271227452903986 Engine time: 0.0630283139180392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 11.819913452025503,
    "estimated_duration": 3600.0041524606504,
    "input_throughput": 6913.103414891693,
    "output_throughput": 6054.076072413149,
    "total_throughput": 12967.179487304842,
    "itl": 99.57958961842101,
    "ttft": 1370717.3885955485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21820949805434783,
    "arrivals": 186004,
    "finished_requests": 100722,
    "scheduler_time": 178.74126686530366
}
#Debug simulation 
Total elapsed time: 11.8200267709326. Arrivals time: 0.4347901220899075 Scheduler time: 11.234535265248269 Scheduler overhead time: 0.05736030824482441 Adapter cache time: 0.009496862534433603 Engine time: 0.057545802323147655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 11.65083970897831,
    "estimated_duration": 3600.0448667313635,
    "input_throughput": 6837.0947894179935,
    "output_throughput": 5989.128135388013,
    "total_throughput": 12826.222924806007,
    "itl": 96.91475180736273,
    "ttft": 1383471.0156606515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2384338105050847,
    "arrivals": 186004,
    "finished_requests": 99611,
    "scheduler_time": 180.54365407424416
}
#Debug simulation 
Total elapsed time: 11.650998655939475. Arrivals time: 0.4392694439738989 Scheduler time: 11.056816340656951 Scheduler overhead time: 0.05919860419817269 Adapter cache time: 0.009653362911194563 Engine time: 0.05895618908107281 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 11.229082521051168,
    "estimated_duration": 3600.02785943405,
    "input_throughput": 6647.952442168715,
    "output_throughput": 5823.785764617936,
    "total_throughput": 12471.738206786651,
    "itl": 90.61620642333247,
    "ttft": 1417693.9908296224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2442959382012487,
    "arrivals": 186004,
    "finished_requests": 96799,
    "scheduler_time": 185.24214065722344
}
#Debug simulation 
Total elapsed time: 11.229188146069646. Arrivals time: 0.39787158998660743 Scheduler time: 10.66938323364593 Scheduler overhead time: 0.061882082372903824 Adapter cache time: 0.010151935508474708 Engine time: 0.06168582336977124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 11.661467480938882,
    "estimated_duration": 3600.068355884146,
    "input_throughput": 6837.113511961356,
    "output_throughput": 5989.244888852849,
    "total_throughput": 12826.358400814204,
    "itl": 96.91389778408535,
    "ttft": 1383523.940965054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22593982174526897,
    "arrivals": 186004,
    "finished_requests": 99611,
    "scheduler_time": 180.54650469092243
}
#Debug simulation 
Total elapsed time: 11.66167965810746. Arrivals time: 0.4124119863845408 Scheduler time: 11.094779742415994 Scheduler overhead time: 0.05862185079604387 Adapter cache time: 0.009710415499284863 Engine time: 0.0590484777931124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 11.676031202077866,
    "estimated_duration": 3600.0213161714996,
    "input_throughput": 6647.8650813799495,
    "output_throughput": 5823.617184104821,
    "total_throughput": 12471.48226548477,
    "itl": 90.61586673719643,
    "ttft": 1417767.3604026835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2424318836117164,
    "arrivals": 186004,
    "finished_requests": 96796,
    "scheduler_time": 185.2417464446245
}
#Debug simulation 
Total elapsed time: 11.67614401713945. Arrivals time: 0.38534314557909966 Scheduler time: 11.127221197355539 Scheduler overhead time: 0.06199448392726481 Adapter cache time: 0.010313824284821749 Engine time: 0.06272410112433136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 11.813085889909416,
    "estimated_duration": 3600.068262193389,
    "input_throughput": 6837.050079990342,
    "output_throughput": 5989.022548940154,
    "total_throughput": 12826.072628930497,
    "itl": 96.91218872166209,
    "ttft": 1383516.3390698137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21066939103882756,
    "arrivals": 186004,
    "finished_requests": 99610,
    "scheduler_time": 180.54660491400583
}
#Debug simulation 
Total elapsed time: 11.81319363694638. Arrivals time: 0.4984719487838447 Scheduler time: 11.158710718154907 Scheduler overhead time: 0.05946944863535464 Adapter cache time: 0.009770577773451805 Engine time: 0.059768034145236015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 11.323528780136257,
    "estimated_duration": 3600.0717652929984,
    "input_throughput": 6647.737756431148,
    "output_throughput": 5823.673906204387,
    "total_throughput": 12471.411662635535,
    "itl": 90.61524320235259,
    "ttft": 1417690.9223240926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24015359466895467,
    "arrivals": 186004,
    "finished_requests": 96797,
    "scheduler_time": 185.24493350176812
}
#Debug simulation 
Total elapsed time: 11.323641530005261. Arrivals time: 0.41018000547774136 Scheduler time: 10.750274679390714 Scheduler overhead time: 0.06188966450281441 Adapter cache time: 0.010404004016891122 Engine time: 0.06251253141090274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 11.114985791966319,
    "estimated_duration": 3600.008366696328,
    "input_throughput": 6932.215000072838,
    "output_throughput": 6055.083983041965,
    "total_throughput": 12987.298983114802,
    "itl": 99.53536858972538,
    "ttft": 1363625.4774282747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2314343161182477,
    "arrivals": 185665,
    "finished_requests": 101161,
    "scheduler_time": 178.60826102236055
}
#Debug simulation 
Total elapsed time: 11.115246276836842. Arrivals time: 0.42551113688386977 Scheduler time: 10.538879541680217 Scheduler overhead time: 0.05750960251316428 Adapter cache time: 0.009416601154953241 Engine time: 0.05750018730759621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.746797133004293,
    "estimated_duration": 3600.0876475162154,
    "input_throughput": 6852.144562930086,
    "output_throughput": 5991.770510054739,
    "total_throughput": 12843.915072984826,
    "itl": 96.88317052358344,
    "ttft": 1375473.4592499915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2539780943328515,
    "arrivals": 185665,
    "finished_requests": 100078,
    "scheduler_time": 180.4147621915072
}
#Debug simulation 
Total elapsed time: 10.746904578991234. Arrivals time: 0.300942953908816 Scheduler time: 10.293075760127977 Scheduler overhead time: 0.058068119222298265 Adapter cache time: 0.009544217260554433 Engine time: 0.058510612696409225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.387990257935598,
    "estimated_duration": 3600.026547152055,
    "input_throughput": 6661.814485499414,
    "output_throughput": 5828.048411642398,
    "total_throughput": 12489.862897141811,
    "itl": 90.59661517714724,
    "ttft": 1410034.7698358695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26044808494858446,
    "arrivals": 185665,
    "finished_requests": 97328,
    "scheduler_time": 185.0740770233652
}
#Debug simulation 
Total elapsed time: 10.388103442033753. Arrivals time: 0.2745955535210669 Scheduler time: 9.952129843411967 Scheduler overhead time: 0.06140271807089448 Adapter cache time: 0.010168081847950816 Engine time: 0.0615991975646466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 10.662220699014142,
    "estimated_duration": 3600.0635854295447,
    "input_throughput": 6851.94730999641,
    "output_throughput": 5991.708892948982,
    "total_throughput": 12843.656202945393,
    "itl": 96.88060531345126,
    "ttft": 1375464.190190148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24148410557303582,
    "arrivals": 185665,
    "finished_requests": 100076,
    "scheduler_time": 180.41589501225505
}
#Debug simulation 
Total elapsed time: 10.66232560412027. Arrivals time: 0.2854304069187492 Scheduler time: 10.224409913877025 Scheduler overhead time: 0.05771211790852249 Adapter cache time: 0.009527930291369557 Engine time: 0.05881099007092416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 10.312923145946115,
    "estimated_duration": 3600.0304460545735,
    "input_throughput": 6661.598383503352,
    "output_throughput": 5827.63793650482,
    "total_throughput": 12489.236320008173,
    "itl": 90.59794052173224,
    "ttft": 1410206.4300410075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25858403035905214,
    "arrivals": 185665,
    "finished_requests": 97323,
    "scheduler_time": 185.07234577804508
}
#Debug simulation 
Total elapsed time: 10.31302844104357. Arrivals time: 0.26917569944635034 Scheduler time: 9.88298805989325 Scheduler overhead time: 0.061397308483719826 Adapter cache time: 0.01004043803550303 Engine time: 0.061365710804238915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 10.731429303064942,
    "estimated_duration": 3600.071781741287,
    "input_throughput": 6852.050041087962,
    "output_throughput": 5991.748861620377,
    "total_throughput": 12843.79890270834,
    "itl": 96.88028923255759,
    "ttft": 1375492.824028834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22343723291996861,
    "arrivals": 185665,
    "finished_requests": 100077,
    "scheduler_time": 180.41668278226757
}
#Debug simulation 
Total elapsed time: 10.731535739963874. Arrivals time: 0.2742121145129204 Scheduler time: 10.304242530604824 Scheduler overhead time: 0.05816423334181309 Adapter cache time: 0.009615212446078658 Engine time: 0.058642094489187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.330976584926248,
    "estimated_duration": 3600.0593903049644,
    "input_throughput": 6661.628434404367,
    "output_throughput": 5827.913299569954,
    "total_throughput": 12489.54173397432,
    "itl": 90.5976356180021,
    "ttft": 1410099.7237371013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.255891507063061,
    "arrivals": 185665,
    "finished_requests": 97326,
    "scheduler_time": 185.07601793527203
}
#Debug simulation 
Total elapsed time: 10.331086786929518. Arrivals time: 0.28302341094240546 Scheduler time: 9.887009797617793 Scheduler overhead time: 0.061348465736955404 Adapter cache time: 0.010123067302629352 Engine time: 0.061265533324331045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.551622326951474,
    "estimated_duration": 3600.0704286159134,
    "input_throughput": 6957.33889006987,
    "output_throughput": 6058.604250246746,
    "total_throughput": 13015.943140316616,
    "itl": 99.68149641753044,
    "ttft": 1355740.314427959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 184921,
    "finished_requests": 101465,
    "scheduler_time": 178.06963725523144
}
#Debug simulation 
Total elapsed time: 9.551732979016379. Arrivals time: 0.27811961411498487 Scheduler time: 9.125248348806053 Scheduler overhead time: 0.05650000600144267 Adapter cache time: 0.009330360684543848 Engine time: 0.05663945432752371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.417619773885235,
    "estimated_duration": 3600.0600901659404,
    "input_throughput": 6888.751126056031,
    "output_throughput": 5995.133542064062,
    "total_throughput": 12883.884668120094,
    "itl": 97.00080810531297,
    "ttft": 1368827.0030549655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2306616685912013,
    "arrivals": 184921,
    "finished_requests": 100418,
    "scheduler_time": 179.86853961373015
}
#Debug simulation 
Total elapsed time: 9.417728581931442. Arrivals time: 0.2752662026323378 Scheduler time: 8.991490842076018 Scheduler overhead time: 0.05720497854053974 Adapter cache time: 0.009505444904789329 Engine time: 0.057810823898762465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.253129266900942,
    "estimated_duration": 3600.01958517545,
    "input_throughput": 6696.523568725278,
    "output_throughput": 5831.0385550241235,
    "total_throughput": 12527.562123749402,
    "itl": 90.75289574476889,
    "ttft": 1402592.529351992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2362198648275808,
    "arrivals": 184921,
    "finished_requests": 97615,
    "scheduler_time": 184.515423592541
}
#Debug simulation 
Total elapsed time: 9.253205996006727. Arrivals time: 0.27133304881863296 Scheduler time: 8.821364644914865 Scheduler overhead time: 0.06103699002414942 Adapter cache time: 0.010121976491063833 Engine time: 0.06113461614586413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 9.484012397006154,
    "estimated_duration": 3600.0890247914363,
    "input_throughput": 6888.370490070496,
    "output_throughput": 5994.9828605281045,
    "total_throughput": 12883.353350598602,
    "itl": 97.00102648284283,
    "ttft": 1368785.0074223925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2181676798313856,
    "arrivals": 184921,
    "finished_requests": 100415,
    "scheduler_time": 179.87084320144297
}
#Debug simulation 
Total elapsed time: 9.484086900018156. Arrivals time: 0.277158371405676 Scheduler time: 9.055350170936435 Scheduler overhead time: 0.05744091095402837 Adapter cache time: 0.009600409772247076 Engine time: 0.05796938925050199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 9.202879288932309,
    "estimated_duration": 3600.0717569708536,
    "input_throughput": 6696.509855204861,
    "output_throughput": 5830.992384902608,
    "total_throughput": 12527.50224010747,
    "itl": 90.7515763397356,
    "ttft": 1402557.1352860944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2343558102380485,
    "arrivals": 184921,
    "finished_requests": 97615,
    "scheduler_time": 184.5186017684078
}
#Debug simulation 
Total elapsed time: 9.202953346073627. Arrivals time: 0.2678047812078148 Scheduler time: 8.774785103276372 Scheduler overhead time: 0.061098410515114665 Adapter cache time: 0.010075657395645976 Engine time: 0.06114402576349676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.482167836045846,
    "estimated_duration": 3600.0639200136775,
    "input_throughput": 6888.418525609341,
    "output_throughput": 5995.024666094819,
    "total_throughput": 12883.443191704158,
    "itl": 97.0011291896824,
    "ttft": 1368819.023314502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 184921,
    "finished_requests": 100415,
    "scheduler_time": 179.86925244254772
}
#Debug simulation 
Total elapsed time: 9.482244597980753. Arrivals time: 0.2722915669437498 Scheduler time: 9.058246016036719 Scheduler overhead time: 0.057615299709141254 Adapter cache time: 0.009570559486746788 Engine time: 0.05793136730790138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.206770761171356,
    "estimated_duration": 3600.0018825167945,
    "input_throughput": 6696.640109295146,
    "output_throughput": 5831.1322285543465,
    "total_throughput": 12527.772337849494,
    "itl": 90.75029388316408,
    "ttft": 1402521.639145747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23228463847190148,
    "arrivals": 184921,
    "finished_requests": 97616,
    "scheduler_time": 184.51524301228415
}
#Debug simulation 
Total elapsed time: 9.20684269699268. Arrivals time: 0.26799766696058214 Scheduler time: 8.77824622252956 Scheduler overhead time: 0.061062929686158895 Adapter cache time: 0.010056457482278347 Engine time: 0.06145186326466501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 81.28635199996643,
    "estimated_duration": 3600.05907194299,
    "input_throughput": 6541.626825942521,
    "output_throughput": 5774.058309765751,
    "total_throughput": 12315.685135708272,
    "itl": 92.39358041245269,
    "ttft": 1266622.209232018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0712102631758886,
    "arrivals": 149510,
    "finished_requests": 95847,
    "scheduler_time": 174.37783809104738
}
#Debug simulation 
Total elapsed time: 81.28650276502594. Arrivals time: 0.370188822504133 Scheduler time: 80.70182231953368 Scheduler overhead time: 0.08352201734669507 Adapter cache time: 0.0158919848036021 Engine time: 0.08210152853280306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 100.69092407403514,
    "estimated_duration": 3600.0853798816465,
    "input_throughput": 6355.800650692409,
    "output_throughput": 5604.583189264062,
    "total_throughput": 11960.383839956472,
    "itl": 87.15550355624467,
    "ttft": 1354703.5227171779,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2160426621930684,
    "arrivals": 149510,
    "finished_requests": 93086,
    "scheduler_time": 179.6222459339517
}
#Debug simulation 
Total elapsed time: 100.69106775196269. Arrivals time: 0.38468550611287355 Scheduler time: 100.07446534605697 Scheduler overhead time: 0.09181673475541174 Adapter cache time: 0.017156047746539116 Engine time: 0.08809539559297264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 99.87357575306669,
    "estimated_duration": 3600.005163180021,
    "input_throughput": 6243.630489725499,
    "output_throughput": 5505.238493184056,
    "total_throughput": 11748.868982909555,
    "itl": 82.79618587334589,
    "ttft": 1353009.8583597243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3137341109057903,
    "arrivals": 149510,
    "finished_requests": 91432,
    "scheduler_time": 182.9780029791566
}
#Debug simulation 
Total elapsed time: 99.8737244810909. Arrivals time: 0.3805606074165553 Scheduler time: 99.25593661167659 Scheduler overhead time: 0.09224096336401999 Adapter cache time: 0.017881079809740186 Engine time: 0.09090616041794419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 132.8471629000269,
    "estimated_duration": 3600.0158938570485,
    "input_throughput": 6379.899610774334,
    "output_throughput": 5615.544096484689,
    "total_throughput": 11995.443707259023,
    "itl": 87.14679472901297,
    "ttft": 1352980.3132441696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1538465715153137,
    "arrivals": 149510,
    "finished_requests": 93368,
    "scheduler_time": 179.87175563913547
}
#Debug simulation 
Total elapsed time: 132.847376849968. Arrivals time: 0.7374138871673495 Scheduler time: 131.77896068408154 Scheduler overhead time: 0.13080934341996908 Adapter cache time: 0.025808501057326794 Engine time: 0.13265432347543538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 103.1047670179978,
    "estimated_duration": 3600.00425926762,
    "input_throughput": 6369.29218652223,
    "output_throughput": 5617.623353621874,
    "total_throughput": 11986.915540144104,
    "itl": 85.09416637438596,
    "ttft": 1306727.2550209642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6140934663591933,
    "arrivals": 149510,
    "finished_requests": 93325,
    "scheduler_time": 180.15621907007858
}
#Debug simulation 
Total elapsed time: 103.10494860797189. Arrivals time: 0.6674799893517047 Scheduler time: 102.14048066013493 Scheduler overhead time: 0.11782823409885168 Adapter cache time: 0.023191457148641348 Engine time: 0.11701684724539518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 133.0836729381699,
    "estimated_duration": 3600.1036311796083,
    "input_throughput": 6381.711293258544,
    "output_throughput": 5616.594984898981,
    "total_throughput": 11998.306278157526,
    "itl": 87.11695644473627,
    "ttft": 1352292.5708571537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0661147970752767,
    "arrivals": 149510,
    "finished_requests": 93369,
    "scheduler_time": 179.8993395373652
}
#Debug simulation 
Total elapsed time: 133.0838850911241. Arrivals time: 0.693369145039469 Scheduler time: 132.0635065047536 Scheduler overhead time: 0.13046377780847251 Adapter cache time: 0.02497304230928421 Engine time: 0.13070421083830297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.51041738805361,
    "estimated_duration": 3600.0398901726435,
    "input_throughput": 6330.572908987147,
    "output_throughput": 5593.860794424213,
    "total_throughput": 11924.43370341136,
    "itl": 85.25772045615726,
    "ttft": 1278354.9686478241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 163,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1940401962585754,
    "arrivals": 149510,
    "finished_requests": 92777,
    "scheduler_time": 179.39485269710698
}
#Debug simulation 
Total elapsed time: 109.51060852920637. Arrivals time: 0.6799843383487314 Scheduler time: 108.53342121094465 Scheduler overhead time: 0.1181499536614865 Adapter cache time: 0.0215821189340204 Engine time: 0.11798533424735069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 110.1356590280775,
    "estimated_duration": 3600.002603000204,
    "input_throughput": 6552.430262228525,
    "output_throughput": 5722.357251306363,
    "total_throughput": 12274.787513534888,
    "itl": 91.92655743097042,
    "ttft": 1262432.8328549957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6398774399235825,
    "arrivals": 143761,
    "finished_requests": 95346,
    "scheduler_time": 171.43975714840866
}
#Debug simulation 
Total elapsed time: 110.13585042301565. Arrivals time: 0.6521544172428548 Scheduler time: 109.19117174949497 Scheduler overhead time: 0.11519480240531266 Adapter cache time: 0.02367655630223453 Engine time: 0.11610843846574426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 111.75972579605877,
    "estimated_duration": 3600.0947268556683,
    "input_throughput": 6490.7536531432,
    "output_throughput": 5655.753124525021,
    "total_throughput": 12146.50677766822,
    "itl": 89.41973980929123,
    "ttft": 1224904.4614308684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3106965861329822,
    "arrivals": 143761,
    "finished_requests": 94316,
    "scheduler_time": 173.4943417797905
}
#Debug simulation 
Total elapsed time: 111.7599344230257. Arrivals time: 0.6374775711447 Scheduler time: 110.83570762025192 Scheduler overhead time: 0.1133276354521513 Adapter cache time: 0.021619702456519008 Engine time: 0.1137485015206039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 98.6067900469061,
    "estimated_duration": 3600.025408587652,
    "input_throughput": 6354.51459465526,
    "output_throughput": 5538.64674189133,
    "total_throughput": 11893.16133654659,
    "itl": 84.45475496368931,
    "ttft": 1262848.0153854042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3707345831207953,
    "arrivals": 143761,
    "finished_requests": 92310,
    "scheduler_time": 177.31382567207004
}
#Debug simulation 
Total elapsed time: 98.60698446398601. Arrivals time: 0.5674954482819885 Scheduler time: 97.76587028382346 Scheduler overhead time: 0.10869986796751618 Adapter cache time: 0.020484609296545386 Engine time: 0.10721571324393153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 92.14726325799711,
    "estimated_duration": 3600.0330268795115,
    "input_throughput": 6489.471020284033,
    "output_throughput": 5662.858048185986,
    "total_throughput": 12152.32906847002,
    "itl": 89.63506914561925,
    "ttft": 1295047.6848157055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9965430910373083,
    "arrivals": 143761,
    "finished_requests": 94375,
    "scheduler_time": 173.19070099989216
}
#Debug simulation 
Total elapsed time: 92.14743954897858. Arrivals time: 0.5671281118411571 Scheduler time: 91.31540671898983 Scheduler overhead time: 0.10345150507055223 Adapter cache time: 0.02076629246585071 Engine time: 0.10433857911266387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 93.99856717698276,
    "estimated_duration": 3600.048561296994,
    "input_throughput": 6347.3921562234345,
    "output_throughput": 5540.933590299527,
    "total_throughput": 11888.325746522962,
    "itl": 84.42957831202646,
    "ttft": 1308817.3102132508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6921635320643,
    "arrivals": 143761,
    "finished_requests": 92335,
    "scheduler_time": 177.19166471429415
}
#Debug simulation 
Total elapsed time: 93.99875659891404. Arrivals time: 0.5734320250339806 Scheduler time: 93.15446623251773 Scheduler overhead time: 0.10626756865531206 Adapter cache time: 0.021149422973394394 Engine time: 0.10554963955655694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 108.5745987079572,
    "estimated_duration": 3600.101045065331,
    "input_throughput": 6506.5815394564615,
    "output_throughput": 5673.449368316091,
    "total_throughput": 12180.030907772552,
    "itl": 89.81589948970633,
    "ttft": 1228265.3495442912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.340623397519808,
    "arrivals": 143761,
    "finished_requests": 94667,
    "scheduler_time": 172.89924822470536
}
#Debug simulation 
Total elapsed time: 108.57480895495974. Arrivals time: 0.6093424963764846 Scheduler time: 107.68500020820647 Scheduler overhead time: 0.11103446036577225 Adapter cache time: 0.020750292809680104 Engine time: 0.11062372592277825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 96.78752151899971,
    "estimated_duration": 3600.0227732113754,
    "input_throughput": 6349.338168105501,
    "output_throughput": 5539.706067528547,
    "total_throughput": 11889.044235634048,
    "itl": 84.4917641133536,
    "ttft": 1270051.9958731614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3592882763966936,
    "arrivals": 143761,
    "finished_requests": 92336,
    "scheduler_time": 177.26599924172723
}
#Debug simulation 
Total elapsed time: 96.78771242406219. Arrivals time: 0.580563914263621 Scheduler time: 95.93051527231 Scheduler overhead time: 0.10991017846390605 Adapter cache time: 0.02095240098424256 Engine time: 0.10841376055032015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 96.53951255301945,
    "estimated_duration": 3600.018018587025,
    "input_throughput": 6635.6167876559575,
    "output_throughput": 5695.906768835611,
    "total_throughput": 12331.52355649157,
    "itl": 90.86261182159899,
    "ttft": 1251460.9209514942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.322481806389986,
    "arrivals": 140843,
    "finished_requests": 95650,
    "scheduler_time": 169.955980407454
}
#Debug simulation 
Total elapsed time: 96.539678680012. Arrivals time: 0.5967151348013431 Scheduler time: 95.67213281104341 Scheduler overhead time: 0.10604427522048354 Adapter cache time: 0.02126327739097178 Engine time: 0.10701300436630845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 94.3623037559446,
    "estimated_duration": 3600.085716501363,
    "input_throughput": 6590.243918707822,
    "output_throughput": 5658.596934685593,
    "total_throughput": 12248.840853393414,
    "itl": 88.978930207635,
    "ttft": 1263482.057216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.49028688719962,
    "arrivals": 140843,
    "finished_requests": 95058,
    "scheduler_time": 171.16841455937404
}
#Debug simulation 
Total elapsed time: 94.36248323600739. Arrivals time: 0.5708740053232759 Scheduler time: 93.52247569174506 Scheduler overhead time: 0.10697059892117977 Adapter cache time: 0.02038733451627195 Engine time: 0.10573167447000742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 82.81271466286853,
    "estimated_duration": 3600.045836009133,
    "input_throughput": 6506.272160680506,
    "output_throughput": 5586.735535094164,
    "total_throughput": 12093.00769577467,
    "itl": 85.287818905974,
    "ttft": 1240577.9355857607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4302167509263402,
    "arrivals": 140843,
    "finished_requests": 93627,
    "scheduler_time": 173.57637928503772
}
#Debug simulation 
Total elapsed time: 82.81289652385749. Arrivals time: 0.551468278747052 Scheduler time: 81.9999991254881 Scheduler overhead time: 0.10297493101097643 Adapter cache time: 0.01999489264562726 Engine time: 0.10269460408017039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 96.76273601409048,
    "estimated_duration": 3600.00576999689,
    "input_throughput": 6581.873617391582,
    "output_throughput": 5650.534276787443,
    "total_throughput": 12232.407894179025,
    "itl": 88.81509456936065,
    "ttft": 1258441.1173571511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3994953400082877,
    "arrivals": 140843,
    "finished_requests": 94999,
    "scheduler_time": 171.38764674180334
}
#Debug simulation 
Total elapsed time: 96.76290699513629. Arrivals time: 0.5763860931620002 Scheduler time: 95.91230183560401 Scheduler overhead time: 0.10866342158988118 Adapter cache time: 0.021073052426800132 Engine time: 0.10800279793329537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 90.69539254903793,
    "estimated_duration": 3600.015701002189,
    "input_throughput": 6507.6927285283955,
    "output_throughput": 5581.111491932291,
    "total_throughput": 12088.804220460686,
    "itl": 85.02184258191706,
    "ttft": 1227923.7661825907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3491958932671713,
    "arrivals": 140843,
    "finished_requests": 93597,
    "scheduler_time": 173.79680017041963
}
#Debug simulation 
Total elapsed time: 90.69555521500297. Arrivals time: 0.5741366646252573 Scheduler time: 89.85283457883634 Scheduler overhead time: 0.10538497986271977 Adapter cache time: 0.020803555380553007 Engine time: 0.10559991491027176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 97.37386252498254,
    "estimated_duration": 3600.063292687455,
    "input_throughput": 6579.3468265160145,
    "output_throughput": 5651.409807523714,
    "total_throughput": 12230.756634039728,
    "itl": 88.8387969354522,
    "ttft": 1258798.9800301949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.353391239400949,
    "arrivals": 140843,
    "finished_requests": 94939,
    "scheduler_time": 171.35831001836561
}
#Debug simulation 
Total elapsed time: 97.3740436709486. Arrivals time: 0.5762184478808194 Scheduler time: 96.52072401880287 Scheduler overhead time: 0.10878876340575516 Adapter cache time: 0.020867137238383293 Engine time: 0.10972243174910545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 87.87560809589922,
    "estimated_duration": 3600.039614213397,
    "input_throughput": 6508.365604504466,
    "output_throughput": 5586.411305197343,
    "total_throughput": 12094.776909701808,
    "itl": 85.28978939194748,
    "ttft": 1232684.0021030484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3956296789459903,
    "arrivals": 140843,
    "finished_requests": 93669,
    "scheduler_time": 173.58025237942874
}
#Debug simulation 
Total elapsed time: 87.87578916386701. Arrivals time: 0.5720561714842916 Scheduler time: 87.03663959656842 Scheduler overhead time: 0.10563651705160737 Adapter cache time: 0.020406452240422368 Engine time: 0.10399862704798579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 100.3249913470354,
    "estimated_duration": 3600.0611499319016,
    "input_throughput": 6526.923021972689,
    "output_throughput": 5657.89361671962,
    "total_throughput": 12184.81663869231,
    "itl": 89.77996168415903,
    "ttft": 1271238.4772245593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3555438515497356,
    "arrivals": 139377,
    "finished_requests": 94567,
    "scheduler_time": 170.1400070149242
}
#Debug simulation 
Total elapsed time: 100.32519338908605. Arrivals time: 0.5719209720846266 Scheduler time: 99.47777261165902 Scheduler overhead time: 0.10862108576111495 Adapter cache time: 0.02142622065730393 Engine time: 0.1082104432862252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 96.90447653015144,
    "estimated_duration": 3600.055020417337,
    "input_throughput": 6486.2550343169605,
    "output_throughput": 5621.711858629831,
    "total_throughput": 12107.966892946792,
    "itl": 88.03647559247078,
    "ttft": 1275860.3686255899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5488565435167427,
    "arrivals": 139377,
    "finished_requests": 93953,
    "scheduler_time": 171.21464570506407
}
#Debug simulation 
Total elapsed time: 96.90465095918626. Arrivals time: 0.5639734608121216 Scheduler time: 96.06547687319107 Scheduler overhead time: 0.10829921485856175 Adapter cache time: 0.02119266358204186 Engine time: 0.10953275533393025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 94.66557127400301,
    "estimated_duration": 3600.01369422692,
    "input_throughput": 6357.543593987602,
    "output_throughput": 5520.102057352724,
    "total_throughput": 11877.645651340326,
    "itl": 83.48749136552236,
    "ttft": 1301674.9408313534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8161594664678047,
    "arrivals": 139377,
    "finished_requests": 92241,
    "scheduler_time": 174.50654585904212
}
#Debug simulation 
Total elapsed time: 94.66577257914469. Arrivals time: 0.5528163302224129 Scheduler time: 93.83627482643351 Scheduler overhead time: 0.10923298727720976 Adapter cache time: 0.021241586422547698 Engine time: 0.1085207590367645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 97.75578579097055,
    "estimated_duration": 3600.022271142637,
    "input_throughput": 6472.988010877979,
    "output_throughput": 5615.322483431117,
    "total_throughput": 12088.310494309095,
    "itl": 87.86945830600091,
    "ttft": 1279063.296265588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3420400515431532,
    "arrivals": 139377,
    "finished_requests": 93865,
    "scheduler_time": 171.44619961795962
}
#Debug simulation 
Total elapsed time: 97.75595772103406. Arrivals time: 0.5711921397596598 Scheduler time: 96.91191637492739 Scheduler overhead time: 0.10821613809093833 Adapter cache time: 0.020899918396025896 Engine time: 0.10716151469387114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 94.85370366601273,
    "estimated_duration": 3600.0325207947876,
    "input_throughput": 6347.959044257389,
    "output_throughput": 5511.269935867054,
    "total_throughput": 11859.228980124444,
    "itl": 83.405857036214,
    "ttft": 1303558.3870517819,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8060092282993787,
    "arrivals": 139377,
    "finished_requests": 92111,
    "scheduler_time": 174.78232031466632
}
#Debug simulation 
Total elapsed time: 94.85388527298346. Arrivals time: 0.5468850200995803 Scheduler time: 94.02845205133781 Scheduler overhead time: 0.11031661904416978 Adapter cache time: 0.02173326932825148 Engine time: 0.10950636258348823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 96.74265236104839,
    "estimated_duration": 3600.107368718543,
    "input_throughput": 6486.522097342947,
    "output_throughput": 5622.065934995832,
    "total_throughput": 12108.58803233878,
    "itl": 88.03522628277044,
    "ttft": 1275739.3137322527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.353391239400949,
    "arrivals": 139377,
    "finished_requests": 93961,
    "scheduler_time": 171.22033422177068
}
#Debug simulation 
Total elapsed time: 96.74283742206171. Arrivals time: 0.577470698626712 Scheduler time: 95.89313746034168 Scheduler overhead time: 0.10745825688354671 Adapter cache time: 0.021279491251334548 Engine time: 0.10720169660635293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 95.80975171481259,
    "estimated_duration": 3600.0033646044794,
    "input_throughput": 6346.206290979413,
    "output_throughput": 5511.503459991881,
    "total_throughput": 11857.709750971293,
    "itl": 83.42075081547675,
    "ttft": 1301853.006797805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8332878475450054,
    "arrivals": 139377,
    "finished_requests": 92109,
    "scheduler_time": 174.7345035697882
}
#Debug simulation 
Total elapsed time: 95.80991677590646. Arrivals time: 0.5564704877324402 Scheduler time: 94.97621698165312 Scheduler overhead time: 0.10942082456313074 Adapter cache time: 0.021607964765280485 Engine time: 0.10838417336344719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 122.01161384000443,
    "estimated_duration": 3600.0065452966073,
    "input_throughput": 6334.104039280644,
    "output_throughput": 5536.507433865743,
    "total_throughput": 11870.611473146388,
    "itl": 86.02180671950747,
    "ttft": 1275939.82655082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4745672141248343,
    "arrivals": 138774,
    "finished_requests": 92437,
    "scheduler_time": 173.72775931960985
}
#Debug simulation 
Total elapsed time: 122.01175570907071. Arrivals time: 0.41493830317631364 Scheduler time: 121.35960478777997 Scheduler overhead time: 0.09322702186182141 Adapter cache time: 0.017333005322143435 Engine time: 0.09097870090045035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 117.82755960593931,
    "estimated_duration": 3600.081322003725,
    "input_throughput": 6310.523837654713,
    "output_throughput": 5508.079464428132,
    "total_throughput": 11818.603302082845,
    "itl": 84.6353010673333,
    "ttft": 1282942.9285224352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7129025607556139,
    "arrivals": 138774,
    "finished_requests": 92011,
    "scheduler_time": 174.59186114621946
}
#Debug simulation 
Total elapsed time: 117.82771057402715. Arrivals time: 0.3660035487264395 Scheduler time: 117.23212001984939 Scheduler overhead time: 0.09121605241671205 Adapter cache time: 0.017240616027265787 Engine time: 0.08647739025764167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 116.61467876797542,
    "estimated_duration": 3600.0647592787045,
    "input_throughput": 6207.063898616403,
    "output_throughput": 5424.264924587773,
    "total_throughput": 11631.328823204176,
    "itl": 80.95620530177871,
    "ttft": 1290147.8195385046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8093242408894072,
    "arrivals": 138774,
    "finished_requests": 90546,
    "scheduler_time": 177.28042595244747
}
#Debug simulation 
Total elapsed time: 116.61481696204282. Arrivals time: 0.37123150238767266 Scheduler time: 116.00900278217159 Scheduler overhead time: 0.09190104180015624 Adapter cache time: 0.017611641436815262 Engine time: 0.08867196063511074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 120.04656932502985,
    "estimated_duration": 3600.023060434299,
    "input_throughput": 6296.404945046512,
    "output_throughput": 5503.855577416684,
    "total_throughput": 11800.260522463197,
    "itl": 84.45365959312507,
    "ttft": 1278317.6634742375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5396677475795137,
    "arrivals": 138774,
    "finished_requests": 91891,
    "scheduler_time": 174.78373713099057
}
#Debug simulation 
Total elapsed time: 120.04671455710195. Arrivals time: 0.3723399715963751 Scheduler time: 119.44429135508835 Scheduler overhead time: 0.09057107358239591 Adapter cache time: 0.017316211480647326 Engine time: 0.08715689857490361 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 116.05320960795507,
    "estimated_duration": 3600.0306194306518,
    "input_throughput": 6207.57636876274,
    "output_throughput": 5423.588036895061,
    "total_throughput": 11631.1644056578,
    "itl": 80.95092068662898,
    "ttft": 1289338.233580808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7995882370742107,
    "arrivals": 138774,
    "finished_requests": 90529,
    "scheduler_time": 177.25465320674712
}
#Debug simulation 
Total elapsed time: 116.05335596599616. Arrivals time: 0.3612461856100708 Scheduler time: 115.45871607982554 Scheduler overhead time: 0.09218679997138679 Adapter cache time: 0.017860489431768656 Engine time: 0.08817166509106755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 120.81084722606465,
    "estimated_duration": 3600.0044843914334,
    "input_throughput": 6294.30243719002,
    "output_throughput": 5500.206481922547,
    "total_throughput": 11794.508919112568,
    "itl": 84.41353025975206,
    "ttft": 1277654.5257306723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.404462606925513,
    "arrivals": 138774,
    "finished_requests": 91862,
    "scheduler_time": 174.81796813688138
}
#Debug simulation 
Total elapsed time: 120.81099471193738. Arrivals time: 0.3762151531409472 Scheduler time: 120.20093365176581 Scheduler overhead time: 0.09308360167779028 Adapter cache time: 0.01744376146234572 Engine time: 0.08798101334832609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 116.48268597386777,
    "estimated_duration": 3600.020302381876,
    "input_throughput": 6203.684736228735,
    "output_throughput": 5422.694140664656,
    "total_throughput": 11626.378876893392,
    "itl": 80.91750810674972,
    "ttft": 1290995.1307971014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8351500468701198,
    "arrivals": 138774,
    "finished_requests": 90491,
    "scheduler_time": 177.44224814211702
}
#Debug simulation 
Total elapsed time: 116.48283419199288. Arrivals time: 0.36753417761065066 Scheduler time: 115.88219499727711 Scheduler overhead time: 0.09128945227712393 Adapter cache time: 0.017565439455211163 Engine time: 0.08856768836267292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 65.42997190495953,
    "estimated_duration": 3600.048046972673,
    "input_throughput": 6607.93264134471,
    "output_throughput": 5737.224539924809,
    "total_throughput": 12345.157181269518,
    "itl": 91.48827931177932,
    "ttft": 1214155.2804251378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.309256988326086,
    "arrivals": 138381,
    "finished_requests": 95725,
    "scheduler_time": 166.8735133007173
}
#Debug simulation 
Total elapsed time: 65.4301237880718. Arrivals time: 0.3486892036162317 Scheduler time: 64.8801844175905 Scheduler overhead time: 0.07862980919890106 Adapter cache time: 0.01448889379389584 Engine time: 0.07608859823085368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 62.606450038030744,
    "estimated_duration": 3600.0381136273204,
    "input_throughput": 6561.7794185513,
    "output_throughput": 5699.544102694493,
    "total_throughput": 12261.323521245793,
    "itl": 89.59320151763649,
    "ttft": 1221411.1523803042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4422658147430065,
    "arrivals": 138381,
    "finished_requests": 95074,
    "scheduler_time": 167.97756078432704
}
#Debug simulation 
Total elapsed time: 62.60660639894195. Arrivals time: 0.3510943711735308 Scheduler time: 62.053220708156005 Scheduler overhead time: 0.0790199323091656 Adapter cache time: 0.014528641244396567 Engine time: 0.0767725077457726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 58.553578790975735,
    "estimated_duration": 3600.0666379849577,
    "input_throughput": 6420.836424555257,
    "output_throughput": 5580.068932069567,
    "total_throughput": 12000.905356624824,
    "itl": 84.72634491584174,
    "ttft": 1251971.400124786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.455685818842615,
    "arrivals": 138381,
    "finished_requests": 93062,
    "scheduler_time": 171.85798198059658
}
#Debug simulation 
Total elapsed time: 58.553730092942715. Arrivals time: 0.3449164421763271 Scheduler time: 58.00096595007926 Scheduler overhead time: 0.0811432097107172 Adapter cache time: 0.014660570304840803 Engine time: 0.07898755301721394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 62.94465747196227,
    "estimated_duration": 3600.0646750855285,
    "input_throughput": 6561.73100541278,
    "output_throughput": 5699.502051171492,
    "total_throughput": 12261.233056584273,
    "itl": 89.5856399939076,
    "ttft": 1221293.4551953233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.348423972483724,
    "arrivals": 138381,
    "finished_requests": 95074,
    "scheduler_time": 167.9842570124224
}
#Debug simulation 
Total elapsed time: 62.944808933883905. Arrivals time: 0.35065671196207404 Scheduler time: 62.39016456902027 Scheduler overhead time: 0.07983966916799545 Adapter cache time: 0.014843123964965343 Engine time: 0.07711514853872359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 58.422855564858764,
    "estimated_duration": 3600.0993841063555,
    "input_throughput": 6421.132455969187,
    "output_throughput": 5580.0765080785695,
    "total_throughput": 12001.208964047757,
    "itl": 84.73578629761244,
    "ttft": 1251841.0053526373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4494189380481863,
    "arrivals": 138381,
    "finished_requests": 93064,
    "scheduler_time": 171.84494175753144
}
#Debug simulation 
Total elapsed time: 58.42301949579269. Arrivals time: 0.35020620259456336 Scheduler time: 57.866449155146256 Scheduler overhead time: 0.08040808350779116 Adapter cache time: 0.015003007603809237 Engine time: 0.07820483483374119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 81.89870640006848,
    "estimated_duration": 3600.0899195285947,
    "input_throughput": 6561.777213357286,
    "output_throughput": 5699.559860628927,
    "total_throughput": 12261.337073986213,
    "itl": 89.58733023063616,
    "ttft": 1221370.8683135377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2512485043518211,
    "arrivals": 138381,
    "finished_requests": 95074,
    "scheduler_time": 167.98528357092732
}
#Debug simulation 
Total elapsed time: 81.89889094000682. Arrivals time: 0.5544145407620817 Scheduler time: 81.08404817711562 Scheduler overhead time: 0.10370837966911495 Adapter cache time: 0.018521662335842848 Engine time: 0.10243027377873659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 75.9372578850016,
    "estimated_duration": 3600.0693560546133,
    "input_throughput": 6421.237680080216,
    "output_throughput": 5580.151106315311,
    "total_throughput": 12001.388786395526,
    "itl": 84.73520422650107,
    "ttft": 1251719.125792651,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4353349700383866,
    "arrivals": 138381,
    "finished_requests": 93066,
    "scheduler_time": 171.84753596604227
}
#Debug simulation 
Total elapsed time: 75.93743756297044. Arrivals time: 0.5515302063431591 Scheduler time: 75.12587861111388 Scheduler overhead time: 0.10211083525791764 Adapter cache time: 0.019574346719309688 Engine time: 0.10164081654511392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 179.33688266412355,
    "estimated_duration": 3600.040998285327,
    "input_throughput": 6342.733599666147,
    "output_throughput": 5551.230113634412,
    "total_throughput": 11893.963713300558,
    "itl": 77.57362966548239,
    "ttft": 564436.7005639311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5025430864281948,
    "arrivals": 109384,
    "finished_requests": 92414,
    "scheduler_time": 120.31214463593828
}
#Debug simulation 
Total elapsed time: 179.33705886313692. Arrivals time: 0.6381124267354608 Scheduler time: 178.30450888886116 Scheduler overhead time: 0.15955070639029145 Adapter cache time: 0.02898381371051073 Engine time: 0.15881084674037993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 175.40281707397662,
    "estimated_duration": 3600.0689113841436,
    "input_throughput": 6342.684421343705,
    "output_throughput": 5551.187072226448,
    "total_throughput": 11893.871493570154,
    "itl": 77.49210147679185,
    "ttft": 564648.2556199242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5490361562557518,
    "arrivals": 109384,
    "finished_requests": 92414,
    "scheduler_time": 120.33640987033067
}
#Debug simulation 
Total elapsed time: 175.40300515294075. Arrivals time: 0.6317380950786173 Scheduler time: 174.38241779245436 Scheduler overhead time: 0.1560951005667448 Adapter cache time: 0.02979306923225522 Engine time: 0.1570785988587886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 174.0810359849129,
    "estimated_duration": 3600.0159613372452,
    "input_throughput": 6342.757155864992,
    "output_throughput": 5551.16594332452,
    "total_throughput": 11893.923099189513,
    "itl": 77.3047782598816,
    "ttft": 565463.2576871595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5625173845607788,
    "arrivals": 109384,
    "finished_requests": 92413,
    "scheduler_time": 120.42202478287771
}
#Debug simulation 
Total elapsed time: 174.08121218904853. Arrivals time: 0.632452990161255 Scheduler time: 173.067389825359 Scheduler overhead time: 0.15116265695542097 Adapter cache time: 0.028645995538681746 Engine time: 0.15658934134989977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 178.9156802049838,
    "estimated_duration": 3600.0109318162313,
    "input_throughput": 6342.643795384336,
    "output_throughput": 5551.052588031449,
    "total_throughput": 11893.696383415785,
    "itl": 77.48998533901761,
    "ttft": 564712.8648682046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5171070738695565,
    "arrivals": 109384,
    "finished_requests": 92412,
    "scheduler_time": 120.33303343749182
}
#Debug simulation 
Total elapsed time: 178.91584706003778. Arrivals time: 0.6334728302899748 Scheduler time: 177.88746042968705 Scheduler overhead time: 0.15745703037828207 Adapter cache time: 0.030033904826268554 Engine time: 0.1610287653747946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 175.82044565794058,
    "estimated_duration": 3600.062797714367,
    "input_throughput": 6342.695192566383,
    "output_throughput": 5551.196499318844,
    "total_throughput": 11893.891691885226,
    "itl": 77.30431347081554,
    "ttft": 565427.9975726895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5577536894986407,
    "arrivals": 109384,
    "finished_requests": 92414,
    "scheduler_time": 120.42389012927502
}
#Debug simulation 
Total elapsed time: 175.8206114480272. Arrivals time: 0.6225821955595165 Scheduler time: 174.8078028166201 Scheduler overhead time: 0.15707332594320178 Adapter cache time: 0.030947561841458082 Engine time: 0.15539704239927232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 178.8797452908475,
    "estimated_duration": 3600.0159784666516,
    "input_throughput": 6342.757125685219,
    "output_throughput": 5551.165916911255,
    "total_throughput": 11893.923042596474,
    "itl": 77.48988472352802,
    "ttft": 564680.6646622114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48517799148336116,
    "arrivals": 109384,
    "finished_requests": 92413,
    "scheduler_time": 120.33341674893651
}
#Debug simulation 
Total elapsed time: 178.88009211490862. Arrivals time: 0.6335271801799536 Scheduler time: 177.85616635065526 Scheduler overhead time: 0.1581011989619583 Adapter cache time: 0.0295940563082695 Engine time: 0.1575074135325849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 176.55186676001176,
    "estimated_duration": 3600.036797848428,
    "input_throughput": 6342.741000216127,
    "output_throughput": 5551.236590677042,
    "total_throughput": 11893.977590893168,
    "itl": 77.30618661848632,
    "ttft": 565428.7514637178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5529899944365025,
    "arrivals": 109384,
    "finished_requests": 92414,
    "scheduler_time": 120.42302655227827
}
#Debug simulation 
Total elapsed time: 176.5520765229594. Arrivals time: 0.6335512937512249 Scheduler time: 175.52536695986055 Scheduler overhead time: 0.1614181154873222 Adapter cache time: 0.028066611383110285 Engine time: 0.15712266601622105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 147.7585803179536,
    "estimated_duration": 3600.006245124351,
    "input_throughput": 6335.525676070645,
    "output_throughput": 5523.94958395776,
    "total_throughput": 11859.475260028406,
    "itl": 76.10706112057306,
    "ttft": 494847.5679924012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4165817690128457,
    "arrivals": 106465,
    "finished_requests": 91952,
    "scheduler_time": 113.10603317199212
}
#Debug simulation 
Total elapsed time: 147.75875905202702. Arrivals time: 0.591932010371238 Scheduler time: 146.8008601742331 Scheduler overhead time: 0.14563166559673846 Adapter cache time: 0.026237586978822947 Engine time: 0.14820746495388448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 149.54723023506813,
    "estimated_duration": 3600.0205983520073,
    "input_throughput": 6335.500694201822,
    "output_throughput": 5523.928115606725,
    "total_throughput": 11859.428809808547,
    "itl": 76.07464444138844,
    "ttft": 494875.18295144953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4507747533218936,
    "arrivals": 106465,
    "finished_requests": 91953,
    "scheduler_time": 113.11400416522645
}
#Debug simulation 
Total elapsed time: 149.54742168006487. Arrivals time: 0.5993670800235122 Scheduler time: 148.58458449877799 Scheduler overhead time: 0.14449472725391388 Adapter cache time: 0.027348597766831517 Engine time: 0.14701949921436608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 143.79418033501133,
    "estimated_duration": 3600.056168196395,
    "input_throughput": 6335.661704808075,
    "output_throughput": 5523.874648312192,
    "total_throughput": 11859.536353120267,
    "itl": 75.98204541281483,
    "ttft": 495086.48935929226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46094604349229495,
    "arrivals": 106465,
    "finished_requests": 91955,
    "scheduler_time": 113.14823508929723
}
#Debug simulation 
Total elapsed time: 143.79436129587702. Arrivals time: 0.5879075524862856 Scheduler time: 142.8475137911737 Scheduler overhead time: 0.14381162216886878 Adapter cache time: 0.026674587279558182 Engine time: 0.1448729163967073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 147.9378918230068,
    "estimated_duration": 3600.0292680049674,
    "input_throughput": 6335.603213759352,
    "output_throughput": 5523.915368338211,
    "total_throughput": 11859.518582097564,
    "itl": 76.0742800850568,
    "ttft": 494841.3694068833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42301033385563674,
    "arrivals": 106465,
    "finished_requests": 91954,
    "scheduler_time": 113.11436341853017
}
#Debug simulation 
Total elapsed time: 147.93805184215307. Arrivals time: 0.5977429684717208 Scheduler time: 146.9782565261703 Scheduler overhead time: 0.14635436586104333 Adapter cache time: 0.02642306312918663 Engine time: 0.14452225575223565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 145.92405774607323,
    "estimated_duration": 3600.0031432003493,
    "input_throughput": 6335.531135043423,
    "output_throughput": 5523.954343640216,
    "total_throughput": 11859.485478683639,
    "itl": 75.98174342172672,
    "ttft": 495187.08926735836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4568036999600009,
    "arrivals": 106465,
    "finished_requests": 91952,
    "scheduler_time": 113.1457957668039
}
#Debug simulation 
Total elapsed time: 145.92423720401712. Arrivals time: 0.5887042980175465 Scheduler time: 144.96605557529256 Scheduler overhead time: 0.1473882703576237 Adapter cache time: 0.029370679752901196 Engine time: 0.1480593781452626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 149.404277873924,
    "estimated_duration": 3600.044062478131,
    "input_throughput": 6335.577177435881,
    "output_throughput": 5523.892667666704,
    "total_throughput": 11859.469845102585,
    "itl": 76.0740950346284,
    "ttft": 494841.25412382506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40218701925594397,
    "arrivals": 106465,
    "finished_requests": 91954,
    "scheduler_time": 113.11475053664373
}
#Debug simulation 
Total elapsed time: 149.40446251584217. Arrivals time: 0.5946051189675927 Scheduler time: 148.44754333375022 Scheduler overhead time: 0.14658059389330447 Adapter cache time: 0.02595969964750111 Engine time: 0.14518517046235502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 145.54093633103184,
    "estimated_duration": 3600.0469266446635,
    "input_throughput": 6335.572136904887,
    "output_throughput": 5523.88827290496,
    "total_throughput": 11859.460409809846,
    "itl": 75.98234468556241,
    "ttft": 495120.20841936924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4536969423107804,
    "arrivals": 106465,
    "finished_requests": 91954,
    "scheduler_time": 113.14808427891705
}
#Debug simulation 
Total elapsed time: 145.54110297211446. Arrivals time: 0.5893343216739595 Scheduler time: 144.58215320878662 Scheduler overhead time: 0.14991561463102698 Adapter cache time: 0.024667706107720733 Engine time: 0.14987822133116424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 101.66604238701984,
    "estimated_duration": 3600.031998842665,
    "input_throughput": 6286.717731196789,
    "output_throughput": 5550.877049544067,
    "total_throughput": 11837.594780740856,
    "itl": 76.59931809445067,
    "ttft": 453736.2764529541,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 59,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.390132132885046,
    "arrivals": 105012,
    "finished_requests": 91910,
    "scheduler_time": 109.31185828444545
}
#Debug simulation 
Total elapsed time: 101.66620377404615. Arrivals time: 0.33942044363357127 Scheduler time: 101.07254581246525 Scheduler overhead time: 0.10300265694968402 Adapter cache time: 0.016298661939799786 Engine time: 0.09770654165185988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 102.3266403649468,
    "estimated_duration": 3600.0314834510546,
    "input_throughput": 6286.718631222689,
    "output_throughput": 5550.877844224745,
    "total_throughput": 11837.596475447433,
    "itl": 76.56026580690155,
    "ttft": 453783.6665029646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 59,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4280155115062369,
    "arrivals": 105012,
    "finished_requests": 91910,
    "scheduler_time": 109.31753743284156
}
#Debug simulation 
Total elapsed time: 102.32680158899166. Arrivals time: 0.33813165267929435 Scheduler time: 101.73434054316022 Scheduler overhead time: 0.10089174285531044 Adapter cache time: 0.018019954673945904 Engine time: 0.09709812025539577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 76.29295171797276,
    "estimated_duration": 3600.028296179772,
    "input_throughput": 6428.741136440304,
    "output_throughput": 5669.61376988598,
    "total_throughput": 12098.354906326285,
    "itl": 80.32575742797974,
    "ttft": 432810.261781507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6352020449237898,
    "arrivals": 105012,
    "finished_requests": 93952,
    "scheduler_time": 106.08421888809086
}
#Debug simulation 
Total elapsed time: 76.2931059570983. Arrivals time: 0.32498844526708126 Scheduler time: 75.7378081958741 Scheduler overhead time: 0.09166493220254779 Adapter cache time: 0.016867493744939566 Engine time: 0.08692416455596685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 101.77441619080491,
    "estimated_duration": 3600.0282912626935,
    "input_throughput": 6286.724205731671,
    "output_throughput": 5550.882766254856,
    "total_throughput": 11837.606971986526,
    "itl": 76.55954613784517,
    "ttft": 453783.60876499675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 59,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40441575495991855,
    "arrivals": 105012,
    "finished_requests": 91910,
    "scheduler_time": 109.31724960176543
}
#Debug simulation 
Total elapsed time: 101.77458292501979. Arrivals time: 0.34422497637569904 Scheduler time: 101.17967754486017 Scheduler overhead time: 0.10039089410565794 Adapter cache time: 0.01743332645855844 Engine time: 0.0957861531060189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 78.9640383629594,
    "estimated_duration": 3600.0192394354526,
    "input_throughput": 6417.526814001972,
    "output_throughput": 5660.120584020936,
    "total_throughput": 12077.647398022908,
    "itl": 79.91835106490946,
    "ttft": 436082.6838084149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6378930717054755,
    "arrivals": 105012,
    "finished_requests": 93793,
    "scheduler_time": 106.18522264497093
}
#Debug simulation 
Total elapsed time: 78.96419187402353. Arrivals time: 0.3276069085113704 Scheduler time: 78.40273310546763 Scheduler overhead time: 0.09306065575219691 Adapter cache time: 0.016602933639660478 Engine time: 0.08868378261104226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 101.78000338096172,
    "estimated_duration": 3600.0302551931727,
    "input_throughput": 6286.720776124582,
    "output_throughput": 5550.879738072568,
    "total_throughput": 11837.600514197151,
    "itl": 76.55896117840878,
    "ttft": 453783.8581021856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 59,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37665133549366175,
    "arrivals": 105012,
    "finished_requests": 91910,
    "scheduler_time": 109.31733062566165
}
#Debug simulation 
Total elapsed time: 101.78015721403062. Arrivals time: 0.34325187583453953 Scheduler time: 101.18288569082506 Scheduler overhead time: 0.10166245745494962 Adapter cache time: 0.018047821009531617 Engine time: 0.09679084690287709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 76.78728109807707,
    "estimated_duration": 3600.079101697197,
    "input_throughput": 6428.650134128807,
    "output_throughput": 5669.484592818465,
    "total_throughput": 12098.134726947272,
    "itl": 80.32124327864459,
    "ttft": 432843.52433841035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6238106002099812,
    "arrivals": 105012,
    "finished_requests": 93951,
    "scheduler_time": 106.0855297946632
}
#Debug simulation 
Total elapsed time: 76.78743813303299. Arrivals time: 0.31998646655119956 Scheduler time: 76.23435223964043 Scheduler overhead time: 0.09200904564931989 Adapter cache time: 0.016032567713409662 Engine time: 0.08929612091742456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 65.70481072901748,
    "estimated_duration": 3600.023997445096,
    "input_throughput": 6528.952311618157,
    "output_throughput": 5626.828325137834,
    "total_throughput": 12155.780636755992,
    "itl": 79.81601831000886,
    "ttft": 501095.21936008654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4613423960609344,
    "arrivals": 104295,
    "finished_requests": 94641,
    "scheduler_time": 103.66532681561816
}
#Debug simulation 
Total elapsed time: 65.7049472371582. Arrivals time: 0.3162571236025542 Scheduler time: 65.16271256818436 Scheduler overhead time: 0.08908169041387737 Adapter cache time: 0.017043516505509615 Engine time: 0.08563196868635714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 64.92079443787225,
    "estimated_duration": 3600.0689033888157,
    "input_throughput": 6515.175022878388,
    "output_throughput": 5616.402225237147,
    "total_throughput": 12131.577248115535,
    "itl": 79.41801300493579,
    "ttft": 508118.41049504624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6149150109430794,
    "arrivals": 104295,
    "finished_requests": 94435,
    "scheduler_time": 103.7470299400325
}
#Debug simulation 
Total elapsed time: 64.92094320477918. Arrivals time: 0.3110609252471477 Scheduler time: 64.38090626173653 Scheduler overhead time: 0.09007099620066583 Adapter cache time: 0.015910964459180832 Engine time: 0.08820219477638602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 62.01217591506429,
    "estimated_duration": 3600.044128410723,
    "input_throughput": 6491.700980986895,
    "output_throughput": 5597.661940020785,
    "total_throughput": 12089.36292100768,
    "itl": 78.36554358388167,
    "ttft": 519504.2607389031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7178427101858005,
    "arrivals": 104295,
    "finished_requests": 94102,
    "scheduler_time": 103.95935395270062
}
#Debug simulation 
Total elapsed time: 62.01232454716228. Arrivals time: 0.3091798403766006 Scheduler time: 61.47932367306203 Scheduler overhead time: 0.08734867116436362 Adapter cache time: 0.017002655193209648 Engine time: 0.08482943871058524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 64.89978255797178,
    "estimated_duration": 3600.0258216783113,
    "input_throughput": 6510.443858170286,
    "output_throughput": 5611.827525887469,
    "total_throughput": 12122.271384057754,
    "itl": 79.30540525750746,
    "ttft": 510127.8757246576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5291191637190047,
    "arrivals": 104295,
    "finished_requests": 94371,
    "scheduler_time": 103.72875077569876
}
#Debug simulation 
Total elapsed time: 64.89991870196536. Arrivals time: 0.3041079386603087 Scheduler time: 64.36855099489912 Scheduler overhead time: 0.08967267978005111 Adapter cache time: 0.016092502512037754 Engine time: 0.08657577517442405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 61.85222292295657,
    "estimated_duration": 3600.017465209998,
    "input_throughput": 6491.83933851741,
    "output_throughput": 5596.928124576378,
    "total_throughput": 12088.767463093789,
    "itl": 78.37244799755547,
    "ttft": 519400.14741284255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6959416549606292,
    "arrivals": 104295,
    "finished_requests": 94104,
    "scheduler_time": 103.95283583199392
}
#Debug simulation 
Total elapsed time: 61.852372529916465. Arrivals time: 0.31026735063642263 Scheduler time: 61.318392801098526 Scheduler overhead time: 0.08824750408530235 Adapter cache time: 0.0161361675709486 Engine time: 0.0848901690915227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 74.97672139294446,
    "estimated_duration": 3600.0918575060664,
    "input_throughput": 6513.9949001872055,
    "output_throughput": 5616.36863732884,
    "total_throughput": 12130.363537516047,
    "itl": 79.40634306747913,
    "ttft": 508039.60760556086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.417230448806654,
    "arrivals": 104295,
    "finished_requests": 94438,
    "scheduler_time": 103.72504443232936
}
#Debug simulation 
Total elapsed time: 74.9769015761558. Arrivals time: 0.5395836064126343 Scheduler time: 74.1608409434557 Scheduler overhead time: 0.10563415475189686 Adapter cache time: 0.022382841911166906 Engine time: 0.11106872116215527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 70.19642540812492,
    "estimated_duration": 3600.007459378861,
    "input_throughput": 6490.55127347751,
    "output_throughput": 5596.366737383406,
    "total_throughput": 12086.918010860916,
    "itl": 78.39154670782379,
    "ttft": 519855.9905124529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6799936323612972,
    "arrivals": 104295,
    "finished_requests": 94085,
    "scheduler_time": 103.94036500440089
}
#Debug simulation 
Total elapsed time: 70.19659522199072. Arrivals time: 0.4816788053140044 Scheduler time: 69.45025741471909 Scheduler overhead time: 0.10301014967262745 Adapter cache time: 0.01956773758865893 Engine time: 0.10498698917217553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 121.20215098699555,
    "estimated_duration": 3600.0286593016167,
    "input_throughput": 6345.957535913592,
    "output_throughput": 5486.395767702474,
    "total_throughput": 11832.353303616066,
    "itl": 74.34062251111301,
    "ttft": 428470.8597199643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3636824967572463,
    "arrivals": 103971,
    "finished_requests": 91603,
    "scheduler_time": 106.88525284149641
}
#Debug simulation 
Total elapsed time: 121.20238561695442. Arrivals time: 0.5541397915221751 Scheduler time: 120.30925608542748 Scheduler overhead time: 0.13479239493608475 Adapter cache time: 0.02512936480343342 Engine time: 0.13527581212110817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 120.90490814414807,
    "estimated_duration": 3600.0551032982344,
    "input_throughput": 6345.977865469192,
    "output_throughput": 5486.36518977298,
    "total_throughput": 11832.34305524217,
    "itl": 74.3412428459454,
    "ttft": 428436.3970142889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3955387228773906,
    "arrivals": 103971,
    "finished_requests": 91604,
    "scheduler_time": 106.88638095332588
}
#Debug simulation 
Total elapsed time: 120.9050666440744. Arrivals time: 0.5517367315478623 Scheduler time: 120.01544355461374 Scheduler overhead time: 0.13325720746070147 Adapter cache time: 0.026337207295000553 Engine time: 0.13442296348512173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 120.84688434214331,
    "estimated_duration": 3600.0562219366034,
    "input_throughput": 6345.975893596006,
    "output_throughput": 5486.363485005545,
    "total_throughput": 11832.339378601551,
    "itl": 74.34026001947811,
    "ttft": 428439.49664308334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4048814884759487,
    "arrivals": 103971,
    "finished_requests": 91604,
    "scheduler_time": 106.88699964462177
}
#Debug simulation 
Total elapsed time: 120.84706885111518. Arrivals time: 0.5520012211054564 Scheduler time: 119.95614203019068 Scheduler overhead time: 0.13409941783174872 Adapter cache time: 0.024539607344195247 Engine time: 0.13637384516187012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 119.00229611294344,
    "estimated_duration": 3600.0729306050644,
    "input_throughput": 6345.986162052631,
    "output_throughput": 5486.368298844544,
    "total_throughput": 11832.354460897175,
    "itl": 74.34097221215887,
    "ttft": 428432.2429091752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3761036292510108,
    "arrivals": 103971,
    "finished_requests": 91605,
    "scheduler_time": 106.88695684577677
}
#Debug simulation 
Total elapsed time: 119.0024727510754. Arrivals time: 0.5523003751877695 Scheduler time: 118.11723476322368 Scheduler overhead time: 0.13332608668133616 Adapter cache time: 0.024018367286771536 Engine time: 0.1320335662458092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 118.63292137091048,
    "estimated_duration": 3600.0240058473596,
    "input_throughput": 6345.965738809757,
    "output_throughput": 5486.402859514001,
    "total_throughput": 11832.368598323757,
    "itl": 74.33959741277651,
    "ttft": 428443.45816229156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40198184800334286,
    "arrivals": 103971,
    "finished_requests": 91603,
    "scheduler_time": 106.88570865513283
}
#Debug simulation 
Total elapsed time: 118.63308219378814. Arrivals time: 0.5560855059884489 Scheduler time: 117.74427544372156 Scheduler overhead time: 0.1301442275289446 Adapter cache time: 0.02464313106611371 Engine time: 0.1338029825128615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 118.97101202816702,
    "estimated_duration": 3600.072522080434,
    "input_throughput": 6345.986882174694,
    "output_throughput": 5486.368921419942,
    "total_throughput": 11832.355803594635,
    "itl": 74.34067466550306,
    "ttft": 428432.24113059905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35111565173137954,
    "arrivals": 103971,
    "finished_requests": 91605,
    "scheduler_time": 106.8868319960304
}
#Debug simulation 
Total elapsed time: 118.97117341519333. Arrivals time: 0.5490493504330516 Scheduler time: 118.0928007226903 Scheduler overhead time: 0.12929060426540673 Adapter cache time: 0.02312256721779704 Engine time: 0.1346658505499363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 120.82963348808698,
    "estimated_duration": 3600.0224845744915,
    "input_throughput": 6345.968420444536,
    "output_throughput": 5486.405177920579,
    "total_throughput": 11832.373598365115,
    "itl": 74.33986944531917,
    "ttft": 428443.4244030401,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3982537388242782,
    "arrivals": 103971,
    "finished_requests": 91603,
    "scheduler_time": 106.8856373127625
}
#Debug simulation 
Total elapsed time: 120.82981762196869. Arrivals time: 0.557547491742298 Scheduler time: 119.9380289723631 Scheduler overhead time: 0.1367183628026396 Adapter cache time: 0.022339157294481993 Engine time: 0.1325756956357509 
