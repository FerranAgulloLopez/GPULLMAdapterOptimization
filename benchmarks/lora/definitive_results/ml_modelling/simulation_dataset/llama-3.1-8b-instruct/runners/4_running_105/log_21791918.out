INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 17280, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105195 . Total input tokens: 23484405 . Total output tokens: 20639727
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6801902349106967,
    "estimated_duration": 3600.0041757880913,
    "input_throughput": 2423.791077433912,
    "output_throughput": 2118.7230979614774,
    "total_throughput": 4542.514175395389,
    "itl": 25.545382378634567,
    "ttft": 5961.956316840769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 35272,
    "finished_requests": 35214,
    "scheduler_time": 5.8488071299502495
}
#Debug simulation 
Total elapsed time: 2.680336077697575. Arrivals time: 0.09497307101264596 Scheduler time: 2.2514847069978714 Scheduler overhead time: 0.12548541324213147 Adapter cache time: 0.02160498220473528 Engine time: 0.12609661184251308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 17280, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105195 . Total input tokens: 23484405 . Total output tokens: 20639727
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6582653201185167,
    "estimated_duration": 3599.991603013537,
    "input_throughput": 2423.7995423922075,
    "output_throughput": 2118.730497486474,
    "total_throughput": 4542.530039878681,
    "itl": 25.545169251058375,
    "ttft": 5961.8989269267395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 35272,
    "finished_requests": 35214,
    "scheduler_time": 5.84881104963627
}
#Debug simulation 
Total elapsed time: 2.6583747169934213. Arrivals time: 0.08872855408117175 Scheduler time: 2.240390371065587 Scheduler overhead time: 0.1260736626572907 Adapter cache time: 0.02176145324483514 Engine time: 0.12056290917098522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 17280, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105195 . Total input tokens: 23484405 . Total output tokens: 20639727
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.677711554337293,
    "estimated_duration": 3599.994900649532,
    "input_throughput": 2423.797322164447,
    "output_throughput": 2118.7285567054046,
    "total_throughput": 4542.525878869851,
    "itl": 25.545158053575022,
    "ttft": 5961.87537412751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 35272,
    "finished_requests": 35214,
    "scheduler_time": 5.848922761411435
}
#Debug simulation 
Total elapsed time: 2.6778079462237656. Arrivals time: 0.09133269963786006 Scheduler time: 2.254020218271762 Scheduler overhead time: 0.1272682840935886 Adapter cache time: 0.021560413762927055 Engine time: 0.12288497108966112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 17280, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105195 . Total input tokens: 23484405 . Total output tokens: 20639727
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6571898511610925,
    "estimated_duration": 3599.98175738563,
    "input_throughput": 2423.806171267025,
    "output_throughput": 2118.7362920247574,
    "total_throughput": 4542.542463291783,
    "itl": 25.545299629642184,
    "ttft": 5961.954377695837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 35272,
    "finished_requests": 35214,
    "scheduler_time": 5.848742663265576
}
#Debug simulation 
Total elapsed time: 2.657285104971379. Arrivals time: 0.09351713815703988 Scheduler time: 2.2324764793738723 Scheduler overhead time: 0.12620497262105346 Adapter cache time: 0.02147412719205022 Engine time: 0.12318864604458213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 17280, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105195 . Total input tokens: 23484405 . Total output tokens: 20639727
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.6878679087385535,
    "estimated_duration": 3599.994380964532,
    "input_throughput": 2423.7976720569686,
    "output_throughput": 2118.7288625590627,
    "total_throughput": 4542.526534616031,
    "itl": 25.54510597736918,
    "ttft": 5961.8614851819475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 35272,
    "finished_requests": 35214,
    "scheduler_time": 5.8488736399729415
}
#Debug simulation 
Total elapsed time: 2.6879608468152583. Arrivals time: 0.09408446308225393 Scheduler time: 2.2588998745195568 Scheduler overhead time: 0.12707519112154841 Adapter cache time: 0.021711294073611498 Engine time: 0.12542413314804435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 17280, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105195 . Total input tokens: 23484405 . Total output tokens: 20639727
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6982557591982186,
    "estimated_duration": 3599.9987855016625,
    "input_throughput": 2423.794706581845,
    "output_throughput": 2118.726270330426,
    "total_throughput": 4542.52097691227,
    "itl": 25.545331465169507,
    "ttft": 5961.886736540986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 35272,
    "finished_requests": 35214,
    "scheduler_time": 5.848787990140076
}
#Debug simulation 
Total elapsed time: 2.6983533571474254. Arrivals time: 0.0933200498111546 Scheduler time: 2.269932187628001 Scheduler overhead time: 0.12732230219990015 Adapter cache time: 0.021723370999097824 Engine time: 0.1253941198810935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 17280, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105195 . Total input tokens: 23484405 . Total output tokens: 20639727
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.681478396989405,
    "estimated_duration": 3599.992748233369,
    "input_throughput": 2423.798771339736,
    "output_throughput": 2118.7298234817317,
    "total_throughput": 4542.528594821468,
    "itl": 25.545115935253623,
    "ttft": 5961.8545954016945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 35272,
    "finished_requests": 35214,
    "scheduler_time": 5.848864466126827
}
#Debug simulation 
Total elapsed time: 2.6816032151691616. Arrivals time: 0.09363673301413655 Scheduler time: 2.258199588395655 Scheduler overhead time: 0.12668378418311477 Adapter cache time: 0.021422581747174263 Engine time: 0.121417167596519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 17280, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104685 . Total input tokens: 23366690 . Total output tokens: 20539149
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6650485079735518,
    "estimated_duration": 3600.01379999687,
    "input_throughput": 2411.141590625999,
    "output_throughput": 2081.8742417061057,
    "total_throughput": 4493.015832332105,
    "itl": 25.331180885604542,
    "ttft": 6092.948767761335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 35101,
    "finished_requests": 35042,
    "scheduler_time": 5.181991496045057
}
#Debug simulation 
Total elapsed time: 2.665141997858882. Arrivals time: 0.09302239120006561 Scheduler time: 2.239121075719595 Scheduler overhead time: 0.1269273031502962 Adapter cache time: 0.021402633283287287 Engine time: 0.12394109042361379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 17280, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104685 . Total input tokens: 23366690 . Total output tokens: 20539149
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6822433047927916,
    "estimated_duration": 3600.000415179252,
    "input_throughput": 2411.150555261199,
    "output_throughput": 2081.881982123832,
    "total_throughput": 4493.032537385031,
    "itl": 25.331285171912853,
    "ttft": 6093.006583695884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 35101,
    "finished_requests": 35042,
    "scheduler_time": 5.182055045489715
}
#Debug simulation 
Total elapsed time: 2.682342852000147. Arrivals time: 0.09412108361721039 Scheduler time: 2.254242340102792 Scheduler overhead time: 0.12741618929430842 Adapter cache time: 0.021300070453435183 Engine time: 0.12453551264479756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 17280, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104685 . Total input tokens: 23366690 . Total output tokens: 20539149
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.623848778195679,
    "estimated_duration": 3600.005945968999,
    "input_throughput": 2411.1468509432143,
    "output_throughput": 2081.878783670359,
    "total_throughput": 4493.025634613573,
    "itl": 25.331294841696625,
    "ttft": 6093.014643796114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 35101,
    "finished_requests": 35042,
    "scheduler_time": 5.182089071926053
}
#Debug simulation 
Total elapsed time: 2.6239364380016923. Arrivals time: 0.09099558601155877 Scheduler time: 2.2034104955382645 Scheduler overhead time: 0.12669326923787594 Adapter cache time: 0.021183859556913376 Engine time: 0.12108891224488616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 17280, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104685 . Total input tokens: 23366690 . Total output tokens: 20539149
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6582050947472453,
    "estimated_duration": 3600.02048020073,
    "input_throughput": 2411.1371165077408,
    "output_throughput": 2081.8703785768757,
    "total_throughput": 4493.0074950846165,
    "itl": 25.331250520736816,
    "ttft": 6092.963866674873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 35101,
    "finished_requests": 35042,
    "scheduler_time": 5.182121763888396
}
#Debug simulation 
Total elapsed time: 2.6582978921942413. Arrivals time: 0.09395674476400018 Scheduler time: 2.230822521727532 Scheduler overhead time: 0.126950575504452 Adapter cache time: 0.021390029229223728 Engine time: 0.12425533728674054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 17280, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104685 . Total input tokens: 23366690 . Total output tokens: 20539149
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.6627169246785343,
    "estimated_duration": 3600.0043100714865,
    "input_throughput": 2411.1479466055516,
    "output_throughput": 2081.879729708205,
    "total_throughput": 4493.027676313756,
    "itl": 25.331295455158244,
    "ttft": 6092.987845456927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 35101,
    "finished_requests": 35042,
    "scheduler_time": 5.1821763897309445
}
#Debug simulation 
Total elapsed time: 2.6628119107335806. Arrivals time: 0.09272951539605856 Scheduler time: 2.235306107904762 Scheduler overhead time: 0.12707837158814073 Adapter cache time: 0.021354602184146643 Engine time: 0.1251855492591858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 17280, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104685 . Total input tokens: 23366690 . Total output tokens: 20539149
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6534383171238005,
    "estimated_duration": 3600.00939653114,
    "input_throughput": 2411.1445398903466,
    "output_throughput": 2081.876788216647,
    "total_throughput": 4493.021328106994,
    "itl": 25.33117975153456,
    "ttft": 6093.03104974539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 35101,
    "finished_requests": 35042,
    "scheduler_time": 5.181905262470161
}
#Debug simulation 
Total elapsed time: 2.6535304924473166. Arrivals time: 0.09337082644924521 Scheduler time: 2.2297285720705986 Scheduler overhead time: 0.12689327215775847 Adapter cache time: 0.021195349749177694 Engine time: 0.121520203538239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 17280, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104685 . Total input tokens: 23366690 . Total output tokens: 20539149
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.70009648706764,
    "estimated_duration": 3600.0025855959325,
    "input_throughput": 2411.149101595192,
    "output_throughput": 2081.880726971572,
    "total_throughput": 4493.029828566764,
    "itl": 25.331330827205914,
    "ttft": 6092.979281064689,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 35101,
    "finished_requests": 35042,
    "scheduler_time": 5.182157833662765
}
#Debug simulation 
Total elapsed time: 2.700191122945398. Arrivals time: 0.09508269233629107 Scheduler time: 2.269764503929764 Scheduler overhead time: 0.12813018076121807 Adapter cache time: 0.021207191050052643 Engine time: 0.12483291560783982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 17280, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104520 . Total input tokens: 23329838 . Total output tokens: 20504813
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6454088860191405,
    "estimated_duration": 3600.022404262633,
    "input_throughput": 2423.0254760835746,
    "output_throughput": 2093.209473107005,
    "total_throughput": 4516.23494919058,
    "itl": 25.34171950602118,
    "ttft": 5179.367466874461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 35037,
    "finished_requests": 34987,
    "scheduler_time": 5.302856081292399
}
#Debug simulation 
Total elapsed time: 2.645504144951701. Arrivals time: 0.09289949107915163 Scheduler time: 2.220449232496321 Scheduler overhead time: 0.12750889686867595 Adapter cache time: 0.021101453807204962 Engine time: 0.12266438687220216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 17280, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104520 . Total input tokens: 23329838 . Total output tokens: 20504813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6528272572904825,
    "estimated_duration": 3600.009014552977,
    "input_throughput": 2423.0344881742335,
    "output_throughput": 2093.2172584950363,
    "total_throughput": 4516.25174666927,
    "itl": 25.341847493872333,
    "ttft": 5076.67433193652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 35037,
    "finished_requests": 34987,
    "scheduler_time": 5.302861794072489
}
#Debug simulation 
Total elapsed time: 2.6529262722469866. Arrivals time: 0.0934130297973752 Scheduler time: 2.2268205881118774 Scheduler overhead time: 0.12763191387057304 Adapter cache time: 0.02107834303751588 Engine time: 0.12325884774327278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 17280, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104520 . Total input tokens: 23329838 . Total output tokens: 20504813
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6543169668875635,
    "estimated_duration": 3600.0127944378864,
    "input_throughput": 2423.031944074526,
    "output_throughput": 2093.2150606916452,
    "total_throughput": 4516.247004766172,
    "itl": 25.341891312526748,
    "ttft": 5076.732174133261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 35037,
    "finished_requests": 34987,
    "scheduler_time": 5.302791822983755
}
#Debug simulation 
Total elapsed time: 2.654436938930303. Arrivals time: 0.0931678619235754 Scheduler time: 2.2280439152382314 Scheduler overhead time: 0.12782809417694807 Adapter cache time: 0.021211193408817053 Engine time: 0.12320133298635483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 17280, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104520 . Total input tokens: 23329838 . Total output tokens: 20504813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.635911028366536,
    "estimated_duration": 3600.0013317491093,
    "input_throughput": 2423.039659199748,
    "output_throughput": 2093.221725653842,
    "total_throughput": 4516.261384853589,
    "itl": 25.341808878453826,
    "ttft": 5076.745875137378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 35037,
    "finished_requests": 34987,
    "scheduler_time": 5.302826349908106
}
#Debug simulation 
Total elapsed time: 2.636003687977791. Arrivals time: 0.09317116485908628 Scheduler time: 2.210030673071742 Scheduler overhead time: 0.12635500356554985 Adapter cache time: 0.021165678277611732 Engine time: 0.12463174993172288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 17280, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104520 . Total input tokens: 23329838 . Total output tokens: 20504813
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.707080422900617,
    "estimated_duration": 3600.01166418778,
    "input_throughput": 2423.032704803204,
    "output_throughput": 2093.21571787189,
    "total_throughput": 4516.248422675094,
    "itl": 25.341914244591525,
    "ttft": 5076.704067596765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 35037,
    "finished_requests": 34987,
    "scheduler_time": 5.30280420765189
}
#Debug simulation 
Total elapsed time: 2.707211035769433. Arrivals time: 0.09430802054703236 Scheduler time: 2.277234971988946 Scheduler overhead time: 0.12899481551721692 Adapter cache time: 0.02124421950429678 Engine time: 0.12428432470187545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 17280, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104520 . Total input tokens: 23329838 . Total output tokens: 20504813
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.673016550950706,
    "estimated_duration": 3600.017644128699,
    "input_throughput": 2423.028679936147,
    "output_throughput": 2093.212240859397,
    "total_throughput": 4516.240920795544,
    "itl": 25.34179778876699,
    "ttft": 5179.362093206968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 35037,
    "finished_requests": 34987,
    "scheduler_time": 5.302875596468631
}
#Debug simulation 
Total elapsed time: 2.6730950828641653. Arrivals time: 0.08869006484746933 Scheduler time: 2.2534023821353912 Scheduler overhead time: 0.1265404224395752 Adapter cache time: 0.021218287758529186 Engine time: 0.12229134794324636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 17280, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 17280]
Prompts retrieved: 104520 . Total input tokens: 23329838 . Total output tokens: 20504813
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.717697232030332,
    "estimated_duration": 3600.010061711279,
    "input_throughput": 2423.033783370459,
    "output_throughput": 2093.2166496273408,
    "total_throughput": 4516.2504329978,
    "itl": 25.341859890111277,
    "ttft": 5076.660354834878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 35037,
    "finished_requests": 34987,
    "scheduler_time": 5.302823597706093
}
#Debug simulation 
Total elapsed time: 2.717782439198345. Arrivals time: 0.09099116129800677 Scheduler time: 2.2917250324971974 Scheduler overhead time: 0.12833504704758525 Adapter cache time: 0.020915855187922716 Engine time: 0.12482979381456971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 17280, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 17280]
Prompts retrieved: 104175 . Total input tokens: 23254663 . Total output tokens: 20434481
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6693591908551753,
    "estimated_duration": 3600.017211896063,
    "input_throughput": 2389.8371851029897,
    "output_throughput": 2090.4336165760733,
    "total_throughput": 4480.270801679063,
    "itl": 25.28851766513551,
    "ttft": 5708.191426597768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 34944,
    "finished_requests": 34888,
    "scheduler_time": 5.201088332234453
}
#Debug simulation 
Total elapsed time: 2.6694562677294016. Arrivals time: 0.09293110528960824 Scheduler time: 2.2415462560020387 Scheduler overhead time: 0.12789640808477998 Adapter cache time: 0.02075381437316537 Engine time: 0.12529546348378062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 17280, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 17280]
Prompts retrieved: 104175 . Total input tokens: 23254663 . Total output tokens: 20434481
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.671553243882954,
    "estimated_duration": 3600.00386130715,
    "input_throughput": 2389.8460477973244,
    "output_throughput": 2090.4413689343874,
    "total_throughput": 4480.287416731712,
    "itl": 25.288526355251904,
    "ttft": 5708.16675072753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 34944,
    "finished_requests": 34888,
    "scheduler_time": 5.201036917213878
}
#Debug simulation 
Total elapsed time: 2.6716497410088778. Arrivals time: 0.0916909221559763 Scheduler time: 2.246480465400964 Scheduler overhead time: 0.12718382151797414 Adapter cache time: 0.020889680366963148 Engine time: 0.12445578724145889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 17280, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 17280]
Prompts retrieved: 104175 . Total input tokens: 23254663 . Total output tokens: 20434481
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7095919200219214,
    "estimated_duration": 3600.007886866593,
    "input_throughput": 2389.8433754511443,
    "output_throughput": 2090.4390313850663,
    "total_throughput": 4480.282406836211,
    "itl": 25.288664061440457,
    "ttft": 5708.113928446955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 34944,
    "finished_requests": 34888,
    "scheduler_time": 5.200920702010703
}
#Debug simulation 
Total elapsed time: 2.7096908427774906. Arrivals time: 0.09319727262482047 Scheduler time: 2.2826533941552043 Scheduler overhead time: 0.12763698492199183 Adapter cache time: 0.02084434451535344 Engine time: 0.12418127618730068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 17280, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 17280]
Prompts retrieved: 104175 . Total input tokens: 23254663 . Total output tokens: 20434481
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.746801164932549,
    "estimated_duration": 3600.0225611270203,
    "input_throughput": 2389.8336340721735,
    "output_throughput": 2090.43051042548,
    "total_throughput": 4480.264144497653,
    "itl": 25.28846608636467,
    "ttft": 5708.178305462968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 34944,
    "finished_requests": 34888,
    "scheduler_time": 5.201088081990429
}
#Debug simulation 
Total elapsed time: 2.746897763106972. Arrivals time: 0.09458582662045956 Scheduler time: 2.3106466312892735 Scheduler overhead time: 0.1327590560540557 Adapter cache time: 0.021089929156005383 Engine time: 0.12584436777979136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 17280, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 17280]
Prompts retrieved: 104175 . Total input tokens: 23254663 . Total output tokens: 20434481
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.7171916109509766,
    "estimated_duration": 3600.0070149097833,
    "input_throughput": 2389.843954294518,
    "output_throughput": 2090.4395377097876,
    "total_throughput": 4480.283492004306,
    "itl": 25.288549578102852,
    "ttft": 5708.141796415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 34944,
    "finished_requests": 34888,
    "scheduler_time": 5.2010239488037415
}
#Debug simulation 
Total elapsed time: 2.717287020292133. Arrivals time: 0.09439548011869192 Scheduler time: 2.287927769124508 Scheduler overhead time: 0.12765218038111925 Adapter cache time: 0.021088887937366962 Engine time: 0.12494090339168906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 17280, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 17280]
Prompts retrieved: 104175 . Total input tokens: 23254663 . Total output tokens: 20434481
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7309215972200036,
    "estimated_duration": 3600.0127364865866,
    "input_throughput": 2389.8401560646967,
    "output_throughput": 2090.436215329773,
    "total_throughput": 4480.27637139447,
    "itl": 25.28855590387367,
    "ttft": 5708.106271810115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 34944,
    "finished_requests": 34888,
    "scheduler_time": 5.201079158388329
}
#Debug simulation 
Total elapsed time: 2.731012641917914. Arrivals time: 0.09450122294947505 Scheduler time: 2.299227314069867 Scheduler overhead time: 0.12998449662700295 Adapter cache time: 0.020936955232173204 Engine time: 0.12441850267350674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 17280, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 17280]
Prompts retrieved: 104175 . Total input tokens: 23254663 . Total output tokens: 20434481
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7290307572111487,
    "estimated_duration": 3600.00548042786,
    "input_throughput": 2389.8449729519525,
    "output_throughput": 2090.4404287477873,
    "total_throughput": 4480.28540169974,
    "itl": 25.288537834324256,
    "ttft": 5708.175110271763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 34944,
    "finished_requests": 34888,
    "scheduler_time": 5.2010384600638835
}
#Debug simulation 
Total elapsed time: 2.729124516248703. Arrivals time: 0.09158661309629679 Scheduler time: 2.30508332233876 Scheduler overhead time: 0.12756464257836342 Adapter cache time: 0.02095226338133216 Engine time: 0.12278675939887762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 78840 . Total input tokens: 17556638 . Total output tokens: 15470465
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.2007186338305473,
    "estimated_duration": 3599.961798153842,
    "input_throughput": 1802.9202430227108,
    "output_throughput": 1577.677019492996,
    "total_throughput": 3380.5972625157065,
    "itl": 25.49328299379127,
    "ttft": 6724.511687877888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 26398,
    "finished_requests": 26349,
    "scheduler_time": 0.6847007820815908
}
#Debug simulation 
Total elapsed time: 2.2008167537860572. Arrivals time: 0.07595888571813703 Scheduler time: 1.7929474120028317 Scheduler overhead time: 0.1241775001399219 Adapter cache time: 0.025532911531627178 Engine time: 0.12212732806801796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 78840 . Total input tokens: 17556638 . Total output tokens: 15470465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.1414575381204486,
    "estimated_duration": 3599.972795617462,
    "input_throughput": 1802.9147353283731,
    "output_throughput": 1577.6721998883459,
    "total_throughput": 3380.586935216719,
    "itl": 25.493335987275927,
    "ttft": 6724.443542994463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 26398,
    "finished_requests": 26349,
    "scheduler_time": 0.6847451498480418
}
#Debug simulation 
Total elapsed time: 2.141537464223802. Arrivals time: 0.07133900700137019 Scheduler time: 1.7398865418508649 Scheduler overhead time: 0.12330179149284959 Adapter cache time: 0.025190606247633696 Engine time: 0.12242253264412284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 78840 . Total input tokens: 17556638 . Total output tokens: 15470465
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.194456573110074,
    "estimated_duration": 3599.981383146288,
    "input_throughput": 1802.910434588838,
    "output_throughput": 1577.668436450691,
    "total_throughput": 3380.578871039529,
    "itl": 25.493469136943283,
    "ttft": 6724.39812999968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 26398,
    "finished_requests": 26349,
    "scheduler_time": 0.6848945993695803
}
#Debug simulation 
Total elapsed time: 2.194549552164972. Arrivals time: 0.07646073494106531 Scheduler time: 1.7848127633333206 Scheduler overhead time: 0.12372542172670364 Adapter cache time: 0.025430713314563036 Engine time: 0.12428400292992592 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 78840 . Total input tokens: 17556638 . Total output tokens: 15470465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.2198483808897436,
    "estimated_duration": 3599.9681442266406,
    "input_throughput": 1802.9170648103895,
    "output_throughput": 1577.6742383424921,
    "total_throughput": 3380.5913031528817,
    "itl": 25.49330325836269,
    "ttft": 6724.441643794292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 26398,
    "finished_requests": 26349,
    "scheduler_time": 0.684716961313756
}
#Debug simulation 
Total elapsed time: 2.2199402102269232. Arrivals time: 0.07611314952373505 Scheduler time: 1.81175631377846 Scheduler overhead time: 0.124446215108037 Adapter cache time: 0.02549454616382718 Engine time: 0.12190370261669159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 78840 . Total input tokens: 17556638 . Total output tokens: 15470465
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.1859903912991285,
    "estimated_duration": 3599.978508005346,
    "input_throughput": 1802.9118744923244,
    "output_throughput": 1577.6696964635228,
    "total_throughput": 3380.5815709558474,
    "itl": 25.493464355728076,
    "ttft": 6724.428922188259,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 26398,
    "finished_requests": 26349,
    "scheduler_time": 0.6848685374273114
}
#Debug simulation 
Total elapsed time: 2.18608277104795. Arrivals time: 0.0735651096329093 Scheduler time: 1.7798451273702085 Scheduler overhead time: 0.12484053103253245 Adapter cache time: 0.025442782323807478 Engine time: 0.1219573337584734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 78840 . Total input tokens: 17556638 . Total output tokens: 15470465
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.1417941418476403,
    "estimated_duration": 3599.9818464875834,
    "input_throughput": 1802.91020254243,
    "output_throughput": 1577.6682333944068,
    "total_throughput": 3380.5784359368367,
    "itl": 25.493378404603114,
    "ttft": 6724.389447181475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 26398,
    "finished_requests": 26349,
    "scheduler_time": 0.6849250396178846
}
#Debug simulation 
Total elapsed time: 2.1418887358158827. Arrivals time: 0.07305727945640683 Scheduler time: 1.738408244214952 Scheduler overhead time: 0.12346014706417918 Adapter cache time: 0.025077727623283863 Engine time: 0.12233718018978834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 78840 . Total input tokens: 17556638 . Total output tokens: 15470465
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.1811667759902775,
    "estimated_duration": 3599.9783348929814,
    "input_throughput": 1802.9119611890512,
    "output_throughput": 1577.669772329016,
    "total_throughput": 3380.5817335180673,
    "itl": 25.49347713615351,
    "ttft": 6724.461744475339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 26398,
    "finished_requests": 26349,
    "scheduler_time": 0.6848756679353804
}
#Debug simulation 
Total elapsed time: 2.1812584758736193. Arrivals time: 0.07445351267233491 Scheduler time: 1.7736283540725708 Scheduler overhead time: 0.12414465565234423 Adapter cache time: 0.025433422531932592 Engine time: 0.12390260165557265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 8640, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 76140 . Total input tokens: 16963069 . Total output tokens: 14936264
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.106519592925906,
    "estimated_duration": 3600.0176103933113,
    "input_throughput": 1738.8995492486135,
    "output_throughput": 1519.9861756793384,
    "total_throughput": 3258.885724927952,
    "itl": 25.06787839954705,
    "ttft": 5966.0228298164875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 25522,
    "finished_requests": 25479,
    "scheduler_time": 0.4094363135299527
}
#Debug simulation 
Total elapsed time: 2.106600308790803. Arrivals time: 0.06914510391652584 Scheduler time: 1.7057409877888858 Scheduler overhead time: 0.12487546633929014 Adapter cache time: 0.02462710114195943 Engine time: 0.12193743232637644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 8640, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 76140 . Total input tokens: 16963069 . Total output tokens: 14936264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.091005625203252,
    "estimated_duration": 3600.014717757841,
    "input_throughput": 1738.9009464658222,
    "output_throughput": 1519.9873969982139,
    "total_throughput": 3258.888343464036,
    "itl": 25.06790578836949,
    "ttft": 5966.045314847315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 25522,
    "finished_requests": 25479,
    "scheduler_time": 0.40929507874650206
}
#Debug simulation 
Total elapsed time: 2.091100118122995. Arrivals time: 0.07251142524182796 Scheduler time: 1.6863121329806745 Scheduler overhead time: 0.1247033653780818 Adapter cache time: 0.02460141060873866 Engine time: 0.12255958281457424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 8640, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 76140 . Total input tokens: 16963069 . Total output tokens: 14936264
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.1025328328832984,
    "estimated_duration": 3600.015131722826,
    "input_throughput": 1738.9007465099671,
    "output_throughput": 1519.9872222151819,
    "total_throughput": 3258.8879687251488,
    "itl": 25.067888210032386,
    "ttft": 5966.0068053127015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 25522,
    "finished_requests": 25479,
    "scheduler_time": 0.4093712212352809
}
#Debug simulation 
Total elapsed time: 2.102626997977495. Arrivals time: 0.07155203027650714 Scheduler time: 1.6915199868381023 Scheduler overhead time: 0.1296998872421682 Adapter cache time: 0.024833483155816793 Engine time: 0.12369451113045216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 8640, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 76140 . Total input tokens: 16963069 . Total output tokens: 14936264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.0883025731891394,
    "estimated_duration": 3600.0035507201096,
    "input_throughput": 1738.9063404528579,
    "output_throughput": 1519.9921119259561,
    "total_throughput": 3258.898452378814,
    "itl": 25.067929167644973,
    "ttft": 5966.04725284967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 25522,
    "finished_requests": 25479,
    "scheduler_time": 0.4094596232701866
}
#Debug simulation 
Total elapsed time: 2.088409341406077. Arrivals time: 0.0744373775087297 Scheduler time: 1.6800873982720077 Scheduler overhead time: 0.12459645047783852 Adapter cache time: 0.024666799698024988 Engine time: 0.12253447296097875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 8640, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 76140 . Total input tokens: 16963069 . Total output tokens: 14936264
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.0794158671051264,
    "estimated_duration": 3600.0149239800967,
    "input_throughput": 1738.9008468551033,
    "output_throughput": 1519.9873099276776,
    "total_throughput": 3258.8881567827807,
    "itl": 25.067921938674782,
    "ttft": 5966.027759246416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 25522,
    "finished_requests": 25479,
    "scheduler_time": 0.40933077315486827
}
#Debug simulation 
Total elapsed time: 2.0795071688480675. Arrivals time: 0.0686043482273817 Scheduler time: 1.6814366094768047 Scheduler overhead time: 0.12387612089514732 Adapter cache time: 0.02447574958205223 Engine time: 0.12079283874481916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 8640, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 76140 . Total input tokens: 16963069 . Total output tokens: 14936264
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.0806926423683763,
    "estimated_duration": 3600.017326119186,
    "input_throughput": 1738.8996865602162,
    "output_throughput": 1519.9862957045223,
    "total_throughput": 3258.8859822647382,
    "itl": 25.067916022005853,
    "ttft": 5965.994967590816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 25522,
    "finished_requests": 25479,
    "scheduler_time": 0.40938977730346887
}
#Debug simulation 
Total elapsed time: 2.08078794227913. Arrivals time: 0.07263434259220958 Scheduler time: 1.6762699568644166 Scheduler overhead time: 0.12509753555059433 Adapter cache time: 0.024441839195787907 Engine time: 0.12208589725196362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 8640, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 76140 . Total input tokens: 16963069 . Total output tokens: 14936264
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.1610218049027026,
    "estimated_duration": 3600.016254960126,
    "input_throughput": 1738.9002039573672,
    "output_throughput": 1519.986747965561,
    "total_throughput": 3258.8869519229283,
    "itl": 25.06792639756843,
    "ttft": 5966.055447250765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 25522,
    "finished_requests": 25479,
    "scheduler_time": 0.4092887822244418
}
#Debug simulation 
Total elapsed time: 2.161113578826189. Arrivals time: 0.07619087118655443 Scheduler time: 1.735069281887263 Scheduler overhead time: 0.13320442056283355 Adapter cache time: 0.024908043444156647 Engine time: 0.1255642008036375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 8640, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74790 . Total input tokens: 16669362 . Total output tokens: 14669461
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.0762655660510063,
    "estimated_duration": 3599.974210869193,
    "input_throughput": 1702.197749500117,
    "output_throughput": 1496.4487755870055,
    "total_throughput": 3198.6465250871224,
    "itl": 24.797062637905732,
    "ttft": 7088.5888114216505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 25030,
    "finished_requests": 24981,
    "scheduler_time": 0.26175938535908083
}
#Debug simulation 
Total elapsed time: 2.076340419705957. Arrivals time: 0.06887109763920307 Scheduler time: 1.6714234105311334 Scheduler overhead time: 0.12603944959118962 Adapter cache time: 0.02397919027134776 Engine time: 0.12511297734454274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 8640, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74790 . Total input tokens: 16669362 . Total output tokens: 14669461
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.0681357202120125,
    "estimated_duration": 3599.958815091694,
    "input_throughput": 1702.2050292105685,
    "output_throughput": 1496.4551753803285,
    "total_throughput": 3198.6602045908967,
    "itl": 24.79718742376454,
    "ttft": 7088.600468907859,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 25030,
    "finished_requests": 24981,
    "scheduler_time": 0.2618904453204004
}
#Debug simulation 
Total elapsed time: 2.0682303020730615. Arrivals time: 0.07072261581197381 Scheduler time: 1.6649239328689873 Scheduler overhead time: 0.12565013533458114 Adapter cache time: 0.02385814907029271 Engine time: 0.12206593481823802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 8640, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74790 . Total input tokens: 16669362 . Total output tokens: 14669461
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.072737565729767,
    "estimated_duration": 3599.9618996400704,
    "input_throughput": 1702.2035707135326,
    "output_throughput": 1496.453893175541,
    "total_throughput": 3198.657463889074,
    "itl": 24.797159869824668,
    "ttft": 7088.6198754144525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 25030,
    "finished_requests": 24981,
    "scheduler_time": 0.26172294021869136
}
#Debug simulation 
Total elapsed time: 2.0728288986720145. Arrivals time: 0.07154059549793601 Scheduler time: 1.6616864674724638 Scheduler overhead time: 0.12622846197336912 Adapter cache time: 0.023712768219411373 Engine time: 0.12871046410873532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 8640, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74790 . Total input tokens: 16669362 . Total output tokens: 14669461
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.0558688640594482,
    "estimated_duration": 3599.9808909412004,
    "input_throughput": 1702.194590926813,
    "output_throughput": 1496.4459987984949,
    "total_throughput": 3198.6405897253076,
    "itl": 24.79715347705711,
    "ttft": 7088.5270693447255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 25030,
    "finished_requests": 24981,
    "scheduler_time": 0.2618400312758977
}
#Debug simulation 
Total elapsed time: 2.0559657653793693. Arrivals time: 0.07127392012625933 Scheduler time: 1.650630476884544 Scheduler overhead time: 0.12577745970338583 Adapter cache time: 0.023705829866230488 Engine time: 0.12391895009204745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 8640, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74790 . Total input tokens: 16669362 . Total output tokens: 14669461
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.06389043526724,
    "estimated_duration": 3599.9589936074008,
    "input_throughput": 1702.2049448011808,
    "output_throughput": 1496.4551011737183,
    "total_throughput": 3198.6600459748993,
    "itl": 24.79714303386614,
    "ttft": 7088.601483539905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 25030,
    "finished_requests": 24981,
    "scheduler_time": 0.26177327100920805
}
#Debug simulation 
Total elapsed time: 2.0639825533144176. Arrivals time: 0.0704343756660819 Scheduler time: 1.6603907188400626 Scheduler overhead time: 0.12558512249961495 Adapter cache time: 0.02396299596875906 Engine time: 0.12280204007402062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 8640, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74790 . Total input tokens: 16669362 . Total output tokens: 14669461
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.0704285809770226,
    "estimated_duration": 3599.966592025753,
    "input_throughput": 1702.2013519719249,
    "output_throughput": 1496.451942618878,
    "total_throughput": 3198.653294590803,
    "itl": 24.797001067281343,
    "ttft": 7088.648595240986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 25030,
    "finished_requests": 24981,
    "scheduler_time": 0.2617034669105086
}
#Debug simulation 
Total elapsed time: 2.0705486969091. Arrivals time: 0.07197830639779568 Scheduler time: 1.663221943192184 Scheduler overhead time: 0.1258725170046091 Adapter cache time: 0.023745637387037277 Engine time: 0.1246643103659153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 8640, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74790 . Total input tokens: 16669362 . Total output tokens: 14669461
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.0700501520186663,
    "estimated_duration": 3599.958974953349,
    "input_throughput": 1702.2049536215645,
    "output_throughput": 1496.4551089279598,
    "total_throughput": 3198.6600625495244,
    "itl": 24.797150945335073,
    "ttft": 7088.588426546915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 25030,
    "finished_requests": 24981,
    "scheduler_time": 0.26178469656932674
}
#Debug simulation 
Total elapsed time: 2.070157077629119. Arrivals time: 0.07204080885276198 Scheduler time: 1.6560925133526325 Scheduler overhead time: 0.13165848143398762 Adapter cache time: 0.023997661657631397 Engine time: 0.12437257636338472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 8640, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74115 . Total input tokens: 16520830 . Total output tokens: 14548682
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.0394320846535265,
    "estimated_duration": 3599.788021522544,
    "input_throughput": 1700.55124451768,
    "output_throughput": 1482.3397844807184,
    "total_throughput": 3182.8910289983987,
    "itl": 24.72416244210002,
    "ttft": 7735.30564442555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 24797,
    "finished_requests": 24744,
    "scheduler_time": 0.2791490459211855
}
#Debug simulation 
Total elapsed time: 2.0395225468091667. Arrivals time: 0.0689557297155261 Scheduler time: 1.636181138921529 Scheduler overhead time: 0.12637239508330822 Adapter cache time: 0.023545240983366966 Engine time: 0.1233890731818974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 8640, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74115 . Total input tokens: 16520830 . Total output tokens: 14548682
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.05519532924518,
    "estimated_duration": 3599.8007445459275,
    "input_throughput": 1700.5649574563272,
    "output_throughput": 1482.365935971576,
    "total_throughput": 3182.9308934279034,
    "itl": 24.724312384926375,
    "ttft": 7590.114142041166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 24797,
    "finished_requests": 24745,
    "scheduler_time": 0.2791432080191219
}
#Debug simulation 
Total elapsed time: 2.0552869611419737. Arrivals time: 0.0705177066847682 Scheduler time: 1.6488634403795004 Scheduler overhead time: 0.1265956680290401 Adapter cache time: 0.023704261519014835 Engine time: 0.12431094562634826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 8640, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74115 . Total input tokens: 16520830 . Total output tokens: 14548682
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.038018028717488,
    "estimated_duration": 3599.8044638918946,
    "input_throughput": 1700.563200419388,
    "output_throughput": 1482.364404379563,
    "total_throughput": 3182.927604798951,
    "itl": 24.724331487426124,
    "ttft": 7590.177776195768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 24797,
    "finished_requests": 24745,
    "scheduler_time": 0.27923302778203885
}
#Debug simulation 
Total elapsed time: 2.038110266905278. Arrivals time: 0.06977080460637808 Scheduler time: 1.6355775562115014 Scheduler overhead time: 0.1261540623381734 Adapter cache time: 0.023376667872071266 Engine time: 0.12225482007488608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 8640, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74115 . Total input tokens: 16520830 . Total output tokens: 14548682
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.0431489599868655,
    "estimated_duration": 3599.792812672274,
    "input_throughput": 1700.5489811664097,
    "output_throughput": 1482.3378115583232,
    "total_throughput": 3182.886792724733,
    "itl": 24.724127971884304,
    "ttft": 7735.344268846509,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 24797,
    "finished_requests": 24744,
    "scheduler_time": 0.2790368336580287
}
#Debug simulation 
Total elapsed time: 2.043245800770819. Arrivals time: 0.06881670793518424 Scheduler time: 1.6352181006222963 Scheduler overhead time: 0.13118362706154585 Adapter cache time: 0.023635934107005596 Engine time: 0.12251594429835677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 8640, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74115 . Total input tokens: 16520830 . Total output tokens: 14548682
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.0550197130069137,
    "estimated_duration": 3599.8035250456223,
    "input_throughput": 1700.563643934544,
    "output_throughput": 1482.3647909874112,
    "total_throughput": 3182.928434921955,
    "itl": 24.72434263755789,
    "ttft": 7590.109080514822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 24797,
    "finished_requests": 24745,
    "scheduler_time": 0.27917248078342377
}
#Debug simulation 
Total elapsed time: 2.055135285947472. Arrivals time: 0.06885975832119584 Scheduler time: 1.6447412427514791 Scheduler overhead time: 0.1312875892035663 Adapter cache time: 0.023713724222034216 Engine time: 0.12452778359875083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 8640, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74115 . Total input tokens: 16520830 . Total output tokens: 14548682
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.0582505627535284,
    "estimated_duration": 3599.8063039860385,
    "input_throughput": 1700.5623311513991,
    "output_throughput": 1482.3636466471103,
    "total_throughput": 3182.9259777985094,
    "itl": 24.72419880320775,
    "ttft": 7590.108787081882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 24797,
    "finished_requests": 24745,
    "scheduler_time": 0.2791433331411261
}
#Debug simulation 
Total elapsed time: 2.0583546911366284. Arrivals time: 0.06973596755415201 Scheduler time: 1.6530815395526588 Scheduler overhead time: 0.1262496286071837 Adapter cache time: 0.023554205894470215 Engine time: 0.12477794429287314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 8640, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 74115 . Total input tokens: 16520830 . Total output tokens: 14548682
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.0635676658712327,
    "estimated_duration": 3599.801710656457,
    "input_throughput": 1700.5645010607132,
    "output_throughput": 1482.3655381359576,
    "total_throughput": 3182.930039196671,
    "itl": 24.724330109452442,
    "ttft": 7590.130068637849,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 24797,
    "finished_requests": 24745,
    "scheduler_time": 0.2791465439631584
}
#Debug simulation 
Total elapsed time: 2.0636583920568228. Arrivals time: 0.06838281825184822 Scheduler time: 1.6532679218798876 Scheduler overhead time: 0.1317352163605392 Adapter cache time: 0.02369001880288124 Engine time: 0.12424086080864072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 8640, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73770 . Total input tokens: 16440716 . Total output tokens: 14475015
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.027422587852925,
    "estimated_duration": 3599.671488245175,
    "input_throughput": 1684.4070409757974,
    "output_throughput": 1471.6995751694483,
    "total_throughput": 3156.1066161452454,
    "itl": 24.636357617430125,
    "ttft": 5578.332849212919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 24704,
    "finished_requests": 24666,
    "scheduler_time": 0.18462124626410822
}
#Debug simulation 
Total elapsed time: 2.0275183650664985. Arrivals time: 0.06982536287978292 Scheduler time: 1.6246868344023824 Scheduler overhead time: 0.12631244165822864 Adapter cache time: 0.02318922383710742 Engine time: 0.12236364278942347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 8640, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73770 . Total input tokens: 16440716 . Total output tokens: 14475015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.0296433470211923,
    "estimated_duration": 3599.6867035311893,
    "input_throughput": 1684.3999212631657,
    "output_throughput": 1471.6933545364302,
    "total_throughput": 3156.093275799596,
    "itl": 24.6364846015989,
    "ttft": 5578.345713388979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 24704,
    "finished_requests": 24666,
    "scheduler_time": 0.18464989341839044
}
#Debug simulation 
Total elapsed time: 2.0297330389730632. Arrivals time: 0.06972951255738735 Scheduler time: 1.6265002652071416 Scheduler overhead time: 0.12604108871892095 Adapter cache time: 0.02301717596128583 Engine time: 0.12331596715375781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 8640, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73770 . Total input tokens: 16440716 . Total output tokens: 14475015
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.0335986288264394,
    "estimated_duration": 3599.691035861747,
    "input_throughput": 1684.3978940399466,
    "output_throughput": 1471.6915833116145,
    "total_throughput": 3156.089477351561,
    "itl": 24.636506439425713,
    "ttft": 5578.356447839809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 24704,
    "finished_requests": 24666,
    "scheduler_time": 0.18476231405753635
}
#Debug simulation 
Total elapsed time: 2.0336889778263867. Arrivals time: 0.0696722180582583 Scheduler time: 1.6306866160593927 Scheduler overhead time: 0.12540447060018778 Adapter cache time: 0.02317368471994996 Engine time: 0.12360333604738116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 8640, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73770 . Total input tokens: 16440716 . Total output tokens: 14475015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.042920909821987,
    "estimated_duration": 3599.6780391752854,
    "input_throughput": 1684.4039755814251,
    "output_throughput": 1471.6968968740687,
    "total_throughput": 3156.100872455494,
    "itl": 24.636422335150353,
    "ttft": 5578.311046547732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 24704,
    "finished_requests": 24666,
    "scheduler_time": 0.18447409032460899
}
#Debug simulation 
Total elapsed time: 2.043017685879022. Arrivals time: 0.06964721251279116 Scheduler time: 1.6362561956048012 Scheduler overhead time: 0.12692975625395775 Adapter cache time: 0.023073609452694654 Engine time: 0.12585382908582687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 8640, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73770 . Total input tokens: 16440716 . Total output tokens: 14475015
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.021660386584699,
    "estimated_duration": 3599.6877403391604,
    "input_throughput": 1684.3994361101772,
    "output_throughput": 1471.692930648718,
    "total_throughput": 3156.092366758895,
    "itl": 24.63647246842226,
    "ttft": 5578.328062616373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 24704,
    "finished_requests": 24666,
    "scheduler_time": 0.1847104404170053
}
#Debug simulation 
Total elapsed time: 2.021753702778369. Arrivals time: 0.06940528517588973 Scheduler time: 1.6160313519649208 Scheduler overhead time: 0.12701186491176486 Adapter cache time: 0.02329108025878668 Engine time: 0.12492274353280663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 8640, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73770 . Total input tokens: 16440716 . Total output tokens: 14475015
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.018698262050748,
    "estimated_duration": 3599.667811872615,
    "input_throughput": 1684.4087612755998,
    "output_throughput": 1471.7010782292355,
    "total_throughput": 3156.1098395048352,
    "itl": 24.636348584703512,
    "ttft": 5578.361481525942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 24704,
    "finished_requests": 24666,
    "scheduler_time": 0.18463621614425263
}
#Debug simulation 
Total elapsed time: 2.0187894329428673. Arrivals time: 0.06957482732832432 Scheduler time: 1.6167067275382578 Scheduler overhead time: 0.12648015515878797 Adapter cache time: 0.023181164637207985 Engine time: 0.1217724340967834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 8640, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73770 . Total input tokens: 16440716 . Total output tokens: 14475015
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.0207595857791603,
    "estimated_duration": 3599.687739207344,
    "input_throughput": 1684.3994366397874,
    "output_throughput": 1471.6929311114486,
    "total_throughput": 3156.092367751236,
    "itl": 24.636486768402477,
    "ttft": 5578.313265895166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 24704,
    "finished_requests": 24666,
    "scheduler_time": 0.18470472763694598
}
#Debug simulation 
Total elapsed time: 2.0208832607604563. Arrivals time: 0.06928993435576558 Scheduler time: 1.6189258079975843 Scheduler overhead time: 0.12578216008841991 Adapter cache time: 0.023282958660274744 Engine time: 0.12244493560865521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 8640, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73605 . Total input tokens: 16403489 . Total output tokens: 14440022
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.0313531099818647,
    "estimated_duration": 3599.9176351316482,
    "input_throughput": 1683.5096283469188,
    "output_throughput": 1464.0893304235988,
    "total_throughput": 3147.5989587705176,
    "itl": 24.593922626213036,
    "ttft": 6025.771218005911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 24661,
    "finished_requests": 24620,
    "scheduler_time": 0.21650221245899526
}
#Debug simulation 
Total elapsed time: 2.0314485132694244. Arrivals time: 0.06779511692002416 Scheduler time: 1.622192567680031 Scheduler overhead time: 0.13069115579128265 Adapter cache time: 0.023107762448489666 Engine time: 0.12565255584195256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 8640, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73605 . Total input tokens: 16403489 . Total output tokens: 14440022
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.0271287467330694,
    "estimated_duration": 3599.9385760726263,
    "input_throughput": 1683.4998353254496,
    "output_throughput": 1464.080813775993,
    "total_throughput": 3147.5806491014428,
    "itl": 24.618770687831386,
    "ttft": 6025.944066544391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 24661,
    "finished_requests": 24620,
    "scheduler_time": 0.21820787119646456
}
#Debug simulation 
Total elapsed time: 2.027217990718782. Arrivals time: 0.0682689156383276 Scheduler time: 1.6265909927897155 Scheduler overhead time: 0.12557323928922415 Adapter cache time: 0.022560354322195053 Engine time: 0.12296318309381604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 8640, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73605 . Total input tokens: 16403489 . Total output tokens: 14440022
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.008701973594725,
    "estimated_duration": 3599.9171215574715,
    "input_throughput": 1683.509868521079,
    "output_throughput": 1464.0895392946497,
    "total_throughput": 3147.5994078157287,
    "itl": 24.618642740184235,
    "ttft": 6025.937287061365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 24661,
    "finished_requests": 24620,
    "scheduler_time": 0.21815207786989643
}
#Debug simulation 
Total elapsed time: 2.008790352847427. Arrivals time: 0.06867324793711305 Scheduler time: 1.6086491090245545 Scheduler overhead time: 0.12642114190384746 Adapter cache time: 0.02246836433187127 Engine time: 0.12150161759927869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 8640, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73605 . Total input tokens: 16403489 . Total output tokens: 14440022
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.01713356282562,
    "estimated_duration": 3599.930722902144,
    "input_throughput": 1683.5035078436815,
    "output_throughput": 1464.0840076364073,
    "total_throughput": 3147.5875154800888,
    "itl": 24.618648265718818,
    "ttft": 6025.834949683386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 24661,
    "finished_requests": 24620,
    "scheduler_time": 0.21811246377549265
}
#Debug simulation 
Total elapsed time: 2.01722371019423. Arrivals time: 0.06899553909897804 Scheduler time: 1.6130927596241236 Scheduler overhead time: 0.1276475489139557 Adapter cache time: 0.02265609474852681 Engine time: 0.12346793571487069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 8640, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73605 . Total input tokens: 16403489 . Total output tokens: 14440022
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.0161536508239806,
    "estimated_duration": 3599.91677064431,
    "input_throughput": 1683.510032626476,
    "output_throughput": 1464.0896820113628,
    "total_throughput": 3147.5997146378386,
    "itl": 24.618664074120915,
    "ttft": 6025.9182608609135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 24661,
    "finished_requests": 24620,
    "scheduler_time": 0.2181519527478925
}
#Debug simulation 
Total elapsed time: 2.016242649871856. Arrivals time: 0.06868453649803996 Scheduler time: 1.6126225404441357 Scheduler overhead time: 0.12660389579832554 Adapter cache time: 0.02256388869136572 Engine time: 0.12474380061030388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 8640, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73605 . Total input tokens: 16403489 . Total output tokens: 14440022
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.0058238678611815,
    "estimated_duration": 3599.9176964971357,
    "input_throughput": 1683.5095996492103,
    "output_throughput": 1464.0893054662072,
    "total_throughput": 3147.5989051154174,
    "itl": 24.593943178461267,
    "ttft": 6025.848868342307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 24661,
    "finished_requests": 24620,
    "scheduler_time": 0.21645701070653622
}
#Debug simulation 
Total elapsed time: 2.0059022810310125. Arrivals time: 0.06606737803667784 Scheduler time: 1.6077930214814842 Scheduler overhead time: 0.12580560939386487 Adapter cache time: 0.02303859917446971 Engine time: 0.12212106073275208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 8640, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 8640]
Prompts retrieved: 73605 . Total input tokens: 16403489 . Total output tokens: 14440022
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.008688999339938,
    "estimated_duration": 3599.9400545877256,
    "input_throughput": 1683.4991439028458,
    "output_throughput": 1464.080212469983,
    "total_throughput": 3147.579356372829,
    "itl": 24.618780779239632,
    "ttft": 6025.904325806433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 24661,
    "finished_requests": 24620,
    "scheduler_time": 0.21819978158038209
}
#Debug simulation 
Total elapsed time: 2.0087798363529146. Arrivals time: 0.06922265514731407 Scheduler time: 1.6069949786178768 Scheduler overhead time: 0.1264719944447279 Adapter cache time: 0.022364893462508917 Engine time: 0.1226850706152618 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 8640, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 59940 . Total input tokens: 13317150 . Total output tokens: 11780890
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.7488488079980016,
    "estimated_duration": 3600.0009798563674,
    "input_throughput": 1372.1385154170384,
    "output_throughput": 1207.854115687904,
    "total_throughput": 2579.9926311049426,
    "itl": 23.65419041342532,
    "ttft": 7190.178103200933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 20138,
    "finished_requests": 20097,
    "scheduler_time": 0.0013867454763313923
}
#Debug simulation 
Total elapsed time: 1.7489466578699648. Arrivals time: 0.05834433436393738 Scheduler time: 1.3432812332175672 Scheduler overhead time: 0.13084936328232288 Adapter cache time: 0.026553614996373653 Engine time: 0.12666425202041864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 8640, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 59940 . Total input tokens: 13317150 . Total output tokens: 11780890
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7515302561223507,
    "estimated_duration": 3600.0134638747563,
    "input_throughput": 1372.1337571563736,
    "output_throughput": 1207.849927127738,
    "total_throughput": 2579.9836842841114,
    "itl": 23.654322095075187,
    "ttft": 7368.404029781867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 20138,
    "finished_requests": 20097,
    "scheduler_time": 0.001392208012382929
}
#Debug simulation 
Total elapsed time: 1.7516315253451467. Arrivals time: 0.05955773266032338 Scheduler time: 1.3462156313471496 Scheduler overhead time: 0.12935318471863866 Adapter cache time: 0.02637850120663643 Engine time: 0.1271718665957451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 8640, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 59940 . Total input tokens: 13317150 . Total output tokens: 11780890
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7510355091653764,
    "estimated_duration": 3600.0153575198246,
    "input_throughput": 1372.1330353999185,
    "output_throughput": 1207.8492917862657,
    "total_throughput": 2579.982327186184,
    "itl": 23.654286573056236,
    "ttft": 7368.453313801319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 20138,
    "finished_requests": 20097,
    "scheduler_time": 0.0013792396022501545
}
#Debug simulation 
Total elapsed time: 1.751112683210522. Arrivals time: 0.057401233818382025 Scheduler time: 1.348090092651546 Scheduler overhead time: 0.13031362695619464 Adapter cache time: 0.02643294259905815 Engine time: 0.12595396162942052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 8640, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 59940 . Total input tokens: 13317150 . Total output tokens: 11780890
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7394364927895367,
    "estimated_duration": 3600.0080051633254,
    "input_throughput": 1372.1358377301428,
    "output_throughput": 1207.8517585970556,
    "total_throughput": 2579.9875963271984,
    "itl": 23.65414639257342,
    "ttft": 7368.543527542581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 20138,
    "finished_requests": 20097,
    "scheduler_time": 0.001366396314121263
}
#Debug simulation 
Total elapsed time: 1.739531829021871. Arrivals time: 0.0580262653529644 Scheduler time: 1.338193730916828 Scheduler overhead time: 0.1293435492552817 Adapter cache time: 0.026174148079007864 Engine time: 0.1250353967770934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 8640, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 59940 . Total input tokens: 13317150 . Total output tokens: 11780890
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.7742780507542193,
    "estimated_duration": 3600.0135312929106,
    "input_throughput": 1372.1337314601576,
    "output_throughput": 1207.8499045080973,
    "total_throughput": 2579.983635968255,
    "itl": 23.654318622273994,
    "ttft": 7368.39034036016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 20138,
    "finished_requests": 20097,
    "scheduler_time": 0.0013792396022501545
}
#Debug simulation 
Total elapsed time: 1.7743769688531756. Arrivals time: 0.06008803052827716 Scheduler time: 1.3649920434691012 Scheduler overhead time: 0.13172606751322746 Adapter cache time: 0.026652508415281773 Engine time: 0.12741705821827054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 8640, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 59940 . Total input tokens: 13317150 . Total output tokens: 11780890
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.7679611979983747,
    "estimated_duration": 3600.0263238409325,
    "input_throughput": 1372.2069106232107,
    "output_throughput": 1207.9164452776763,
    "total_throughput": 2580.123355900887,
    "itl": 23.654406902037486,
    "ttft": 7189.810843784152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 20138,
    "finished_requests": 20098,
    "scheduler_time": 0.001388413448349445
}
#Debug simulation 
Total elapsed time: 1.7680553519167006. Arrivals time: 0.06028449535369873 Scheduler time: 1.3611516854725778 Scheduler overhead time: 0.12871276959776878 Adapter cache time: 0.026332798413932323 Engine time: 0.12863413663581014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 8640, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 59940 . Total input tokens: 13317150 . Total output tokens: 11780890
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7520346636883914,
    "estimated_duration": 3600.0135196295646,
    "input_throughput": 1372.133735905605,
    "output_throughput": 1207.8499084212967,
    "total_throughput": 2579.983644326902,
    "itl": 23.65433835912327,
    "ttft": 7368.393205736238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 20138,
    "finished_requests": 20097,
    "scheduler_time": 0.0013832844102914037
}
#Debug simulation 
Total elapsed time: 1.752114719711244. Arrivals time: 0.05722347414121032 Scheduler time: 1.3502190317958593 Scheduler overhead time: 0.12962647806853056 Adapter cache time: 0.026940734591335058 Engine time: 0.12534669181331992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 8640, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 58590 . Total input tokens: 13016047 . Total output tokens: 11522829
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.779549467843026,
    "estimated_duration": 3599.802534429846,
    "input_throughput": 1342.9531075002951,
    "output_throughput": 1197.5101297279255,
    "total_throughput": 2540.4632372282204,
    "itl": 23.468075061282825,
    "ttft": 5152.95062935366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 19710,
    "finished_requests": 19682,
    "scheduler_time": 0.0006889528051380879
}
#Debug simulation 
Total elapsed time: 1.7796298419125378. Arrivals time: 0.05665941769257188 Scheduler time: 1.3753896779380739 Scheduler overhead time: 0.12986540142446756 Adapter cache time: 0.025527209043502808 Engine time: 0.12730736192315817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 8640, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 58590 . Total input tokens: 13016047 . Total output tokens: 11522829
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7379813427105546,
    "estimated_duration": 3599.8098888377117,
    "input_throughput": 1342.9503638484907,
    "output_throughput": 1197.5076832159737,
    "total_throughput": 2540.4580470644646,
    "itl": 23.468239344910394,
    "ttft": 5152.986754953211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 19710,
    "finished_requests": 19682,
    "scheduler_time": 0.0006962084352115603
}
#Debug simulation 
Total elapsed time: 1.7380763259716332. Arrivals time: 0.058408357203006744 Scheduler time: 1.3327032583765686 Scheduler overhead time: 0.13042451115325093 Adapter cache time: 0.02578808693215251 Engine time: 0.12751370761543512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 8640, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 58590 . Total input tokens: 13016047 . Total output tokens: 11522829
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7397095169872046,
    "estimated_duration": 3599.7927536832817,
    "input_throughput": 1342.9567563447956,
    "output_throughput": 1197.513383399425,
    "total_throughput": 2540.470139744221,
    "itl": 23.468371182546854,
    "ttft": 5152.8926254663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 19710,
    "finished_requests": 19682,
    "scheduler_time": 0.0007051320373030852
}
#Debug simulation 
Total elapsed time: 1.7398060359992087. Arrivals time: 0.05873843841254711 Scheduler time: 1.3340245094150305 Scheduler overhead time: 0.13014180772006512 Adapter cache time: 0.02553813997656107 Engine time: 0.12817461602389812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 8640, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 58590 . Total input tokens: 13016047 . Total output tokens: 11522829
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7568163201212883,
    "estimated_duration": 3599.8056547472743,
    "input_throughput": 1342.9519434263455,
    "output_throughput": 1197.5090917241869,
    "total_throughput": 2540.461035150532,
    "itl": 23.46818868554346,
    "ttft": 5152.938563365845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 19710,
    "finished_requests": 19682,
    "scheduler_time": 0.0006766932590104575
}
#Debug simulation 
Total elapsed time: 1.7569127371534705. Arrivals time: 0.058464222587645054 Scheduler time: 1.35073066316545 Scheduler overhead time: 0.1313625299371779 Adapter cache time: 0.025700439233332872 Engine time: 0.12703330675140023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 8640, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 58590 . Total input tokens: 13016047 . Total output tokens: 11522829
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.7393417148850858,
    "estimated_duration": 3599.811099071562,
    "input_throughput": 1342.9499123570251,
    "output_throughput": 1197.5072806214225,
    "total_throughput": 2540.4571929784474,
    "itl": 23.468211764433217,
    "ttft": 5152.943101140149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 19710,
    "finished_requests": 19682,
    "scheduler_time": 0.0007123876673765574
}
#Debug simulation 
Total elapsed time: 1.7394416029565036. Arrivals time: 0.058203800581395626 Scheduler time: 1.3341207597404718 Scheduler overhead time: 0.1302848127670586 Adapter cache time: 0.02551198983564973 Engine time: 0.12687576049938798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 8640, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 58590 . Total input tokens: 13016047 . Total output tokens: 11522829
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.7528116409666836,
    "estimated_duration": 3599.794155880346,
    "input_throughput": 1342.9562332343235,
    "output_throughput": 1197.5129169422673,
    "total_throughput": 2540.4691501765906,
    "itl": 23.468082281618276,
    "ttft": 5152.812593461996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 19710,
    "finished_requests": 19682,
    "scheduler_time": 0.0007042980512940589
}
#Debug simulation 
Total elapsed time: 1.7529146592132747. Arrivals time: 0.058804885018616915 Scheduler time: 1.3477779645472765 Scheduler overhead time: 0.13011799193918705 Adapter cache time: 0.02591205621138215 Engine time: 0.12709638103842735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 8640, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 58590 . Total input tokens: 13016047 . Total output tokens: 11522829
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7594316080212593,
    "estimated_duration": 3599.809994586948,
    "input_throughput": 1342.9503243975264,
    "output_throughput": 1197.5076480375828,
    "total_throughput": 2540.4579724351092,
    "itl": 23.468234556457027,
    "ttft": 5152.971907422571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 19710,
    "finished_requests": 19682,
    "scheduler_time": 0.0007034640652850324
}
#Debug simulation 
Total elapsed time: 1.7595219598151743. Arrivals time: 0.05900958878919482 Scheduler time: 1.353256628382951 Scheduler overhead time: 0.1298020826652646 Adapter cache time: 0.025608471129089594 Engine time: 0.12847430678084493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 8640, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57915 . Total input tokens: 12860002 . Total output tokens: 11395221
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.7070294925943017,
    "estimated_duration": 3599.928327342967,
    "input_throughput": 1339.9105652639998,
    "output_throughput": 1168.8285480687935,
    "total_throughput": 2508.7391133327933,
    "itl": 23.314802111388932,
    "ttft": 5395.938446891344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 19487,
    "finished_requests": 19458,
    "scheduler_time": 0.0017717529722733737
}
#Debug simulation 
Total elapsed time: 1.707123956643045. Arrivals time: 0.05744005925953388 Scheduler time: 1.3029398801736534 Scheduler overhead time: 0.13090420933440328 Adapter cache time: 0.025131524074822664 Engine time: 0.12730798590928316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 8640, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57915 . Total input tokens: 12860002 . Total output tokens: 11395221
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7389886151067913,
    "estimated_duration": 3599.9295627107654,
    "input_throughput": 1339.9101054543462,
    "output_throughput": 1168.8281469684039,
    "total_throughput": 2508.73825242275,
    "itl": 23.43191131134554,
    "ttft": 5396.200500266298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 19487,
    "finished_requests": 19458,
    "scheduler_time": 0.001923078841817651
}
#Debug simulation 
Total elapsed time: 1.7390959658659995. Arrivals time: 0.05855308938771486 Scheduler time: 1.3290135217830539 Scheduler overhead time: 0.13532836828380823 Adapter cache time: 0.024689813144505024 Engine time: 0.12726810853928328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 8640, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57915 . Total input tokens: 12860002 . Total output tokens: 11395221
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7099124412052333,
    "estimated_duration": 3599.936475968638,
    "input_throughput": 1339.9075323133625,
    "output_throughput": 1168.8259023703552,
    "total_throughput": 2508.7334346837174,
    "itl": 23.431900907346986,
    "ttft": 5396.043445307483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 19487,
    "finished_requests": 19458,
    "scheduler_time": 0.0019078587176655627
}
#Debug simulation 
Total elapsed time: 1.710010071285069. Arrivals time: 0.05738824466243386 Scheduler time: 1.3064672821201384 Scheduler overhead time: 0.13016786705702543 Adapter cache time: 0.024726102594286203 Engine time: 0.12764290813356638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 8640, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57915 . Total input tokens: 12860002 . Total output tokens: 11395221
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7106672222726047,
    "estimated_duration": 3599.9305706988976,
    "input_throughput": 1339.9097302766982,
    "output_throughput": 1168.827819694064,
    "total_throughput": 2508.7375499707623,
    "itl": 23.314762663935827,
    "ttft": 5395.94616725081,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 19487,
    "finished_requests": 19458,
    "scheduler_time": 0.0017958966985169868
}
#Debug simulation 
Total elapsed time: 1.7107606059871614. Arrivals time: 0.05825775768607855 Scheduler time: 1.3053491152822971 Scheduler overhead time: 0.1299197287298739 Adapter cache time: 0.025266777258366346 Engine time: 0.12869498878717422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 8640, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57915 . Total input tokens: 12860002 . Total output tokens: 11395221
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.7091184970922768,
    "estimated_duration": 3599.9347058473018,
    "input_throughput": 1339.908191158343,
    "output_throughput": 1168.8264770929093,
    "total_throughput": 2508.734668251252,
    "itl": 23.431859613010023,
    "ttft": 5396.157743800435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 19487,
    "finished_requests": 19458,
    "scheduler_time": 0.0019223699778125071
}
#Debug simulation 
Total elapsed time: 1.7092151250690222. Arrivals time: 0.057306062430143356 Scheduler time: 1.3069333280436695 Scheduler overhead time: 0.1299760420806706 Adapter cache time: 0.024788993410766125 Engine time: 0.12711237976327538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 8640, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57915 . Total input tokens: 12860002 . Total output tokens: 11395221
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.706730902660638,
    "estimated_duration": 3599.9272204987737,
    "input_throughput": 1339.9109772368363,
    "output_throughput": 1168.8289074402505,
    "total_throughput": 2508.739884677087,
    "itl": 23.31478261004338,
    "ttft": 5395.982227247205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 19487,
    "finished_requests": 19458,
    "scheduler_time": 0.0017773406303287934
}
#Debug simulation 
Total elapsed time: 1.7068244060501456. Arrivals time: 0.057405998930335045 Scheduler time: 1.3027956965379417 Scheduler overhead time: 0.13021925650537014 Adapter cache time: 0.025143798906356096 Engine time: 0.12776456912979484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 8640, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57915 . Total input tokens: 12860002 . Total output tokens: 11395221
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.706422931049019,
    "estimated_duration": 3599.929438110406,
    "input_throughput": 1339.9101518311666,
    "output_throughput": 1168.8281874237543,
    "total_throughput": 2508.738339254921,
    "itl": 23.431930956588076,
    "ttft": 5396.178092406087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 19487,
    "finished_requests": 19458,
    "scheduler_time": 0.0019271236498589004
}
#Debug simulation 
Total elapsed time: 1.7065200163051486. Arrivals time: 0.05756195215508342 Scheduler time: 1.3031792789697647 Scheduler overhead time: 0.13012358359992504 Adapter cache time: 0.024647850543260574 Engine time: 0.12770842108875513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 8640, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57570 . Total input tokens: 12780407 . Total output tokens: 11322293
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.7111933277919888,
    "estimated_duration": 3599.8716692582975,
    "input_throughput": 1339.2613523340774,
    "output_throughput": 1146.1339122819593,
    "total_throughput": 2485.3952646160365,
    "itl": 23.173278267057352,
    "ttft": 8959.831127857626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 19369,
    "finished_requests": 19321,
    "scheduler_time": 0.002272642173401306
}
#Debug simulation 
Total elapsed time: 1.7112918458878994. Arrivals time: 0.05773541145026684 Scheduler time: 1.2904244679957628 Scheduler overhead time: 0.1309458720497787 Adapter cache time: 0.02525505144149065 Engine time: 0.14306341158226132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 8640, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57570 . Total input tokens: 12780407 . Total output tokens: 11322293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6938331658020616,
    "estimated_duration": 3599.8575631341987,
    "input_throughput": 1339.2666002602816,
    "output_throughput": 1146.1384034338778,
    "total_throughput": 2485.4050036941594,
    "itl": 23.1733474702629,
    "ttft": 8959.780602926876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 19369,
    "finished_requests": 19321,
    "scheduler_time": 0.00227026533737811
}
#Debug simulation 
Total elapsed time: 1.693911919835955. Arrivals time: 0.054695768281817436 Scheduler time: 1.2943996009416878 Scheduler overhead time: 0.13104647863656282 Adapter cache time: 0.024900930002331734 Engine time: 0.1252442100085318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 8640, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57570 . Total input tokens: 12780407 . Total output tokens: 11322293
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6823716084472835,
    "estimated_duration": 3599.8611319291767,
    "input_throughput": 1339.265272551311,
    "output_throughput": 1146.1372671864424,
    "total_throughput": 2485.4025397377536,
    "itl": 23.173346115397454,
    "ttft": 8959.777039066417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 19369,
    "finished_requests": 19321,
    "scheduler_time": 0.00226622052933686
}
#Debug simulation 
Total elapsed time: 1.6824782653711736. Arrivals time: 0.05678637186065316 Scheduler time: 1.278949520085007 Scheduler overhead time: 0.13137002382427454 Adapter cache time: 0.02502284338697791 Engine time: 0.12692748941481113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 8640, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57570 . Total input tokens: 12780407 . Total output tokens: 11322293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7033682791516185,
    "estimated_duration": 3599.8810368548416,
    "input_throughput": 1339.257867313354,
    "output_throughput": 1146.130929816715,
    "total_throughput": 2485.3887971300687,
    "itl": 23.173351383100293,
    "ttft": 8959.77912144537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 19369,
    "finished_requests": 19321,
    "scheduler_time": 0.002252543255198942
}
#Debug simulation 
Total elapsed time: 1.7034598481841385. Arrivals time: 0.05785350175574422 Scheduler time: 1.295382293406874 Scheduler overhead time: 0.13108209008350968 Adapter cache time: 0.025226625613868237 Engine time: 0.13009972730651498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 8640, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57570 . Total input tokens: 12780407 . Total output tokens: 11322293
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.6848047054372728,
    "estimated_duration": 3599.8610091252935,
    "input_throughput": 1339.265318238346,
    "output_throughput": 1146.137306285204,
    "total_throughput": 2485.40262452355,
    "itl": 23.173332878501142,
    "ttft": 8959.783209531863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 19369,
    "finished_requests": 19321,
    "scheduler_time": 0.00226622052933686
}
#Debug simulation 
Total elapsed time: 1.684897430241108. Arrivals time: 0.05734321614727378 Scheduler time: 1.2787171183153987 Scheduler overhead time: 0.13073001336306334 Adapter cache time: 0.02504438068717718 Engine time: 0.12956665642559528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 8640, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57570 . Total input tokens: 12780407 . Total output tokens: 11322293
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6936947819776833,
    "estimated_duration": 3599.8651334837255,
    "input_throughput": 1339.2637838446944,
    "output_throughput": 1146.1359931579373,
    "total_throughput": 2485.3997770026317,
    "itl": 23.173292428925826,
    "ttft": 8959.857300655518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 19369,
    "finished_requests": 19321,
    "scheduler_time": 0.002288946527570186
}
#Debug simulation 
Total elapsed time: 1.69379074126482. Arrivals time: 0.05766951199620962 Scheduler time: 1.2862087432295084 Scheduler overhead time: 0.1328222774900496 Adapter cache time: 0.025706839747726917 Engine time: 0.1273377942852676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 8640, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57570 . Total input tokens: 12780407 . Total output tokens: 11322293
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6922438223846257,
    "estimated_duration": 3599.8576229224213,
    "input_throughput": 1339.2665780170769,
    "output_throughput": 1146.1383843982421,
    "total_throughput": 2485.404962415319,
    "itl": 23.173309042210718,
    "ttft": 8959.743420905581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 19369,
    "finished_requests": 19321,
    "scheduler_time": 0.00226622052933686
}
#Debug simulation 
Total elapsed time: 1.6923388242721558. Arrivals time: 0.055962005630135536 Scheduler time: 1.2894414789043367 Scheduler overhead time: 0.13024885021150112 Adapter cache time: 0.024876317009329796 Engine time: 0.12818131688982248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 8640, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57405 . Total input tokens: 12742383 . Total output tokens: 11290536
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.696229434106499,
    "estimated_duration": 3600.011009209942,
    "input_throughput": 1323.054287282663,
    "output_throughput": 1154.091470656863,
    "total_throughput": 2477.145757939526,
    "itl": 23.188814700341982,
    "ttft": 5635.415255572495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 19297,
    "finished_requests": 19266,
    "scheduler_time": 0.0011445151318032426
}
#Debug simulation 
Total elapsed time: 1.696320737246424. Arrivals time: 0.05621750373393297 Scheduler time: 1.291124563664198 Scheduler overhead time: 0.13117599207907915 Adapter cache time: 0.02494154777377844 Engine time: 0.12866603676229715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 8640, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57405 . Total input tokens: 12742383 . Total output tokens: 11290536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.699385333340615,
    "estimated_duration": 3600.0085423148685,
    "input_throughput": 1323.0551939016516,
    "output_throughput": 1154.0922614945875,
    "total_throughput": 2477.147455396239,
    "itl": 23.19057039209832,
    "ttft": 5635.407473774889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 19297,
    "finished_requests": 19266,
    "scheduler_time": 0.0011436811457942158
}
#Debug simulation 
Total elapsed time: 1.6994821364060044. Arrivals time: 0.05644044326618314 Scheduler time: 1.2964315824210644 Scheduler overhead time: 0.1298186001367867 Adapter cache time: 0.024864823557436466 Engine time: 0.1285625617019832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 8640, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57405 . Total input tokens: 12742383 . Total output tokens: 11290536
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7118666842579842,
    "estimated_duration": 3600.0078662070096,
    "input_throughput": 1323.0554423811125,
    "output_throughput": 1154.0924782415716,
    "total_throughput": 2477.147920622684,
    "itl": 23.189082944779802,
    "ttft": 5635.370254859332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 19297,
    "finished_requests": 19266,
    "scheduler_time": 0.0011485599398444917
}
#Debug simulation 
Total elapsed time: 1.711969472002238. Arrivals time: 0.05718063563108444 Scheduler time: 1.3062070016749203 Scheduler overhead time: 0.131315550301224 Adapter cache time: 0.024928941391408443 Engine time: 0.12863363046199083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 8640, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57405 . Total input tokens: 12742383 . Total output tokens: 11290536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7062236578203738,
    "estimated_duration": 3600.0055649748265,
    "input_throughput": 1323.0562881180729,
    "output_throughput": 1154.0932159723072,
    "total_throughput": 2477.14950409038,
    "itl": 23.190553541861565,
    "ttft": 5635.32999100041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 19297,
    "finished_requests": 19266,
    "scheduler_time": 0.0011388023517439401
}
#Debug simulation 
Total elapsed time: 1.7063191616907716. Arrivals time: 0.05740146432071924 Scheduler time: 1.3011658559553325 Scheduler overhead time: 0.13129850197583437 Adapter cache time: 0.024805220309644938 Engine time: 0.12802778976038098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 8640, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57405 . Total input tokens: 12742383 . Total output tokens: 11290536
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.699849784374237,
    "estimated_duration": 3600.0073720415458,
    "input_throughput": 1323.055623994159,
    "output_throughput": 1154.0926366614265,
    "total_throughput": 2477.1482606555855,
    "itl": 23.189056525767075,
    "ttft": 5635.338956284782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 19297,
    "finished_requests": 19266,
    "scheduler_time": 0.001146891967826439
}
#Debug simulation 
Total elapsed time: 1.6999298422597349. Arrivals time: 0.055102272890508175 Scheduler time: 1.2957621379755437 Scheduler overhead time: 0.13225045427680016 Adapter cache time: 0.02500175964087248 Engine time: 0.12805712735280395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 8640, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57405 . Total input tokens: 12742383 . Total output tokens: 11290536
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.711828505154699,
    "estimated_duration": 3600.0097152552767,
    "input_throughput": 1323.0547628292318,
    "output_throughput": 1154.0918854729778,
    "total_throughput": 2477.1466483022095,
    "itl": 23.188832170984192,
    "ttft": 5635.365938370726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 19297,
    "finished_requests": 19266,
    "scheduler_time": 0.0011404703237619931
}
#Debug simulation 
Total elapsed time: 1.7119268509559333. Arrivals time: 0.05803850153461099 Scheduler time: 1.3044104324653745 Scheduler overhead time: 0.13118981756269932 Adapter cache time: 0.02492636116221547 Engine time: 0.129339802544564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 8640, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 8640]
Prompts retrieved: 57405 . Total input tokens: 12742383 . Total output tokens: 11290536
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.7036912967450917,
    "estimated_duration": 3600.0103672962005,
    "input_throughput": 1323.0545231949636,
    "output_throughput": 1154.0916764415965,
    "total_throughput": 2477.14619963656,
    "itl": 23.190548388703778,
    "ttft": 5635.455656300357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 19297,
    "finished_requests": 19266,
    "scheduler_time": 0.0011388023517439401
}
#Debug simulation 
Total elapsed time: 1.7037885547615588. Arrivals time: 0.0570874847471714 Scheduler time: 1.2984802098944783 Scheduler overhead time: 0.13152917427942157 Adapter cache time: 0.02472880994901061 Engine time: 0.12823018478229642 
