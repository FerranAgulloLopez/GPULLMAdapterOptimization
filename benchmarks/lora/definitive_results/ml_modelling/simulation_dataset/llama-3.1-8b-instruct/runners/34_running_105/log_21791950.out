INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.441093624103814,
    "estimated_duration": 3600.0515370065636,
    "input_throughput": 7182.832449532473,
    "output_throughput": 6238.621800029818,
    "total_throughput": 13421.45424956229,
    "itl": 96.47179398824,
    "ttft": 1558302.1202536356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 906,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.990842582946769,
    "arrivals": 256011,
    "finished_requests": 104015,
    "scheduler_time": 157.67398725357523
}
#Debug simulation 
Total elapsed time: 7.441321717109531. Arrivals time: 0.3563899928703904 Scheduler time: 6.928545675240457 Scheduler overhead time: 0.05532060796394944 Adapter cache time: 0.017542154528200626 Engine time: 0.05710719199851155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.749566792976111,
    "estimated_duration": 3600.033367389277,
    "input_throughput": 7117.333475878693,
    "output_throughput": 6182.975469514061,
    "total_throughput": 13300.308945392755,
    "itl": 93.74475349070504,
    "ttft": 1567927.663614082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.565419735498736,
    "arrivals": 256011,
    "finished_requests": 103080,
    "scheduler_time": 158.84027529392918
}
#Debug simulation 
Total elapsed time: 7.749658591113985. Arrivals time: 0.3477542456239462 Scheduler time: 7.24321654252708 Scheduler overhead time: 0.05631272401660681 Adapter cache time: 0.017386233434081078 Engine time: 0.05814014654606581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.205873411614448,
    "estimated_duration": 3600.0360605859983,
    "input_throughput": 6952.969242181861,
    "output_throughput": 6039.617002186582,
    "total_throughput": 12992.586244368444,
    "itl": 87.27180077171296,
    "ttft": 1591186.9683194738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 868,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.528148285066732,
    "arrivals": 256011,
    "finished_requests": 100697,
    "scheduler_time": 161.97295893532726
}
#Debug simulation 
Total elapsed time: 7.205978836864233. Arrivals time: 0.34699462074786425 Scheduler time: 6.689853832125664 Scheduler overhead time: 0.06009627366438508 Adapter cache time: 0.01832599099725485 Engine time: 0.06216535344719887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.340987788978964,
    "estimated_duration": 3600.0232323539717,
    "input_throughput": 7117.7423994691935,
    "output_throughput": 6183.38704037876,
    "total_throughput": 13301.129439847953,
    "itl": 93.73336010311355,
    "ttft": 1567607.6773150675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.061495522186147,
    "arrivals": 256011,
    "finished_requests": 103088,
    "scheduler_time": 158.86127627638254
}
#Debug simulation 
Total elapsed time: 7.341117089148611. Arrivals time: 0.36214142525568604 Scheduler time: 6.819188762456179 Scheduler overhead time: 0.05663278326392174 Adapter cache time: 0.017489221412688494 Engine time: 0.05855791736394167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.211645308881998,
    "estimated_duration": 3600.041703598294,
    "input_throughput": 6953.354450027561,
    "output_throughput": 6040.099473921634,
    "total_throughput": 12993.453923949195,
    "itl": 87.27120191832947,
    "ttft": 1591110.973688825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 869,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.464561815331713,
    "arrivals": 256011,
    "finished_requests": 100703,
    "scheduler_time": 161.97626105139705
}
#Debug simulation 
Total elapsed time: 7.211815760936588. Arrivals time: 0.3423991431482136 Scheduler time: 6.70036478433758 Scheduler overhead time: 0.06018225057050586 Adapter cache time: 0.01801143493503332 Engine time: 0.06215372774749994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.389243511017412,
    "estimated_duration": 3600.002872961619,
    "input_throughput": 7118.334319249642,
    "output_throughput": 6183.661731826995,
    "total_throughput": 13301.996051076636,
    "itl": 93.72280890890728,
    "ttft": 1567482.2854464026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 895,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.713609241810599,
    "arrivals": 256011,
    "finished_requests": 103093,
    "scheduler_time": 158.87411655806957
}
#Debug simulation 
Total elapsed time: 7.389381061773747. Arrivals time: 0.35182109428569674 Scheduler time: 6.876563110854477 Scheduler overhead time: 0.056711618322879076 Adapter cache time: 0.01841090153902769 Engine time: 0.058797957841306925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.258290403988212,
    "estimated_duration": 3600.088261047268,
    "input_throughput": 6953.5242429633645,
    "output_throughput": 6040.07469352277,
    "total_throughput": 12993.598936486134,
    "itl": 87.26985406128287,
    "ttft": 1591194.738540365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 870,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.422515531964625,
    "arrivals": 256011,
    "finished_requests": 100705,
    "scheduler_time": 161.98062403521567
}
#Debug simulation 
Total elapsed time: 7.258419129066169. Arrivals time: 0.3930096053518355 Scheduler time: 6.693020234815776 Scheduler overhead time: 0.06016592588275671 Adapter cache time: 0.018381720408797264 Engine time: 0.06486211018636823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.4558927761390805,
    "estimated_duration": 3600.009861098467,
    "input_throughput": 7209.21275256755,
    "output_throughput": 6285.653060154513,
    "total_throughput": 13494.865812722062,
    "itl": 95.7259101664547,
    "ttft": 1551002.9347610702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.503050550757985,
    "arrivals": 255515,
    "finished_requests": 104506,
    "scheduler_time": 158.89929392904955
}
#Debug simulation 
Total elapsed time: 7.456052321009338. Arrivals time: 0.4012878402136266 Scheduler time: 6.899027850944549 Scheduler overhead time: 0.055406794883310795 Adapter cache time: 0.016792851965874434 Engine time: 0.05712368618696928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.413628746755421,
    "estimated_duration": 3600.0005328661578,
    "input_throughput": 7144.185053640434,
    "output_throughput": 6228.5274113968135,
    "total_throughput": 13372.712465037248,
    "itl": 93.0246572032907,
    "ttft": 1560336.8361699174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.975209371270619,
    "arrivals": 255515,
    "finished_requests": 103554,
    "scheduler_time": 160.05066207798134
}
#Debug simulation 
Total elapsed time: 7.413762425072491. Arrivals time: 0.3497859137132764 Scheduler time: 6.905038723256439 Scheduler overhead time: 0.05664905486628413 Adapter cache time: 0.016456010285764933 Engine time: 0.05883231107145548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.312174077145755,
    "estimated_duration": 3600.032893880433,
    "input_throughput": 6978.253460601399,
    "output_throughput": 6079.59917177549,
    "total_throughput": 13057.852632376887,
    "itl": 86.6696800500732,
    "ttft": 1584726.5380029103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.0313092101598444,
    "arrivals": 255515,
    "finished_requests": 101124,
    "scheduler_time": 163.07643679945923
}
#Debug simulation 
Total elapsed time: 7.312347845174372. Arrivals time: 0.4086094778031111 Scheduler time: 6.734242124017328 Scheduler overhead time: 0.06035927124321461 Adapter cache time: 0.017521511763334274 Engine time: 0.06271513411775231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.418330781161785,
    "estimated_duration": 3600.0214216698396,
    "input_throughput": 7145.234982537577,
    "output_throughput": 6229.617097555363,
    "total_throughput": 13374.85208009294,
    "itl": 93.01507430651887,
    "ttft": 1560243.8565463396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.558743079276744,
    "arrivals": 255515,
    "finished_requests": 103571,
    "scheduler_time": 160.06913664557226
}
#Debug simulation 
Total elapsed time: 7.418536295183003. Arrivals time: 0.35641135182231665 Scheduler time: 6.901841846294701 Scheduler overhead time: 0.057109377812594175 Adapter cache time: 0.0164870317094028 Engine time: 0.05919771268963814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.666112311650068,
    "estimated_duration": 3600.018906940325,
    "input_throughput": 6978.455849653247,
    "output_throughput": 6079.8919577348815,
    "total_throughput": 13058.347807388129,
    "itl": 86.66794441019847,
    "ttft": 1584666.5321351632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.96384237607944,
    "arrivals": 255515,
    "finished_requests": 101126,
    "scheduler_time": 163.0804535807891
}
#Debug simulation 
Total elapsed time: 7.666230869945139. Arrivals time: 0.35913925850763917 Scheduler time: 7.137991322670132 Scheduler overhead time: 0.060566497035324574 Adapter cache time: 0.016734041273593903 Engine time: 0.06292863469570875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.4497422548010945,
    "estimated_duration": 3600.0822952036883,
    "input_throughput": 7146.051087297279,
    "output_throughput": 6230.025638547526,
    "total_throughput": 13376.076725844805,
    "itl": 93.00769445414804,
    "ttft": 1560203.826566374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3346823186473715,
    "arrivals": 255515,
    "finished_requests": 103580,
    "scheduler_time": 160.08130979240357
}
#Debug simulation 
Total elapsed time: 7.4498603558167815. Arrivals time: 0.41116945585235953 Scheduler time: 6.879607640672475 Scheduler overhead time: 0.05660543078556657 Adapter cache time: 0.016729366965591908 Engine time: 0.05877880658954382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.304082243237644,
    "estimated_duration": 3600.0450814389087,
    "input_throughput": 6978.405112071183,
    "output_throughput": 6079.847753254149,
    "total_throughput": 13058.252865325332,
    "itl": 86.66756190478915,
    "ttft": 1584662.0596497115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.929875159114629,
    "arrivals": 255515,
    "finished_requests": 101126,
    "scheduler_time": 163.08148836122257
}
#Debug simulation 
Total elapsed time: 7.3041902729310095. Arrivals time: 0.39236503979191184 Scheduler time: 6.74282913422212 Scheduler overhead time: 0.06056097615510225 Adapter cache time: 0.017306912690401077 Engine time: 0.06230877013877034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.575431263074279,
    "estimated_duration": 3600.0987687949255,
    "input_throughput": 7225.5450948939715,
    "output_throughput": 6317.299179992447,
    "total_throughput": 13542.844274886418,
    "itl": 95.343647184287,
    "ttft": 1540922.1732750158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7161738759559175,
    "arrivals": 255262,
    "finished_requests": 105096,
    "scheduler_time": 159.39069975480024
}
#Debug simulation 
Total elapsed time: 7.5755674238316715. Arrivals time: 0.35293178306892514 Scheduler time: 7.067573108710349 Scheduler overhead time: 0.05542811844497919 Adapter cache time: 0.015425097662955523 Engine time: 0.05756862089037895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.444352824240923,
    "estimated_duration": 3600.05650682909,
    "input_throughput": 7155.928233662867,
    "output_throughput": 6257.894551728362,
    "total_throughput": 13413.822785391229,
    "itl": 92.66402943780905,
    "ttft": 1549714.3049769064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.113068280438898,
    "arrivals": 255262,
    "finished_requests": 104103,
    "scheduler_time": 160.52826673733873
}
#Debug simulation 
Total elapsed time: 7.4445022931322455. Arrivals time: 0.39532820088788867 Scheduler time: 6.889996580313891 Scheduler overhead time: 0.05691276118159294 Adapter cache time: 0.01626992831006646 Engine time: 0.0588168827816844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.247867938131094,
    "estimated_duration": 3600.032390147903,
    "input_throughput": 6981.019967702981,
    "output_throughput": 6110.9705735442485,
    "total_throughput": 13091.990541247229,
    "itl": 86.33766319451253,
    "ttft": 1574208.1064616353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.055671235746738,
    "arrivals": 255262,
    "finished_requests": 101606,
    "scheduler_time": 163.5799431535273
}
#Debug simulation 
Total elapsed time: 7.248044771142304. Arrivals time: 0.36016096035018563 Scheduler time: 6.720274716150016 Scheduler overhead time: 0.06020304886624217 Adapter cache time: 0.016302240546792746 Engine time: 0.062156562227755785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.471403533127159,
    "estimated_duration": 3600.033151650781,
    "input_throughput": 7156.151322714004,
    "output_throughput": 6258.093481630651,
    "total_throughput": 13414.244804344655,
    "itl": 92.65628093876376,
    "ttft": 1549639.4623728236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7529618644248597,
    "arrivals": 255262,
    "finished_requests": 104107,
    "scheduler_time": 160.5419327650658
}
#Debug simulation 
Total elapsed time: 7.471503951121122. Arrivals time: 0.3926581204868853 Scheduler time: 6.920175673440099 Scheduler overhead time: 0.05691135115921497 Adapter cache time: 0.016031351871788502 Engine time: 0.05863881390541792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.162884327117354,
    "estimated_duration": 3600.010881039108,
    "input_throughput": 6980.9883443313365,
    "output_throughput": 6110.857363200569,
    "total_throughput": 13091.845707531906,
    "itl": 86.3369398804452,
    "ttft": 1574146.7343623939,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.003477707239834,
    "arrivals": 255262,
    "finished_requests": 101605,
    "scheduler_time": 163.58012599091967
}
#Debug simulation 
Total elapsed time: 7.162991624791175. Arrivals time: 0.3022946254350245 Scheduler time: 6.694305048324168 Scheduler overhead time: 0.06011105980724096 Adapter cache time: 0.01505941990762949 Engine time: 0.06246168399229646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.505553594790399,
    "estimated_duration": 3600.092984213407,
    "input_throughput": 7156.555431478472,
    "output_throughput": 6258.708899687783,
    "total_throughput": 13415.264331166256,
    "itl": 92.65288821582422,
    "ttft": 1549522.4653644464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5877635686006233,
    "arrivals": 255262,
    "finished_requests": 104118,
    "scheduler_time": 160.55218311502844
}
#Debug simulation 
Total elapsed time: 7.505652536638081. Arrivals time: 0.4307005973532796 Scheduler time: 6.914365930482745 Scheduler overhead time: 0.05748806335031986 Adapter cache time: 0.016439350321888924 Engine time: 0.059191117528826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.3237000759691,
    "estimated_duration": 3600.093879029409,
    "input_throughput": 6981.054618157832,
    "output_throughput": 6110.934530943737,
    "total_throughput": 13091.98914910157,
    "itl": 86.33505402746509,
    "ttft": 1574212.7675903603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.980073466282373,
    "arrivals": 255262,
    "finished_requests": 101608,
    "scheduler_time": 163.5871653156984
}
#Debug simulation 
Total elapsed time: 7.323806044179946. Arrivals time: 0.356683196965605 Scheduler time: 6.797811316326261 Scheduler overhead time: 0.06037635216489434 Adapter cache time: 0.016664016526192427 Engine time: 0.063350151758641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.62001766404137,
    "estimated_duration": 3600.0573521116785,
    "input_throughput": 7295.5409959216795,
    "output_throughput": 6380.264466211054,
    "total_throughput": 13675.805462132734,
    "itl": 94.59065948217524,
    "ttft": 1531832.5491732666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 474,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1342818811443043,
    "arrivals": 254534,
    "finished_requests": 106367,
    "scheduler_time": 160.9738255742425
}
#Debug simulation 
Total elapsed time: 7.62013432290405. Arrivals time: 0.36206024093553424 Scheduler time: 7.101100948173553 Scheduler overhead time: 0.056360372342169285 Adapter cache time: 0.014946844428777695 Engine time: 0.05860990937799215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.541859166696668,
    "estimated_duration": 3600.062798627982,
    "input_throughput": 7227.877805330723,
    "output_throughput": 6319.74559129102,
    "total_throughput": 13547.623396621744,
    "itl": 91.97186850620024,
    "ttft": 1541282.455956134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.45744139540941,
    "arrivals": 254534,
    "finished_requests": 105388,
    "scheduler_time": 162.0695384834704
}
#Debug simulation 
Total elapsed time: 7.542040406726301. Arrivals time: 0.36198042100295424 Scheduler time: 7.019628648180515 Scheduler overhead time: 0.057828986551612616 Adapter cache time: 0.015197624918073416 Engine time: 0.059802164789289236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.373638607095927,
    "estimated_duration": 3600.0173360749714,
    "input_throughput": 7051.884374445499,
    "output_throughput": 6166.780858951305,
    "total_throughput": 13218.665233396803,
    "itl": 85.77338832583601,
    "ttft": 1566654.491358913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4672168246703503,
    "arrivals": 254534,
    "finished_requests": 102844,
    "scheduler_time": 164.97094801407093
}
#Debug simulation 
Total elapsed time: 7.37376920087263. Arrivals time: 0.34704944118857384 Scheduler time: 6.858291607350111 Scheduler overhead time: 0.06071509560570121 Adapter cache time: 0.015269290190190077 Engine time: 0.06318691978231072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.510573151987046,
    "estimated_duration": 3600.0052590388786,
    "input_throughput": 7228.234440676126,
    "output_throughput": 6320.260211529396,
    "total_throughput": 13548.494652205522,
    "itl": 91.96606856109055,
    "ttft": 1541223.6035508104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 471,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1886837105126986,
    "arrivals": 254534,
    "finished_requests": 105395,
    "scheduler_time": 162.0780217752138
}
#Debug simulation 
Total elapsed time: 7.510720144025981. Arrivals time: 0.36023288033902645 Scheduler time: 6.991842820774764 Scheduler overhead time: 0.0571222216822207 Adapter cache time: 0.014946784358471632 Engine time: 0.05918063363060355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.416958780027926,
    "estimated_duration": 3600.0654332629297,
    "input_throughput": 7051.681551518595,
    "output_throughput": 6166.827078995083,
    "total_throughput": 13218.508630513677,
    "itl": 85.77529621758647,
    "ttft": 1566668.5685357458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4284859126434006,
    "arrivals": 254534,
    "finished_requests": 102844,
    "scheduler_time": 164.97390459769696
}
#Debug simulation 
Total elapsed time: 7.417061088141054. Arrivals time: 0.3611201075837016 Scheduler time: 6.885585131589323 Scheduler overhead time: 0.061402915976941586 Adapter cache time: 0.016088987700641155 Engine time: 0.0636104429140687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.517100316938013,
    "estimated_duration": 3600.0762667531503,
    "input_throughput": 7228.302700228407,
    "output_throughput": 6320.382490263555,
    "total_throughput": 13548.685190491962,
    "itl": 91.96162880151826,
    "ttft": 1541238.4423200889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0132106839492785,
    "arrivals": 254534,
    "finished_requests": 105399,
    "scheduler_time": 162.0876524955391
}
#Debug simulation 
Total elapsed time: 7.517210856080055. Arrivals time: 0.3594649136066437 Scheduler time: 6.998083627317101 Scheduler overhead time: 0.0576053443364799 Adapter cache time: 0.015051913913339376 Engine time: 0.05955525767058134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.364006520714611,
    "estimated_duration": 3600.0394601408693,
    "input_throughput": 7051.712427342846,
    "output_throughput": 6166.837404368696,
    "total_throughput": 13218.549831711542,
    "itl": 85.77498978286248,
    "ttft": 1566667.6663486955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.409843870233772,
    "arrivals": 254534,
    "finished_requests": 102843,
    "scheduler_time": 164.97306684947176
}
#Debug simulation 
Total elapsed time: 7.364104318898171. Arrivals time: 0.3534449078142643 Scheduler time: 6.84183347504586 Scheduler overhead time: 0.061266438104212284 Adapter cache time: 0.015331421047449112 Engine time: 0.06315614096820354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.970677684061229,
    "estimated_duration": 3600.034269728706,
    "input_throughput": 7385.734970240632,
    "output_throughput": 6429.390740700992,
    "total_throughput": 13815.125710941624,
    "itl": 93.96370978127221,
    "ttft": 1523071.162994229,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2746687069907847,
    "arrivals": 254308,
    "finished_requests": 107535,
    "scheduler_time": 162.04069582903557
}
#Debug simulation 
Total elapsed time: 7.970765640027821. Arrivals time: 0.3702688687480986 Scheduler time: 7.443925834726542 Scheduler overhead time: 0.05664564995095134 Adapter cache time: 0.014092421624809504 Engine time: 0.05877526104450226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.817931800149381,
    "estimated_duration": 3600.080928352279,
    "input_throughput": 7312.966159360681,
    "output_throughput": 6366.926315317839,
    "total_throughput": 13679.89247467852,
    "itl": 91.37594449416501,
    "ttft": 1532193.3784683365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5209125113114745,
    "arrivals": 254308,
    "finished_requests": 106490,
    "scheduler_time": 163.1164815140844
}
#Debug simulation 
Total elapsed time: 7.818087519146502. Arrivals time: 0.3736257813870907 Scheduler time: 7.284351353533566 Scheduler overhead time: 0.057901785243302584 Adapter cache time: 0.014559898525476456 Engine time: 0.0600968636572361 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.768981480970979,
    "estimated_duration": 3600.02625220116,
    "input_throughput": 7120.001412302962,
    "output_throughput": 6205.7439126562385,
    "total_throughput": 13325.745324959202,
    "itl": 85.26264600853598,
    "ttft": 1557443.0151644798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5566553958458966,
    "arrivals": 254308,
    "finished_requests": 103662,
    "scheduler_time": 165.92036743256352
}
#Debug simulation 
Total elapsed time: 7.769084773026407. Arrivals time: 0.7390818130224943 Scheduler time: 6.859001019503921 Scheduler overhead time: 0.060866575222462416 Adapter cache time: 0.014731715898960829 Engine time: 0.06607227819040418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.857493464834988,
    "estimated_duration": 3600.0720030907705,
    "input_throughput": 7313.73871894643,
    "output_throughput": 6367.585420602492,
    "total_throughput": 13681.324139548922,
    "itl": 91.3710683398275,
    "ttft": 1532128.9518669124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3112911443412254,
    "arrivals": 254308,
    "finished_requests": 106500,
    "scheduler_time": 163.12558152971613
}
#Debug simulation 
Total elapsed time: 7.857602275907993. Arrivals time: 0.3595531890168786 Scheduler time: 7.339343494735658 Scheduler overhead time: 0.057544860523194075 Adapter cache time: 0.013856704346835613 Engine time: 0.05978915560990572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.741585237905383,
    "estimated_duration": 3600.058174065366,
    "input_throughput": 7119.938278956988,
    "output_throughput": 6205.688886069194,
    "total_throughput": 13325.627165026182,
    "itl": 85.26199362086898,
    "ttft": 1557422.5778768787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5260020537069208,
    "arrivals": 254308,
    "finished_requests": 103662,
    "scheduler_time": 165.92292544069875
}
#Debug simulation 
Total elapsed time: 7.741658091079444. Arrivals time: 0.3581036003306508 Scheduler time: 7.2144184950739145 Scheduler overhead time: 0.061347234062850475 Adapter cache time: 0.014940820634365082 Engine time: 0.06369183212518692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.899036672897637,
    "estimated_duration": 3600.097627075139,
    "input_throughput": 7314.277202363883,
    "output_throughput": 6368.186470161383,
    "total_throughput": 13682.463672525266,
    "itl": 91.36814636883052,
    "ttft": 1532069.9357460607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.196068803556255,
    "arrivals": 254308,
    "finished_requests": 106510,
    "scheduler_time": 163.130355488915
}
#Debug simulation 
Total elapsed time: 7.899124901741743. Arrivals time: 0.3620904120616615 Scheduler time: 7.378050619736314 Scheduler overhead time: 0.05778172053396702 Adapter cache time: 0.013916811440140009 Engine time: 0.05965675227344036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.697443064302206,
    "estimated_duration": 3600.0078986837298,
    "input_throughput": 7120.002989263398,
    "output_throughput": 6205.757495190067,
    "total_throughput": 13325.760484453465,
    "itl": 85.26212556940199,
    "ttft": 1557452.1802708816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.508397093694671,
    "arrivals": 254308,
    "finished_requests": 103662,
    "scheduler_time": 165.92077626086808
}
#Debug simulation 
Total elapsed time: 7.697521172929555. Arrivals time: 0.3615924441255629 Scheduler time: 7.164668171200901 Scheduler overhead time: 0.061633660923689604 Adapter cache time: 0.014960117638111115 Engine time: 0.06542745186015964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.780897320713848,
    "estimated_duration": 3600.074636543206,
    "input_throughput": 7443.708174264789,
    "output_throughput": 6467.360916260429,
    "total_throughput": 13911.069090525218,
    "itl": 93.10915407598165,
    "ttft": 1514256.1275284751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6729394850833321,
    "arrivals": 253785,
    "finished_requests": 107784,
    "scheduler_time": 163.17933243356325
}
#Debug simulation 
Total elapsed time: 7.781081021763384. Arrivals time: 0.3694773833267391 Scheduler time: 7.255624336656183 Scheduler overhead time: 0.05704921204596758 Adapter cache time: 0.012901969719678164 Engine time: 0.05886300001293421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.654514889698476,
    "estimated_duration": 3600.065482300822,
    "input_throughput": 7371.891742102032,
    "output_throughput": 6403.54046706578,
    "total_throughput": 13775.432209167811,
    "itl": 90.53452536235291,
    "ttft": 1523925.7031008056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.829201358659195,
    "arrivals": 253785,
    "finished_requests": 106775,
    "scheduler_time": 164.2842083364601
}
#Debug simulation 
Total elapsed time: 7.65464543690905. Arrivals time: 0.37529465463012457 Scheduler time: 7.117672432679683 Scheduler overhead time: 0.059306187089532614 Adapter cache time: 0.013521534856408834 Engine time: 0.06088038021698594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.484058175235987,
    "estimated_duration": 3600.086250511901,
    "input_throughput": 7182.593193794516,
    "output_throughput": 6240.3210469764645,
    "total_throughput": 13422.91424077098,
    "itl": 84.50261557780796,
    "ttft": 1551131.0881844673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8240725118387533,
    "arrivals": 253785,
    "finished_requests": 104060,
    "scheduler_time": 167.09249557656094
}
#Debug simulation 
Total elapsed time: 7.484198701102287. Arrivals time: 0.380949632730335 Scheduler time: 6.932856968138367 Scheduler overhead time: 0.06240109680220485 Adapter cache time: 0.013913949020206928 Engine time: 0.0643974095582962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.669316018931568,
    "estimated_duration": 3600.015736777816,
    "input_throughput": 7372.285551105635,
    "output_throughput": 6403.840062275492,
    "total_throughput": 13776.125613381126,
    "itl": 90.5305268941141,
    "ttft": 1523888.5431309508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7028732500877208,
    "arrivals": 253785,
    "finished_requests": 106778,
    "scheduler_time": 164.28680331188795
}
#Debug simulation 
Total elapsed time: 7.669430200941861. Arrivals time: 0.36535694636404514 Scheduler time: 7.144039568956941 Scheduler overhead time: 0.05838249484077096 Adapter cache time: 0.013210364151746035 Engine time: 0.06048496998846531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 9.286464654840529,
    "estimated_duration": 3600.061980151946,
    "input_throughput": 7182.573839717236,
    "output_throughput": 6240.273118589979,
    "total_throughput": 13422.846958307215,
    "itl": 84.50202032401168,
    "ttft": 1551124.035959154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8139222736703275,
    "arrivals": 253785,
    "finished_requests": 104058,
    "scheduler_time": 167.09189544859603
}
#Debug simulation 
Total elapsed time: 9.286564231850207. Arrivals time: 0.38464909279718995 Scheduler time: 8.731376942247152 Scheduler overhead time: 0.06231384398415685 Adapter cache time: 0.013904638588428497 Engine time: 0.06439533503726125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 9.5531169436872,
    "estimated_duration": 3600.0829381077756,
    "input_throughput": 7372.4134849932825,
    "output_throughput": 6403.958018844345,
    "total_throughput": 13776.371503837627,
    "itl": 90.52834259558034,
    "ttft": 1523940.0331855272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.595980235142628,
    "arrivals": 253785,
    "finished_requests": 106782,
    "scheduler_time": 164.29476742791547
}
#Debug simulation 
Total elapsed time: 9.553308469709009. Arrivals time: 0.3657077639363706 Scheduler time: 9.026912508998066 Scheduler overhead time: 0.05848090769723058 Adapter cache time: 0.01331682875752449 Engine time: 0.060398660600185394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 9.324119039811194,
    "estimated_duration": 3600.064435966627,
    "input_throughput": 7182.688937915362,
    "output_throughput": 6240.293583514142,
    "total_throughput": 13422.982521429503,
    "itl": 84.49996626366365,
    "ttft": 1551132.8880135738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.798388485424225,
    "arrivals": 253785,
    "finished_requests": 104059,
    "scheduler_time": 167.0933970082627
}
#Debug simulation 
Total elapsed time: 9.324221687857062. Arrivals time: 0.3876671944744885 Scheduler time: 8.766818029806018 Scheduler overhead time: 0.06205561989918351 Adapter cache time: 0.013618829660117626 Engine time: 0.06391428131610155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 98.8888881248422,
    "estimated_duration": 3600.0605414204056,
    "input_throughput": 7008.312974104972,
    "output_throughput": 6107.406180265237,
    "total_throughput": 13115.71915437021,
    "itl": 97.94962011262065,
    "ttft": 1473857.8094934497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7009153573866926,
    "arrivals": 216813,
    "finished_requests": 101933,
    "scheduler_time": 152.68828581819122
}
#Debug simulation 
Total elapsed time: 98.88907196000218. Arrivals time: 0.6508818436414003 Scheduler time: 97.99978090310469 Scheduler overhead time: 0.09342226898297668 Adapter cache time: 0.016542796045541763 Engine time: 0.09386993758380413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 89.0931373606436,
    "estimated_duration": 3600.075407056941,
    "input_throughput": 6918.502582244954,
    "output_throughput": 6027.317638254347,
    "total_throughput": 12945.820220499301,
    "itl": 95.56016067514706,
    "ttft": 1488266.333281011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7802550087729468,
    "arrivals": 216813,
    "finished_requests": 100587,
    "scheduler_time": 153.10457528184367
}
#Debug simulation 
Total elapsed time: 89.09332864778116. Arrivals time: 0.5777558810077608 Scheduler time: 88.27786243986338 Scheduler overhead time: 0.0932417893782258 Adapter cache time: 0.016663560178130865 Engine time: 0.09384342096745968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 63.557804889045656,
    "estimated_duration": 3600.034811619817,
    "input_throughput": 6744.162840213117,
    "output_throughput": 5865.331893971113,
    "total_throughput": 12609.49473418423,
    "itl": 89.02075217814892,
    "ttft": 1516985.2596126732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8153573547350245,
    "arrivals": 216813,
    "finished_requests": 97898,
    "scheduler_time": 155.79209103448207
}
#Debug simulation 
Total elapsed time: 63.557989194989204. Arrivals time: 0.5489322948269546 Scheduler time: 62.78441156633198 Scheduler overhead time: 0.08736670855432749 Adapter cache time: 0.015292983036488295 Engine time: 0.08807143662124872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 89.87785015208647,
    "estimated_duration": 3600.0006565298277,
    "input_throughput": 6929.257069648903,
    "output_throughput": 6034.120010672286,
    "total_throughput": 12963.377080321188,
    "itl": 95.40738136863733,
    "ttft": 1487566.8564298358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7380511956475672,
    "arrivals": 216813,
    "finished_requests": 100718,
    "scheduler_time": 153.35378642159162
}
#Debug simulation 
Total elapsed time: 89.87802369799465. Arrivals time: 0.5783900134265423 Scheduler time: 89.06034471513703 Scheduler overhead time: 0.09378090919926763 Adapter cache time: 0.01638037245720625 Engine time: 0.09472641116008162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 67.3843916640617,
    "estimated_duration": 3600.055457659438,
    "input_throughput": 6746.10413245958,
    "output_throughput": 5868.721537344346,
    "total_throughput": 12614.825669803926,
    "itl": 89.14431848966734,
    "ttft": 1513705.1162201439,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.800032180179842,
    "arrivals": 216813,
    "finished_requests": 97927,
    "scheduler_time": 155.74251209272936
}
#Debug simulation 
Total elapsed time: 67.38458295213059. Arrivals time: 0.5479410230182111 Scheduler time: 66.60096223047003 Scheduler overhead time: 0.09273731568828225 Adapter cache time: 0.01617592480033636 Engine time: 0.09257515566423535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 89.88717552600428,
    "estimated_duration": 3600.0395565859276,
    "input_throughput": 6930.471070618216,
    "output_throughput": 6038.702258209787,
    "total_throughput": 12969.173328828003,
    "itl": 95.54749681525419,
    "ttft": 1488066.4391624667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6830795406410467,
    "arrivals": 216813,
    "finished_requests": 100821,
    "scheduler_time": 153.32737456698874
}
#Debug simulation 
Total elapsed time: 89.88733769673854. Arrivals time: 0.5985974422656 Scheduler time: 89.04752332204953 Scheduler overhead time: 0.09438144601881504 Adapter cache time: 0.01655706437304616 Engine time: 0.09581633796915412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 67.46805919613689,
    "estimated_duration": 3600.0306377247525,
    "input_throughput": 6748.386179111589,
    "output_throughput": 5870.251152461378,
    "total_throughput": 12618.637331572967,
    "itl": 89.12881324216386,
    "ttft": 1513444.416240061,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8085209913924338,
    "arrivals": 216813,
    "finished_requests": 98011,
    "scheduler_time": 155.74213915621033
}
#Debug simulation 
Total elapsed time: 67.46829370595515. Arrivals time: 0.5583882937207818 Scheduler time: 66.67651087138802 Scheduler overhead time: 0.09113677218556404 Adapter cache time: 0.015652551781386137 Engine time: 0.09192792698740959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 130.88780132588,
    "estimated_duration": 3600.0376032981176,
    "input_throughput": 6997.944681722201,
    "output_throughput": 6090.740546685407,
    "total_throughput": 13088.685228407609,
    "itl": 98.35656538271888,
    "ttft": 1400339.371511415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47609345030039507,
    "arrivals": 194341,
    "finished_requests": 101409,
    "scheduler_time": 150.05833155355597
}
#Debug simulation 
Total elapsed time: 130.88806266477332. Arrivals time: 0.6497277403250337 Scheduler time: 129.9720949223265 Scheduler overhead time: 0.1066752839833498 Adapter cache time: 0.017536164727061987 Engine time: 0.10565413674339652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 129.21547673409805,
    "estimated_duration": 3600.024220497426,
    "input_throughput": 6923.6036963540255,
    "output_throughput": 6029.1549919075105,
    "total_throughput": 12952.758688261536,
    "itl": 95.71775744947426,
    "ttft": 1410852.2053386136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5354372773272913,
    "arrivals": 194341,
    "finished_requests": 100361,
    "scheduler_time": 150.96547963759602
}
#Debug simulation 
Total elapsed time: 129.21565059991553. Arrivals time: 0.6445107297040522 Scheduler time: 128.30220790486783 Scheduler overhead time: 0.10759820742532611 Adapter cache time: 0.019220745656639338 Engine time: 0.1054719272069633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 126.49417559616268,
    "estimated_duration": 3600.0229561704978,
    "input_throughput": 6741.785898448182,
    "output_throughput": 5867.506751254062,
    "total_throughput": 12609.292649702244,
    "itl": 89.47262344876101,
    "ttft": 1436519.0160129678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5421747358283029,
    "arrivals": 194341,
    "finished_requests": 97707,
    "scheduler_time": 153.33546386900275
}
#Debug simulation 
Total elapsed time: 126.49433113122359. Arrivals time: 0.6188179296441376 Scheduler time: 125.60254054702818 Scheduler overhead time: 0.10966680059209466 Adapter cache time: 0.01894186483696103 Engine time: 0.10635678144171834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 130.24620978720486,
    "estimated_duration": 3600.0821677599024,
    "input_throughput": 6924.059740422695,
    "output_throughput": 6029.579600819736,
    "total_throughput": 12953.63934124243,
    "itl": 95.71871899895841,
    "ttft": 1410838.0127105045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5007317529944704,
    "arrivals": 194341,
    "finished_requests": 100368,
    "scheduler_time": 150.96884702810482
}
#Debug simulation 
Total elapsed time: 130.2463607992977. Arrivals time: 0.6193383801728487 Scheduler time: 129.3578396094963 Scheduler overhead time: 0.10856448486447334 Adapter cache time: 0.01814782014116645 Engine time: 0.10564079275354743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 126.25887416210026,
    "estimated_duration": 3600.0891696328895,
    "input_throughput": 6742.401884027558,
    "output_throughput": 5867.994653630816,
    "total_throughput": 12610.396537658375,
    "itl": 89.47136268992429,
    "ttft": 1436454.8188030263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 74,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5531489531602711,
    "arrivals": 194341,
    "finished_requests": 97714,
    "scheduler_time": 153.33892145053036
}
#Debug simulation 
Total elapsed time: 126.25901093892753. Arrivals time: 0.6267807953990996 Scheduler time: 125.35297643765807 Scheduler overhead time: 0.11114096641540527 Adapter cache time: 0.01964445598423481 Engine time: 0.11009809980168939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 128.19762360025197,
    "estimated_duration": 3600.060392645463,
    "input_throughput": 6923.722460578932,
    "output_throughput": 6029.393574714301,
    "total_throughput": 12953.116035293235,
    "itl": 95.71561873343201,
    "ttft": 1410881.5754842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45964230772107895,
    "arrivals": 194341,
    "finished_requests": 100364,
    "scheduler_time": 150.9692677434995
}
#Debug simulation 
Total elapsed time: 128.19778925227. Arrivals time: 0.6348348157480359 Scheduler time: 127.29877398256212 Scheduler overhead time: 0.10562138399109244 Adapter cache time: 0.01795134972780943 Engine time: 0.10401709144935012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 126.98488189000636,
    "estimated_duration": 3600.0249182717093,
    "input_throughput": 6741.844168026444,
    "output_throughput": 5867.6616077816,
    "total_throughput": 12609.505775808044,
    "itl": 89.47325133079596,
    "ttft": 1436566.2212668648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 74,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5477639065682889,
    "arrivals": 194341,
    "finished_requests": 97709,
    "scheduler_time": 153.33636981385263
}
#Debug simulation 
Total elapsed time: 126.98502729600295. Arrivals time: 0.6198639310896397 Scheduler time: 126.0926303807646 Scheduler overhead time: 0.10950335022062063 Adapter cache time: 0.0192281361669302 Engine time: 0.1058205752633512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 145.2509163073264,
    "estimated_duration": 3600.0074962150716,
    "input_throughput": 6998.742926643831,
    "output_throughput": 6095.045641729722,
    "total_throughput": 13093.788568373553,
    "itl": 97.94844949208492,
    "ttft": 1366385.534234302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 75,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49593067739624486,
    "arrivals": 190487,
    "finished_requests": 101523,
    "scheduler_time": 149.66507130902423
}
#Debug simulation 
Total elapsed time: 145.2510767141357. Arrivals time: 0.6487421663478017 Scheduler time: 144.32076084055007 Scheduler overhead time: 0.1129827001132071 Adapter cache time: 0.020797159522771835 Engine time: 0.1102502765133977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 142.8988403188996,
    "estimated_duration": 3600.068917203515,
    "input_throughput": 6936.544986865958,
    "output_throughput": 6035.323350664546,
    "total_throughput": 12971.868337530505,
    "itl": 95.3377722219778,
    "ttft": 1374939.5309674707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 75,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5495933401817453,
    "arrivals": 190487,
    "finished_requests": 100551,
    "scheduler_time": 150.55270653747363
}
#Debug simulation 
Total elapsed time: 142.89898925786838. Arrivals time: 0.6572942873463035 Scheduler time: 141.95911322068423 Scheduler overhead time: 0.11267546704038978 Adapter cache time: 0.02036535693332553 Engine time: 0.11190510168671608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 141.8636369900778,
    "estimated_duration": 3600.053706152351,
    "input_throughput": 6748.419602319086,
    "output_throughput": 5872.782665399846,
    "total_throughput": 12621.202267718932,
    "itl": 89.06597184937742,
    "ttft": 1390066.239414226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.580846296302043,
    "arrivals": 190487,
    "finished_requests": 97735,
    "scheduler_time": 152.95152759084243
}
#Debug simulation 
Total elapsed time: 141.86377785215154. Arrivals time: 0.6356464917771518 Scheduler time: 140.93933071102947 Scheduler overhead time: 0.11643318273127079 Adapter cache time: 0.020950591191649437 Engine time: 0.11262627132236958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 143.58081822190434,
    "estimated_duration": 3600.0198709392416,
    "input_throughput": 6936.790599848739,
    "output_throughput": 6035.571963198953,
    "total_throughput": 12972.36256304769,
    "itl": 95.33112252670641,
    "ttft": 1374884.475823472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5290438787033783,
    "arrivals": 190487,
    "finished_requests": 100553,
    "scheduler_time": 150.55804935638508
}
#Debug simulation 
Total elapsed time: 143.58095948863775. Arrivals time: 0.6425259653478861 Scheduler time: 142.65807935362682 Scheduler overhead time: 0.11215041810646653 Adapter cache time: 0.0192858693189919 Engine time: 0.11192280938848853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 141.9891295740381,
    "estimated_duration": 3600.0196351855425,
    "input_throughput": 6748.364581835925,
    "output_throughput": 5872.518803334359,
    "total_throughput": 12620.883385170284,
    "itl": 89.06598481786679,
    "ttft": 1390024.3275786566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5754612497100607,
    "arrivals": 190487,
    "finished_requests": 97732,
    "scheduler_time": 152.94959238563783
}
#Debug simulation 
Total elapsed time: 141.98926804820076. Arrivals time: 0.6502890782430768 Scheduler time: 141.047145715449 Scheduler overhead time: 0.11858445918187499 Adapter cache time: 0.02034972421824932 Engine time: 0.11374505935236812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 145.2840144741349,
    "estimated_duration": 3600.0258990522666,
    "input_throughput": 6935.675381272444,
    "output_throughput": 6034.849639753822,
    "total_throughput": 12970.525021026266,
    "itl": 95.2949862081544,
    "ttft": 1374816.2983628574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 75,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4787940705427906,
    "arrivals": 190487,
    "finished_requests": 100541,
    "scheduler_time": 150.57543454987703
}
#Debug simulation 
Total elapsed time: 145.28432335983962. Arrivals time: 0.6769326101057231 Scheduler time: 144.3202822697349 Scheduler overhead time: 0.11546050943434238 Adapter cache time: 0.020678047090768814 Engine time: 0.1129113333299756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 142.66870639985427,
    "estimated_duration": 3600.0459248329494,
    "input_throughput": 6748.374189456297,
    "output_throughput": 5872.565639834446,
    "total_throughput": 12620.939829290743,
    "itl": 89.06504996494384,
    "ttft": 1390071.4893759037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5698690859414638,
    "arrivals": 190487,
    "finished_requests": 97732,
    "scheduler_time": 152.95191097177
}
#Debug simulation 
Total elapsed time: 142.66883858386427. Arrivals time: 0.6570616420358419 Scheduler time: 141.71886003343388 Scheduler overhead time: 0.11850685393437743 Adapter cache time: 0.021147594787180424 Engine time: 0.1137809599749744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 119.6662378530018,
    "estimated_duration": 3600.0389072679045,
    "input_throughput": 7102.648515375384,
    "output_throughput": 6155.706249524391,
    "total_throughput": 13258.354764899776,
    "itl": 97.59577139492295,
    "ttft": 1368053.4878868933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6017292219074437,
    "arrivals": 188561,
    "finished_requests": 103460,
    "scheduler_time": 150.3973769838531
}
#Debug simulation 
Total elapsed time: 119.6663906336762. Arrivals time: 0.657418348826468 Scheduler time: 118.74311364581808 Scheduler overhead time: 0.10535456566140056 Adapter cache time: 0.020236601121723652 Engine time: 0.10355719458311796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 118.76333131408319,
    "estimated_duration": 3600.050626166989,
    "input_throughput": 7031.631115408041,
    "output_throughput": 6095.4334476578915,
    "total_throughput": 13127.064563065933,
    "itl": 94.81184946050104,
    "ttft": 1380434.7938342828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6847700477857143,
    "arrivals": 188561,
    "finished_requests": 102439,
    "scheduler_time": 151.46161586345602
}
#Debug simulation 
Total elapsed time: 118.76348025817424. Arrivals time: 0.6348477862775326 Scheduler time: 117.8655976462178 Scheduler overhead time: 0.10500352177768946 Adapter cache time: 0.019902009516954422 Engine time: 0.10238564759492874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 112.29082932462916,
    "estimated_duration": 3600.0187143255066,
    "input_throughput": 6857.071576258472,
    "output_throughput": 5942.539385939887,
    "total_throughput": 12799.61096219836,
    "itl": 88.60495623945522,
    "ttft": 1413862.9498004068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 74,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5566180761810392,
    "arrivals": 188561,
    "finished_requests": 99833,
    "scheduler_time": 153.97574045716624
}
#Debug simulation 
Total elapsed time: 112.29095236863941. Arrivals time: 0.5883087585680187 Scheduler time: 111.43768271571025 Scheduler overhead time: 0.10588243650272489 Adapter cache time: 0.02007503528147936 Engine time: 0.10173199232667685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 120.88247135700658,
    "estimated_duration": 3600.0797020449995,
    "input_throughput": 7031.36988484473,
    "output_throughput": 6095.4886603023615,
    "total_throughput": 13126.858545147092,
    "itl": 94.85958749278157,
    "ttft": 1380461.019172959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.635351276672445,
    "arrivals": 188561,
    "finished_requests": 102440,
    "scheduler_time": 151.42744990375752
}
#Debug simulation 
Total elapsed time: 120.88260702695698. Arrivals time: 0.6718137483112514 Scheduler time: 119.93836982687935 Scheduler overhead time: 0.10888695623725653 Adapter cache time: 0.019894917495548725 Engine time: 0.10643134918063879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 115.05785225238651,
    "estimated_duration": 3600.011185328829,
    "input_throughput": 6856.433419038226,
    "output_throughput": 5941.753205426827,
    "total_throughput": 12798.186624465054,
    "itl": 88.5568575309368,
    "ttft": 1413877.2732706994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 74,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5516472639422865,
    "arrivals": 188561,
    "finished_requests": 99821,
    "scheduler_time": 154.00440465576943
}
#Debug simulation 
Total elapsed time: 115.05799863301218. Arrivals time: 0.6251368657685816 Scheduler time: 114.15841366071254 Scheduler overhead time: 0.10972470557317138 Adapter cache time: 0.02078799018636346 Engine time: 0.1063146204687655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 118.9879376962781,
    "estimated_duration": 3600.0731804048055,
    "input_throughput": 7032.668985121335,
    "output_throughput": 6096.695511487359,
    "total_throughput": 13129.364496608694,
    "itl": 94.92429473194132,
    "ttft": 1380479.3188221864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6000885684136302,
    "arrivals": 188561,
    "finished_requests": 102459,
    "scheduler_time": 151.3884805088565
}
#Debug simulation 
Total elapsed time: 118.9880891260691. Arrivals time: 0.6326369545422494 Scheduler time: 118.09203916881233 Scheduler overhead time: 0.10446647554636002 Adapter cache time: 0.019927920307964087 Engine time: 0.10224479343742132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 115.76553894300014,
    "estimated_duration": 3600.08293027926,
    "input_throughput": 6860.580013939877,
    "output_throughput": 5945.649979329675,
    "total_throughput": 12806.229993269553,
    "itl": 88.57812884560155,
    "ttft": 1413891.9376437187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 74,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5462622173503042,
    "arrivals": 188561,
    "finished_requests": 99883,
    "scheduler_time": 154.032772793619
}
#Debug simulation 
Total elapsed time: 115.76569056883454. Arrivals time: 0.6378472065553069 Scheduler time: 114.85508714104071 Scheduler overhead time: 0.10820342041552067 Adapter cache time: 0.01993531547486782 Engine time: 0.10571725433692336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 143.0086432523094,
    "estimated_duration": 3600.0115924534566,
    "input_throughput": 7096.3999264761505,
    "output_throughput": 6174.986782431425,
    "total_throughput": 13271.386708907576,
    "itl": 97.1186188420773,
    "ttft": 1357188.3652585242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5025430864281948,
    "arrivals": 187655,
    "finished_requests": 103466,
    "scheduler_time": 151.0678465603273
}
#Debug simulation 
Total elapsed time: 143.0087911351584. Arrivals time: 0.6703886464238167 Scheduler time: 142.0590005214326 Scheduler overhead time: 0.11224634293466806 Adapter cache time: 0.021105452440679073 Engine time: 0.10861725592985749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 137.6829508249648,
    "estimated_duration": 3600.0729428712016,
    "input_throughput": 7022.552154134088,
    "output_throughput": 6111.971715342884,
    "total_throughput": 13134.52386947697,
    "itl": 94.52983546025006,
    "ttft": 1366089.6029486663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5651376240095124,
    "arrivals": 187655,
    "finished_requests": 102401,
    "scheduler_time": 151.95379983519314
}
#Debug simulation 
Total elapsed time: 137.6830941848457. Arrivals time: 0.6687598824501038 Scheduler time: 136.73573635332286 Scheduler overhead time: 0.11121028615161777 Adapter cache time: 0.020198500249534845 Engine time: 0.1089241923764348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 138.0654928162694,
    "estimated_duration": 3600.0253098453286,
    "input_throughput": 6840.967460047691,
    "output_throughput": 5944.064321292057,
    "total_throughput": 12785.031781339749,
    "itl": 88.47090474768639,
    "ttft": 1388645.7769060112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5744790293229743,
    "arrivals": 187655,
    "finished_requests": 99663,
    "scheduler_time": 154.13964768943575
}
#Debug simulation 
Total elapsed time: 138.06562181841582. Arrivals time: 0.6492700539529324 Scheduler time: 137.12744471337646 Scheduler overhead time: 0.11523098684847355 Adapter cache time: 0.020777575206011534 Engine time: 0.11343848425894976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 136.51044841390103,
    "estimated_duration": 3600.0572470397738,
    "input_throughput": 7022.624159876501,
    "output_throughput": 6112.0458620742875,
    "total_throughput": 13134.670021950787,
    "itl": 94.53883753361465,
    "ttft": 1366102.40462888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5290438787033783,
    "arrivals": 187655,
    "finished_requests": 102403,
    "scheduler_time": 151.9490480867986
}
#Debug simulation 
Total elapsed time: 136.51060080574825. Arrivals time: 0.6547064394690096 Scheduler time: 135.57426183018833 Scheduler overhead time: 0.11355935223400593 Adapter cache time: 0.021160006523132324 Engine time: 0.10808960814028978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 139.08430061722174,
    "estimated_duration": 3600.0196642134065,
    "input_throughput": 6840.747911687001,
    "output_throughput": 5943.927532594591,
    "total_throughput": 12784.675444281593,
    "itl": 88.4667719639301,
    "ttft": 1388503.4854692533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5690939827309921,
    "arrivals": 187655,
    "finished_requests": 99661,
    "scheduler_time": 154.1419681185626
}
#Debug simulation 
Total elapsed time: 139.0845175879076. Arrivals time: 0.6742769349366426 Scheduler time: 138.12501165643334 Scheduler overhead time: 0.11430096020922065 Adapter cache time: 0.02072346117347479 Engine time: 0.1111750965937972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 136.32691081380472,
    "estimated_duration": 3600.0878937838456,
    "input_throughput": 6991.696520371479,
    "output_throughput": 6085.681140682575,
    "total_throughput": 13077.377661054054,
    "itl": 94.1796763865805,
    "ttft": 1364423.0789516456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 75,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4787940705427906,
    "arrivals": 187655,
    "finished_requests": 101945,
    "scheduler_time": 151.89036678722627
}
#Debug simulation 
Total elapsed time: 136.32703967392445. Arrivals time: 0.6557222669944167 Scheduler time: 135.3862403780222 Scheduler overhead time: 0.1144268037751317 Adapter cache time: 0.019945952109992504 Engine time: 0.11215819278731942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 133.70057359104976,
    "estimated_duration": 3600.0608336455066,
    "input_throughput": 6787.203363799052,
    "output_throughput": 5899.261146232967,
    "total_throughput": 12686.464510032018,
    "itl": 88.04021460676893,
    "ttft": 1383002.309988605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 75,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.555632862765342,
    "arrivals": 187655,
    "finished_requests": 98898,
    "scheduler_time": 154.00743364733367
}
#Debug simulation 
Total elapsed time: 133.70071107288823. Arrivals time: 0.639909690245986 Scheduler time: 132.77403188636526 Scheduler overhead time: 0.11652538366615772 Adapter cache time: 0.020348040387034416 Engine time: 0.11064758663997054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 172.86133192060515,
    "estimated_duration": 3600.063244427995,
    "input_throughput": 6976.601046904844,
    "output_throughput": 6120.880802331898,
    "total_throughput": 13097.481849236741,
    "itl": 98.74367610671257,
    "ttft": 1175606.7673340968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30417081546969693,
    "arrivals": 187118,
    "finished_requests": 102135,
    "scheduler_time": 148.93567338355635
}
#Debug simulation 
Total elapsed time: 172.86147099593654. Arrivals time: 0.7298022489994764 Scheduler time: 171.82738732593134 Scheduler overhead time: 0.12235933681949973 Adapter cache time: 0.0216556447558105 Engine time: 0.1209494206123054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 170.0291071399115,
    "estimated_duration": 3600.091075655638,
    "input_throughput": 6908.355782491036,
    "output_throughput": 6047.63811316494,
    "total_throughput": 12955.993895655974,
    "itl": 96.18054384867449,
    "ttft": 1188196.8496730824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24898239436559386,
    "arrivals": 187118,
    "finished_requests": 101107,
    "scheduler_time": 149.67485094416514
}
#Debug simulation 
Total elapsed time: 170.02928956365213. Arrivals time: 0.7137698014266789 Scheduler time: 169.01143455551937 Scheduler overhead time: 0.1234709587879479 Adapter cache time: 0.020411791745573282 Engine time: 0.11946772411465645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 155.67423489829525,
    "estimated_duration": 3600.071031470937,
    "input_throughput": 6720.7443376788615,
    "output_throughput": 5886.865235361978,
    "total_throughput": 12607.60957304084,
    "itl": 89.86878642373003,
    "ttft": 1227309.6482846038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2557896243641153,
    "arrivals": 187118,
    "finished_requests": 98341,
    "scheduler_time": 152.04777968772515
}
#Debug simulation 
Total elapsed time: 155.67441329406574. Arrivals time: 0.6933020735159516 Scheduler time: 154.67586475843564 Scheduler overhead time: 0.12352651869878173 Adapter cache time: 0.020069562830030918 Engine time: 0.11962407547980547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 168.36550965392962,
    "estimated_duration": 3600.060454444312,
    "input_throughput": 6908.149547682741,
    "output_throughput": 6047.396779996703,
    "total_throughput": 12955.546327679443,
    "itl": 96.17862518855286,
    "ttft": 1188319.952054173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23093552171252665,
    "arrivals": 187118,
    "finished_requests": 101104,
    "scheduler_time": 149.67517313433592
}
#Debug simulation 
Total elapsed time: 168.36567731387913. Arrivals time: 0.7252329834736884 Scheduler time: 167.3354093641974 Scheduler overhead time: 0.12301587406545877 Adapter cache time: 0.020755433477461338 Engine time: 0.1201249547302723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 156.8328438908793,
    "estimated_duration": 3600.077833544775,
    "input_throughput": 6720.8771917511585,
    "output_throughput": 5886.833001922212,
    "total_throughput": 12607.71019367337,
    "itl": 89.86532187434356,
    "ttft": 1227271.927652727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25309710106812416,
    "arrivals": 187118,
    "finished_requests": 98344,
    "scheduler_time": 152.0484046637042
}
#Debug simulation 
Total elapsed time: 156.83301224466413. Arrivals time: 0.6955465157516301 Scheduler time: 155.83046637196094 Scheduler overhead time: 0.12530181743204594 Adapter cache time: 0.020285039208829403 Engine time: 0.12032023817300797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 168.34001667425036,
    "estimated_duration": 3600.0092209160675,
    "input_throughput": 6908.260638752355,
    "output_throughput": 6047.729509553831,
    "total_throughput": 12955.990148306186,
    "itl": 96.18289373018824,
    "ttft": 1188195.666523113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2170533119793981,
    "arrivals": 187118,
    "finished_requests": 101105,
    "scheduler_time": 149.67136191797488
}
#Debug simulation 
Total elapsed time: 168.34019551612437. Arrivals time: 0.7185702575370669 Scheduler time: 167.31146841403097 Scheduler overhead time: 0.12633238127455115 Adapter cache time: 0.021001579239964485 Engine time: 0.1214508805423975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 141.54867094522342,
    "estimated_duration": 3600.043802815244,
    "input_throughput": 6720.940723298676,
    "output_throughput": 5886.888649362259,
    "total_throughput": 12607.829372660934,
    "itl": 89.8671539708412,
    "ttft": 1227304.416819625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25102592930197715,
    "arrivals": 187118,
    "finished_requests": 98344,
    "scheduler_time": 152.04667653687594
}
#Debug simulation 
Total elapsed time: 141.54880664218217. Arrivals time: 0.6661138110794127 Scheduler time: 140.59304324211553 Scheduler overhead time: 0.11868516448885202 Adapter cache time: 0.01964799501001835 Engine time: 0.11182628013193607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 161.12820872478187,
    "estimated_duration": 3600.0109355809805,
    "input_throughput": 7005.141220753028,
    "output_throughput": 6148.301601319429,
    "total_throughput": 13153.442822072459,
    "itl": 97.54539938574706,
    "ttft": 1296178.7383726866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.44303140514064543,
    "arrivals": 186916,
    "finished_requests": 102287,
    "scheduler_time": 150.52020909330955
}
#Debug simulation 
Total elapsed time: 161.12834597099572. Arrivals time: 0.716778539121151 Scheduler time: 160.11099878232926 Scheduler overhead time: 0.12260994268581271 Adapter cache time: 0.020476388279348612 Engine time: 0.1186030637472868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 158.54562218068168,
    "estimated_duration": 3600.0860840888818,
    "input_throughput": 6921.848094170398,
    "output_throughput": 6080.534322984136,
    "total_throughput": 13002.382417154535,
    "itl": 95.04495186639402,
    "ttft": 1295391.382414962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48880442584399114,
    "arrivals": 186916,
    "finished_requests": 101075,
    "scheduler_time": 151.23734866650145
}
#Debug simulation 
Total elapsed time: 158.545761867892. Arrivals time: 0.6988706514239311 Scheduler time: 157.54666844941676 Scheduler overhead time: 0.12227535340934992 Adapter cache time: 0.020988120697438717 Engine time: 0.11723867198452353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 155.82009047083557,
    "estimated_duration": 3600.0695830154154,
    "input_throughput": 6715.901024265417,
    "output_throughput": 5900.173457816481,
    "total_throughput": 12616.074482081898,
    "itl": 89.14516672265017,
    "ttft": 1291583.886560405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47461649464908984,
    "arrivals": 186916,
    "finished_requests": 98039,
    "scheduler_time": 153.06808036974493
}
#Debug simulation 
Total elapsed time: 155.82024836400524. Arrivals time: 0.6957310643047094 Scheduler time: 154.81677409587428 Scheduler overhead time: 0.12519832095131278 Adapter cache time: 0.021460335236042738 Engine time: 0.11966487485915422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 159.19445256888866,
    "estimated_duration": 3600.0114648178005,
    "input_throughput": 6921.276846895001,
    "output_throughput": 6080.149525609302,
    "total_throughput": 13001.426372504304,
    "itl": 95.0339817040523,
    "ttft": 1295193.0482809495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4582635644311086,
    "arrivals": 186916,
    "finished_requests": 101065,
    "scheduler_time": 151.23950411592867
}
#Debug simulation 
Total elapsed time: 159.1946722068824. Arrivals time: 0.7070496594533324 Scheduler time: 158.18270208779722 Scheduler overhead time: 0.123781219124794 Adapter cache time: 0.02193961851298809 Engine time: 0.11902221897616982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 154.79238267056644,
    "estimated_duration": 3600.050255244152,
    "input_throughput": 6714.6998753106545,
    "output_throughput": 5899.039872864146,
    "total_throughput": 12613.739748174801,
    "itl": 89.16145508311855,
    "ttft": 1291258.962584384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4700599167635664,
    "arrivals": 186916,
    "finished_requests": 98020,
    "scheduler_time": 153.03875507597908
}
#Debug simulation 
Total elapsed time: 154.79251997964457. Arrivals time: 0.6894829920493066 Scheduler time: 153.79176073009148 Scheduler overhead time: 0.12691330583766103 Adapter cache time: 0.021714514587074518 Engine time: 0.12206643912941217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 159.93110321322456,
    "estimated_duration": 3600.080823055209,
    "input_throughput": 6921.303777522964,
    "output_throughput": 6080.075997133891,
    "total_throughput": 13001.379774656856,
    "itl": 95.03593744706464,
    "ttft": 1295289.3356728205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4277227030182262,
    "arrivals": 186916,
    "finished_requests": 101066,
    "scheduler_time": 151.24512364062443
}
#Debug simulation 
Total elapsed time: 159.9312425670214. Arrivals time: 0.7009078930132091 Scheduler time: 158.9228073982522 Scheduler overhead time: 0.12402780400589108 Adapter cache time: 0.022397192660719156 Engine time: 0.1202160632237792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 156.72975645586848,
    "estimated_duration": 3600.022041299416,
    "input_throughput": 6716.444711341975,
    "output_throughput": 5900.522762447523,
    "total_throughput": 12616.967473789497,
    "itl": 89.1415981656066,
    "ttft": 1291741.3532844847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46571045605465766,
    "arrivals": 186916,
    "finished_requests": 98043,
    "scheduler_time": 153.06967711159422
}
#Debug simulation 
Total elapsed time: 156.7298945770599. Arrivals time: 0.6934938081540167 Scheduler time: 155.72446142556146 Scheduler overhead time: 0.12692160392180085 Adapter cache time: 0.022200698498636484 Engine time: 0.12016888661310077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 51.83198138792068,
    "estimated_duration": 3600.0760977289096,
    "input_throughput": 7001.881436867933,
    "output_throughput": 6106.252868895703,
    "total_throughput": 13108.134305763635,
    "itl": 97.79463368487296,
    "ttft": 1260091.340766426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9125124464090903,
    "arrivals": 164321,
    "finished_requests": 101594,
    "scheduler_time": 145.11056215043868
}
#Debug simulation 
Total elapsed time: 51.83214563690126. Arrivals time: 0.5465822261758149 Scheduler time: 51.07038741046563 Scheduler overhead time: 0.08413008274510503 Adapter cache time: 0.014825856313109398 Engine time: 0.0838705375790596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 50.754292788915336,
    "estimated_duration": 3600.0913440051663,
    "input_throughput": 6939.93668844091,
    "output_throughput": 6046.2996407706605,
    "total_throughput": 12986.23632921157,
    "itl": 95.08030682549266,
    "ttft": 1279118.4174453074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0100856403168297,
    "arrivals": 164321,
    "finished_requests": 100573,
    "scheduler_time": 146.0703935199819
}
#Debug simulation 
Total elapsed time: 50.75442783208564. Arrivals time: 0.5316830859519541 Scheduler time: 50.004606121685356 Scheduler overhead time: 0.08509629871696234 Adapter cache time: 0.015148609410971403 Engine time: 0.08480441384017467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 26.54962965892628,
    "estimated_duration": 3600.05531328412,
    "input_throughput": 6757.556171493203,
    "output_throughput": 5888.278138888424,
    "total_throughput": 12645.834310381626,
    "itl": 89.13905430404525,
    "ttft": 1316465.7552067265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6005871809693089,
    "arrivals": 164321,
    "finished_requests": 97946,
    "scheduler_time": 148.2100876650012
}
#Debug simulation 
Total elapsed time: 26.549750430043787. Arrivals time: 0.4647181243635714 Scheduler time: 25.89593826700002 Scheduler overhead time: 0.07228732435032725 Adapter cache time: 0.013734038453549147 Engine time: 0.07235086616128683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 46.905938354320824,
    "estimated_duration": 3600.048556853786,
    "input_throughput": 6934.500078470447,
    "output_throughput": 6046.448445413031,
    "total_throughput": 12980.948523883479,
    "itl": 95.16287483474603,
    "ttft": 1282060.031086766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 140,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9589953174255781,
    "arrivals": 164321,
    "finished_requests": 100607,
    "scheduler_time": 145.99572006210565
}
#Debug simulation 
Total elapsed time: 46.90605920692906. Arrivals time: 0.5357342148199677 Scheduler time: 46.159616257529706 Scheduler overhead time: 0.08264776645228267 Adapter cache time: 0.014339685440063477 Engine time: 0.08134974120184779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 28.407455153297633,
    "estimated_duration": 3600.0102901239816,
    "input_throughput": 6760.202621299135,
    "output_throughput": 5890.478440624038,
    "total_throughput": 12650.681061923173,
    "itl": 89.0686532995583,
    "ttft": 1316137.9503248737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2936989655345694,
    "arrivals": 164321,
    "finished_requests": 98007,
    "scheduler_time": 148.33428717659726
}
#Debug simulation 
Total elapsed time: 28.407583095133305. Arrivals time: 0.4745023618452251 Scheduler time: 27.744210372678936 Scheduler overhead time: 0.0724193500354886 Adapter cache time: 0.013050162699073553 Engine time: 0.07218757178634405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 49.340035795234144,
    "estimated_duration": 3600.098134123138,
    "input_throughput": 6938.257811153777,
    "output_throughput": 6045.9363020395695,
    "total_throughput": 12984.194113193345,
    "itl": 95.09493575128046,
    "ttft": 1281225.7042396374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8809810897987321,
    "arrivals": 164321,
    "finished_requests": 100547,
    "scheduler_time": 146.07835920822419
}
#Debug simulation 
Total elapsed time: 49.34018733119592. Arrivals time: 0.533505929633975 Scheduler time: 48.588745130226016 Scheduler overhead time: 0.08447845466434956 Adapter cache time: 0.014838614035397768 Engine time: 0.08539709122851491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 24.349242647644132,
    "estimated_duration": 3600.0964067749137,
    "input_throughput": 6754.78799796497,
    "output_throughput": 5891.439451478579,
    "total_throughput": 12646.227449443548,
    "itl": 89.03711906794321,
    "ttft": 1314545.8413367043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5016505081579115,
    "arrivals": 164321,
    "finished_requests": 97957,
    "scheduler_time": 148.36809044019316
}
#Debug simulation 
Total elapsed time: 24.3493324466981. Arrivals time: 0.4762609926983714 Scheduler time: 23.690140277147293 Scheduler overhead time: 0.06955943396314979 Adapter cache time: 0.013109429739415646 Engine time: 0.06979021802544594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 45.390345555730164,
    "estimated_duration": 3600.057136275284,
    "input_throughput": 7011.641216927011,
    "output_throughput": 6104.505336472951,
    "total_throughput": 13116.146553399962,
    "itl": 98.00347532125404,
    "ttft": 1241337.6958207565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2563577160704866,
    "arrivals": 160553,
    "finished_requests": 101914,
    "scheduler_time": 143.57065899252603
}
#Debug simulation 
Total elapsed time: 45.39050056878477. Arrivals time: 0.5582103580236435 Scheduler time: 44.626564152073115 Scheduler overhead time: 0.08070978941395879 Adapter cache time: 0.014179135207086802 Engine time: 0.0795346749946475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 26.34416812984273,
    "estimated_duration": 3600.0856423709606,
    "input_throughput": 6948.181372575941,
    "output_throughput": 6049.619971167294,
    "total_throughput": 12997.801343743235,
    "itl": 95.37544341519836,
    "ttft": 1254355.0004511757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.698189313975165,
    "arrivals": 160553,
    "finished_requests": 101003,
    "scheduler_time": 144.5193443898656
}
#Debug simulation 
Total elapsed time: 26.344317134004086. Arrivals time: 0.5127566563896835 Scheduler time: 25.65541976504028 Scheduler overhead time: 0.06735207187011838 Adapter cache time: 0.012845024932175875 Engine time: 0.0667490866035223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.076711632311344,
    "estimated_duration": 3600.084051001575,
    "input_throughput": 6729.30760970986,
    "output_throughput": 5872.066790806922,
    "total_throughput": 12601.374400516781,
    "itl": 88.99064670451787,
    "ttft": 1294511.532817926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.128318576444879,
    "arrivals": 160553,
    "finished_requests": 97955,
    "scheduler_time": 146.82061564866135
}
#Debug simulation 
Total elapsed time: 25.076844554394484. Arrivals time: 0.4746888941153884 Scheduler time: 24.417031392920762 Scheduler overhead time: 0.07042285986244678 Adapter cache time: 0.01383973564952612 Engine time: 0.07013976713642478 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 25.58165438193828,
    "estimated_duration": 3600.1004537144254,
    "input_throughput": 6957.6497439554305,
    "output_throughput": 6058.314005515275,
    "total_throughput": 13015.963749470706,
    "itl": 95.3464879538365,
    "ttft": 1253339.6787064534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4916466751229003,
    "arrivals": 160553,
    "finished_requests": 101167,
    "scheduler_time": 144.70659140180499
}
#Debug simulation 
Total elapsed time: 25.581757856998593. Arrivals time: 0.49995371513068676 Scheduler time: 24.90453276457265 Scheduler overhead time: 0.06801446387544274 Adapter cache time: 0.012565755750983953 Engine time: 0.06755457911640406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 23.417685139924288,
    "estimated_duration": 3600.0140313586376,
    "input_throughput": 6767.17945757724,
    "output_throughput": 5892.343978446784,
    "total_throughput": 12659.523436024025,
    "itl": 89.19850852830238,
    "ttft": 1286149.877831694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4301100199343662,
    "arrivals": 160553,
    "finished_requests": 98404,
    "scheduler_time": 146.90663476032142
}
#Debug simulation 
Total elapsed time: 23.417862285859883. Arrivals time: 0.4818410356529057 Scheduler time: 22.75221474841237 Scheduler overhead time: 0.07054674113169312 Adapter cache time: 0.012761151418089867 Engine time: 0.06994874402880669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 32.28622711217031,
    "estimated_duration": 3600.0466896233665,
    "input_throughput": 6940.2874890530775,
    "output_throughput": 6048.60099808264,
    "total_throughput": 12988.888487135719,
    "itl": 95.45710676593998,
    "ttft": 1254764.798400354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3342394765792376,
    "arrivals": 160553,
    "finished_requests": 100927,
    "scheduler_time": 144.4623869068216
}
#Debug simulation 
Total elapsed time: 32.28637718083337. Arrivals time: 0.5211518476717174 Scheduler time: 31.576000966131687 Scheduler overhead time: 0.07294215587899089 Adapter cache time: 0.013750841375440359 Engine time: 0.07277525309473276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.484877778682858,
    "estimated_duration": 3600.0600195198613,
    "input_throughput": 6753.597958970324,
    "output_throughput": 5885.204936895945,
    "total_throughput": 12638.802895866269,
    "itl": 89.17089346801335,
    "ttft": 1291007.9196162815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9722265419736555,
    "arrivals": 160553,
    "finished_requests": 98195,
    "scheduler_time": 146.865928359975
}
#Debug simulation 
Total elapsed time: 18.484989219810814. Arrivals time: 0.4492471790872514 Scheduler time: 17.860153127461672 Scheduler overhead time: 0.0670938272960484 Adapter cache time: 0.013102351687848568 Engine time: 0.06509837415069342 
