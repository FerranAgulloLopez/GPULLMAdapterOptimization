INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.3619079468771815,
    "estimated_duration": 3600.0794259356094,
    "input_throughput": 5708.10461901391,
    "output_throughput": 4942.550398141759,
    "total_throughput": 10650.65501715567,
    "itl": 116.78416812982081,
    "ttft": 1941484.482812522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.053914200915216,
    "arrivals": 426676,
    "finished_requests": 82985,
    "scheduler_time": 100.07215617246368
}
#Debug simulation 
Total elapsed time: 6.362001119647175. Arrivals time: 0.2702259486541152 Scheduler time: 5.952797208912671 Scheduler overhead time: 0.04764289874583483 Adapter cache time: 0.021271550562232733 Engine time: 0.047909971326589584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.444733679294586,
    "estimated_duration": 3600.037373946736,
    "input_throughput": 5552.045416152872,
    "output_throughput": 4809.602846155391,
    "total_throughput": 10361.648262308263,
    "itl": 109.13803730955323,
    "ttft": 1960581.54181802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.871422298177956,
    "arrivals": 426676,
    "finished_requests": 80763,
    "scheduler_time": 100.04896225081679
}
#Debug simulation 
Total elapsed time: 6.444805987179279. Arrivals time: 0.5432807290926576 Scheduler time: 5.751984357833862 Scheduler overhead time: 0.05061896098777652 Adapter cache time: 0.024815330747514963 Engine time: 0.050750838592648506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.7801019861362875,
    "estimated_duration": 3600.083958426633,
    "input_throughput": 5253.003879460884,
    "output_throughput": 4549.245014596168,
    "total_throughput": 9802.248894057053,
    "itl": 96.84096745935753,
    "ttft": 1996836.6924367386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2048,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.52305846448486,
    "arrivals": 426676,
    "finished_requests": 76404,
    "scheduler_time": 100.03276940270109
}
#Debug simulation 
Total elapsed time: 5.780229784082621. Arrivals time: 0.22808642638847232 Scheduler time: 5.3826304553076625 Scheduler overhead time: 0.05542070884257555 Adapter cache time: 0.032017388846725225 Engine time: 0.05614648852497339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.133144688326865,
    "estimated_duration": 3600.067749658058,
    "input_throughput": 5552.337730838142,
    "output_throughput": 4810.172531237725,
    "total_throughput": 10362.510262075868,
    "itl": 109.11877979924012,
    "ttft": 1960353.2074801244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.131500519402392,
    "arrivals": 426676,
    "finished_requests": 80770,
    "scheduler_time": 100.06919076691614
}
#Debug simulation 
Total elapsed time: 6.133226134348661. Arrivals time: 0.23461727797985077 Scheduler time: 5.7495617042295635 Scheduler overhead time: 0.05039728106930852 Adapter cache time: 0.024432617239654064 Engine time: 0.050879462622106075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.770144463982433,
    "estimated_duration": 3600.022422814831,
    "input_throughput": 5253.6742216210405,
    "output_throughput": 4549.423885864061,
    "total_throughput": 9803.098107485102,
    "itl": 96.83070089975428,
    "ttft": 1996930.5279364428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.305861953105424,
    "arrivals": 426676,
    "finished_requests": 76409,
    "scheduler_time": 100.03940725828888
}
#Debug simulation 
Total elapsed time: 5.7702265260741115. Arrivals time: 0.22744594514369965 Scheduler time: 5.374131775461137 Scheduler overhead time: 0.05545778898522258 Adapter cache time: 0.03117596684023738 Engine time: 0.05609876848757267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.180932593066245,
    "estimated_duration": 3600.101108387107,
    "input_throughput": 5553.4951375177325,
    "output_throughput": 4811.219873699598,
    "total_throughput": 10364.715011217331,
    "itl": 109.09020159364745,
    "ttft": 1959904.3844230718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1464,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.346060256995399,
    "arrivals": 426676,
    "finished_requests": 80788,
    "scheduler_time": 100.09132257988244
}
#Debug simulation 
Total elapsed time: 6.181068401783705. Arrivals time: 0.26862345449626446 Scheduler time: 5.763474480714649 Scheduler overhead time: 0.050302122719585896 Adapter cache time: 0.024042760487645864 Engine time: 0.051107484847307205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.767579122912139,
    "estimated_duration": 3600.0999224296625,
    "input_throughput": 5253.317798811599,
    "output_throughput": 4549.499834145228,
    "total_throughput": 9802.817632956827,
    "itl": 96.8285102576543,
    "ttft": 1996868.5964467241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2045,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.201513716708526,
    "arrivals": 426676,
    "finished_requests": 76409,
    "scheduler_time": 100.04058049502876
}
#Debug simulation 
Total elapsed time: 5.76768747670576. Arrivals time: 0.26181916845962405 Scheduler time: 5.336768651846796 Scheduler overhead time: 0.055346062406897545 Adapter cache time: 0.03159653255715966 Engine time: 0.05628760112449527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.131964216008782,
    "estimated_duration": 3600.072937926875,
    "input_throughput": 5677.857741340904,
    "output_throughput": 4941.647101806957,
    "total_throughput": 10619.50484314786,
    "itl": 116.76854820371263,
    "ttft": 1946677.292958175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.08545000989944,
    "arrivals": 420888,
    "finished_requests": 82610,
    "scheduler_time": 100.05114392505526
}
#Debug simulation 
Total elapsed time: 6.132068373262882. Arrivals time: 0.2707962291315198 Scheduler time: 5.720368509646505 Scheduler overhead time: 0.04759126575663686 Adapter cache time: 0.022940429858863354 Engine time: 0.04825856629759073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.9878548020496964,
    "estimated_duration": 3600.0162873103554,
    "input_throughput": 5520.415579799489,
    "output_throughput": 4810.891289866799,
    "total_throughput": 10331.306869666289,
    "itl": 109.0463761403081,
    "ttft": 1964728.0026865064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.274251264058805,
    "arrivals": 420888,
    "finished_requests": 80335,
    "scheduler_time": 100.08239759156002
}
#Debug simulation 
Total elapsed time: 5.988009620923549. Arrivals time: 0.2696306803263724 Scheduler time: 5.567436141893268 Scheduler overhead time: 0.05005717324092984 Adapter cache time: 0.026704990305006504 Engine time: 0.050843763165175915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.980636686086655,
    "estimated_duration": 3600.030706945927,
    "input_throughput": 5254.977676579067,
    "output_throughput": 4577.907896230492,
    "total_throughput": 9832.885572809559,
    "itl": 96.38893268007823,
    "ttft": 1998280.723167039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2074,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.51601796008615,
    "arrivals": 420888,
    "finished_requests": 76473,
    "scheduler_time": 100.60443908283166
}
#Debug simulation 
Total elapsed time: 5.980696361977607. Arrivals time: 0.2720488356426358 Scheduler time: 5.5383089003153145 Scheduler overhead time: 0.05464618047699332 Adapter cache time: 0.033986691385507584 Engine time: 0.055898191407322884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.939395866822451,
    "estimated_duration": 3600.0458546892023,
    "input_throughput": 5525.272122323354,
    "output_throughput": 4814.468676120884,
    "total_throughput": 10339.740798444238,
    "itl": 109.17390003516404,
    "ttft": 1964836.8381787331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.439656605976753,
    "arrivals": 420888,
    "finished_requests": 80399,
    "scheduler_time": 100.0961605864386
}
#Debug simulation 
Total elapsed time: 5.939507617149502. Arrivals time: 0.2849153191782534 Scheduler time: 5.503901773598045 Scheduler overhead time: 0.04986049048602581 Adapter cache time: 0.02666907338425517 Engine time: 0.0508804670535028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.7235408131964505,
    "estimated_duration": 3600.1039914651155,
    "input_throughput": 5253.231863533866,
    "output_throughput": 4576.406414664438,
    "total_throughput": 9829.638278198305,
    "itl": 96.29433304071246,
    "ttft": 1998719.1557586284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.35892275402757,
    "arrivals": 420888,
    "finished_requests": 76448,
    "scheduler_time": 100.61482105419968
}
#Debug simulation 
Total elapsed time: 5.723692044150084. Arrivals time: 0.28367335395887494 Scheduler time: 5.268976844847202 Scheduler overhead time: 0.05485441815108061 Adapter cache time: 0.034049061592668295 Engine time: 0.05632746405899525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.953450492117554,
    "estimated_duration": 3600.0315177679076,
    "input_throughput": 5526.996611500992,
    "output_throughput": 4815.859226351841,
    "total_throughput": 10342.855837852834,
    "itl": 109.14185937419019,
    "ttft": 1964471.5905912367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.661147970753104,
    "arrivals": 420888,
    "finished_requests": 80421,
    "scheduler_time": 100.1234388958904
}
#Debug simulation 
Total elapsed time: 5.953536082059145. Arrivals time: 0.23743418045341969 Scheduler time: 5.564349722582847 Scheduler overhead time: 0.05033111106604338 Adapter cache time: 0.026901936624199152 Engine time: 0.05118467519059777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.723012160975486,
    "estimated_duration": 3600.020627049193,
    "input_throughput": 5254.994612491005,
    "output_throughput": 4577.802937064895,
    "total_throughput": 9832.7975495559,
    "itl": 96.38327781097512,
    "ttft": 1998256.2949592646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2077,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.25659902745827,
    "arrivals": 420888,
    "finished_requests": 76473,
    "scheduler_time": 100.60961426830649
}
#Debug simulation 
Total elapsed time: 5.723108499310911. Arrivals time: 0.2691598911769688 Scheduler time: 5.283142888452858 Scheduler overhead time: 0.05488501721993089 Adapter cache time: 0.03393353847786784 Engine time: 0.056225304026156664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.151510575786233,
    "estimated_duration": 3600.0935674715793,
    "input_throughput": 5677.205499509938,
    "output_throughput": 4957.633924091308,
    "total_throughput": 10634.839423601245,
    "itl": 116.70708068450674,
    "ttft": 1940755.5513500238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.179549972522269,
    "arrivals": 418034,
    "finished_requests": 82929,
    "scheduler_time": 100.32716510719366
}
#Debug simulation 
Total elapsed time: 6.151657123118639. Arrivals time: 0.24365224409848452 Scheduler time: 5.768244503065944 Scheduler overhead time: 0.04731191135942936 Adapter cache time: 0.022346612997353077 Engine time: 0.04791379300877452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.955685933120549,
    "estimated_duration": 3600.1062966002505,
    "input_throughput": 5552.088564405921,
    "output_throughput": 4840.148752400809,
    "total_throughput": 10392.23731680673,
    "itl": 108.60863610927441,
    "ttft": 1958203.3833073159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1368,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.975661617815419,
    "arrivals": 418034,
    "finished_requests": 80990,
    "scheduler_time": 100.65249181354282
}
#Debug simulation 
Total elapsed time: 5.955767150036991. Arrivals time: 0.23958567110821605 Scheduler time: 5.567066407296807 Scheduler overhead time: 0.04955640435218811 Adapter cache time: 0.025239709299057722 Engine time: 0.051059884019196033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.712542853318155,
    "estimated_duration": 3600.0790331742296,
    "input_throughput": 5300.480579498572,
    "output_throughput": 4629.401978796342,
    "total_throughput": 9929.882558294914,
    "itl": 95.35072659494571,
    "ttft": 1988093.8841077804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1466,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.923603338291754,
    "arrivals": 418034,
    "finished_requests": 77426,
    "scheduler_time": 101.71967039444826
}
#Debug simulation 
Total elapsed time: 5.712626345921308. Arrivals time: 0.23324212105944753 Scheduler time: 5.311702109873295 Scheduler overhead time: 0.055114638060331345 Adapter cache time: 0.030114945024251938 Engine time: 0.056535950396209955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.985245232004672,
    "estimated_duration": 3600.12314904098,
    "input_throughput": 5552.753939910417,
    "output_throughput": 4840.8766251905845,
    "total_throughput": 10393.630565101003,
    "itl": 108.5956970923091,
    "ttft": 1957991.3499024177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.36540157348388,
    "arrivals": 418034,
    "finished_requests": 81000,
    "scheduler_time": 100.66959155141924
}
#Debug simulation 
Total elapsed time: 5.985384079162031. Arrivals time: 0.2397428909316659 Scheduler time: 5.596869156230241 Scheduler overhead time: 0.04946307325735688 Adapter cache time: 0.025014879181981087 Engine time: 0.050793073140084743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.745066728908569,
    "estimated_duration": 3600.0027305615977,
    "input_throughput": 5300.151257669587,
    "output_throughput": 4629.142044400695,
    "total_throughput": 9929.293302070282,
    "itl": 95.35166582220201,
    "ttft": 1988089.7052031823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1475,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.897544615641122,
    "arrivals": 418034,
    "finished_requests": 77420,
    "scheduler_time": 101.71813255880627
}
#Debug simulation 
Total elapsed time: 5.745147555600852. Arrivals time: 0.22810479905456305 Scheduler time: 5.349893128499389 Scheduler overhead time: 0.054734505247324705 Adapter cache time: 0.03031468251720071 Engine time: 0.056203401647508144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.947771645151079,
    "estimated_duration": 3600.095431098876,
    "input_throughput": 5553.740555676639,
    "output_throughput": 4841.978590184001,
    "total_throughput": 10395.719145860641,
    "itl": 108.57486066803247,
    "ttft": 1957787.46645462,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.701284241997689,
    "arrivals": 418034,
    "finished_requests": 81013,
    "scheduler_time": 100.68664887320845
}
#Debug simulation 
Total elapsed time: 5.947857005987316. Arrivals time: 0.2390760383568704 Scheduler time: 5.559986869804561 Scheduler overhead time: 0.04952155379578471 Adapter cache time: 0.02520989580079913 Engine time: 0.05078181251883507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.751602536067367,
    "estimated_duration": 3600.0490407532843,
    "input_throughput": 5300.467516966724,
    "output_throughput": 4629.570267329641,
    "total_throughput": 9930.037784296366,
    "itl": 95.34595762254744,
    "ttft": 1988071.7996530957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.781561989765502,
    "arrivals": 418034,
    "finished_requests": 77427,
    "scheduler_time": 101.72450866674528
}
#Debug simulation 
Total elapsed time: 5.751748403068632. Arrivals time: 0.2322259838692844 Scheduler time: 5.35119117423892 Scheduler overhead time: 0.05463162297382951 Adapter cache time: 0.030295726377516985 Engine time: 0.057247384916990995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.094648411031812,
    "estimated_duration": 3600.0337456872226,
    "input_throughput": 5683.470613160653,
    "output_throughput": 4992.665144189899,
    "total_throughput": 10676.135757350552,
    "itl": 116.36253895243101,
    "ttft": 1937349.474684831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 957,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.3280754435762265,
    "arrivals": 416602,
    "finished_requests": 83191,
    "scheduler_time": 100.88222321059425
}
#Debug simulation 
Total elapsed time: 6.094731173012406. Arrivals time: 0.2738951318897307 Scheduler time: 5.682587715797126 Scheduler overhead time: 0.04679923877120018 Adapter cache time: 0.02141991537064314 Engine time: 0.04800450894981623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.974776469171047,
    "estimated_duration": 3600.0951971091404,
    "input_throughput": 5551.798190239378,
    "output_throughput": 4881.847572839945,
    "total_throughput": 10433.645763079323,
    "itl": 108.20759774407495,
    "ttft": 1952164.3385634832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.141918025049389,
    "arrivals": 416602,
    "finished_requests": 81328,
    "scheduler_time": 101.3931692332078
}
#Debug simulation 
Total elapsed time: 5.974882497917861. Arrivals time: 0.23659329488873482 Scheduler time: 5.590648065321147 Scheduler overhead time: 0.04948448995128274 Adapter cache time: 0.024128457065671682 Engine time: 0.050702224019914865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.059762865304947,
    "estimated_duration": 3600.0883225348007,
    "input_throughput": 5310.892480142806,
    "output_throughput": 4676.190551943678,
    "total_throughput": 9987.083032086484,
    "itl": 94.86071328504177,
    "ttft": 1981490.0375887123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 990,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.373307112297058,
    "arrivals": 416602,
    "finished_requests": 77771,
    "scheduler_time": 102.57124925056638
}
#Debug simulation 
Total elapsed time: 6.059890463016927. Arrivals time: 0.4992106268182397 Scheduler time: 5.394541800953448 Scheduler overhead time: 0.054790642112493515 Adapter cache time: 0.028961701318621635 Engine time: 0.05636110482737422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.962065157014877,
    "estimated_duration": 3600.1082098387146,
    "input_throughput": 5552.233109375203,
    "output_throughput": 4882.384077224016,
    "total_throughput": 10434.61718659922,
    "itl": 108.19697374910784,
    "ttft": 1952021.1310897798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 988,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.7196155183575925,
    "arrivals": 416602,
    "finished_requests": 81334,
    "scheduler_time": 101.40495057406396
}
#Debug simulation 
Total elapsed time: 5.962169826030731. Arrivals time: 0.25530706997960806 Scheduler time: 5.560021822806448 Scheduler overhead time: 0.049170101061463356 Adapter cache time: 0.023955088108778 Engine time: 0.0505541586317122 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.759448916185647,
    "estimated_duration": 3600.0469394487454,
    "input_throughput": 5310.752698943303,
    "output_throughput": 4676.166528700251,
    "total_throughput": 9986.919227643553,
    "itl": 94.8608834057751,
    "ttft": 1981503.0342688442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 993,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.327892092093841,
    "arrivals": 416602,
    "finished_requests": 77770,
    "scheduler_time": 102.56990722299119
}
#Debug simulation 
Total elapsed time: 5.7595351547934115. Arrivals time: 0.23232021695002913 Scheduler time: 5.3608111762441695 Scheduler overhead time: 0.05484497966244817 Adapter cache time: 0.029001343064010143 Engine time: 0.05653943354263902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.986582315061241,
    "estimated_duration": 3600.006396958686,
    "input_throughput": 5552.718466524827,
    "output_throughput": 4882.72826816361,
    "total_throughput": 10435.446734688436,
    "itl": 108.18094332922789,
    "ttft": 1951893.7474213282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 988,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.307313889283655,
    "arrivals": 416602,
    "finished_requests": 81338,
    "scheduler_time": 101.41235717820868
}
#Debug simulation 
Total elapsed time: 5.986740613821894. Arrivals time: 0.23800606140866876 Scheduler time: 5.601025908719748 Scheduler overhead time: 0.04943204624578357 Adapter cache time: 0.02407487155869603 Engine time: 0.050792221911251545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.770359302870929,
    "estimated_duration": 3600.0682908160375,
    "input_throughput": 5310.865643514261,
    "output_throughput": 4676.1990718192865,
    "total_throughput": 9987.064715333547,
    "itl": 94.85879325165695,
    "ttft": 1981493.5614596335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 987,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.224549958799052,
    "arrivals": 416602,
    "finished_requests": 77772,
    "scheduler_time": 102.57398676613303
}
#Debug simulation 
Total elapsed time: 5.770475037861615. Arrivals time: 0.26515701971948147 Scheduler time: 5.339182431809604 Scheduler overhead time: 0.054738329257816076 Adapter cache time: 0.028840916696935892 Engine time: 0.056582671124488115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.115237063728273,
    "estimated_duration": 3600.055825497454,
    "input_throughput": 5738.593233382795,
    "output_throughput": 5022.366284417716,
    "total_throughput": 10760.959517800511,
    "itl": 115.26303255292035,
    "ttft": 1929319.4634071784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5773132862849644,
    "arrivals": 415862,
    "finished_requests": 83963,
    "scheduler_time": 101.59712108441714
}
#Debug simulation 
Total elapsed time: 6.115319198928773. Arrivals time: 0.2734381118789315 Scheduler time: 5.7058757725171745 Scheduler overhead time: 0.046429796144366264 Adapter cache time: 0.019676706288009882 Engine time: 0.04788153851404786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.015185084193945,
    "estimated_duration": 3600.1062017005197,
    "input_throughput": 5612.98024776464,
    "output_throughput": 4915.334161987048,
    "total_throughput": 10528.314409751689,
    "itl": 107.11862957430907,
    "ttft": 1944739.9940147398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9706671371636966,
    "arrivals": 415862,
    "finished_requests": 82134,
    "scheduler_time": 102.13892244431267
}
#Debug simulation 
Total elapsed time: 6.015337934251875. Arrivals time: 0.24037591321393847 Scheduler time: 5.628133061341941 Scheduler overhead time: 0.049514065496623516 Adapter cache time: 0.022387461736798286 Engine time: 0.05138164293020964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.818758967798203,
    "estimated_duration": 3600.0413007539037,
    "input_throughput": 5370.692551763507,
    "output_throughput": 4706.609614854049,
    "total_throughput": 10077.302166617557,
    "itl": 93.9050141093278,
    "ttft": 1974809.123008297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9140244584065247,
    "arrivals": 415862,
    "finished_requests": 78612,
    "scheduler_time": 103.33175619631751
}
#Debug simulation 
Total elapsed time: 5.818840392865241. Arrivals time: 0.2636466817930341 Scheduler time: 5.3892042697407305 Scheduler overhead time: 0.0551341911777854 Adapter cache time: 0.02753685647621751 Engine time: 0.057069551665335894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.011038410011679,
    "estimated_duration": 3600.1000739180827,
    "input_throughput": 5613.128686727671,
    "output_throughput": 4915.421970684545,
    "total_throughput": 10528.550657412217,
    "itl": 107.10963194593484,
    "ttft": 1944750.718058154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 544,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7255092088133046,
    "arrivals": 415862,
    "finished_requests": 82135,
    "scheduler_time": 102.14664031395007
}
#Debug simulation 
Total elapsed time: 6.011148124933243. Arrivals time: 0.270820836070925 Scheduler time: 5.5941419173032045 Scheduler overhead time: 0.049533810932189226 Adapter cache time: 0.022237062454223633 Engine time: 0.05094652809202671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.7902979529462755,
    "estimated_duration": 3600.0537792199993,
    "input_throughput": 5371.384758643712,
    "output_throughput": 4707.003572505371,
    "total_throughput": 10078.388331149083,
    "itl": 93.90192965204089,
    "ttft": 1974890.0846420464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8488404028117973,
    "arrivals": 415862,
    "finished_requests": 78620,
    "scheduler_time": 103.33585872731052
}
#Debug simulation 
Total elapsed time: 5.790451257955283. Arrivals time: 0.2637536064721644 Scheduler time: 5.360686623957008 Scheduler overhead time: 0.05517164245247841 Adapter cache time: 0.027560795657336712 Engine time: 0.057034142315387726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.016425332985818,
    "estimated_duration": 3600.098487751174,
    "input_throughput": 5613.31559921389,
    "output_throughput": 4915.749683018996,
    "total_throughput": 10529.065282232885,
    "itl": 107.09992389375114,
    "ttft": 1944602.5591877138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4983886754326363,
    "arrivals": 415862,
    "finished_requests": 82137,
    "scheduler_time": 102.15192586825464
}
#Debug simulation 
Total elapsed time: 6.0165303917601705. Arrivals time: 0.23736496502533555 Scheduler time: 5.632084592245519 Scheduler overhead time: 0.049678067676723 Adapter cache time: 0.022500083781778812 Engine time: 0.051373844500631094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.829114824999124,
    "estimated_duration": 3600.0967803754493,
    "input_throughput": 5371.437263968025,
    "output_throughput": 4706.97596030537,
    "total_throughput": 10078.413224273396,
    "itl": 93.90331301847728,
    "ttft": 1974925.693553015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.836149896513702,
    "arrivals": 415862,
    "finished_requests": 78621,
    "scheduler_time": 103.33610074156636
}
#Debug simulation 
Total elapsed time: 5.829194314777851. Arrivals time: 0.265819969587028 Scheduler time: 5.39622893370688 Scheduler overhead time: 0.05567103158682585 Adapter cache time: 0.027679494582116604 Engine time: 0.057513921055942774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.1428132159635425,
    "estimated_duration": 3600.0425405631927,
    "input_throughput": 5761.191921014172,
    "output_throughput": 5039.699891202307,
    "total_throughput": 10800.89181221648,
    "itl": 115.22410485389828,
    "ttft": 1925596.4694807457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2283818437671337,
    "arrivals": 415494,
    "finished_requests": 84360,
    "scheduler_time": 101.87312176667487
}
#Debug simulation 
Total elapsed time: 6.142967781983316. Arrivals time: 0.28173726610839367 Scheduler time: 5.725193890742958 Scheduler overhead time: 0.04691628832370043 Adapter cache time: 0.018995375372469425 Engine time: 0.048027136363089085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.022093737963587,
    "estimated_duration": 3600.005294600695,
    "input_throughput": 5637.8597638288265,
    "output_throughput": 4931.646635805638,
    "total_throughput": 10569.506399634465,
    "itl": 107.08570075333068,
    "ttft": 1941425.8851582655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.384064251929524,
    "arrivals": 415494,
    "finished_requests": 82548,
    "scheduler_time": 102.39869514934144
}
#Debug simulation 
Total elapsed time: 6.022178363054991. Arrivals time: 0.23777939099818468 Scheduler time: 5.638136273249984 Scheduler overhead time: 0.04974894970655441 Adapter cache time: 0.021696642972528934 Engine time: 0.051373102236539125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.807316726073623,
    "estimated_duration": 3600.0670689232516,
    "input_throughput": 5393.367575734444,
    "output_throughput": 4718.969306613819,
    "total_throughput": 10112.336882348261,
    "itl": 93.96002641970833,
    "ttft": 1972468.2063480755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3342478847690025,
    "arrivals": 415494,
    "finished_requests": 78983,
    "scheduler_time": 103.4968812872798
}
#Debug simulation 
Total elapsed time: 5.807425423990935. Arrivals time: 0.26853060722351074 Scheduler time: 5.373483249451965 Scheduler overhead time: 0.05544109269976616 Adapter cache time: 0.026397428009659052 Engine time: 0.05711340997368097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.028882231097668,
    "estimated_duration": 3600.0784120356584,
    "input_throughput": 5637.954699026416,
    "output_throughput": 4931.835356875037,
    "total_throughput": 10569.790055901454,
    "itl": 107.07908735180787,
    "ttft": 1941368.2987398421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.201385745061559,
    "arrivals": 415494,
    "finished_requests": 82552,
    "scheduler_time": 102.40584831269925
}
#Debug simulation 
Total elapsed time: 6.029032074380666. Arrivals time: 0.24005977995693684 Scheduler time: 5.642666831612587 Scheduler overhead time: 0.04983069607988 Adapter cache time: 0.021623354870826006 Engine time: 0.051190437749028206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.091176320333034,
    "estimated_duration": 3600.0706079240163,
    "input_throughput": 5393.1722775837825,
    "output_throughput": 4718.577175294818,
    "total_throughput": 10111.7494528786,
    "itl": 93.96249558972427,
    "ttft": 1972480.488283286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.308411603188152,
    "arrivals": 415494,
    "finished_requests": 78979,
    "scheduler_time": 103.49617196449343
}
#Debug simulation 
Total elapsed time: 6.091272949241102. Arrivals time: 0.5010457988828421 Scheduler time: 5.425166770815849 Scheduler overhead time: 0.0553845651447773 Adapter cache time: 0.026160283479839563 Engine time: 0.05725546879693866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.042598122730851,
    "estimated_duration": 3600.0193095130257,
    "input_throughput": 5638.047258903615,
    "output_throughput": 4931.916324193749,
    "total_throughput": 10569.963583097364,
    "itl": 107.07893658740359,
    "ttft": 1941329.8942738438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.081158226625986,
    "arrivals": 415494,
    "finished_requests": 82552,
    "scheduler_time": 102.40623585337545
}
#Debug simulation 
Total elapsed time: 6.042680164799094. Arrivals time: 0.2749570091255009 Scheduler time: 5.621165340766311 Scheduler overhead time: 0.04968421859666705 Adapter cache time: 0.021853215992450714 Engine time: 0.05144052952528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.80978740286082,
    "estimated_duration": 3600.086650081192,
    "input_throughput": 5393.346851682612,
    "output_throughput": 4719.017804645578,
    "total_throughput": 10112.36465632819,
    "itl": 93.960016718813,
    "ttft": 1972508.7896152164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.287285651173452,
    "arrivals": 415494,
    "finished_requests": 78987,
    "scheduler_time": 103.49739488842206
}
#Debug simulation 
Total elapsed time: 5.809948791749775. Arrivals time: 0.27275741659104824 Scheduler time: 5.372667375020683 Scheduler overhead time: 0.05517347855493426 Adapter cache time: 0.02596139768138528 Engine time: 0.05704376520588994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.323873044922948,
    "estimated_duration": 3600.0605152149187,
    "input_throughput": 5927.559247910603,
    "output_throughput": 5205.551384705329,
    "total_throughput": 11133.110632615932,
    "itl": 111.66725329282575,
    "ttft": 1879480.9612483026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1941,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.834685931015175,
    "arrivals": 386239,
    "finished_requests": 86718,
    "scheduler_time": 105.03983522867027
}
#Debug simulation 
Total elapsed time: 6.323952946811914. Arrivals time: 0.24577353475615382 Scheduler time: 5.924030274152756 Scheduler overhead time: 0.04852893343195319 Adapter cache time: 0.03299538977444172 Engine time: 0.04976082779467106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.240045636892319,
    "estimated_duration": 3600.0132969123583,
    "input_throughput": 5828.030140331667,
    "output_throughput": 5119.521368381403,
    "total_throughput": 10947.55150871307,
    "itl": 103.1856564169372,
    "ttft": 1891867.698708508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.160634058061728,
    "arrivals": 386239,
    "finished_requests": 85284,
    "scheduler_time": 106.14284049427596
}
#Debug simulation 
Total elapsed time: 6.240154546685517. Arrivals time: 0.24302930105477571 Scheduler time: 5.832788897212595 Scheduler overhead time: 0.051743889693170786 Adapter cache time: 0.03488220879808068 Engine time: 0.05336310248821974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.043665918055922,
    "estimated_duration": 3600.077115892225,
    "input_throughput": 5616.494133067709,
    "output_throughput": 4939.4061370246345,
    "total_throughput": 10555.900270092343,
    "itl": 89.81630228622356,
    "ttft": 1918292.5842527563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1853,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.949932963014318,
    "arrivals": 386239,
    "finished_requests": 82206,
    "scheduler_time": 108.15283529192915
}
#Debug simulation 
Total elapsed time: 6.043809917289764. Arrivals time: 0.23853801377117634 Scheduler time: 5.6234374740161 Scheduler overhead time: 0.05796851171180606 Adapter cache time: 0.036331957671791315 Engine time: 0.06003204174339771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.291740285232663,
    "estimated_duration": 3600.0244113317035,
    "input_throughput": 5829.512137179319,
    "output_throughput": 5120.732498916779,
    "total_throughput": 10950.244636096098,
    "itl": 103.15831003300809,
    "ttft": 1891670.6929907051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1935,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.238572000977802,
    "arrivals": 386239,
    "finished_requests": 85303,
    "scheduler_time": 106.16939341684512
}
#Debug simulation 
Total elapsed time: 6.291846384294331. Arrivals time: 0.23968300828710198 Scheduler time: 5.8879023301415145 Scheduler overhead time: 0.05212957784533501 Adapter cache time: 0.034348100889474154 Engine time: 0.05312135210260749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.054925750941038,
    "estimated_duration": 3600.0198675003003,
    "input_throughput": 5616.732335991285,
    "output_throughput": 4939.715794498603,
    "total_throughput": 10556.448130489889,
    "itl": 89.80914795524711,
    "ttft": 1918240.034618696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1851,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.800863457862029,
    "arrivals": 386239,
    "finished_requests": 82209,
    "scheduler_time": 108.15516867686121
}
#Debug simulation 
Total elapsed time: 6.055001779925078. Arrivals time: 0.2374032041989267 Scheduler time: 5.635846993420273 Scheduler overhead time: 0.05804590927436948 Adapter cache time: 0.03632470779120922 Engine time: 0.059869612101465464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.255937722045928,
    "estimated_duration": 3600.080990410179,
    "input_throughput": 5830.954930157909,
    "output_throughput": 5122.098099770535,
    "total_throughput": 10953.053029928444,
    "itl": 103.13260959069086,
    "ttft": 1891580.8831470613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1929,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.314583494361093,
    "arrivals": 386239,
    "finished_requests": 85322,
    "scheduler_time": 106.19694352039681
}
#Debug simulation 
Total elapsed time: 6.256068411283195. Arrivals time: 0.24581604031845927 Scheduler time: 5.846345318946987 Scheduler overhead time: 0.051759060472249985 Adapter cache time: 0.034382383804768324 Engine time: 0.05336148804053664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.086508607957512,
    "estimated_duration": 3600.0837894471197,
    "input_throughput": 5617.109262644575,
    "output_throughput": 4939.830026214789,
    "total_throughput": 10556.939288859365,
    "itl": 89.80591251114343,
    "ttft": 1918205.3439532255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1848,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.652831035107118,
    "arrivals": 386239,
    "finished_requests": 82214,
    "scheduler_time": 108.1616281036638
}
#Debug simulation 
Total elapsed time: 6.086600236129016. Arrivals time: 0.2599193034693599 Scheduler time: 5.645435365382582 Scheduler overhead time: 0.05801637517288327 Adapter cache time: 0.03598740417510271 Engine time: 0.05977221950888634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.497541144955903,
    "estimated_duration": 3600.050310623493,
    "input_throughput": 6144.968289670556,
    "output_throughput": 5356.171813237928,
    "total_throughput": 11501.140102908485,
    "itl": 107.83112485019817,
    "ttft": 1858351.309861207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.410984288640524,
    "arrivals": 383390,
    "finished_requests": 89539,
    "scheduler_time": 108.20173492582282
}
#Debug simulation 
Total elapsed time: 6.497619037982076. Arrivals time: 0.26804976304993033 Scheduler time: 6.0766332424245775 Scheduler overhead time: 0.0498090130276978 Adapter cache time: 0.02798810787498951 Engine time: 0.051610955968499184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.425336688756943,
    "estimated_duration": 3600.0068915457246,
    "input_throughput": 6029.673179508772,
    "output_throughput": 5262.0015935209485,
    "total_throughput": 11291.674773029721,
    "itl": 99.84763806004587,
    "ttft": 1872449.1797491305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.125740662622237,
    "arrivals": 383390,
    "finished_requests": 87887,
    "scheduler_time": 109.19703650185244
}
#Debug simulation 
Total elapsed time: 6.425473446026444. Arrivals time: 0.26884777564555407 Scheduler time: 5.993924608454108 Scheduler overhead time: 0.05329162394627929 Adapter cache time: 0.02919056359678507 Engine time: 0.05501385498791933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.225954690016806,
    "estimated_duration": 3600.050123219128,
    "input_throughput": 5787.9285806631815,
    "output_throughput": 5052.718539300406,
    "total_throughput": 10840.647119963587,
    "itl": 87.289278247597,
    "ttft": 1900485.0009413764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.182794381189128,
    "arrivals": 383390,
    "finished_requests": 84398,
    "scheduler_time": 110.79201072914493
}
#Debug simulation 
Total elapsed time: 6.2260389062576. Arrivals time: 0.2684508138336241 Scheduler time: 5.776923796162009 Scheduler overhead time: 0.05950124096125364 Adapter cache time: 0.031321174930781126 Engine time: 0.06151191471144557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.402544208802283,
    "estimated_duration": 3600.0325267314697,
    "input_throughput": 6030.543568369093,
    "output_throughput": 5263.156890752536,
    "total_throughput": 11293.70045912163,
    "itl": 99.83007028851964,
    "ttft": 1872157.202846389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.585987079441557,
    "arrivals": 383390,
    "finished_requests": 87899,
    "scheduler_time": 109.2135223614462
}
#Debug simulation 
Total elapsed time: 6.402634468860924. Arrivals time: 0.263908046297729 Scheduler time: 5.975674720015377 Scheduler overhead time: 0.053188640624284744 Adapter cache time: 0.02959432266652584 Engine time: 0.05515163252130151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.205050867050886,
    "estimated_duration": 3600.0916610777795,
    "input_throughput": 5788.5237271323385,
    "output_throughput": 5053.013565369601,
    "total_throughput": 10841.53729250194,
    "itl": 87.28441125591543,
    "ttft": 1900522.5296969085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.084727570465736,
    "arrivals": 383390,
    "finished_requests": 84404,
    "scheduler_time": 110.79680278338417
}
#Debug simulation 
Total elapsed time: 6.205176533199847. Arrivals time: 0.2586308908648789 Scheduler time: 5.766039937268943 Scheduler overhead time: 0.05948220193386078 Adapter cache time: 0.03163709910586476 Engine time: 0.06126785511150956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.411012505181134,
    "estimated_duration": 3600.0484424007577,
    "input_throughput": 6031.662169945535,
    "output_throughput": 5263.966666893652,
    "total_throughput": 11295.628836839187,
    "itl": 99.81127837783835,
    "ttft": 1872081.3365600088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.005436859475408,
    "arrivals": 383390,
    "finished_requests": 87915,
    "scheduler_time": 109.2328278826546
}
#Debug simulation 
Total elapsed time: 6.411095784977078. Arrivals time: 0.26511590788140893 Scheduler time: 5.983448499348015 Scheduler overhead time: 0.0534875662997365 Adapter cache time: 0.029126652516424656 Engine time: 0.05478887911885977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.205534478183836,
    "estimated_duration": 3600.0812523976765,
    "input_throughput": 5788.599628444722,
    "output_throughput": 5053.139283421508,
    "total_throughput": 10841.73891186623,
    "itl": 87.28326972701603,
    "ttft": 1900640.6799728943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.0286443397775,
    "arrivals": 383390,
    "finished_requests": 84407,
    "scheduler_time": 110.79859373244904
}
#Debug simulation 
Total elapsed time: 6.2056174031458795. Arrivals time: 0.2601478914730251 Scheduler time: 5.76425523776561 Scheduler overhead time: 0.05994865903630853 Adapter cache time: 0.031385931186378 Engine time: 0.061541929841041565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.545847963076085,
    "estimated_duration": 3600.0761292640327,
    "input_throughput": 6225.337241571512,
    "output_throughput": 5410.622248141747,
    "total_throughput": 11635.95948971326,
    "itl": 106.89372693366684,
    "ttft": 1848395.389527762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 803,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.309764452655903,
    "arrivals": 381970,
    "finished_requests": 90644,
    "scheduler_time": 109.22511599268589
}
#Debug simulation 
Total elapsed time: 6.546000257134438. Arrivals time: 0.2507253033109009 Scheduler time: 6.144110060296953 Scheduler overhead time: 0.05040225526317954 Adapter cache time: 0.02523028338328004 Engine time: 0.051613382529467344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.4540714411996305,
    "estimated_duration": 3600.027136502969,
    "input_throughput": 6107.1184094897635,
    "output_throughput": 5309.95830730573,
    "total_throughput": 11417.076716795495,
    "itl": 99.05494029131034,
    "ttft": 1863359.5548000268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 797,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.795977686024278,
    "arrivals": 381970,
    "finished_requests": 88931,
    "scheduler_time": 110.08959102029456
}
#Debug simulation 
Total elapsed time: 6.454159259330481. Arrivals time: 0.2568735210224986 Scheduler time: 6.035703491419554 Scheduler overhead time: 0.05366333434358239 Adapter cache time: 0.02655646437779069 Engine time: 0.055726215709000826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.259086521808058,
    "estimated_duration": 3600.0549704286727,
    "input_throughput": 5846.829610352762,
    "output_throughput": 5093.952217573044,
    "total_throughput": 10940.781827925806,
    "itl": 86.71599212285271,
    "ttft": 1892747.506342015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 769,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.725199408284408,
    "arrivals": 381970,
    "finished_requests": 85234,
    "scheduler_time": 111.56838124668182
}
#Debug simulation 
Total elapsed time: 6.259188421070576. Arrivals time: 0.25935555901378393 Scheduler time: 5.821298427414149 Scheduler overhead time: 0.060012741945683956 Adapter cache time: 0.028345399536192417 Engine time: 0.0616978881880641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.44082934781909,
    "estimated_duration": 3600.0397080846806,
    "input_throughput": 6107.600133026867,
    "output_throughput": 5310.197817282055,
    "total_throughput": 11417.797950308923,
    "itl": 99.04260018191499,
    "ttft": 1863240.4815621395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 796,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.431432753968975,
    "arrivals": 381970,
    "finished_requests": 88935,
    "scheduler_time": 110.09980584508368
}
#Debug simulation 
Total elapsed time: 6.440975551959127. Arrivals time: 0.2486410946585238 Scheduler time: 6.031848852522671 Scheduler overhead time: 0.053588828071951866 Adapter cache time: 0.026402113027870655 Engine time: 0.05508198123425245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.232619957067072,
    "estimated_duration": 3600.092308351966,
    "input_throughput": 5846.940910700137,
    "output_throughput": 5094.104103234858,
    "total_throughput": 10941.045013934994,
    "itl": 86.71444313886028,
    "ttft": 1892637.2649706365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 769,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.674921803348717,
    "arrivals": 381970,
    "finished_requests": 85239,
    "scheduler_time": 111.57212320413876
}
#Debug simulation 
Total elapsed time: 6.232699204236269. Arrivals time: 0.24320051353424788 Scheduler time: 5.811744092032313 Scheduler overhead time: 0.05957324989140034 Adapter cache time: 0.02833416685461998 Engine time: 0.06145840557292104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.485445347148925,
    "estimated_duration": 3600.1067833252932,
    "input_throughput": 6108.587417978518,
    "output_throughput": 5310.860802395375,
    "total_throughput": 11419.448220373893,
    "itl": 99.03229601610897,
    "ttft": 1863299.7902536315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 799,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.100752831515831,
    "arrivals": 381970,
    "finished_requests": 88948,
    "scheduler_time": 110.11180982273336
}
#Debug simulation 
Total elapsed time: 6.4855547291226685. Arrivals time: 0.26609143847599626 Scheduler time: 6.058727881405503 Scheduler overhead time: 0.053854858968406916 Adapter cache time: 0.02621097071096301 Engine time: 0.05528254620730877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.2267359290272,
    "estimated_duration": 3600.0681961838545,
    "input_throughput": 5846.980349514747,
    "output_throughput": 5094.214887218043,
    "total_throughput": 10941.19523673279,
    "itl": 86.71284064111903,
    "ttft": 1892641.533223719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 770,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.634118223041315,
    "arrivals": 381970,
    "finished_requests": 85240,
    "scheduler_time": 111.57306285353866
}
#Debug simulation 
Total elapsed time: 6.226884142961353. Arrivals time: 0.24439860973507166 Scheduler time: 5.804195311386138 Scheduler overhead time: 0.060090945567935705 Adapter cache time: 0.028093356639146805 Engine time: 0.06163850286975503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.823499627877027,
    "estimated_duration": 3600.1114398235263,
    "input_throughput": 6202.845765545147,
    "output_throughput": 5436.628650850715,
    "total_throughput": 11639.47441639586,
    "itl": 106.79363366994721,
    "ttft": 1850480.055937275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6764994217642166,
    "arrivals": 381260,
    "finished_requests": 90735,
    "scheduler_time": 109.6340447108547
}
#Debug simulation 
Total elapsed time: 6.823599171824753. Arrivals time: 0.5141520448960364 Scheduler time: 6.15873239049688 Scheduler overhead time: 0.050878169015049934 Adapter cache time: 0.02406285423785448 Engine time: 0.05198361864313483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.49545239796862,
    "estimated_duration": 3600.0108915690616,
    "input_throughput": 6078.700498170478,
    "output_throughput": 5328.6066564201665,
    "total_throughput": 11407.307154590644,
    "itl": 99.02909701000362,
    "ttft": 1865054.6397594844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.013135325727058,
    "arrivals": 381260,
    "finished_requests": 88899,
    "scheduler_time": 110.41796278548459
}
#Debug simulation 
Total elapsed time: 6.49553326703608. Arrivals time: 0.24887235229834914 Scheduler time: 6.086907679680735 Scheduler overhead time: 0.05381212895736098 Adapter cache time: 0.02510782890021801 Engine time: 0.05551850004121661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.47680776193738,
    "estimated_duration": 3600.0179462712895,
    "input_throughput": 5832.317314347564,
    "output_throughput": 5112.633402026101,
    "total_throughput": 10944.950716373665,
    "itl": 86.64253676620794,
    "ttft": 1895152.4742793674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.882961012707125,
    "arrivals": 381260,
    "finished_requests": 85269,
    "scheduler_time": 111.91203728161355
}
#Debug simulation 
Total elapsed time: 6.476931661833078. Arrivals time: 0.2548979357816279 Scheduler time: 6.044394131284207 Scheduler overhead time: 0.06007014540955424 Adapter cache time: 0.027214495465159416 Engine time: 0.06194914085790515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.49493614397943,
    "estimated_duration": 3600.0340745202952,
    "input_throughput": 6080.147450525638,
    "output_throughput": 5329.61483220312,
    "total_throughput": 11409.76228272876,
    "itl": 99.02016694933293,
    "ttft": 1865093.063354273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7799142022104886,
    "arrivals": 381260,
    "finished_requests": 88919,
    "scheduler_time": 110.42641112922804
}
#Debug simulation 
Total elapsed time: 6.495015428867191. Arrivals time: 0.2484813118353486 Scheduler time: 6.086245201528072 Scheduler overhead time: 0.05409469688311219 Adapter cache time: 0.025260468013584614 Engine time: 0.05551181547343731 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.493563320953399,
    "estimated_duration": 3600.0858270202602,
    "input_throughput": 5832.378728975739,
    "output_throughput": 5112.743941228381,
    "total_throughput": 10945.12267020412,
    "itl": 86.641597991151,
    "ttft": 1895268.6603334758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.844283466176162,
    "arrivals": 381260,
    "finished_requests": 85273,
    "scheduler_time": 111.91595342904395
}
#Debug simulation 
Total elapsed time: 6.493662079796195. Arrivals time: 0.2515239054337144 Scheduler time: 6.065026935655624 Scheduler overhead time: 0.05993108497932553 Adapter cache time: 0.02713844971731305 Engine time: 0.06158760329708457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.4949607430025935,
    "estimated_duration": 3600.0616631000853,
    "input_throughput": 6080.86417640658,
    "output_throughput": 5330.37036467742,
    "total_throughput": 11411.234541083999,
    "itl": 99.01453042773399,
    "ttft": 1865133.557122176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 550,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5111565173137773,
    "arrivals": 381260,
    "finished_requests": 88928,
    "scheduler_time": 110.43498510914958
}
#Debug simulation 
Total elapsed time: 6.4951197896152735. Arrivals time: 0.2796268812380731 Scheduler time: 6.055565072223544 Scheduler overhead time: 0.05380987608805299 Adapter cache time: 0.025288766250014305 Engine time: 0.05532079562544823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.507721915841103,
    "estimated_duration": 3600.0630241966546,
    "input_throughput": 5832.281506984266,
    "output_throughput": 5112.569940107413,
    "total_throughput": 10944.851447091678,
    "itl": 86.64268970736822,
    "ttft": 1895203.9703086687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8065362710506045,
    "arrivals": 381260,
    "finished_requests": 85270,
    "scheduler_time": 111.91703904336654
}
#Debug simulation 
Total elapsed time: 6.507781792897731. Arrivals time: 0.2541829007677734 Scheduler time: 6.076083904597908 Scheduler overhead time: 0.059966479893773794 Adapter cache time: 0.02719892468303442 Engine time: 0.06200288748368621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.64590888377279,
    "estimated_duration": 3600.0348423817136,
    "input_throughput": 6316.324978942794,
    "output_throughput": 5486.090792091807,
    "total_throughput": 11802.4157710346,
    "itl": 105.64895725346368,
    "ttft": 1844631.131027954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5259402502048904,
    "arrivals": 380865,
    "finished_requests": 91995,
    "scheduler_time": 110.65497120515865
}
#Debug simulation 
Total elapsed time: 6.646015563979745. Arrivals time: 0.2543018111027777 Scheduler time: 6.241200001444668 Scheduler overhead time: 0.050822645891457796 Adapter cache time: 0.023143505677580833 Engine time: 0.05246255174279213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.541219607926905,
    "estimated_duration": 3600.0364404597835,
    "input_throughput": 6192.550927943598,
    "output_throughput": 5376.978905671231,
    "total_throughput": 11569.529833614828,
    "itl": 97.95014358154049,
    "ttft": 1860009.3945824823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.620826510656627,
    "arrivals": 380865,
    "finished_requests": 90166,
    "scheduler_time": 111.46899894428914
}
#Debug simulation 
Total elapsed time: 6.541363750118762. Arrivals time: 0.2524617053568363 Scheduler time: 6.128150502219796 Scheduler overhead time: 0.054337067529559135 Adapter cache time: 0.02456682175397873 Engine time: 0.05620431527495384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.327710755169392,
    "estimated_duration": 3600.054182967142,
    "input_throughput": 5930.299355218026,
    "output_throughput": 5155.376851773733,
    "total_throughput": 11085.676206991759,
    "itl": 85.76907944599952,
    "ttft": 1890915.3757559345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6187611618358777,
    "arrivals": 380865,
    "finished_requests": 86340,
    "scheduler_time": 112.8548922924902
}
#Debug simulation 
Total elapsed time: 6.327790093142539. Arrivals time: 0.24412035197019577 Scheduler time: 5.905371609609574 Scheduler overhead time: 0.06055546971037984 Adapter cache time: 0.026486030779778957 Engine time: 0.06245263898745179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.560747442767024,
    "estimated_duration": 3600.022310239169,
    "input_throughput": 6193.279118461563,
    "output_throughput": 5377.371397099452,
    "total_throughput": 11570.650515561016,
    "itl": 97.9475329573831,
    "ttft": 1859904.6960478716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4728440504381397,
    "arrivals": 380865,
    "finished_requests": 90173,
    "scheduler_time": 111.47316819488235
}
#Debug simulation 
Total elapsed time: 6.560862134676427. Arrivals time: 0.2617708323523402 Scheduler time: 6.137821992393583 Scheduler overhead time: 0.05478325113654137 Adapter cache time: 0.02464395435526967 Engine time: 0.056110619101673365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.597101320978254,
    "estimated_duration": 3600.022718254254,
    "input_throughput": 5930.394797717544,
    "output_throughput": 5155.329966639738,
    "total_throughput": 11085.724764357281,
    "itl": 85.76815834565441,
    "ttft": 1890873.4526280111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.606746869077919,
    "arrivals": 380865,
    "finished_requests": 86340,
    "scheduler_time": 112.85433050477934
}
#Debug simulation 
Total elapsed time: 6.597248591948301. Arrivals time: 0.25601505441591144 Scheduler time: 6.1628489177674055 Scheduler overhead time: 0.060599439311772585 Adapter cache time: 0.02629580907523632 Engine time: 0.06265745079144835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.558953283820301,
    "estimated_duration": 3600.099365833968,
    "input_throughput": 6193.106004682498,
    "output_throughput": 5377.451018081935,
    "total_throughput": 11570.557022764433,
    "itl": 97.94326505091851,
    "ttft": 1859858.0911471637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3173633014270942,
    "arrivals": 380865,
    "finished_requests": 90175,
    "scheduler_time": 111.47926450506189
}
#Debug simulation 
Total elapsed time: 6.559033844154328. Arrivals time: 0.2517749397084117 Scheduler time: 6.147078993264586 Scheduler overhead time: 0.054198932368308306 Adapter cache time: 0.024480527266860008 Engine time: 0.05575980106368661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.316749594639987,
    "estimated_duration": 3600.0222428277316,
    "input_throughput": 5930.395580897976,
    "output_throughput": 5155.330647463475,
    "total_throughput": 11085.726228361451,
    "itl": 85.76872326097696,
    "ttft": 1890900.684768818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.582462290432317,
    "arrivals": 380865,
    "finished_requests": 86340,
    "scheduler_time": 112.8544493805402
}
#Debug simulation 
Total elapsed time: 6.3168317917734385. Arrivals time: 0.24366877553984523 Scheduler time: 5.8950772900134325 Scheduler overhead time: 0.06051551643759012 Adapter cache time: 0.026468155439943075 Engine time: 0.06236672634258866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.726761106867343,
    "estimated_duration": 3600.0448783384495,
    "input_throughput": 6318.942337879755,
    "output_throughput": 5554.511867426803,
    "total_throughput": 11873.454205306558,
    "itl": 104.34081424145211,
    "ttft": 1834013.5308576976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1028,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.797556484844687,
    "arrivals": 377686,
    "finished_requests": 92282,
    "scheduler_time": 112.08008303529145
}
#Debug simulation 
Total elapsed time: 6.726904354058206. Arrivals time: 0.28820486832410097 Scheduler time: 6.284931416157633 Scheduler overhead time: 0.051763436291366816 Adapter cache time: 0.024247130379080772 Engine time: 0.05322524206712842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.611042580101639,
    "estimated_duration": 3600.069761043438,
    "input_throughput": 6192.561666787936,
    "output_throughput": 5448.087759920312,
    "total_throughput": 11640.649426708249,
    "itl": 96.75142286743093,
    "ttft": 1849251.2291899747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1009,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.396472258674924,
    "arrivals": 377686,
    "finished_requests": 90459,
    "scheduler_time": 112.9336284167926
}
#Debug simulation 
Total elapsed time: 6.611145092174411. Arrivals time: 0.2528556431643665 Scheduler time: 6.1952163078822196 Scheduler overhead time: 0.054656587075442076 Adapter cache time: 0.025569811463356018 Engine time: 0.05677409935742617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.441630403045565,
    "estimated_duration": 3600.004737667628,
    "input_throughput": 5935.736077348705,
    "output_throughput": 5223.159515112847,
    "total_throughput": 11158.895592461553,
    "itl": 84.739686989829,
    "ttft": 1880181.9276264138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.354226436768685,
    "arrivals": 377686,
    "finished_requests": 86717,
    "scheduler_time": 114.35217628613673
}
#Debug simulation 
Total elapsed time: 6.44171100994572. Arrivals time: 0.27863814448937774 Scheduler time: 5.981430057436228 Scheduler overhead time: 0.061734165996313095 Adapter cache time: 0.027181563898921013 Engine time: 0.06350925657898188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.622319917194545,
    "estimated_duration": 3600.0276198364236,
    "input_throughput": 6193.321650408083,
    "output_throughput": 5448.350143738959,
    "total_throughput": 11641.671794147042,
    "itl": 96.7397289355511,
    "ttft": 1849155.709133196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1009,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.923088906775218,
    "arrivals": 377686,
    "finished_requests": 90465,
    "scheduler_time": 112.9462100159689
}
#Debug simulation 
Total elapsed time: 6.622476059012115. Arrivals time: 0.28064823942258954 Scheduler time: 6.178995137568563 Scheduler overhead time: 0.05476576089859009 Adapter cache time: 0.025342550594359636 Engine time: 0.05662366934120655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.400014997925609,
    "estimated_duration": 3600.0094818156376,
    "input_throughput": 5935.57131111309,
    "output_throughput": 5222.8670771492425,
    "total_throughput": 11158.438388262333,
    "itl": 84.74147842083623,
    "ttft": 1880176.2831279938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.2856706513092195,
    "arrivals": 377686,
    "finished_requests": 86714,
    "scheduler_time": 114.3533055330124
}
#Debug simulation 
Total elapsed time: 6.400127260945737. Arrivals time: 0.2574094096198678 Scheduler time: 5.962069715373218 Scheduler overhead time: 0.061349169816821814 Adapter cache time: 0.027007187251001596 Engine time: 0.06326656229794025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.656852756161243,
    "estimated_duration": 3600.0519787218473,
    "input_throughput": 6193.769459938904,
    "output_throughput": 5448.946047430289,
    "total_throughput": 11642.715507369194,
    "itl": 96.72678068375602,
    "ttft": 1848930.1346453838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1009,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.441376229035636,
    "arrivals": 377686,
    "finished_requests": 90474,
    "scheduler_time": 112.96179075273861
}
#Debug simulation 
Total elapsed time: 6.656975745223463. Arrivals time: 0.26650745095685124 Scheduler time: 6.2271892707794905 Scheduler overhead time: 0.054966829251497984 Adapter cache time: 0.025317533407360315 Engine time: 0.0569649962708354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.646282435860485,
    "estimated_duration": 3600.029793535431,
    "input_throughput": 5935.581988341035,
    "output_throughput": 5222.903719786937,
    "total_throughput": 11158.485708127972,
    "itl": 84.73762205472495,
    "ttft": 1880152.937455358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.231817192360786,
    "arrivals": 377686,
    "finished_requests": 86715,
    "scheduler_time": 114.3574190147565
}
#Debug simulation 
Total elapsed time: 6.646376685705036. Arrivals time: 0.26025872817263007 Scheduler time: 6.205491697881371 Scheduler overhead time: 0.061543258372694254 Adapter cache time: 0.026813406497240067 Engine time: 0.06312981713563204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.8349043326452374,
    "estimated_duration": 3600.049380547878,
    "input_throughput": 6449.073205897718,
    "output_throughput": 5640.53846308899,
    "total_throughput": 12089.611668986709,
    "itl": 102.89846557793197,
    "ttft": 1822076.0234098143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.589011868173337,
    "arrivals": 376204,
    "finished_requests": 93940,
    "scheduler_time": 113.72590465264001
}
#Debug simulation 
Total elapsed time: 6.835016249679029. Arrivals time: 0.29412861727178097 Scheduler time: 6.387671142350882 Scheduler overhead time: 0.05255613895133138 Adapter cache time: 0.022154485806822777 Engine time: 0.05378991039469838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.704980233218521,
    "estimated_duration": 3600.1046009685665,
    "input_throughput": 6313.164621351635,
    "output_throughput": 5525.009466294024,
    "total_throughput": 11838.174087645659,
    "itl": 95.55485084781857,
    "ttft": 1838601.6022533246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 677,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.943837472810417,
    "arrivals": 376204,
    "finished_requests": 92015,
    "scheduler_time": 114.40158876327159
}
#Debug simulation 
Total elapsed time: 6.7050977111794055. Arrivals time: 0.2947227656841278 Scheduler time: 6.248535077087581 Scheduler overhead time: 0.05577010661363602 Adapter cache time: 0.022759574465453625 Engine time: 0.05705226073041558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.453864771407098,
    "estimated_duration": 3600.0779513882153,
    "input_throughput": 6030.722471336505,
    "output_throughput": 5282.9331077862225,
    "total_throughput": 11313.655579122727,
    "itl": 83.863019152093,
    "ttft": 1870783.7192372729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.99371546947866,
    "arrivals": 376204,
    "finished_requests": 87955,
    "scheduler_time": 115.58695488174236
}
#Debug simulation 
Total elapsed time: 6.454024234320968. Arrivals time: 0.295258903875947 Scheduler time: 5.980033487081528 Scheduler overhead time: 0.06174020003527403 Adapter cache time: 0.02420108951628208 Engine time: 0.06349194468930364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.72339963587001,
    "estimated_duration": 3600.0639513403908,
    "input_throughput": 6313.2906268339275,
    "output_throughput": 5525.1318501145415,
    "total_throughput": 11838.422476948468,
    "itl": 95.54575628946827,
    "ttft": 1838466.6876514326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 677,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.641205300628202,
    "arrivals": 376204,
    "finished_requests": 92016,
    "scheduler_time": 114.40868482640253
}
#Debug simulation 
Total elapsed time: 6.723494749050587. Arrivals time: 0.29535761987790465 Scheduler time: 6.266575314104557 Scheduler overhead time: 0.05566594796255231 Adapter cache time: 0.022793478332459927 Engine time: 0.05692872265353799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.471038172021508,
    "estimated_duration": 3600.052697939652,
    "input_throughput": 6030.764775311615,
    "output_throughput": 5282.970166210277,
    "total_throughput": 11313.734941521892,
    "itl": 83.86580925379884,
    "ttft": 1870728.6010638622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.955138309132339,
    "arrivals": 376204,
    "finished_requests": 87955,
    "scheduler_time": 115.5859047371978
}
#Debug simulation 
Total elapsed time: 6.471167317125946. Arrivals time: 0.2917111157439649 Scheduler time: 5.999708354007453 Scheduler overhead time: 0.06210849154740572 Adapter cache time: 0.02420602086931467 Engine time: 0.06412795092910528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.706983530893922,
    "estimated_duration": 3600.041730906609,
    "input_throughput": 6313.712923065459,
    "output_throughput": 5525.627336267132,
    "total_throughput": 11839.340259332592,
    "itl": 95.53607484199885,
    "ttft": 1838299.437343932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.328298397706801,
    "arrivals": 376204,
    "finished_requests": 92022,
    "scheduler_time": 114.41849729842723
}
#Debug simulation 
Total elapsed time: 6.7071486711502075. Arrivals time: 0.29568321676924825 Scheduler time: 6.250329445581883 Scheduler overhead time: 0.05541084846481681 Adapter cache time: 0.022356092929840088 Engine time: 0.05708179576322436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.481583097949624,
    "estimated_duration": 3600.0259285144707,
    "input_throughput": 6030.673787107623,
    "output_throughput": 5282.981950035017,
    "total_throughput": 11313.655737142639,
    "itl": 83.86283287900595,
    "ttft": 1870703.222915776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.895336307100984,
    "arrivals": 376204,
    "finished_requests": 87954,
    "scheduler_time": 115.58754154151875
}
#Debug simulation 
Total elapsed time: 6.481698971241713. Arrivals time: 0.28212537290528417 Scheduler time: 6.020804857369512 Scheduler overhead time: 0.06166543811559677 Adapter cache time: 0.0239242073148489 Engine time: 0.06394051993265748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.889982994180173,
    "estimated_duration": 3600.0134699305427,
    "input_throughput": 6505.475103250615,
    "output_throughput": 5674.729044942755,
    "total_throughput": 12180.204148193368,
    "itl": 102.06562154845977,
    "ttft": 1814101.1290601778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0615453817928526,
    "arrivals": 375496,
    "finished_requests": 94666,
    "scheduler_time": 114.49391423504873
}
#Debug simulation 
Total elapsed time: 6.8900976912118495. Arrivals time: 0.2816365067847073 Scheduler time: 6.455490398686379 Scheduler overhead time: 0.052894027438014746 Adapter cache time: 0.020782162435352802 Engine time: 0.05447003897279501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.992703593336046,
    "estimated_duration": 3600.038670377167,
    "input_throughput": 6364.139137871999,
    "output_throughput": 5552.298414035052,
    "total_throughput": 11916.43755190705,
    "itl": 94.81537881815454,
    "ttft": 1829751.327913726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.369171392410067,
    "arrivals": 375496,
    "finished_requests": 92569,
    "scheduler_time": 115.09673651001623
}
#Debug simulation 
Total elapsed time: 6.992801061365753. Arrivals time: 0.5558320418931544 Scheduler time: 6.27565283048898 Scheduler overhead time: 0.05605220561847091 Adapter cache time: 0.0213615745306015 Engine time: 0.057591607328504324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.452207379043102,
    "estimated_duration": 3600.09098822208,
    "input_throughput": 6083.401522808678,
    "output_throughput": 5307.55390975171,
    "total_throughput": 11390.95543256039,
    "itl": 83.27519895198809,
    "ttft": 1863239.814591713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3196707389503968,
    "arrivals": 375496,
    "finished_requests": 88443,
    "scheduler_time": 116.19469978038842
}
#Debug simulation 
Total elapsed time: 6.452321791090071. Arrivals time: 0.2900538840331137 Scheduler time: 5.983487064950168 Scheduler overhead time: 0.06248672166839242 Adapter cache time: 0.02251966204494238 Engine time: 0.06432573264464736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.779760172124952,
    "estimated_duration": 3600.0544038315397,
    "input_throughput": 6364.257711109892,
    "output_throughput": 5552.398591178446,
    "total_throughput": 11916.656302288337,
    "itl": 94.80820792109348,
    "ttft": 1829766.6063877153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.16093824641313,
    "arrivals": 375496,
    "finished_requests": 92571,
    "scheduler_time": 115.10429901373533
}
#Debug simulation 
Total elapsed time: 6.779846386052668. Arrivals time: 0.2966200076043606 Scheduler time: 6.321518449578434 Scheduler overhead time: 0.05615365272387862 Adapter cache time: 0.021280602551996708 Engine time: 0.057800740003585815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.498095847200602,
    "estimated_duration": 3600.0004213401544,
    "input_throughput": 6083.518454658165,
    "output_throughput": 5307.612712136011,
    "total_throughput": 11391.131166794177,
    "itl": 83.2717118219672,
    "ttft": 1863225.2912225397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2979219388915477,
    "arrivals": 375496,
    "finished_requests": 88442,
    "scheduler_time": 116.1931422320634
}
#Debug simulation 
Total elapsed time: 6.4982369020581245. Arrivals time: 0.2899898625910282 Scheduler time: 6.029604445211589 Scheduler overhead time: 0.062410262413322926 Adapter cache time: 0.022530293557792902 Engine time: 0.06426109606400132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.744959997013211,
    "estimated_duration": 3600.040449118901,
    "input_throughput": 6364.313213649471,
    "output_throughput": 5552.556501106078,
    "total_throughput": 11916.86971475555,
    "itl": 94.80357427553453,
    "ttft": 1829679.3944878334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.942987553603003,
    "arrivals": 375496,
    "finished_requests": 92572,
    "scheduler_time": 115.11015105289054
}
#Debug simulation 
Total elapsed time: 6.745051896199584. Arrivals time: 0.29687662702053785 Scheduler time: 6.28693258529529 Scheduler overhead time: 0.05600587744265795 Adapter cache time: 0.021408727392554283 Engine time: 0.05751272430643439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.514676693826914,
    "estimated_duration": 3600.043325970011,
    "input_throughput": 6083.503729527736,
    "output_throughput": 5307.602511936871,
    "total_throughput": 11391.106241464608,
    "itl": 83.27152054803828,
    "ttft": 1863210.9656289043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2528252563998348,
    "arrivals": 375496,
    "finished_requests": 88443,
    "scheduler_time": 116.19531813822681
}
#Debug simulation 
Total elapsed time: 6.514771056827158. Arrivals time: 0.2909474945627153 Scheduler time: 6.0438957256264985 Scheduler overhead time: 0.06270014680922031 Adapter cache time: 0.022565899416804314 Engine time: 0.06530560180544853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.896679664961994,
    "estimated_duration": 3600.0792117926208,
    "input_throughput": 6556.097966590964,
    "output_throughput": 5702.756742893199,
    "total_throughput": 12258.854709484162,
    "itl": 101.42169747316949,
    "ttft": 1809622.3730494094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.970497891521079,
    "arrivals": 375142,
    "finished_requests": 95208,
    "scheduler_time": 115.02829100317548
}
#Debug simulation 
Total elapsed time: 6.896822832059115. Arrivals time: 0.2686207601800561 Scheduler time: 6.475604582577944 Scheduler overhead time: 0.05304853850975633 Adapter cache time: 0.019846242852509022 Engine time: 0.05472162738442421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.799169596750289,
    "estimated_duration": 3600.0974311616046,
    "input_throughput": 6408.7301638821455,
    "output_throughput": 5580.85895845248,
    "total_throughput": 11989.589122334624,
    "itl": 94.22629893937119,
    "ttft": 1826356.496689607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1428539994778135,
    "arrivals": 375142,
    "finished_requests": 93131,
    "scheduler_time": 115.62662581585057
}
#Debug simulation 
Total elapsed time: 6.7992572439834476. Arrivals time: 0.2687777918763459 Scheduler time: 6.3688226910308 Scheduler overhead time: 0.05639945529401302 Adapter cache time: 0.020481891464442015 Engine time: 0.05821708124130964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.5431144600734115,
    "estimated_duration": 3600.0565247532945,
    "input_throughput": 6106.340511280297,
    "output_throughput": 5324.664451293199,
    "total_throughput": 11431.004962573496,
    "itl": 82.86165517818016,
    "ttft": 1859118.2801792475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1401171932043574,
    "arrivals": 375142,
    "finished_requests": 88761,
    "scheduler_time": 116.5976306506112
}
#Debug simulation 
Total elapsed time: 6.543204309884459. Arrivals time: 0.2756162192672491 Scheduler time: 6.088880006223917 Scheduler overhead time: 0.06254090229049325 Adapter cache time: 0.021811881102621555 Engine time: 0.06479071360081434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.763248013798147,
    "estimated_duration": 3600.0752159979056,
    "input_throughput": 6409.102481378107,
    "output_throughput": 5580.988672324364,
    "total_throughput": 11990.091153702471,
    "itl": 94.21983425690398,
    "ttft": 1826347.5038473287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0145804860070324,
    "arrivals": 375142,
    "finished_requests": 93136,
    "scheduler_time": 115.63085756657368
}
#Debug simulation 
Total elapsed time: 6.763371197972447. Arrivals time: 0.29407861921936274 Scheduler time: 6.307557662948966 Scheduler overhead time: 0.05646173097193241 Adapter cache time: 0.020429623313248158 Engine time: 0.05818238575011492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.519065398257226,
    "estimated_duration": 3600.0073313795283,
    "input_throughput": 6106.742008099071,
    "output_throughput": 5325.007488985863,
    "total_throughput": 11431.749497084935,
    "itl": 82.85991374175264,
    "ttft": 1859108.6053426987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.119819709896116,
    "arrivals": 375142,
    "finished_requests": 88765,
    "scheduler_time": 116.59781814329885
}
#Debug simulation 
Total elapsed time: 6.51917979400605. Arrivals time: 0.26069268165156245 Scheduler time: 6.079613528680056 Scheduler overhead time: 0.06292411545291543 Adapter cache time: 0.02190204383805394 Engine time: 0.06446977704763412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.797860674094409,
    "estimated_duration": 3600.0682062874175,
    "input_throughput": 6408.807744171389,
    "output_throughput": 5580.904818667191,
    "total_throughput": 11989.71256283858,
    "itl": 94.21968011920443,
    "ttft": 1826311.7641581702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9087923612305824,
    "arrivals": 375142,
    "finished_requests": 93132,
    "scheduler_time": 115.63233309609205
}
#Debug simulation 
Total elapsed time: 6.797974080312997. Arrivals time: 0.2667806865647435 Scheduler time: 6.369189413730055 Scheduler overhead time: 0.05661648605018854 Adapter cache time: 0.020443121436983347 Engine time: 0.05837187310680747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.525503545999527,
    "estimated_duration": 3600.0432950884565,
    "input_throughput": 6106.668225349725,
    "output_throughput": 5324.915404810146,
    "total_throughput": 11431.58363015987,
    "itl": 82.86093812483101,
    "ttft": 1859142.5639336745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0958474829047975,
    "arrivals": 375142,
    "finished_requests": 88766,
    "scheduler_time": 116.59905346964361
}
#Debug simulation 
Total elapsed time: 6.525656419340521. Arrivals time: 0.2587491115555167 Scheduler time: 6.088113142177463 Scheduler overhead time: 0.06273665791377425 Adapter cache time: 0.02180540282279253 Engine time: 0.06460746005177498 
