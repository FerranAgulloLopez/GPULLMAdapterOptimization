INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 234360 . Total input tokens: 52341149 . Total output tokens: 45938204
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.377117045689374,
    "estimated_duration": 3599.9616948493685,
    "input_throughput": 5380.03502306999,
    "output_throughput": 4699.9616757610975,
    "total_throughput": 10079.996698831086,
    "itl": 42.126088441454094,
    "ttft": 8752.850217669798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 78358,
    "finished_requests": 78169,
    "scheduler_time": 53.25948341497843
}
#Debug simulation 
Total elapsed time: 5.37728232704103. Arrivals time: 0.20287600625306368 Scheduler time: 4.922151383012533 Scheduler overhead time: 0.09475600719451904 Adapter cache time: 0.018158474005758762 Engine time: 0.0959634380415082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 234360 . Total input tokens: 52341149 . Total output tokens: 45938204
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.352350743953139,
    "estimated_duration": 3599.972410925325,
    "input_throughput": 5380.019008262825,
    "output_throughput": 4699.947685335461,
    "total_throughput": 10079.966693598286,
    "itl": 42.12622692774519,
    "ttft": 8752.902669311323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 78358,
    "finished_requests": 78169,
    "scheduler_time": 53.259621649513306
}
#Debug simulation 
Total elapsed time: 5.352498773019761. Arrivals time: 0.202622233889997 Scheduler time: 4.901204120367765 Scheduler overhead time: 0.09456071211025119 Adapter cache time: 0.01781446347013116 Engine time: 0.0931468359194696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 234360 . Total input tokens: 52341149 . Total output tokens: 45938204
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.339495518710464,
    "estimated_duration": 3599.9723582433967,
    "input_throughput": 5380.019086993923,
    "output_throughput": 4699.947754114408,
    "total_throughput": 10079.966841108331,
    "itl": 42.126164075203775,
    "ttft": 8752.902522982988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 78358,
    "finished_requests": 78169,
    "scheduler_time": 53.259613518922286
}
#Debug simulation 
Total elapsed time: 5.339657500851899. Arrivals time: 0.20165187632665038 Scheduler time: 4.88929033651948 Scheduler overhead time: 0.09398722602054477 Adapter cache time: 0.017889052163809538 Engine time: 0.09362733596935868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 234360 . Total input tokens: 52341149 . Total output tokens: 45938204
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.362355073913932,
    "estimated_duration": 3599.9616177471594,
    "input_throughput": 5380.035138296936,
    "output_throughput": 4699.961776422568,
    "total_throughput": 10079.996914719504,
    "itl": 42.12602785164635,
    "ttft": 8752.854599214372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 78358,
    "finished_requests": 78169,
    "scheduler_time": 53.259439674126384
}
#Debug simulation 
Total elapsed time: 5.362481568008661. Arrivals time: 0.2015693373978138 Scheduler time: 4.910281665623188 Scheduler overhead time: 0.09559940872713923 Adapter cache time: 0.017920394893735647 Engine time: 0.09349006973206997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 234360 . Total input tokens: 52341149 . Total output tokens: 45938204
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.362516560126096,
    "estimated_duration": 3599.9720690609342,
    "input_throughput": 5380.019519165934,
    "output_throughput": 4699.948131656911,
    "total_throughput": 10079.967650822844,
    "itl": 42.12618593909389,
    "ttft": 8752.899092797728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 78358,
    "finished_requests": 78169,
    "scheduler_time": 53.25962565334651
}
#Debug simulation 
Total elapsed time: 5.3626704281196. Arrivals time: 0.20267592510208488 Scheduler time: 4.911160318180919 Scheduler overhead time: 0.09423252940177917 Adapter cache time: 0.01765751000493765 Engine time: 0.09359659440815449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 234360 . Total input tokens: 52341149 . Total output tokens: 45938204
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.3750360598787665,
    "estimated_duration": 3599.961552580031,
    "input_throughput": 5380.035235687265,
    "output_throughput": 4699.961861502091,
    "total_throughput": 10079.997097189356,
    "itl": 42.12614470997154,
    "ttft": 8752.888125837084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 78358,
    "finished_requests": 78169,
    "scheduler_time": 53.25951969532603
}
#Debug simulation 
Total elapsed time: 5.375177284702659. Arrivals time: 0.20831453055143356 Scheduler time: 4.916909441817552 Scheduler overhead time: 0.09456260409206152 Adapter cache time: 0.017929787281900644 Engine time: 0.09411884285509586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 234360 . Total input tokens: 52341149 . Total output tokens: 45938204
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.370937487110496,
    "estimated_duration": 3599.9720494338508,
    "input_throughput": 5380.019548497854,
    "output_throughput": 4699.948157281075,
    "total_throughput": 10079.967705778929,
    "itl": 42.12620759903753,
    "ttft": 8752.88934592133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 78358,
    "finished_requests": 78169,
    "scheduler_time": 53.25957311181678
}
#Debug simulation 
Total elapsed time: 5.371098042000085. Arrivals time: 0.20616520754992962 Scheduler time: 4.915972015354782 Scheduler overhead time: 0.09380282321944833 Adapter cache time: 0.01784263551235199 Engine time: 0.09407138032838702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 34560, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 231660 . Total input tokens: 51721888 . Total output tokens: 45407552
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.310211782809347,
    "estimated_duration": 3600.025409071363,
    "input_throughput": 5356.543026448884,
    "output_throughput": 4641.672238727457,
    "total_throughput": 9998.21526517634,
    "itl": 40.732760745869705,
    "ttft": 9828.452107213927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 77453,
    "finished_requests": 77241,
    "scheduler_time": 51.892866465671595
}
#Debug simulation 
Total elapsed time: 5.310359119903296. Arrivals time: 0.2017337870784104 Scheduler time: 4.854339318815619 Scheduler overhead time: 0.09634294919669628 Adapter cache time: 0.017488052137196064 Engine time: 0.09577161679044366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 34560, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 231660 . Total input tokens: 51721888 . Total output tokens: 45407552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.313685648608953,
    "estimated_duration": 3600.042021970824,
    "input_throughput": 5356.518307928874,
    "output_throughput": 4641.650819079085,
    "total_throughput": 9998.16912700796,
    "itl": 40.732772683253394,
    "ttft": 9874.767556403749,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 77453,
    "finished_requests": 77241,
    "scheduler_time": 51.89316173665885
}
#Debug simulation 
Total elapsed time: 5.313813503831625. Arrivals time: 0.2019781586714089 Scheduler time: 4.858309385366738 Scheduler overhead time: 0.09624268533661962 Adapter cache time: 0.017560925334692 Engine time: 0.09542951406911016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 34560, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 231660 . Total input tokens: 51721888 . Total output tokens: 45407552
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.312274879775941,
    "estimated_duration": 3600.012013394328,
    "input_throughput": 5356.562958193595,
    "output_throughput": 4641.689510431545,
    "total_throughput": 9998.25246862514,
    "itl": 40.73297380775113,
    "ttft": 9828.451564714253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 77453,
    "finished_requests": 77241,
    "scheduler_time": 51.892800668907526
}
#Debug simulation 
Total elapsed time: 5.312412952072918. Arrivals time: 0.2013180204667151 Scheduler time: 4.857007113751024 Scheduler overhead time: 0.09632891044020653 Adapter cache time: 0.017641061451286077 Engine time: 0.09565520100295544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 34560, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 231660 . Total input tokens: 51721888 . Total output tokens: 45407552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.328806990291923,
    "estimated_duration": 3600.026596780894,
    "input_throughput": 5356.5412592349385,
    "output_throughput": 4641.670707361448,
    "total_throughput": 9998.211966596387,
    "itl": 40.73267184418013,
    "ttft": 9828.42658680794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 77453,
    "finished_requests": 77241,
    "scheduler_time": 51.89284203292361
}
#Debug simulation 
Total elapsed time: 5.328961288090795. Arrivals time: 0.19914245419204235 Scheduler time: 4.877378740347922 Scheduler overhead time: 0.09597355872392654 Adapter cache time: 0.017335849814116955 Engine time: 0.09465382946655154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 34560, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 231660 . Total input tokens: 51721888 . Total output tokens: 45407552
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.362568908836693,
    "estimated_duration": 3600.0093963843137,
    "input_throughput": 5356.566852122015,
    "output_throughput": 4641.692884686053,
    "total_throughput": 9998.259736808068,
    "itl": 40.73290266224658,
    "ttft": 9782.14238150929,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 77453,
    "finished_requests": 77241,
    "scheduler_time": 51.89271172410536
}
#Debug simulation 
Total elapsed time: 5.362692866008729. Arrivals time: 0.19782652286812663 Scheduler time: 4.912313662469387 Scheduler overhead time: 0.09620275953784585 Adapter cache time: 0.017333969473838806 Engine time: 0.09462096961215138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 34560, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 231660 . Total input tokens: 51721888 . Total output tokens: 45407552
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.3248937600292265,
    "estimated_duration": 3600.0109380377457,
    "input_throughput": 5356.56455824852,
    "output_throughput": 4641.690896947157,
    "total_throughput": 9998.255455195678,
    "itl": 40.73275571729564,
    "ttft": 9828.439917907801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 77453,
    "finished_requests": 77241,
    "scheduler_time": 51.89264391927962
}
#Debug simulation 
Total elapsed time: 5.325039712712169. Arrivals time: 0.195681260433048 Scheduler time: 4.87727320054546 Scheduler overhead time: 0.09592050034552813 Adapter cache time: 0.017287029419094324 Engine time: 0.09458413394168019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 34560, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 231660 . Total input tokens: 51721888 . Total output tokens: 45407552
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.447058963123709,
    "estimated_duration": 3600.0422678280916,
    "input_throughput": 5356.517942116792,
    "output_throughput": 4641.650502087366,
    "total_throughput": 9998.168444204159,
    "itl": 40.732769750882895,
    "ttft": 9874.782363980396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 77453,
    "finished_requests": 77241,
    "scheduler_time": 51.893161695683844
}
#Debug simulation 
Total elapsed time: 5.447183784097433. Arrivals time: 0.20626218290999532 Scheduler time: 4.9821641175076365 Scheduler overhead time: 0.09893483249470592 Adapter cache time: 0.01759542152285576 Engine time: 0.09703306574374437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 34560, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 230310 . Total input tokens: 51420682 . Total output tokens: 45139390
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.779814191162586,
    "estimated_duration": 3599.926626296537,
    "input_throughput": 5270.48547639957,
    "output_throughput": 4624.597312175505,
    "total_throughput": 9895.082788575075,
    "itl": 40.06282464582959,
    "ttft": 9647.661820204727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 77024,
    "finished_requests": 76819,
    "scheduler_time": 51.39511623259906
}
#Debug simulation 
Total elapsed time: 5.779952734243125. Arrivals time: 0.21738309040665627 Scheduler time: 5.2867743810638785 Scheduler overhead time: 0.10502783488482237 Adapter cache time: 0.018254566472023726 Engine time: 0.10397834470495582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 34560, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 230310 . Total input tokens: 51420682 . Total output tokens: 45139390
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.709513539914042,
    "estimated_duration": 3599.9595945230794,
    "input_throughput": 5270.437487372294,
    "output_throughput": 4624.555515936435,
    "total_throughput": 9894.99300330873,
    "itl": 40.06278729691876,
    "ttft": 9600.903062508403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 77024,
    "finished_requests": 76820,
    "scheduler_time": 51.39569122445286
}
#Debug simulation 
Total elapsed time: 5.709652684163302. Arrivals time: 0.2165293786674738 Scheduler time: 5.219735688064247 Scheduler overhead time: 0.10492925718426704 Adapter cache time: 0.018122870940715075 Engine time: 0.10238431766629219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 34560, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 230310 . Total input tokens: 51420682 . Total output tokens: 45139390
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.805154026020318,
    "estimated_duration": 3599.960419169749,
    "input_throughput": 5270.436280067708,
    "output_throughput": 4624.554456584703,
    "total_throughput": 9894.990736652411,
    "itl": 40.062740287219405,
    "ttft": 9600.961746664774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 77024,
    "finished_requests": 76820,
    "scheduler_time": 51.395659029888066
}
#Debug simulation 
Total elapsed time: 5.805317027959973. Arrivals time: 0.22267762385308743 Scheduler time: 5.304494488984346 Scheduler overhead time: 0.10586642008274794 Adapter cache time: 0.018460405990481377 Engine time: 0.10492149880155921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 34560, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 230310 . Total input tokens: 51420682 . Total output tokens: 45139390
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.357942676637322,
    "estimated_duration": 3599.9355005454977,
    "input_throughput": 5270.472484055608,
    "output_throughput": 4624.585912019063,
    "total_throughput": 9895.058396074672,
    "itl": 40.06291572290476,
    "ttft": 9647.703203463256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 77024,
    "finished_requests": 76819,
    "scheduler_time": 51.395290870406384
}
#Debug simulation 
Total elapsed time: 5.358069675974548. Arrivals time: 0.19821621431037784 Scheduler time: 4.905306828673929 Scheduler overhead time: 0.09708812972530723 Adapter cache time: 0.01680893963202834 Engine time: 0.09572185901924968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 34560, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 230310 . Total input tokens: 51420682 . Total output tokens: 45139390
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.38399538397789,
    "estimated_duration": 3599.960166285378,
    "input_throughput": 5270.436650297072,
    "output_throughput": 4624.554781443172,
    "total_throughput": 9894.991431740244,
    "itl": 40.06276434987376,
    "ttft": 9600.894260101533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 77024,
    "finished_requests": 76820,
    "scheduler_time": 51.3956469364389
}
#Debug simulation 
Total elapsed time: 5.38413185114041. Arrivals time: 0.19541715132072568 Scheduler time: 4.9338288265280426 Scheduler overhead time: 0.09712947672232985 Adapter cache time: 0.017078911885619164 Engine time: 0.0958134806714952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 34560, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 230310 . Total input tokens: 51420682 . Total output tokens: 45139390
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.403822235763073,
    "estimated_duration": 3599.966880428609,
    "input_throughput": 5270.426820632597,
    "output_throughput": 4624.54615638516,
    "total_throughput": 9894.972977017756,
    "itl": 40.06273194304189,
    "ttft": 9600.973506669725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 77024,
    "finished_requests": 76820,
    "scheduler_time": 51.39565094027204
}
#Debug simulation 
Total elapsed time: 5.403983925934881. Arrivals time: 0.20504790311679244 Scheduler time: 4.941865420434624 Scheduler overhead time: 0.09727409854531288 Adapter cache time: 0.017058135010302067 Engine time: 0.0975528359413147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 34560, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 230310 . Total input tokens: 51420682 . Total output tokens: 45139390
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.361141442786902,
    "estimated_duration": 3599.9598539173126,
    "input_throughput": 5270.437107612201,
    "output_throughput": 4624.55518271521,
    "total_throughput": 9894.992290327411,
    "itl": 40.062796616361055,
    "ttft": 9600.91040565646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 77024,
    "finished_requests": 76820,
    "scheduler_time": 51.39567508619539
}
#Debug simulation 
Total elapsed time: 5.36125876987353. Arrivals time: 0.20119018526747823 Scheduler time: 4.903902395628393 Scheduler overhead time: 0.09729881677776575 Adapter cache time: 0.016996322199702263 Engine time: 0.09693401260301471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 34560, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229635 . Total input tokens: 51263080 . Total output tokens: 45006036
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.324802918359637,
    "estimated_duration": 3600.0208626340827,
    "input_throughput": 5260.523125453042,
    "output_throughput": 4626.8601309750375,
    "total_throughput": 9887.38325642808,
    "itl": 39.76600402297615,
    "ttft": 12070.449482304044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 76770,
    "finished_requests": 76514,
    "scheduler_time": 51.324702972068536
}
#Debug simulation 
Total elapsed time: 5.324934387113899. Arrivals time: 0.20004917215555906 Scheduler time: 4.868681295774877 Scheduler overhead time: 0.09748497093096375 Adapter cache time: 0.016617839690297842 Engine time: 0.09707290818914771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 34560, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229635 . Total input tokens: 51263080 . Total output tokens: 45006036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.345767777878791,
    "estimated_duration": 3600.04752604368,
    "input_throughput": 5260.543329774425,
    "output_throughput": 4626.826418123765,
    "total_throughput": 9887.369747898189,
    "itl": 39.76594786227746,
    "ttft": 12023.519003478845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 76770,
    "finished_requests": 76515,
    "scheduler_time": 51.32516182627387
}
#Debug simulation 
Total elapsed time: 5.345899180974811. Arrivals time: 0.20329460129141808 Scheduler time: 4.886438205372542 Scheduler overhead time: 0.09766438696533442 Adapter cache time: 0.016708890441805124 Engine time: 0.09649806935340166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 34560, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229635 . Total input tokens: 51263080 . Total output tokens: 45006036
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.318745357915759,
    "estimated_duration": 3600.005935903469,
    "input_throughput": 5260.544937198072,
    "output_throughput": 4626.879315358617,
    "total_throughput": 9887.42425255669,
    "itl": 39.76617286508851,
    "ttft": 11976.951668108832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 76770,
    "finished_requests": 76514,
    "scheduler_time": 51.324413741661324
}
#Debug simulation 
Total elapsed time: 5.3189006401225924. Arrivals time: 0.20228567952290177 Scheduler time: 4.860775623936206 Scheduler overhead time: 0.09773111762478948 Adapter cache time: 0.016635640058666468 Engine time: 0.09634003322571516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 34560, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229635 . Total input tokens: 51263080 . Total output tokens: 45006036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.395929737016559,
    "estimated_duration": 3600.033128203166,
    "input_throughput": 5260.564368598563,
    "output_throughput": 4626.8449224837195,
    "total_throughput": 9887.409291082284,
    "itl": 39.76589749280609,
    "ttft": 12023.431281267109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 76770,
    "finished_requests": 76515,
    "scheduler_time": 51.324834155847945
}
#Debug simulation 
Total elapsed time: 5.3960530841723084. Arrivals time: 0.20625317050144076 Scheduler time: 4.9332232074812055 Scheduler overhead time: 0.0980051108635962 Adapter cache time: 0.016676578670740128 Engine time: 0.09682182129472494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 34560, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229635 . Total input tokens: 51263080 . Total output tokens: 45006036
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.372761313803494,
    "estimated_duration": 3600.005552352827,
    "input_throughput": 5260.54549766537,
    "output_throughput": 4626.8798083141155,
    "total_throughput": 9887.425305979486,
    "itl": 39.766090575507434,
    "ttft": 11976.907081594753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 76770,
    "finished_requests": 76514,
    "scheduler_time": 51.324425876085414
}
#Debug simulation 
Total elapsed time: 5.372878698632121. Arrivals time: 0.20622269855812192 Scheduler time: 4.911890432238579 Scheduler overhead time: 0.09742287825793028 Adapter cache time: 0.016598881222307682 Engine time: 0.09566946281120181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 34560, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229635 . Total input tokens: 51263080 . Total output tokens: 45006036
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.384337070863694,
    "estimated_duration": 3600.020232058756,
    "input_throughput": 5260.52404688011,
    "output_throughput": 4626.8609414104385,
    "total_throughput": 9887.384988290549,
    "itl": 39.766074297990414,
    "ttft": 12070.413257756192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 76770,
    "finished_requests": 76514,
    "scheduler_time": 51.324719110325844
}
#Debug simulation 
Total elapsed time: 5.384462364017963. Arrivals time: 0.20689802197739482 Scheduler time: 4.920084161683917 Scheduler overhead time: 0.09785259189084172 Adapter cache time: 0.016602303832769394 Engine time: 0.09786981297656894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 34560, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229635 . Total input tokens: 51263080 . Total output tokens: 45006036
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.350586869288236,
    "estimated_duration": 3600.048871965357,
    "input_throughput": 5260.541363056874,
    "output_throughput": 4626.824688328922,
    "total_throughput": 9887.366051385796,
    "itl": 39.76585882940969,
    "ttft": 12023.531106101946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 76770,
    "finished_requests": 76515,
    "scheduler_time": 51.32517479468412
}
#Debug simulation 
Total elapsed time: 5.350716710090637. Arrivals time: 0.20458263950422406 Scheduler time: 4.89142352854833 Scheduler overhead time: 0.09755845880135894 Adapter cache time: 0.01649112068116665 Engine time: 0.09569123154506087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 34560, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229290 . Total input tokens: 51182153 . Total output tokens: 44937046
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.3470475021749735,
    "estimated_duration": 3599.9702567206655,
    "input_throughput": 5233.692685328748,
    "output_throughput": 4616.262306327571,
    "total_throughput": 9849.95499165632,
    "itl": 39.506583176798074,
    "ttft": 8565.231188374928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 76660,
    "finished_requests": 76479,
    "scheduler_time": 51.02575311986453
}
#Debug simulation 
Total elapsed time: 5.347171918023378. Arrivals time: 0.20180808892473578 Scheduler time: 4.889524410944432 Scheduler overhead time: 0.09791853278875351 Adapter cache time: 0.01633490016683936 Engine time: 0.09642487158998847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 34560, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229290 . Total input tokens: 51182153 . Total output tokens: 44937046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.366237172856927,
    "estimated_duration": 3599.9865053374165,
    "input_throughput": 5233.739340985127,
    "output_throughput": 4616.29035980022,
    "total_throughput": 9850.029700785348,
    "itl": 39.506757218374325,
    "ttft": 8518.252175805492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 76660,
    "finished_requests": 76480,
    "scheduler_time": 51.026036338377025
}
#Debug simulation 
Total elapsed time: 5.366357239894569. Arrivals time: 0.19879727670922875 Scheduler time: 4.909753659274429 Scheduler overhead time: 0.09804263431578875 Adapter cache time: 0.016347541473805904 Engine time: 0.0980055215768516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 34560, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229290 . Total input tokens: 51182153 . Total output tokens: 44937046
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.335634903982282,
    "estimated_duration": 3599.9538493560253,
    "input_throughput": 5233.716260937728,
    "output_throughput": 4616.239456228791,
    "total_throughput": 9849.95571716652,
    "itl": 39.50673634994653,
    "ttft": 8612.179340078217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 76660,
    "finished_requests": 76478,
    "scheduler_time": 51.02561063564681
}
#Debug simulation 
Total elapsed time: 5.335821657907218. Arrivals time: 0.19776540249586105 Scheduler time: 4.879936118610203 Scheduler overhead time: 0.0988913499750197 Adapter cache time: 0.016484811436384916 Engine time: 0.09723501140251756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 34560, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229290 . Total input tokens: 51182153 . Total output tokens: 44937046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.32394588412717,
    "estimated_duration": 3599.971692191134,
    "input_throughput": 5233.760876750708,
    "output_throughput": 4616.30935488969,
    "total_throughput": 9850.070231640399,
    "itl": 39.50656262488727,
    "ttft": 8518.22260596844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 76660,
    "finished_requests": 76480,
    "scheduler_time": 51.02574887018216
}
#Debug simulation 
Total elapsed time: 5.324087014421821. Arrivals time: 0.1947319977916777 Scheduler time: 4.873622955288738 Scheduler overhead time: 0.09782885666936636 Adapter cache time: 0.01630012784153223 Engine time: 0.09625321067869663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 34560, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229290 . Total input tokens: 51182153 . Total output tokens: 44937046
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.333770074881613,
    "estimated_duration": 3599.9910063686675,
    "input_throughput": 5233.7327972953535,
    "output_throughput": 4616.284588100475,
    "total_throughput": 9850.017385395828,
    "itl": 39.50675837728906,
    "ttft": 8518.254336696793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 76660,
    "finished_requests": 76480,
    "scheduler_time": 51.02609688757296
}
#Debug simulation 
Total elapsed time: 5.333901676815003. Arrivals time: 0.19282916747033596 Scheduler time: 4.884016888681799 Scheduler overhead time: 0.09829059848561883 Adapter cache time: 0.016254768706858158 Engine time: 0.09725813707336783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 34560, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229290 . Total input tokens: 51182153 . Total output tokens: 44937046
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.34357352880761,
    "estimated_duration": 3599.954975545892,
    "input_throughput": 5233.71462365108,
    "output_throughput": 4616.238012110147,
    "total_throughput": 9849.952635761227,
    "itl": 39.50653058476472,
    "ttft": 8612.103077728298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 76660,
    "finished_requests": 76478,
    "scheduler_time": 51.02550626364906
}
#Debug simulation 
Total elapsed time: 5.343698740936816. Arrivals time: 0.1986485687084496 Scheduler time: 4.8886290192604065 Scheduler overhead time: 0.09811287885531783 Adapter cache time: 0.01628004154190421 Engine time: 0.09693736024200916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 34560, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229290 . Total input tokens: 51182153 . Total output tokens: 44937046
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.410080739296973,
    "estimated_duration": 3599.9887326375638,
    "input_throughput": 5233.736102889324,
    "output_throughput": 4616.287503717893,
    "total_throughput": 9850.023606607218,
    "itl": 39.506785380714476,
    "ttft": 8518.26794621873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 76660,
    "finished_requests": 76480,
    "scheduler_time": 51.02608475314877
}
#Debug simulation 
Total elapsed time: 5.410225230269134. Arrivals time: 0.2030733129940927 Scheduler time: 4.9506178461015224 Scheduler overhead time: 0.097958168014884 Adapter cache time: 0.0163833093829453 Engine time: 0.09706805599853396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 34560, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229125 . Total input tokens: 51144922 . Total output tokens: 44907295
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.258090818300843,
    "estimated_duration": 3600.0256789757177,
    "input_throughput": 5299.401643553854,
    "output_throughput": 4566.333539231091,
    "total_throughput": 9865.735182784945,
    "itl": 38.98868958138073,
    "ttft": 9134.855213526598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 76601,
    "finished_requests": 76408,
    "scheduler_time": 50.11530236551796
}
#Debug simulation 
Total elapsed time: 5.258284593001008. Arrivals time: 0.19738580286502838 Scheduler time: 4.803389040753245 Scheduler overhead time: 0.09899914730340242 Adapter cache time: 0.016131330747157335 Engine time: 0.09647754533216357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 34560, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229125 . Total input tokens: 51144922 . Total output tokens: 44907295
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.347766685765237,
    "estimated_duration": 3600.0097208454686,
    "input_throughput": 5299.425134751998,
    "output_throughput": 4566.3537808834835,
    "total_throughput": 9865.778915635481,
    "itl": 38.988737310329405,
    "ttft": 9088.190854717539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 76601,
    "finished_requests": 76408,
    "scheduler_time": 50.11515254372044
}
#Debug simulation 
Total elapsed time: 5.347883094102144. Arrivals time: 0.1991504249162972 Scheduler time: 4.888030289672315 Scheduler overhead time: 0.09978095255792141 Adapter cache time: 0.016555463429540396 Engine time: 0.09826992359012365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 34560, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229125 . Total input tokens: 51144922 . Total output tokens: 44907295
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.2787200678139925,
    "estimated_duration": 3600.0124482110805,
    "input_throughput": 5299.421119913138,
    "output_throughput": 4566.350321418703,
    "total_throughput": 9865.77144133184,
    "itl": 38.98856873357975,
    "ttft": 9088.173157147057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 76601,
    "finished_requests": 76408,
    "scheduler_time": 50.1152213874067
}
#Debug simulation 
Total elapsed time: 5.2788302837871015. Arrivals time: 0.18226116336882114 Scheduler time: 4.838276239577681 Scheduler overhead time: 0.09906073193997145 Adapter cache time: 0.016135973390191793 Engine time: 0.09733294742181897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 34560, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229125 . Total input tokens: 51144922 . Total output tokens: 44907295
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.302720078732818,
    "estimated_duration": 3600.0368348540514,
    "input_throughput": 5299.385221644111,
    "output_throughput": 4566.319388969932,
    "total_throughput": 9865.704610614042,
    "itl": 38.98860744795461,
    "ttft": 9134.920918796106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 76601,
    "finished_requests": 76408,
    "scheduler_time": 50.11548421798023
}
#Debug simulation 
Total elapsed time: 5.30286285886541. Arrivals time: 0.20502390060573816 Scheduler time: 4.838225677609444 Scheduler overhead time: 0.09848270146176219 Adapter cache time: 0.01646334445104003 Engine time: 0.099002615083009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 34560, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229125 . Total input tokens: 51144922 . Total output tokens: 44907295
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.278279119171202,
    "estimated_duration": 3600.010791059649,
    "input_throughput": 5299.423559334518,
    "output_throughput": 4566.352423394062,
    "total_throughput": 9865.775982728579,
    "itl": 38.98866697257924,
    "ttft": 9088.203409414264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 76601,
    "finished_requests": 76408,
    "scheduler_time": 50.11519699563387
}
#Debug simulation 
Total elapsed time: 5.278415571898222. Arrivals time: 0.20306415297091007 Scheduler time: 4.819813326001167 Scheduler overhead time: 0.09804130485281348 Adapter cache time: 0.016054126899689436 Engine time: 0.09580359421670437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 34560, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229125 . Total input tokens: 51144922 . Total output tokens: 44907295
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.296314400155097,
    "estimated_duration": 3600.0235139666624,
    "input_throughput": 5299.404830547634,
    "output_throughput": 4566.336285366893,
    "total_throughput": 9865.741115914529,
    "itl": 38.98866096222715,
    "ttft": 9134.893343545104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 76601,
    "finished_requests": 76408,
    "scheduler_time": 50.115269884128544
}
#Debug simulation 
Total elapsed time: 5.2964447950944304. Arrivals time: 0.19207680691033602 Scheduler time: 4.847076823934913 Scheduler overhead time: 0.09867634531110525 Adapter cache time: 0.016225495375692844 Engine time: 0.0966933136805892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 34560, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 34560]
Prompts retrieved: 229125 . Total input tokens: 51144922 . Total output tokens: 44907295
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.331594391260296,
    "estimated_duration": 3600.0096400662596,
    "input_throughput": 5299.425253663727,
    "output_throughput": 4566.353883346111,
    "total_throughput": 9865.779137009838,
    "itl": 38.9887228600582,
    "ttft": 9088.200530407381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 76601,
    "finished_requests": 76408,
    "scheduler_time": 50.11516467814474
}
#Debug simulation 
Total elapsed time: 5.331710948143154. Arrivals time: 0.20064617786556482 Scheduler time: 4.874236817471683 Scheduler overhead time: 0.09833136340603232 Adapter cache time: 0.016230858396738768 Engine time: 0.09678144380450249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 34560, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 215460 . Total input tokens: 48086638 . Total output tokens: 42232676
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.058104244992137,
    "estimated_duration": 3600.0001785021036,
    "input_throughput": 4959.680309635175,
    "output_throughput": 4335.965340561407,
    "total_throughput": 9295.645650196582,
    "itl": 37.34186192804522,
    "ttft": 9500.041211245341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 72090,
    "finished_requests": 71900,
    "scheduler_time": 45.962924843528256
}
#Debug simulation 
Total elapsed time: 5.0582295139320195. Arrivals time: 0.1941904928535223 Scheduler time: 4.595803691074252 Scheduler overhead time: 0.10186677426099777 Adapter cache time: 0.019885113928467035 Engine time: 0.09930001012980938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 34560, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 215460 . Total input tokens: 48086638 . Total output tokens: 42232676
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.034309050999582,
    "estimated_duration": 3600.0152706438685,
    "input_throughput": 4959.659517446055,
    "output_throughput": 4335.947163137511,
    "total_throughput": 9295.606680583567,
    "itl": 37.341935920472096,
    "ttft": 9500.031989200219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 72090,
    "finished_requests": 71900,
    "scheduler_time": 45.9631434270619
}
#Debug simulation 
Total elapsed time: 5.0344501649960876. Arrivals time: 0.1874622479081154 Scheduler time: 4.582017404958606 Scheduler overhead time: 0.10096071986481547 Adapter cache time: 0.019917049910873175 Engine time: 0.09744964865967631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 34560, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 215460 . Total input tokens: 48086638 . Total output tokens: 42232676
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.000952338799834,
    "estimated_duration": 3600.0163561719382,
    "input_throughput": 4959.658021939066,
    "output_throughput": 4335.94585570113,
    "total_throughput": 9295.603877640197,
    "itl": 37.34174034725361,
    "ttft": 9499.992375034737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 72090,
    "finished_requests": 71900,
    "scheduler_time": 45.96311507243054
}
#Debug simulation 
Total elapsed time: 5.001100565772504. Arrivals time: 0.17704470548778772 Scheduler time: 4.559419688303024 Scheduler overhead time: 0.0997666409239173 Adapter cache time: 0.0202276143245399 Engine time: 0.09817969240248203 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 34560, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 215460 . Total input tokens: 48086638 . Total output tokens: 42232676
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 5.072808863129467,
    "estimated_duration": 3600.009908680373,
    "input_throughput": 4959.666904512746,
    "output_throughput": 4335.953621228182,
    "total_throughput": 9295.62052574093,
    "itl": 37.34202401066067,
    "ttft": 9500.05965202633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 72090,
    "finished_requests": 71900,
    "scheduler_time": 45.96307474727523
}
#Debug simulation 
Total elapsed time: 5.072915620170534. Arrivals time: 0.18614242505282164 Scheduler time: 4.61806675652042 Scheduler overhead time: 0.10159113770350814 Adapter cache time: 0.019957176875323057 Engine time: 0.09994461247697473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 34560, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 215460 . Total input tokens: 48086638 . Total output tokens: 42232676
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 5.0526620130985975,
    "estimated_duration": 3600.0164749394125,
    "input_throughput": 4959.657858315911,
    "output_throughput": 4335.945712654747,
    "total_throughput": 9295.603570970658,
    "itl": 37.34180122668165,
    "ttft": 9499.992955083393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 72090,
    "finished_requests": 71900,
    "scheduler_time": 45.96313133361255
}
#Debug simulation 
Total elapsed time: 5.052777492906898. Arrivals time: 0.18649959610775113 Scheduler time: 4.598088196013123 Scheduler overhead time: 0.10095339454710484 Adapter cache time: 0.01995084062218666 Engine time: 0.10028470493853092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 34560, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 215460 . Total input tokens: 48086638 . Total output tokens: 42232676
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.0725739686749876,
    "estimated_duration": 3600.031824134982,
    "input_throughput": 4959.636712181058,
    "output_throughput": 4335.927225796304,
    "total_throughput": 9295.563937977362,
    "itl": 37.341786665672366,
    "ttft": 9549.836897221781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 72090,
    "finished_requests": 71900,
    "scheduler_time": 45.963330281242904
}
#Debug simulation 
Total elapsed time: 5.072760730981827. Arrivals time: 0.18692959053441882 Scheduler time: 4.616337920539081 Scheduler overhead time: 0.10149424662813544 Adapter cache time: 0.02006534719839692 Engine time: 0.10070510627701879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 34560, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 215460 . Total input tokens: 48086638 . Total output tokens: 42232676
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.062131850980222,
    "estimated_duration": 3600.0148319536074,
    "input_throughput": 4959.6601218197675,
    "output_throughput": 4335.947691506944,
    "total_throughput": 9295.607813326711,
    "itl": 37.34186510575901,
    "ttft": 9500.012075068338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 72090,
    "finished_requests": 71900,
    "scheduler_time": 45.963119199188554
}
#Debug simulation 
Total elapsed time: 5.0622430997900665. Arrivals time: 0.19405080657452345 Scheduler time: 4.600716534536332 Scheduler overhead time: 0.10127464169636369 Adapter cache time: 0.019961365964263678 Engine time: 0.09934669081121683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 34560, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 214110 . Total input tokens: 47774854 . Total output tokens: 41974656
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 5.042799819726497,
    "estimated_duration": 3599.9452378763035,
    "input_throughput": 4969.248368498287,
    "output_throughput": 4267.8543657690825,
    "total_throughput": 9237.10273426737,
    "itl": 36.287785874885046,
    "ttft": 9502.331637932932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 71677,
    "finished_requests": 71489,
    "scheduler_time": 44.515707629028576
}
#Debug simulation 
Total elapsed time: 5.042923844885081. Arrivals time: 0.19102215440943837 Scheduler time: 4.577685405034572 Scheduler overhead time: 0.10399613482877612 Adapter cache time: 0.019811143167316914 Engine time: 0.10242745745927095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 34560, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 214110 . Total input tokens: 47774854 . Total output tokens: 41974656
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.973490728996694,
    "estimated_duration": 3599.952961052579,
    "input_throughput": 4969.237707697571,
    "output_throughput": 4267.845209707339,
    "total_throughput": 9237.082917404909,
    "itl": 36.28775064087578,
    "ttft": 9502.355942260754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 71677,
    "finished_requests": 71489,
    "scheduler_time": 44.515797325866856
}
#Debug simulation 
Total elapsed time: 4.973613867070526. Arrivals time: 0.18765042815357447 Scheduler time: 4.512925528921187 Scheduler overhead time: 0.10355089930817485 Adapter cache time: 0.01973876729607582 Engine time: 0.10185042070224881 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 34560, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 214110 . Total input tokens: 47774854 . Total output tokens: 41974656
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.96325716516003,
    "estimated_duration": 3599.966995107937,
    "input_throughput": 4969.218335698558,
    "output_throughput": 4267.828572005934,
    "total_throughput": 9237.046907704493,
    "itl": 36.28797786474376,
    "ttft": 9502.403025016865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 71677,
    "finished_requests": 71489,
    "scheduler_time": 44.516011946542605
}
#Debug simulation 
Total elapsed time: 4.963391459081322. Arrivals time: 0.17978560319170356 Scheduler time: 4.513379046693444 Scheduler overhead time: 0.10266728000715375 Adapter cache time: 0.019939469173550606 Engine time: 0.09966924740001559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 34560, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 214110 . Total input tokens: 47774854 . Total output tokens: 41974656
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.982414137106389,
    "estimated_duration": 3599.945182093915,
    "input_throughput": 4969.2484454985,
    "output_throughput": 4267.854431900953,
    "total_throughput": 9237.102877399453,
    "itl": 36.28781229170004,
    "ttft": 9502.390746133324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 71677,
    "finished_requests": 71489,
    "scheduler_time": 44.51569628761543
}
#Debug simulation 
Total elapsed time: 4.9825401389971375. Arrivals time: 0.1830420703627169 Scheduler time: 4.528273550793529 Scheduler overhead time: 0.10362310474738479 Adapter cache time: 0.019909517839550972 Engine time: 0.09955548867583275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 34560, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 214110 . Total input tokens: 47774854 . Total output tokens: 41974656
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.9971655500121415,
    "estimated_duration": 3599.9554199117592,
    "input_throughput": 4969.234313584497,
    "output_throughput": 4267.842294662803,
    "total_throughput": 9237.0766082473,
    "itl": 36.28774390301849,
    "ttft": 9502.341337131784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 71677,
    "finished_requests": 71489,
    "scheduler_time": 44.515797407816756
}
#Debug simulation 
Total elapsed time: 4.9972757287323475. Arrivals time: 0.18352032406255603 Scheduler time: 4.542659662663937 Scheduler overhead time: 0.1031839856877923 Adapter cache time: 0.019881646614521742 Engine time: 0.10019820462912321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 34560, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 214110 . Total input tokens: 47774854 . Total output tokens: 41974656
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.977187322918326,
    "estimated_duration": 3599.9703531952846,
    "input_throughput": 4969.213700363379,
    "output_throughput": 4267.824590933947,
    "total_throughput": 9237.038291297325,
    "itl": 36.28771545037072,
    "ttft": 9502.513123507448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 71677,
    "finished_requests": 71489,
    "scheduler_time": 44.51598359191107
}
#Debug simulation 
Total elapsed time: 4.977285102941096. Arrivals time: 0.18750214530155063 Scheduler time: 4.518544910475612 Scheduler overhead time: 0.10307948710396886 Adapter cache time: 0.019868658389896154 Engine time: 0.10050490219146013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 34560, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 214110 . Total input tokens: 47774854 . Total output tokens: 41974656
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.997456789016724,
    "estimated_duration": 3599.9532567483357,
    "input_throughput": 4969.237299530464,
    "output_throughput": 4267.8448591517545,
    "total_throughput": 9237.082158682219,
    "itl": 36.28773574349088,
    "ttft": 9502.356927138675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 71677,
    "finished_requests": 71489,
    "scheduler_time": 44.515789277225686
}
#Debug simulation 
Total elapsed time: 4.997595802880824. Arrivals time: 0.1939321756362915 Scheduler time: 4.529834712389857 Scheduler overhead time: 0.10403338260948658 Adapter cache time: 0.02034802595153451 Engine time: 0.10131639754399657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 34560, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213435 . Total input tokens: 47623437 . Total output tokens: 41836731
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.9858089638873935,
    "estimated_duration": 3600.023846314099,
    "input_throughput": 4948.569720792249,
    "output_throughput": 4257.022079365102,
    "total_throughput": 9205.591800157352,
    "itl": 35.955609165834225,
    "ttft": 8724.50551479725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 71461,
    "finished_requests": 71288,
    "scheduler_time": 44.183045394887955
}
#Debug simulation 
Total elapsed time: 4.985953996889293. Arrivals time: 0.18671479728072882 Scheduler time: 4.526172718498856 Scheduler overhead time: 0.1040536523796618 Adapter cache time: 0.019463450647890568 Engine time: 0.10116431443020701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 34560, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213435 . Total input tokens: 47623437 . Total output tokens: 41836731
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.993924228940159,
    "estimated_duration": 3600.006175231093,
    "input_throughput": 4948.593733691697,
    "output_throughput": 4257.04241993869,
    "total_throughput": 9205.636153630387,
    "itl": 35.95572805738761,
    "ttft": 8774.850633479644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 71461,
    "finished_requests": 71287,
    "scheduler_time": 44.182926933669705
}
#Debug simulation 
Total elapsed time: 4.994077274110168. Arrivals time: 0.18369715241715312 Scheduler time: 4.536478253547102 Scheduler overhead time: 0.10440913960337639 Adapter cache time: 0.019794518128037453 Engine time: 0.10145143000409007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 34560, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213435 . Total input tokens: 47623437 . Total output tokens: 41836731
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.020766745787114,
    "estimated_duration": 3600.0128008576035,
    "input_throughput": 4948.584626075795,
    "output_throughput": 4257.034585085129,
    "total_throughput": 9205.619211160923,
    "itl": 35.95589006630033,
    "ttft": 8774.827806875071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 71461,
    "finished_requests": 71287,
    "scheduler_time": 44.183036266411506
}
#Debug simulation 
Total elapsed time: 5.020893387962133. Arrivals time: 0.1787757407873869 Scheduler time: 4.566003255546093 Scheduler overhead time: 0.1059383312240243 Adapter cache time: 0.019532110076397657 Engine time: 0.10217067366465926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 34560, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213435 . Total input tokens: 47623437 . Total output tokens: 41836731
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.9781492301262915,
    "estimated_duration": 3600.0350144229196,
    "input_throughput": 4948.6721458612765,
    "output_throughput": 4257.023595215405,
    "total_throughput": 9205.69574107668,
    "itl": 35.95562457065967,
    "ttft": 8674.140020564144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 71461,
    "finished_requests": 71289,
    "scheduler_time": 44.18327586699646
}
#Debug simulation 
Total elapsed time: 4.978255437687039. Arrivals time: 0.1812102128751576 Scheduler time: 4.525038052350283 Scheduler overhead time: 0.104394746478647 Adapter cache time: 0.019386892672628164 Engine time: 0.10006141848862171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 34560, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213435 . Total input tokens: 47623437 . Total output tokens: 41836731
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.980218927375972,
    "estimated_duration": 3600.009545550552,
    "input_throughput": 4948.589100831271,
    "output_throughput": 4257.038434506784,
    "total_throughput": 9205.627535338055,
    "itl": 35.95579077905993,
    "ttft": 8774.849067840034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 71461,
    "finished_requests": 71287,
    "scheduler_time": 44.18300386697253
}
#Debug simulation 
Total elapsed time: 4.980318878311664. Arrivals time: 0.18183276057243347 Scheduler time: 4.527690509334207 Scheduler overhead time: 0.10338702378794551 Adapter cache time: 0.019333106465637684 Engine time: 0.10004043532535434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 34560, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213435 . Total input tokens: 47623437 . Total output tokens: 41836731
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.982198899146169,
    "estimated_duration": 3600.018038108825,
    "input_throughput": 4948.577426950513,
    "output_throughput": 4257.0283920162765,
    "total_throughput": 9205.60581896679,
    "itl": 35.95577740010785,
    "ttft": 8774.901711247643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 71461,
    "finished_requests": 71287,
    "scheduler_time": 44.183016917331834
}
#Debug simulation 
Total elapsed time: 4.982371496967971. Arrivals time: 0.18777965242043138 Scheduler time: 4.523976178839803 Scheduler overhead time: 0.10333413258194923 Adapter cache time: 0.019405472557991743 Engine time: 0.09987365687265992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 34560, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213435 . Total input tokens: 47623437 . Total output tokens: 41836731
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.985829881392419,
    "estimated_duration": 3600.0062344622374,
    "input_throughput": 4948.593652272152,
    "output_throughput": 4257.042349897285,
    "total_throughput": 9205.636002169436,
    "itl": 35.95578880040282,
    "ttft": 8774.814931543582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 71461,
    "finished_requests": 71287,
    "scheduler_time": 44.18293506426068
}
#Debug simulation 
Total elapsed time: 4.98593874135986. Arrivals time: 0.1869029882363975 Scheduler time: 4.527309990487993 Scheduler overhead time: 0.10357749881222844 Adapter cache time: 0.019463001284748316 Engine time: 0.1005027424544096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 34560, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213090 . Total input tokens: 47535929 . Total output tokens: 41767132
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.9439365696161985,
    "estimated_duration": 3599.97068598679,
    "input_throughput": 4948.021123987942,
    "output_throughput": 4266.02001504641,
    "total_throughput": 9214.041139034352,
    "itl": 35.829921791748,
    "ttft": 8836.370971684391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 71367,
    "finished_requests": 71193,
    "scheduler_time": 44.25413006529352
}
#Debug simulation 
Total elapsed time: 4.9440646739676595. Arrivals time: 0.1821375684812665 Scheduler time: 4.492370108142495 Scheduler overhead time: 0.10296269506216049 Adapter cache time: 0.01899733440950513 Engine time: 0.09950427897274494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 34560, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213090 . Total input tokens: 47535929 . Total output tokens: 41767132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.942816617898643,
    "estimated_duration": 3599.95235241624,
    "input_throughput": 4947.949654957118,
    "output_throughput": 4266.041185154081,
    "total_throughput": 9213.990840111199,
    "itl": 35.83008072852601,
    "ttft": 8887.000282312343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 71367,
    "finished_requests": 71192,
    "scheduler_time": 44.253959431318805
}
#Debug simulation 
Total elapsed time: 4.942966062109917. Arrivals time: 0.1755621898919344 Scheduler time: 4.497504313476384 Scheduler overhead time: 0.10348523454740644 Adapter cache time: 0.018972566816955805 Engine time: 0.09945201501250267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 34560, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213090 . Total input tokens: 47535929 . Total output tokens: 41767132
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.973567510955036,
    "estimated_duration": 3599.9599142702723,
    "input_throughput": 4948.03592934193,
    "output_throughput": 4266.032779732505,
    "total_throughput": 9214.068709074434,
    "itl": 35.83001886931749,
    "ttft": 8836.528458074383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 71367,
    "finished_requests": 71193,
    "scheduler_time": 44.25408073458501
}
#Debug simulation 
Total elapsed time: 4.97367669083178. Arrivals time: 0.1797504872083664 Scheduler time: 4.521534094586968 Scheduler overhead time: 0.10368198715150356 Adapter cache time: 0.018953478429466486 Engine time: 0.10162588348612189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 34560, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213090 . Total input tokens: 47535929 . Total output tokens: 41767132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.9645574083551764,
    "estimated_duration": 3599.9488939642633,
    "input_throughput": 4947.954408426339,
    "output_throughput": 4266.04528351742,
    "total_throughput": 9213.99969194376,
    "itl": 35.830130861689916,
    "ttft": 8886.978701125196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 71367,
    "finished_requests": 71192,
    "scheduler_time": 44.253955427485685
}
#Debug simulation 
Total elapsed time: 4.96469203196466. Arrivals time: 0.1800042507238686 Scheduler time: 4.513961074408144 Scheduler overhead time: 0.10332630155608058 Adapter cache time: 0.01899915048852563 Engine time: 0.10024737054482102 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 34560, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213090 . Total input tokens: 47535929 . Total output tokens: 41767132
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.974669920280576,
    "estimated_duration": 3599.9526948386797,
    "input_throughput": 4948.045852252017,
    "output_throughput": 4266.0413349370965,
    "total_throughput": 9214.087187189114,
    "itl": 35.82989753849766,
    "ttft": 8836.517200272338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 71367,
    "finished_requests": 71193,
    "scheduler_time": 44.253971606717855
}
#Debug simulation 
Total elapsed time: 4.974801424890757. Arrivals time: 0.18694208515807986 Scheduler time: 4.514439602848142 Scheduler overhead time: 0.10405385121703148 Adapter cache time: 0.01950430264696479 Engine time: 0.101543671451509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 34560, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213090 . Total input tokens: 47535929 . Total output tokens: 41767132
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.978157347068191,
    "estimated_duration": 3599.959986810637,
    "input_throughput": 4948.035829637396,
    "output_throughput": 4266.032693770557,
    "total_throughput": 9214.068523407954,
    "itl": 35.82987031720648,
    "ttft": 8836.546281471341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 71367,
    "finished_requests": 71193,
    "scheduler_time": 44.253944045097676
}
#Debug simulation 
Total elapsed time: 4.97826784895733. Arrivals time: 0.18393290229141712 Scheduler time: 4.522543657105416 Scheduler overhead time: 0.10414296388626099 Adapter cache time: 0.018946231342852116 Engine time: 0.10052357101812959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 34560, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 213090 . Total input tokens: 47535929 . Total output tokens: 41767132
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.9740973631851375,
    "estimated_duration": 3599.9521755440255,
    "input_throughput": 4947.949898058907,
    "output_throughput": 4266.041394752464,
    "total_throughput": 9213.99129281137,
    "itl": 35.83009643658002,
    "ttft": 8886.963081517395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 71367,
    "finished_requests": 71192,
    "scheduler_time": 44.25395538651089
}
#Debug simulation 
Total elapsed time: 4.9742019372060895. Arrivals time: 0.18149851821362972 Scheduler time: 4.5218461798503995 Scheduler overhead time: 0.10359705472365022 Adapter cache time: 0.019036381039768457 Engine time: 0.10003695264458656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 34560, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 212925 . Total input tokens: 47502410 . Total output tokens: 41732745
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.9681005771271884,
    "estimated_duration": 3599.996083042194,
    "input_throughput": 4879.181697652454,
    "output_throughput": 4271.888259112795,
    "total_throughput": 9151.06995676525,
    "itl": 35.922590232793866,
    "ttft": 10361.211640158368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 71288,
    "finished_requests": 71084,
    "scheduler_time": 44.3709624237764
}
#Debug simulation 
Total elapsed time: 4.968320538289845. Arrivals time: 0.17616841895505786 Scheduler time: 4.522310060448945 Scheduler overhead time: 0.10280545148998499 Adapter cache time: 0.018712916411459446 Engine time: 0.10022959671914577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 34560, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 212925 . Total input tokens: 47502410 . Total output tokens: 41732745
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.9349887440912426,
    "estimated_duration": 3600.0139168070914,
    "input_throughput": 4879.337804221402,
    "output_throughput": 4271.869874780842,
    "total_throughput": 9151.207679002244,
    "itl": 35.922420236630145,
    "ttft": 10310.735586537197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 71288,
    "finished_requests": 71085,
    "scheduler_time": 44.37122125051614
}
#Debug simulation 
Total elapsed time: 4.9351171450689435. Arrivals time: 0.17715949518606067 Scheduler time: 4.490037519019097 Scheduler overhead time: 0.10247485619038343 Adapter cache time: 0.018573930021375418 Engine time: 0.09899668162688613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 34560, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 212925 . Total input tokens: 47502410 . Total output tokens: 41732745
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.985612830147147,
    "estimated_duration": 3600.0137976309925,
    "input_throughput": 4879.337965748684,
    "output_throughput": 4271.870016198297,
    "total_throughput": 9151.20798194698,
    "itl": 35.922396534021416,
    "ttft": 10310.688804111274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 71288,
    "finished_requests": 71085,
    "scheduler_time": 44.37121395391114
}
#Debug simulation 
Total elapsed time: 4.985727832186967. Arrivals time: 0.18588302191346884 Scheduler time: 4.5271148681640625 Scheduler overhead time: 0.10465274611487985 Adapter cache time: 0.0188927398994565 Engine time: 0.10086394753307104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 34560, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 212925 . Total input tokens: 47502410 . Total output tokens: 41732745
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.982497815974057,
    "estimated_duration": 3600.0047791060215,
    "input_throughput": 4879.169911647137,
    "output_throughput": 4271.8779400673375,
    "total_throughput": 9151.047851714475,
    "itl": 35.92252361906275,
    "ttft": 10361.343731195288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 71288,
    "finished_requests": 71084,
    "scheduler_time": 44.371099865299975
}
#Debug simulation 
Total elapsed time: 4.982636123895645. Arrivals time: 0.18564003007486463 Scheduler time: 4.52433060714975 Scheduler overhead time: 0.10435315640643239 Adapter cache time: 0.018894925713539124 Engine time: 0.10079895006492734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 34560, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 212925 . Total input tokens: 47502410 . Total output tokens: 41732745
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.994984054937959,
    "estimated_duration": 3600.0128787303183,
    "input_throughput": 4879.3392111961575,
    "output_throughput": 4271.871106590018,
    "total_throughput": 9151.210317786175,
    "itl": 35.92239565768364,
    "ttft": 10310.718163228006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 71288,
    "finished_requests": 71085,
    "scheduler_time": 44.37120907511692
}
#Debug simulation 
Total elapsed time: 4.995119312778115. Arrivals time: 0.18168300669640303 Scheduler time: 4.539604316465557 Scheduler overhead time: 0.10553257772698998 Adapter cache time: 0.018778827507048845 Engine time: 0.10128985857591033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 34560, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 212925 . Total input tokens: 47502410 . Total output tokens: 41732745
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.964722057804465,
    "estimated_duration": 3599.9908369107015,
    "input_throughput": 4879.188807900764,
    "output_throughput": 4271.894484375176,
    "total_throughput": 9151.08329227594,
    "itl": 35.922504763955324,
    "ttft": 10361.150722696155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 71288,
    "finished_requests": 71084,
    "scheduler_time": 44.370873519948944
}
#Debug simulation 
Total elapsed time: 4.964849234092981. Arrivals time: 0.17399435909464955 Scheduler time: 4.520368316676468 Scheduler overhead time: 0.10376726277172565 Adapter cache time: 0.01888824114575982 Engine time: 0.0999642820097506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 34560, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 34560]
Prompts retrieved: 212925 . Total input tokens: 47502410 . Total output tokens: 41732745
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 5.039014585316181,
    "estimated_duration": 3600.013922859212,
    "input_throughput": 4879.337796018561,
    "output_throughput": 4271.869867599239,
    "total_throughput": 9151.207663617799,
    "itl": 35.92239942725743,
    "ttft": 10310.714060510627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 71288,
    "finished_requests": 71085,
    "scheduler_time": 44.37123742974829
}
#Debug simulation 
Total elapsed time: 5.039132316131145. Arrivals time: 0.17882593907415867 Scheduler time: 4.589100468438119 Scheduler overhead time: 0.10395654337480664 Adapter cache time: 0.01879809470847249 Engine time: 0.10018304781988263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 34560, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 211410 . Total input tokens: 47153734 . Total output tokens: 41441352
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.951246007345617,
    "estimated_duration": 3600.018346235034,
    "input_throughput": 4910.814973620634,
    "output_throughput": 4217.695450324435,
    "total_throughput": 9128.51042394507,
    "itl": 35.38817587291073,
    "ttft": 7793.219518790124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 70747,
    "finished_requests": 70595,
    "scheduler_time": 43.308750345029246
}
#Debug simulation 
Total elapsed time: 4.951341752894223. Arrivals time: 0.18143457546830177 Scheduler time: 4.4943601856939495 Scheduler overhead time: 0.10525580728426576 Adapter cache time: 0.019401364494115114 Engine time: 0.10224839067086577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 34560, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 211410 . Total input tokens: 47153734 . Total output tokens: 41441352
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.956987239886075,
    "estimated_duration": 3600.0330706441005,
    "input_throughput": 4910.794888013891,
    "output_throughput": 4217.678199629258,
    "total_throughput": 9128.47308764315,
    "itl": 35.38828962630172,
    "ttft": 7793.165971468766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 70747,
    "finished_requests": 70595,
    "scheduler_time": 43.308985818856286
}
#Debug simulation 
Total elapsed time: 4.957127342931926. Arrivals time: 0.18595149274915457 Scheduler time: 4.496880973689258 Scheduler overhead time: 0.10472595784813166 Adapter cache time: 0.01941810781136155 Engine time: 0.10141379246488214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 34560, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 211410 . Total input tokens: 47153734 . Total output tokens: 41441352
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.955259589944035,
    "estimated_duration": 3600.0087863683802,
    "input_throughput": 4910.828014348893,
    "output_throughput": 4217.706650465459,
    "total_throughput": 9128.534664814351,
    "itl": 35.38838233088591,
    "ttft": 7793.151624437014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 70747,
    "finished_requests": 70595,
    "scheduler_time": 43.308746423146005
}
#Debug simulation 
Total elapsed time: 4.955407642759383. Arrivals time: 0.2047243774868548 Scheduler time: 4.474669576622546 Scheduler overhead time: 0.10534580750390887 Adapter cache time: 0.01947455620393157 Engine time: 0.10220386181026697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 34560, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 211410 . Total input tokens: 47153734 . Total output tokens: 41441352
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.957583614159375,
    "estimated_duration": 3600.0264137403815,
    "input_throughput": 4910.803968694141,
    "output_throughput": 4217.685998649173,
    "total_throughput": 9128.489967343314,
    "itl": 35.38817074473886,
    "ttft": 7793.249019869264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 70747,
    "finished_requests": 70595,
    "scheduler_time": 43.308864351690254
}
#Debug simulation 
Total elapsed time: 4.957726145163178. Arrivals time: 0.17990906396880746 Scheduler time: 4.501157260965556 Scheduler overhead time: 0.10717500001192093 Adapter cache time: 0.019347524270415306 Engine time: 0.10144455637782812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 34560, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 211410 . Total input tokens: 47153734 . Total output tokens: 41441352
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.982533745933324,
    "estimated_duration": 3600.0079927537486,
    "input_throughput": 4910.829096931202,
    "output_throughput": 4217.707580250535,
    "total_throughput": 9128.536677181737,
    "itl": 35.388392518889496,
    "ttft": 7793.159511879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 70747,
    "finished_requests": 70595,
    "scheduler_time": 43.30873833352976
}
#Debug simulation 
Total elapsed time: 4.982668923214078. Arrivals time: 0.1740037640556693 Scheduler time: 4.534134215209633 Scheduler overhead time: 0.1049014893360436 Adapter cache time: 0.019481272902339697 Engine time: 0.10146229714155197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 34560, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 211410 . Total input tokens: 47153734 . Total output tokens: 41441352
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.9208799209445715,
    "estimated_duration": 3600.0141195390743,
    "input_throughput": 4910.820739298523,
    "output_throughput": 4217.700402226214,
    "total_throughput": 9128.521141524736,
    "itl": 35.38822922586195,
    "ttft": 7793.146052213798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 70747,
    "finished_requests": 70595,
    "scheduler_time": 43.308697803499506
}
#Debug simulation 
Total elapsed time: 4.920997553970665. Arrivals time: 0.1753594516776502 Scheduler time: 4.472720813937485 Scheduler overhead time: 0.10383175360038877 Adapter cache time: 0.019495871849358082 Engine time: 0.10112222284078598 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 34560, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 211410 . Total input tokens: 47153734 . Total output tokens: 41441352
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.931907874997705,
    "estimated_duration": 3600.007449124228,
    "input_throughput": 4910.82983850513,
    "output_throughput": 4217.708217157648,
    "total_throughput": 9128.538055662779,
    "itl": 35.388401008779844,
    "ttft": 7793.182813987482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 70747,
    "finished_requests": 70595,
    "scheduler_time": 43.30874642314588
}
#Debug simulation 
Total elapsed time: 4.932076552882791. Arrivals time: 0.1762052709236741 Scheduler time: 4.481837334111333 Scheduler overhead time: 0.1048307977616787 Adapter cache time: 0.019341018982231617 Engine time: 0.1011960911564529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 34560, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210735 . Total input tokens: 47002281 . Total output tokens: 41303866
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.95049884961918,
    "estimated_duration": 3600.023478422002,
    "input_throughput": 4881.697329291671,
    "output_throughput": 4213.129745100527,
    "total_throughput": 9094.827074392198,
    "itl": 35.122015893500105,
    "ttft": 9346.13119774365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 70543,
    "finished_requests": 70361,
    "scheduler_time": 43.12229174536687
}
#Debug simulation 
Total elapsed time: 4.950616339687258. Arrivals time: 0.1825743461959064 Scheduler time: 4.494301843922585 Scheduler overhead time: 0.10445966385304928 Adapter cache time: 0.01912694564089179 Engine time: 0.10136584145948291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 34560, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210735 . Total input tokens: 47002281 . Total output tokens: 41303866
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.93013997701928,
    "estimated_duration": 3600.001484218308,
    "input_throughput": 4881.651320712149,
    "output_throughput": 4213.104651898039,
    "total_throughput": 9094.755972610188,
    "itl": 35.122152962579165,
    "ttft": 9397.095061998041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 70543,
    "finished_requests": 70360,
    "scheduler_time": 43.122032043666174
}
#Debug simulation 
Total elapsed time: 4.930337619036436. Arrivals time: 0.18565824581310153 Scheduler time: 4.46776218758896 Scheduler overhead time: 0.10595067963004112 Adapter cache time: 0.019167416263371706 Engine time: 0.10273415548726916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 34560, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210735 . Total input tokens: 47002281 . Total output tokens: 41303866
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.9040417349897325,
    "estimated_duration": 3600.0131817853535,
    "input_throughput": 4881.711291758221,
    "output_throughput": 4213.141795352552,
    "total_throughput": 9094.853087110774,
    "itl": 35.122226033986664,
    "ttft": 9345.989237089174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 70543,
    "finished_requests": 70361,
    "scheduler_time": 43.12222227256926
}
#Debug simulation 
Total elapsed time: 4.904172263108194. Arrivals time: 0.17868410842493176 Scheduler time: 4.4513252787292 Scheduler overhead time: 0.10484823444858193 Adapter cache time: 0.01898092171177268 Engine time: 0.10148462885990739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 34560, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210735 . Total input tokens: 47002281 . Total output tokens: 41303866
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.918143626768142,
    "estimated_duration": 3600.0316527762707,
    "input_throughput": 4881.6862447437425,
    "output_throughput": 4213.120178624885,
    "total_throughput": 9094.806423368627,
    "itl": 35.122058859422026,
    "ttft": 9346.11702092059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 70543,
    "finished_requests": 70361,
    "scheduler_time": 43.12244945190551
}
#Debug simulation 
Total elapsed time: 4.918326621875167. Arrivals time: 0.17360602039843798 Scheduler time: 4.470241978764534 Scheduler overhead time: 0.10500279627740383 Adapter cache time: 0.019103574566543102 Engine time: 0.10133480653166771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 34560, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210735 . Total input tokens: 47002281 . Total output tokens: 41303866
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.945862464606762,
    "estimated_duration": 3600.0121272571632,
    "input_throughput": 4881.71272172623,
    "output_throughput": 4213.143029480838,
    "total_throughput": 9094.855751207067,
    "itl": 35.12214591008914,
    "ttft": 9345.99754063133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 70543,
    "finished_requests": 70361,
    "scheduler_time": 43.122218186786135
}
#Debug simulation 
Total elapsed time: 4.945987327955663. Arrivals time: 0.17759384587407112 Scheduler time: 4.49258692516014 Scheduler overhead time: 0.10493904491886497 Adapter cache time: 0.019175332970917225 Engine time: 0.10270864889025688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 34560, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210735 . Total input tokens: 47002281 . Total output tokens: 41303866
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.905865012202412,
    "estimated_duration": 3600.017745464056,
    "input_throughput": 4881.705103299321,
    "output_throughput": 4213.1364544273565,
    "total_throughput": 9094.841557726679,
    "itl": 35.12198967362646,
    "ttft": 9346.09717070494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 70543,
    "finished_requests": 70361,
    "scheduler_time": 43.122194710949024
}
#Debug simulation 
Total elapsed time: 4.9059955989941955. Arrivals time: 0.17649925593286753 Scheduler time: 4.454661489930004 Scheduler overhead time: 0.10467851487919688 Adapter cache time: 0.019148836843669415 Engine time: 0.10197123372927308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 34560, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210735 . Total input tokens: 47002281 . Total output tokens: 41303866
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.935128860641271,
    "estimated_duration": 3600.0064878432436,
    "input_throughput": 4881.720368934302,
    "output_throughput": 4213.14962937379,
    "total_throughput": 9094.869998308091,
    "itl": 35.122275260003725,
    "ttft": 9345.983858598025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 70543,
    "finished_requests": 70361,
    "scheduler_time": 43.12214129445848
}
#Debug simulation 
Total elapsed time: 4.93524214765057. Arrivals time: 0.18439020030200481 Scheduler time: 4.475782519672066 Scheduler overhead time: 0.10523865185678005 Adapter cache time: 0.01919804373756051 Engine time: 0.1014876700937748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 34560, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210390 . Total input tokens: 46923824 . Total output tokens: 41244191
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.928874361328781,
    "estimated_duration": 3600.0303522749236,
    "input_throughput": 4874.361403345169,
    "output_throughput": 4208.263130455698,
    "total_throughput": 9082.624533800867,
    "itl": 34.99412745901334,
    "ttft": 8034.014372592932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 70411,
    "finished_requests": 70254,
    "scheduler_time": 42.943856707223254
}
#Debug simulation 
Total elapsed time: 4.928980716969818. Arrivals time: 0.18031041137874126 Scheduler time: 4.472785068675876 Scheduler overhead time: 0.10553071508184075 Adapter cache time: 0.01881040120497346 Engine time: 0.10242525674402714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 34560, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210390 . Total input tokens: 46923824 . Total output tokens: 41244191
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.916891003027558,
    "estimated_duration": 3600.016635515157,
    "input_throughput": 4874.37997560501,
    "output_throughput": 4208.27916475227,
    "total_throughput": 9082.65914035728,
    "itl": 34.9941830594579,
    "ttft": 7931.9999525145995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 70411,
    "finished_requests": 70254,
    "scheduler_time": 42.94375479401102
}
#Debug simulation 
Total elapsed time: 4.917009931989014. Arrivals time: 0.17675004666671157 Scheduler time: 4.464394323062152 Scheduler overhead time: 0.10542565118521452 Adapter cache time: 0.01903708791360259 Engine time: 0.10245880344882607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 34560, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210390 . Total input tokens: 46923824 . Total output tokens: 41244191
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.919611827004701,
    "estimated_duration": 3600.024247041715,
    "input_throughput": 4874.369669709801,
    "output_throughput": 4208.270267193134,
    "total_throughput": 9082.639936902935,
    "itl": 34.99421126763543,
    "ttft": 7983.0432804052525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 70411,
    "finished_requests": 70254,
    "scheduler_time": 42.94388823170174
}
#Debug simulation 
Total elapsed time: 4.9197091637179255. Arrivals time: 0.1735877636820078 Scheduler time: 4.46979085309431 Scheduler overhead time: 0.10593031207099557 Adapter cache time: 0.0187965570949018 Engine time: 0.10242093168199062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 34560, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210390 . Total input tokens: 46923824 . Total output tokens: 41244191
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.9430865263566375,
    "estimated_duration": 3600.0102597757523,
    "input_throughput": 4874.388608296097,
    "output_throughput": 4208.286617756389,
    "total_throughput": 9082.675226052486,
    "itl": 34.99427811307677,
    "ttft": 7880.947122396118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 70411,
    "finished_requests": 70254,
    "scheduler_time": 42.94366172245054
}
#Debug simulation 
Total elapsed time: 4.943320061080158. Arrivals time: 0.18847821559756994 Scheduler time: 4.479169446974993 Scheduler overhead time: 0.10523682273924351 Adapter cache time: 0.018883778247982264 Engine time: 0.10229324083775282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 34560, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210390 . Total input tokens: 46923824 . Total output tokens: 41244191
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.961234393063933,
    "estimated_duration": 3600.0216382500957,
    "input_throughput": 4874.373201970443,
    "output_throughput": 4208.273316758195,
    "total_throughput": 9082.646518728638,
    "itl": 34.9941958657233,
    "ttft": 7983.055289646989,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 70411,
    "finished_requests": 70254,
    "scheduler_time": 42.94383569017184
}
#Debug simulation 
Total elapsed time: 4.961339083965868. Arrivals time: 0.1763169327750802 Scheduler time: 4.50727767450735 Scheduler overhead time: 0.10672438889741898 Adapter cache time: 0.01901529310271144 Engine time: 0.10252415481954813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 34560, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210390 . Total input tokens: 46923824 . Total output tokens: 41244191
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.929778635967523,
    "estimated_duration": 3600.026870403054,
    "input_throughput": 4874.366117727162,
    "output_throughput": 4208.267200601155,
    "total_throughput": 9082.633318328319,
    "itl": 34.99412006308141,
    "ttft": 7983.030023192946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 70411,
    "finished_requests": 70254,
    "scheduler_time": 42.94377172527927
}
#Debug simulation 
Total elapsed time: 4.92988219903782. Arrivals time: 0.183782531414181 Scheduler time: 4.469072420150042 Scheduler overhead time: 0.10618525790050626 Adapter cache time: 0.019260132685303688 Engine time: 0.10241112438961864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 34560, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210390 . Total input tokens: 46923824 . Total output tokens: 41244191
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.930460900068283,
    "estimated_duration": 3600.0166441798847,
    "input_throughput": 4874.379963873071,
    "output_throughput": 4208.27915462354,
    "total_throughput": 9082.659118496611,
    "itl": 34.99420721651503,
    "ttft": 7931.991297970696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 70411,
    "finished_requests": 70254,
    "scheduler_time": 42.9437467043949
}
#Debug simulation 
Total elapsed time: 4.930572365876287. Arrivals time: 0.18098385212942958 Scheduler time: 4.4720824379473925 Scheduler overhead time: 0.10558655951172113 Adapter cache time: 0.018839051015675068 Engine time: 0.10384040419012308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 34560, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210225 . Total input tokens: 46887639 . Total output tokens: 41212892
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.95557555090636,
    "estimated_duration": 3599.924025921796,
    "input_throughput": 4836.392622353143,
    "output_throughput": 4223.200792718694,
    "total_throughput": 9059.593415071837,
    "itl": 35.01913599761166,
    "ttft": 8960.639385161236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 70360,
    "finished_requests": 70186,
    "scheduler_time": 43.201846907305175
}
#Debug simulation 
Total elapsed time: 4.955746989697218. Arrivals time: 0.17819333216175437 Scheduler time: 4.500839990098029 Scheduler overhead time: 0.10583899170160294 Adapter cache time: 0.018608303740620613 Engine time: 0.10313509358093143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 34560, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210225 . Total input tokens: 46887639 . Total output tokens: 41212892
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.937897305004299,
    "estimated_duration": 3599.9433112499764,
    "input_throughput": 4836.366713217674,
    "output_throughput": 4223.178168525416,
    "total_throughput": 9059.544881743092,
    "itl": 35.01920941023646,
    "ttft": 8960.720255762286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 70360,
    "finished_requests": 70186,
    "scheduler_time": 43.20219872365559
}
#Debug simulation 
Total elapsed time: 4.937992012593895. Arrivals time: 0.17534754145890474 Scheduler time: 4.487645203247666 Scheduler overhead time: 0.10523701692000031 Adapter cache time: 0.018677531741559505 Engine time: 0.1021667211316526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 34560, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210225 . Total input tokens: 46887639 . Total output tokens: 41212892
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.914768958929926,
    "estimated_duration": 3599.94890400724,
    "input_throughput": 4836.43114506966,
    "output_throughput": 4223.349387841596,
    "total_throughput": 9059.780532911254,
    "itl": 35.01939132203441,
    "ttft": 8909.516898986263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 70360,
    "finished_requests": 70187,
    "scheduler_time": 43.20228374657404
}
#Debug simulation 
Total elapsed time: 4.914877052884549. Arrivals time: 0.1754383435472846 Scheduler time: 4.466500662732869 Scheduler overhead time: 0.10474374517798424 Adapter cache time: 0.018877331633120775 Engine time: 0.10067727090790868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 34560, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210225 . Total input tokens: 46887639 . Total output tokens: 41212892
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.947648024186492,
    "estimated_duration": 3599.9347456738888,
    "input_throughput": 4836.37822072267,
    "output_throughput": 4223.188217028095,
    "total_throughput": 9059.566437750766,
    "itl": 35.01912091477999,
    "ttft": 8960.7713078032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 70360,
    "finished_requests": 70186,
    "scheduler_time": 43.202077461364
}
#Debug simulation 
Total elapsed time: 4.947746709920466. Arrivals time: 0.18095747148618102 Scheduler time: 4.490792158991098 Scheduler overhead time: 0.10570309590548277 Adapter cache time: 0.0188449677079916 Engine time: 0.10251898178830743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 34560, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210225 . Total input tokens: 46887639 . Total output tokens: 41212892
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.964247570838779,
    "estimated_duration": 3599.9492195389803,
    "input_throughput": 4836.430721161586,
    "output_throughput": 4223.34901766949,
    "total_throughput": 9059.779738831076,
    "itl": 35.01942168824067,
    "ttft": 8909.516968282263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 70360,
    "finished_requests": 70187,
    "scheduler_time": 43.202295840023275
}
#Debug simulation 
Total elapsed time: 4.96435395302251. Arrivals time: 0.1855033114552498 Scheduler time: 4.502476319205016 Scheduler overhead time: 0.10588427307084203 Adapter cache time: 0.018641792703419924 Engine time: 0.1026304429396987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 34560, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210225 . Total input tokens: 46887639 . Total output tokens: 41212892
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.954112835694104,
    "estimated_duration": 3599.9155458330606,
    "input_throughput": 4836.404015131134,
    "output_throughput": 4223.210741040262,
    "total_throughput": 9059.614756171395,
    "itl": 35.01903651816555,
    "ttft": 8960.652222887715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 70360,
    "finished_requests": 70186,
    "scheduler_time": 43.201737574563744
}
#Debug simulation 
Total elapsed time: 4.9542198637500405. Arrivals time: 0.17959304293617606 Scheduler time: 4.499191767070442 Scheduler overhead time: 0.10552354389801621 Adapter cache time: 0.018547975923866034 Engine time: 0.10224403766915202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 34560, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 34560]
Prompts retrieved: 210225 . Total input tokens: 46887639 . Total output tokens: 41212892
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.936887696851045,
    "estimated_duration": 3599.9450471095383,
    "input_throughput": 4836.364381167242,
    "output_throughput": 4223.176132148719,
    "total_throughput": 9059.540513315962,
    "itl": 35.01926235548642,
    "ttft": 8960.687034517103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 70360,
    "finished_requests": 70186,
    "scheduler_time": 43.20222295152892
}
#Debug simulation 
Total elapsed time: 4.937033368740231. Arrivals time: 0.17430394003167748 Scheduler time: 4.487100842874497 Scheduler overhead time: 0.10595124587416649 Adapter cache time: 0.018627463839948177 Engine time: 0.10160667169839144 
