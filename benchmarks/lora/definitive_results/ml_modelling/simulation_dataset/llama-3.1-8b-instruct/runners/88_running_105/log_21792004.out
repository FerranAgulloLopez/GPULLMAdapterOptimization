INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 78.7169924909249,
    "estimated_duration": 3600.0764577714895,
    "input_throughput": 6884.990996925459,
    "output_throughput": 5980.421875075241,
    "total_throughput": 12865.4128720007,
    "itl": 100.03439982911252,
    "ttft": 1982783.6435682678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6134278037957828,
    "arrivals": 863203,
    "finished_requests": 100271,
    "scheduler_time": 213.1641386041678
}
#Debug simulation 
Total elapsed time: 78.71720614982769. Arrivals time: 0.5758067849092185 Scheduler time: 77.94458362879232 Scheduler overhead time: 0.0763855604454875 Adapter cache time: 0.014099999330937862 Engine time: 0.07557455589994788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.63438177201897,
    "estimated_duration": 3600.0258443655475,
    "input_throughput": 6789.279315384325,
    "output_throughput": 5888.529115194725,
    "total_throughput": 12677.808430579049,
    "itl": 97.28188646280067,
    "ttft": 1990398.4459736827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8044967119442317,
    "arrivals": 863203,
    "finished_requests": 98916,
    "scheduler_time": 215.37363308860463
}
#Debug simulation 
Total elapsed time: 79.63458374887705. Arrivals time: 0.5674958443269134 Scheduler time: 78.8665732583031 Scheduler overhead time: 0.07822845410555601 Adapter cache time: 0.014846817590296268 Engine time: 0.07650967128574848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 72.37314673187211,
    "estimated_duration": 3600.090453380699,
    "input_throughput": 6609.705313835817,
    "output_throughput": 5734.78563034782,
    "total_throughput": 12344.490944183637,
    "itl": 91.10284334864244,
    "ttft": 2006656.4658414386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6749806477269202,
    "arrivals": 863203,
    "finished_requests": 96220,
    "scheduler_time": 221.53560190537414
}
#Debug simulation 
Total elapsed time: 72.37332414183766. Arrivals time: 0.6520205088891089 Scheduler time: 71.51526376325637 Scheduler overhead time: 0.08025995036587119 Adapter cache time: 0.014723567757755518 Engine time: 0.07906046230345964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 78.64436960499734,
    "estimated_duration": 3600.0368505170795,
    "input_throughput": 6806.163941483174,
    "output_throughput": 5920.218010251417,
    "total_throughput": 12726.381951734591,
    "itl": 97.5583292282605,
    "ttft": 1985099.0739992503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5305073846923174,
    "arrivals": 863203,
    "finished_requests": 99159,
    "scheduler_time": 215.26854259472609
}
#Debug simulation 
Total elapsed time: 78.64454141911119. Arrivals time: 0.6586437067016959 Scheduler time: 77.78560386225581 Scheduler overhead time: 0.07895340118557215 Adapter cache time: 0.014371398836374283 Engine time: 0.07562942476943135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 74.12220818083733,
    "estimated_duration": 3600.0097884972074,
    "input_throughput": 6609.127862936214,
    "output_throughput": 5730.199419433051,
    "total_throughput": 12339.327282369266,
    "itl": 91.15299093156187,
    "ttft": 2006694.0294654383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6109904192388098,
    "arrivals": 863203,
    "finished_requests": 96171,
    "scheduler_time": 221.5041357759842
}
#Debug simulation 
Total elapsed time: 74.12238822318614. Arrivals time: 0.6464911592192948 Scheduler time: 73.2692911545746 Scheduler overhead time: 0.08121866127476096 Adapter cache time: 0.014744562562555075 Engine time: 0.07855849154293537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 77.62235239334404,
    "estimated_duration": 3600.097572955216,
    "input_throughput": 6821.376227267462,
    "output_throughput": 5912.06809501237,
    "total_throughput": 12733.444322279833,
    "itl": 97.51004742053792,
    "ttft": 1990233.7638744977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.583212393261487,
    "arrivals": 863203,
    "finished_requests": 99312,
    "scheduler_time": 215.29093299010324
}
#Debug simulation 
Total elapsed time: 77.62251432938501. Arrivals time: 0.574995921459049 Scheduler time: 76.84890730911866 Scheduler overhead time: 0.07838797336444259 Adapter cache time: 0.01434580935165286 Engine time: 0.07566427160054445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 73.1564208730124,
    "estimated_duration": 3600.064400002391,
    "input_throughput": 6614.070570510958,
    "output_throughput": 5736.869873768448,
    "total_throughput": 12350.940444279406,
    "itl": 91.14796535577683,
    "ttft": 2007853.205593398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.718977903239434,
    "arrivals": 863203,
    "finished_requests": 96298,
    "scheduler_time": 221.4936302535727
}
#Debug simulation 
Total elapsed time: 73.15659642871469. Arrivals time: 0.5455690072849393 Scheduler time: 72.40569106303155 Scheduler overhead time: 0.08045834954828024 Adapter cache time: 0.014867598656564951 Engine time: 0.07821269985288382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 74.29381454456598,
    "estimated_duration": 3600.025263652133,
    "input_throughput": 6848.425273268399,
    "output_throughput": 5972.14614493814,
    "total_throughput": 12820.57141820654,
    "itl": 100.23421059493258,
    "ttft": 1974480.6914452408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4282803509011848,
    "arrivals": 802767,
    "finished_requests": 100068,
    "scheduler_time": 212.19916011557146
}
#Debug simulation 
Total elapsed time: 74.29399086162448. Arrivals time: 0.5702880010940135 Scheduler time: 73.52735331002623 Scheduler overhead time: 0.07738887751474977 Adapter cache time: 0.01386085944250226 Engine time: 0.07449987577274442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.07688337564468,
    "estimated_duration": 3600.036730468766,
    "input_throughput": 6757.309666902316,
    "output_throughput": 5897.398162722495,
    "total_throughput": 12654.707829624811,
    "itl": 97.0165261354565,
    "ttft": 1980091.1431684277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.639619657658042,
    "arrivals": 802767,
    "finished_requests": 98853,
    "scheduler_time": 215.3305422431991
}
#Debug simulation 
Total elapsed time: 77.07704435568303. Arrivals time: 0.5798322781920433 Scheduler time: 76.29589880956337 Scheduler overhead time: 0.07909603556618094 Adapter cache time: 0.014557563234120607 Engine time: 0.07698088651522994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 67.842269689776,
    "estimated_duration": 3600.0877072314574,
    "input_throughput": 6602.476920841254,
    "output_throughput": 5762.383499249079,
    "total_throughput": 12364.860420090332,
    "itl": 91.31065719778378,
    "ttft": 1990599.0004617276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.038240921539261,
    "arrivals": 802767,
    "finished_requests": 96614,
    "scheduler_time": 221.1437812357126
}
#Debug simulation 
Total elapsed time: 67.8424371550791. Arrivals time: 0.5304567175917327 Scheduler time: 67.10822737030685 Scheduler overhead time: 0.07977446727454662 Adapter cache time: 0.014901890885084867 Engine time: 0.07692260947078466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 75.65047370828688,
    "estimated_duration": 3600.0144749692317,
    "input_throughput": 6816.1384268349975,
    "output_throughput": 5937.505570774875,
    "total_throughput": 12753.643997609874,
    "itl": 97.5978218693099,
    "ttft": 1977155.1731513045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6345955246407513,
    "arrivals": 802767,
    "finished_requests": 99612,
    "scheduler_time": 214.58991113578654
}
#Debug simulation 
Total elapsed time: 75.65075486898422. Arrivals time: 0.5753266732208431 Scheduler time: 74.87318141199648 Scheduler overhead time: 0.07913134945556521 Adapter cache time: 0.014833227265626192 Engine time: 0.07665985962375998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 74.34830301674083,
    "estimated_duration": 3600.0168841675318,
    "input_throughput": 6565.039487437067,
    "output_throughput": 5733.206444328305,
    "total_throughput": 12298.245931765372,
    "itl": 91.01195105944309,
    "ttft": 1994358.7876054866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.55823788378388,
    "arrivals": 802767,
    "finished_requests": 96028,
    "scheduler_time": 221.29329171480495
}
#Debug simulation 
Total elapsed time: 74.34847310464829. Arrivals time: 0.5335187343880534 Scheduler time: 73.60918053472415 Scheduler overhead time: 0.08038409566506743 Adapter cache time: 0.014558422844856977 Engine time: 0.0786891644820571 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 71.7333482299,
    "estimated_duration": 3600.0951180329284,
    "input_throughput": 6748.6072460427085,
    "output_throughput": 5899.637732795519,
    "total_throughput": 12648.244978838227,
    "itl": 97.25904899473241,
    "ttft": 1977622.8065580179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.404462606925513,
    "arrivals": 802767,
    "finished_requests": 98746,
    "scheduler_time": 215.1749025635009
}
#Debug simulation 
Total elapsed time: 71.73354551102966. Arrivals time: 0.5544893420301378 Scheduler time: 70.98226231895387 Scheduler overhead time: 0.07637071656063199 Adapter cache time: 0.01412513293325901 Engine time: 0.075398750603199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.57418351992965,
    "estimated_duration": 3600.0329548398704,
    "input_throughput": 6579.67643550463,
    "output_throughput": 5743.602422361637,
    "total_throughput": 12323.278857866268,
    "itl": 91.19987540798456,
    "ttft": 1994543.186505899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5960779825225508,
    "arrivals": 802767,
    "finished_requests": 96220,
    "scheduler_time": 220.92330786769847
}
#Debug simulation 
Total elapsed time: 71.57435510121286. Arrivals time: 0.5537152886390686 Scheduler time: 70.81717655900866 Scheduler overhead time: 0.0792641662992537 Adapter cache time: 0.014590029139071703 Engine time: 0.07713324064388871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 82.59619914926589,
    "estimated_duration": 3600.06082415011,
    "input_throughput": 6909.517148469938,
    "output_throughput": 5977.038736582965,
    "total_throughput": 12886.555885052901,
    "itl": 100.09567835984716,
    "ttft": 1955040.0849547097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6531022579874823,
    "arrivals": 757465,
    "finished_requests": 100380,
    "scheduler_time": 212.56272445148858
}
#Debug simulation 
Total elapsed time: 82.59636454097927. Arrivals time: 0.5672684004530311 Scheduler time: 81.82714764401317 Scheduler overhead time: 0.07926368294283748 Adapter cache time: 0.01505253603681922 Engine time: 0.07715942338109016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.68205190496519,
    "estimated_duration": 3600.0488352142347,
    "input_throughput": 6847.740163650579,
    "output_throughput": 5909.11817415223,
    "total_throughput": 12756.858337802809,
    "itl": 97.56971926188191,
    "ttft": 1967651.3707108404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.295246542687531,
    "arrivals": 757465,
    "finished_requests": 99366,
    "scheduler_time": 214.52670699778733
}
#Debug simulation 
Total elapsed time: 74.68222209997475. Arrivals time: 0.5645870324224234 Scheduler time: 73.91549529926851 Scheduler overhead time: 0.07837995514273643 Adapter cache time: 0.015629610512405634 Engine time: 0.07627979014068842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 65.14157822076231,
    "estimated_duration": 3600.0286251881057,
    "input_throughput": 6656.554570798132,
    "output_throughput": 5750.2617771319365,
    "total_throughput": 12406.81634793007,
    "itl": 91.41699808229636,
    "ttft": 1986380.6475156406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.41936214849354,
    "arrivals": 757465,
    "finished_requests": 96605,
    "scheduler_time": 220.60315506076378
}
#Debug simulation 
Total elapsed time: 65.14174787094817. Arrivals time: 0.5353119410574436 Scheduler time: 64.40551222674549 Scheduler overhead time: 0.0767762316390872 Adapter cache time: 0.01548570767045021 Engine time: 0.07660107361152768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 79.34833156783134,
    "estimated_duration": 3600.0960603179387,
    "input_throughput": 6841.38244850746,
    "output_throughput": 5912.429736144377,
    "total_throughput": 12753.812184651837,
    "itl": 97.76861086540971,
    "ttft": 1960871.9730402704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9718384443223451,
    "arrivals": 757465,
    "finished_requests": 99270,
    "scheduler_time": 214.20388148163192
}
#Debug simulation 
Total elapsed time: 79.34850340988487. Arrivals time: 0.5618062694557011 Scheduler time: 78.58296461822465 Scheduler overhead time: 0.07938328292220831 Adapter cache time: 0.015475828666239977 Engine time: 0.07732796389609575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 68.52801396185532,
    "estimated_duration": 3600.057175689618,
    "input_throughput": 6636.012939273301,
    "output_throughput": 5736.355561087473,
    "total_throughput": 12372.368500360773,
    "itl": 91.50488269685528,
    "ttft": 1979576.2453829264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5915424886159752,
    "arrivals": 757465,
    "finished_requests": 96404,
    "scheduler_time": 221.05194333280076
}
#Debug simulation 
Total elapsed time: 68.52819191897288. Arrivals time: 0.5348570528440177 Scheduler time: 67.78694125497714 Scheduler overhead time: 0.07977505633607507 Adapter cache time: 0.01621506456285715 Engine time: 0.07835651561617851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 73.7469318700023,
    "estimated_duration": 3600.0802891319468,
    "input_throughput": 6813.69609284871,
    "output_throughput": 5892.6508011617525,
    "total_throughput": 12706.346894010463,
    "itl": 97.33531512793516,
    "ttft": 1963404.0265112245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.544908867618064,
    "arrivals": 757465,
    "finished_requests": 98940,
    "scheduler_time": 214.94734375108249
}
#Debug simulation 
Total elapsed time: 73.74710885016248. Arrivals time: 0.5327602284960449 Scheduler time: 73.01493544410914 Scheduler overhead time: 0.07718470925465226 Adapter cache time: 0.01454529445618391 Engine time: 0.07627883739769459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.47571869986132,
    "estimated_duration": 3600.055451922284,
    "input_throughput": 6674.163306893054,
    "output_throughput": 5758.493244577813,
    "total_throughput": 12432.656551470867,
    "itl": 91.46608029332754,
    "ttft": 1983554.0329444113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2761124552227665,
    "arrivals": 757465,
    "finished_requests": 96781,
    "scheduler_time": 220.43928765448067
}
#Debug simulation 
Total elapsed time: 71.4758884049952. Arrivals time: 0.5346604897640646 Scheduler time: 70.73258829116821 Scheduler overhead time: 0.08086495334282517 Adapter cache time: 0.016249495092779398 Engine time: 0.07902936590835452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 71.22669997205958,
    "estimated_duration": 3600.032979945473,
    "input_throughput": 6875.290348138668,
    "output_throughput": 5962.671208730184,
    "total_throughput": 12837.961556868853,
    "itl": 99.97013684326066,
    "ttft": 1960657.7550424396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9771103005530288,
    "arrivals": 749804,
    "finished_requests": 100095,
    "scheduler_time": 212.3040949771616
}
#Debug simulation 
Total elapsed time: 71.2270907331258. Arrivals time: 0.5920944255776703 Scheduler time: 70.43941258825362 Scheduler overhead time: 0.07619370333850384 Adapter cache time: 0.014730690512806177 Engine time: 0.07435763208195567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 70.92113893898204,
    "estimated_duration": 3600.032876441822,
    "input_throughput": 6798.503747051968,
    "output_throughput": 5892.809518164086,
    "total_throughput": 12691.313265216053,
    "itl": 97.2565434822742,
    "ttft": 1964989.7717496955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1681158301187704,
    "arrivals": 749804,
    "finished_requests": 98937,
    "scheduler_time": 214.94347245997142
}
#Debug simulation 
Total elapsed time: 70.9213119642809. Arrivals time: 0.532813508529216 Scheduler time: 70.19002822833136 Scheduler overhead time: 0.07669501472264528 Adapter cache time: 0.015072022564709187 Engine time: 0.07594549842178822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 67.60513638472185,
    "estimated_duration": 3600.0287606018564,
    "input_throughput": 6609.641917422333,
    "output_throughput": 5738.026936358845,
    "total_throughput": 12347.668853781179,
    "itl": 91.0693139021215,
    "ttft": 1981440.8627720135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.356462367898794,
    "arrivals": 749804,
    "finished_requests": 96186,
    "scheduler_time": 220.7682978514644
}
#Debug simulation 
Total elapsed time: 67.60531333461404. Arrivals time: 0.5825977986678481 Scheduler time: 66.81872124969959 Scheduler overhead time: 0.07925816811621189 Adapter cache time: 0.015427601989358664 Engine time: 0.07748566661030054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 68.74337646597996,
    "estimated_duration": 3600.0309678151452,
    "input_throughput": 6770.811478544931,
    "output_throughput": 5881.059965672813,
    "total_throughput": 12651.871444217742,
    "itl": 97.1363602733152,
    "ttft": 1963557.7918959616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.104512563101012,
    "arrivals": 749804,
    "finished_requests": 98606,
    "scheduler_time": 215.40829189110707
}
#Debug simulation 
Total elapsed time: 68.74355091899633. Arrivals time: 0.5391051610931754 Scheduler time: 68.00557929463685 Scheduler overhead time: 0.07798992982134223 Adapter cache time: 0.015335528645664454 Engine time: 0.07498094672337174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 65.78029021807015,
    "estimated_duration": 3600.069749336338,
    "input_throughput": 6606.855604501751,
    "output_throughput": 5738.796311879408,
    "total_throughput": 12345.651916381159,
    "itl": 91.24003557337849,
    "ttft": 1978807.41141711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 330,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4554919443233056,
    "arrivals": 749804,
    "finished_requests": 96084,
    "scheduler_time": 220.67654253928512
}
#Debug simulation 
Total elapsed time: 65.78046739893034. Arrivals time: 0.5776321170851588 Scheduler time: 64.99789933674037 Scheduler overhead time: 0.0793179152533412 Adapter cache time: 0.01566988555714488 Engine time: 0.07743909070268273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 68.65256291395053,
    "estimated_duration": 3600.0400417940778,
    "input_throughput": 6802.74489052498,
    "output_throughput": 5907.843733149387,
    "total_throughput": 12710.588623674368,
    "itl": 97.53971461421102,
    "ttft": 1966100.0626935968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8321853099437364,
    "arrivals": 749804,
    "finished_requests": 99167,
    "scheduler_time": 214.31588751026732
}
#Debug simulation 
Total elapsed time: 68.65273768501356. Arrivals time: 0.5201218212023377 Scheduler time: 67.93473278265446 Scheduler overhead time: 0.07695830939337611 Adapter cache time: 0.014955826103687286 Engine time: 0.07525315973907709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 68.22130097495392,
    "estimated_duration": 3600.0508316881055,
    "input_throughput": 6605.9683909653095,
    "output_throughput": 5731.297407910478,
    "total_throughput": 12337.265798875787,
    "itl": 90.89095039568197,
    "ttft": 1980609.3397203959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.174537044875331,
    "arrivals": 749804,
    "finished_requests": 96058,
    "scheduler_time": 221.048290679765
}
#Debug simulation 
Total elapsed time: 68.22146884026006. Arrivals time: 0.5140216718427837 Scheduler time: 67.50317769264802 Scheduler overhead time: 0.07925586774945259 Adapter cache time: 0.015303641557693481 Engine time: 0.0776186571456492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 76.6414837660268,
    "estimated_duration": 3600.0324403132095,
    "input_throughput": 6825.519605001721,
    "output_throughput": 5961.219060051828,
    "total_throughput": 12786.73866505355,
    "itl": 100.18713864977323,
    "ttft": 1963525.958038262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2283818437671337,
    "arrivals": 746017,
    "finished_requests": 99678,
    "scheduler_time": 212.38348863735985
}
#Debug simulation 
Total elapsed time: 76.64166239602491. Arrivals time: 0.6233610301278532 Scheduler time: 75.82110671699047 Scheduler overhead time: 0.07679744437336922 Adapter cache time: 0.014862553682178259 Engine time: 0.07534493273124099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 76.61336175817996,
    "estimated_duration": 3600.0638581716917,
    "input_throughput": 6755.650165703286,
    "output_throughput": 5902.175860511263,
    "total_throughput": 12657.826026214549,
    "itl": 97.73686513269013,
    "ttft": 1966258.7546083457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5400642741331865,
    "arrivals": 746017,
    "finished_requests": 98625,
    "scheduler_time": 214.55878244771128
}
#Debug simulation 
Total elapsed time: 76.61354694701731. Arrivals time: 0.6286631883122027 Scheduler time: 75.78486378630623 Scheduler overhead time: 0.07810843549668789 Adapter cache time: 0.015285572037100792 Engine time: 0.07577903755009174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 73.0357169220224,
    "estimated_duration": 3600.0487383191808,
    "input_throughput": 6588.766631830247,
    "output_throughput": 5748.268010855264,
    "total_throughput": 12337.034642685512,
    "itl": 91.43784808139107,
    "ttft": 1979825.5407814677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6122519922629137,
    "arrivals": 746017,
    "finished_requests": 96140,
    "scheduler_time": 220.527099020762
}
#Debug simulation 
Total elapsed time: 73.03588560409844. Arrivals time: 0.5988162076100707 Scheduler time: 72.22885564994067 Scheduler overhead time: 0.08159110322594643 Adapter cache time: 0.015602776315063238 Engine time: 0.07865481358021498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 75.94239831203595,
    "estimated_duration": 3600.0116778092583,
    "input_throughput": 6760.38251487291,
    "output_throughput": 5903.109740169562,
    "total_throughput": 12663.492255042473,
    "itl": 97.66993155980904,
    "ttft": 1966272.9812033277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 351,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.392072336231354,
    "arrivals": 746017,
    "finished_requests": 98605,
    "scheduler_time": 214.70006436741727
}
#Debug simulation 
Total elapsed time: 75.94257538812235. Arrivals time: 0.628598980139941 Scheduler time: 75.11356780305505 Scheduler overhead time: 0.07768678572028875 Adapter cache time: 0.01550056179985404 Engine time: 0.07641530456021428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 72.68180546397343,
    "estimated_duration": 3600.091459810955,
    "input_throughput": 6587.921241657959,
    "output_throughput": 5750.501683389762,
    "total_throughput": 12338.42292504772,
    "itl": 91.5214331448296,
    "ttft": 1981412.6981128876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5585112503636758,
    "arrivals": 746017,
    "finished_requests": 96176,
    "scheduler_time": 220.42467650155373
}
#Debug simulation 
Total elapsed time: 72.68199038691819. Arrivals time: 0.5827976297587156 Scheduler time: 71.8913617534563 Scheduler overhead time: 0.08087881002575159 Adapter cache time: 0.01554836193099618 Engine time: 0.07879400439560413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 69.1563115012832,
    "estimated_duration": 3600.084067447195,
    "input_throughput": 6751.5573371694645,
    "output_throughput": 5897.867550369762,
    "total_throughput": 12649.424887539228,
    "itl": 97.64208750065661,
    "ttft": 1974276.836884841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3173633014270942,
    "arrivals": 746017,
    "finished_requests": 98528,
    "scheduler_time": 214.71976238712446
}
#Debug simulation 
Total elapsed time: 69.15647983318195. Arrivals time: 0.5996020152233541 Scheduler time: 68.35871486924589 Scheduler overhead time: 0.07678097439929843 Adapter cache time: 0.015421348623931408 Engine time: 0.0752273490652442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 72.19279170408845,
    "estimated_duration": 3600.023412345006,
    "input_throughput": 6589.92880953143,
    "output_throughput": 5752.825920237444,
    "total_throughput": 12342.754729768872,
    "itl": 91.48030506623627,
    "ttft": 1979154.360997342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 351,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5818009190075233,
    "arrivals": 746017,
    "finished_requests": 96203,
    "scheduler_time": 220.63526685211502
}
#Debug simulation 
Total elapsed time: 72.19296033494174. Arrivals time: 0.5985725210048258 Scheduler time: 71.38751060282812 Scheduler overhead time: 0.08031106041744351 Adapter cache time: 0.015751166734844446 Engine time: 0.07841868372634053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 76.50115105090663,
    "estimated_duration": 3600.027300335205,
    "input_throughput": 6895.258543647343,
    "output_throughput": 5982.088524160574,
    "total_throughput": 12877.347067807916,
    "itl": 99.82414781736959,
    "ttft": 1954442.3214533655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8118000747542806,
    "arrivals": 744176,
    "finished_requests": 100082,
    "scheduler_time": 212.41877903562576
}
#Debug simulation 
Total elapsed time: 76.50146230775863. Arrivals time: 0.5310590667650104 Scheduler time: 75.77203901577741 Scheduler overhead time: 0.07790985377505422 Adapter cache time: 0.014776722062379122 Engine time: 0.07501716865226626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 87.0327469711192,
    "estimated_duration": 3600.0481166306017,
    "input_throughput": 6820.42606224406,
    "output_throughput": 5916.793417732437,
    "total_throughput": 12737.219479976497,
    "itl": 97.12902031511453,
    "ttft": 1959696.5564796443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.018225875734353,
    "arrivals": 744176,
    "finished_requests": 98964,
    "scheduler_time": 214.89290589548847
}
#Debug simulation 
Total elapsed time: 87.03293223120272. Arrivals time: 0.6124539682641625 Scheduler time: 86.19469022052363 Scheduler overhead time: 0.08887859899550676 Adapter cache time: 0.01707673165947199 Engine time: 0.08680625166743994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 83.4693383560516,
    "estimated_duration": 3600.095879809141,
    "input_throughput": 6636.495470578032,
    "output_throughput": 5752.7245638518825,
    "total_throughput": 12389.220034429914,
    "itl": 90.95933356135234,
    "ttft": 1975587.8197082586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1214833508664808,
    "arrivals": 744176,
    "finished_requests": 96244,
    "scheduler_time": 220.96686801038337
}
#Debug simulation 
Total elapsed time: 83.46954364096746. Arrivals time: 0.650045532733202 Scheduler time: 82.57760464446619 Scheduler overhead time: 0.09463029261678457 Adapter cache time: 0.018347835168242455 Engine time: 0.09334761323407292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 92.72872777096927,
    "estimated_duration": 3600.05751598511,
    "input_throughput": 6797.021406282779,
    "output_throughput": 5903.279851956649,
    "total_throughput": 12700.301258239428,
    "itl": 97.27861453617427,
    "ttft": 1959460.2283303656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7767038594279403,
    "arrivals": 744176,
    "finished_requests": 98613,
    "scheduler_time": 214.5188531012043
}
#Debug simulation 
Total elapsed time: 92.72891260311007. Arrivals time: 0.6580527066253126 Scheduler time: 91.83794534485787 Scheduler overhead time: 0.09174584923312068 Adapter cache time: 0.017555248457938433 Engine time: 0.0896268398500979 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 80.66532864468172,
    "estimated_duration": 3600.0139428840002,
    "input_throughput": 6633.167365143576,
    "output_throughput": 5749.392454691092,
    "total_throughput": 12382.559819834669,
    "itl": 90.82606184492289,
    "ttft": 1976216.0709536043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.107812120695605,
    "arrivals": 744176,
    "finished_requests": 96195,
    "scheduler_time": 221.10116119339804
}
#Debug simulation 
Total elapsed time: 80.66551974974573. Arrivals time: 0.6857194826006889 Scheduler time: 79.74851773818955 Scheduler overhead time: 0.09050457878038287 Adapter cache time: 0.017046677879989147 Engine time: 0.0892020184546709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 86.10537310922518,
    "estimated_duration": 3600.101390484751,
    "input_throughput": 6797.022457388384,
    "output_throughput": 5903.939554640726,
    "total_throughput": 12700.962012029111,
    "itl": 96.93231817905931,
    "ttft": 1959238.555899428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.761962179597461,
    "arrivals": 744176,
    "finished_requests": 98600,
    "scheduler_time": 215.0347413318643
}
#Debug simulation 
Total elapsed time: 86.10556827299297. Arrivals time: 0.6541546322405338 Scheduler time: 85.222740476951 Scheduler overhead time: 0.0895481538027525 Adapter cache time: 0.017195158172398806 Engine time: 0.08858370129019022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.63371323794127,
    "estimated_duration": 3600.0292101256005,
    "input_throughput": 6633.81756259883,
    "output_throughput": 5752.640545735312,
    "total_throughput": 12386.458108334142,
    "itl": 90.93845108108374,
    "ttft": 1976023.3146857882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0984903512336377,
    "arrivals": 744176,
    "finished_requests": 96251,
    "scheduler_time": 220.99028175984253
}
#Debug simulation 
Total elapsed time: 79.63390868995339. Arrivals time: 0.6076668677851558 Scheduler time: 78.79985936777666 Scheduler overhead time: 0.08908984158188105 Adapter cache time: 0.016939316410571337 Engine time: 0.08633495774120092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 95.37699354672804,
    "estimated_duration": 3600.081937322308,
    "input_throughput": 6872.198030693054,
    "output_throughput": 6014.359499857551,
    "total_throughput": 12886.557530550604,
    "itl": 99.91907544880179,
    "ttft": 1953990.092371507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.322481806389986,
    "arrivals": 743225,
    "finished_requests": 100195,
    "scheduler_time": 212.86435851782863
}
#Debug simulation 
Total elapsed time: 95.37718313978985. Arrivals time: 0.6705894032493234 Scheduler time: 94.473414295353 Scheduler overhead time: 0.09252332895994186 Adapter cache time: 0.016965187154710293 Engine time: 0.09043681947514415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 90.87714015273377,
    "estimated_duration": 3600.034540898783,
    "input_throughput": 6760.022084100396,
    "output_throughput": 5926.648413399175,
    "total_throughput": 12686.67049749957,
    "itl": 97.24313289075621,
    "ttft": 1959086.0201148305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4394893727963802,
    "arrivals": 743225,
    "finished_requests": 98735,
    "scheduler_time": 215.2884689783716
}
#Debug simulation 
Total elapsed time: 90.87732187379152. Arrivals time: 0.756253276951611 Scheduler time: 89.88907828601077 Scheduler overhead time: 0.09087842470034957 Adapter cache time: 0.016794434282928705 Engine time: 0.09078443050384521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 81.52518291864544,
    "estimated_duration": 3600.0075714574555,
    "input_throughput": 6606.438605453101,
    "output_throughput": 5788.857547197602,
    "total_throughput": 12395.296152650704,
    "itl": 90.9361525961599,
    "ttft": 1975254.6718965797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.507091913274492,
    "arrivals": 743225,
    "finished_requests": 96320,
    "scheduler_time": 221.646107457857
}
#Debug simulation 
Total elapsed time: 81.52538264589384. Arrivals time: 0.7051833663135767 Scheduler time: 80.58385506598279 Scheduler overhead time: 0.09320247545838356 Adapter cache time: 0.016851671505719423 Engine time: 0.09154260531067848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 92.82547733001411,
    "estimated_duration": 3600.095733518074,
    "input_throughput": 6785.656773667948,
    "output_throughput": 5943.893602820651,
    "total_throughput": 12729.550376488598,
    "itl": 97.37417153963028,
    "ttft": 1958863.9154253136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3670185513794404,
    "arrivals": 743225,
    "finished_requests": 99012,
    "scheduler_time": 214.95422822017002
}
#Debug simulation 
Total elapsed time: 92.82566454308107. Arrivals time: 0.814217135310173 Scheduler time: 91.77222109306604 Scheduler overhead time: 0.09533626493066549 Adapter cache time: 0.01722923992201686 Engine time: 0.09299719100818038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 80.81223852187395,
    "estimated_duration": 3600.0407814913187,
    "input_throughput": 6623.540522816575,
    "output_throughput": 5807.045605561125,
    "total_throughput": 12430.5861283777,
    "itl": 90.91405031304151,
    "ttft": 1976818.320163563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5010840186383598,
    "arrivals": 743225,
    "finished_requests": 96602,
    "scheduler_time": 221.8570651870116
}
#Debug simulation 
Total elapsed time: 80.81243371870369. Arrivals time: 0.7286850837990642 Scheduler time: 79.84442208521068 Scheduler overhead time: 0.09407499805092812 Adapter cache time: 0.017217153683304787 Engine time: 0.09292007610201836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 106.62970797112212,
    "estimated_duration": 3600.0576519151227,
    "input_throughput": 6494.7943229635985,
    "output_throughput": 5686.1528284443775,
    "total_throughput": 12180.947151407976,
    "itl": 89.81027988658302,
    "ttft": 1969064.3497202704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1235700855404112,
    "arrivals": 743225,
    "finished_requests": 94798,
    "scheduler_time": 223.16265186752076
}
#Debug simulation 
Total elapsed time: 106.62987383594736. Arrivals time: 0.695825164206326 Scheduler time: 105.67184713250026 Scheduler overhead time: 0.10460884915664792 Adapter cache time: 0.01828497229143977 Engine time: 0.10231392877176404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 85.76265281205997,
    "estimated_duration": 3600.0368403458397,
    "input_throughput": 6591.978374788029,
    "output_throughput": 5773.220642376368,
    "total_throughput": 12365.199017164397,
    "itl": 90.9602464363393,
    "ttft": 1973855.8217426708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4353349700383866,
    "arrivals": 743225,
    "finished_requests": 96153,
    "scheduler_time": 221.52858957603758
}
#Debug simulation 
Total elapsed time: 85.7628318280913. Arrivals time: 0.701354967430234 Scheduler time: 84.8218374918215 Scheduler overhead time: 0.0950543787330389 Adapter cache time: 0.017138578463345766 Engine time: 0.09250072902068496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 93.41082823229954,
    "estimated_duration": 3600.1008265294936,
    "input_throughput": 6914.965774444277,
    "output_throughput": 6012.957703984088,
    "total_throughput": 12927.923478428364,
    "itl": 99.52673025119375,
    "ttft": 1956377.2107948822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.137334353495388,
    "arrivals": 742746,
    "finished_requests": 100516,
    "scheduler_time": 213.22476850813342
}
#Debug simulation 
Total elapsed time: 93.41100918129086. Arrivals time: 0.6896072663366795 Scheduler time: 92.48498007748276 Scheduler overhead time: 0.0933496574871242 Adapter cache time: 0.01718423841521144 Engine time: 0.09134061634540558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 95.30082380119711,
    "estimated_duration": 3600.021575297679,
    "input_throughput": 6814.008607148865,
    "output_throughput": 5934.837765030151,
    "total_throughput": 12748.846372179016,
    "itl": 96.89650603812905,
    "ttft": 1961025.785783915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2471312298486017,
    "arrivals": 742746,
    "finished_requests": 99165,
    "scheduler_time": 215.39393424309043
}
#Debug simulation 
Total elapsed time: 95.3010048083961. Arrivals time: 0.6767906895838678 Scheduler time: 94.37799594784155 Scheduler overhead time: 0.097981839440763 Adapter cache time: 0.01780706411227584 Engine time: 0.09600595384836197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 85.46959235193208,
    "estimated_duration": 3600.005276461392,
    "input_throughput": 6645.974147992777,
    "output_throughput": 5783.167079261718,
    "total_throughput": 12429.141227254495,
    "itl": 90.71599939646673,
    "ttft": 1980324.3351497853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2992907705530545,
    "arrivals": 742746,
    "finished_requests": 96715,
    "scheduler_time": 221.86459793318585
}
#Debug simulation 
Total elapsed time: 85.46978018106893. Arrivals time: 0.6406954010017216 Scheduler time: 84.58262921404094 Scheduler overhead time: 0.09659356297925115 Adapter cache time: 0.017519278917461634 Engine time: 0.0960849947296083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 97.274897929281,
    "estimated_duration": 3600.0752255363427,
    "input_throughput": 6834.854678997485,
    "output_throughput": 5937.111771550738,
    "total_throughput": 12771.966450548223,
    "itl": 96.89708142013822,
    "ttft": 1962426.4553872787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1602304924558833,
    "arrivals": 742746,
    "finished_requests": 99373,
    "scheduler_time": 215.2995319049421
}
#Debug simulation 
Total elapsed time: 97.2750971922651. Arrivals time: 0.6936405040323734 Scheduler time: 96.33279499737546 Scheduler overhead time: 0.0989943016320467 Adapter cache time: 0.01782977720722556 Engine time: 0.0963612524792552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 85.07113470695913,
    "estimated_duration": 3600.027469698509,
    "input_throughput": 6625.799997575468,
    "output_throughput": 5757.330235520615,
    "total_throughput": 12383.130233096084,
    "itl": 90.60151818662222,
    "ttft": 1980282.3204871805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2492277653655053,
    "arrivals": 742746,
    "finished_requests": 96393,
    "scheduler_time": 221.56973715076055
}
#Debug simulation 
Total elapsed time: 85.07131727598608. Arrivals time: 0.6547434362582862 Scheduler time: 84.1718228617683 Scheduler overhead time: 0.09707710845395923 Adapter cache time: 0.017389371991157532 Engine time: 0.0946532879024744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 91.58209881884977,
    "estimated_duration": 3600.004225412727,
    "input_throughput": 6857.590006625531,
    "output_throughput": 5956.687730704403,
    "total_throughput": 12814.277737329934,
    "itl": 96.79370726288263,
    "ttft": 1960476.125924885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1108022436592702,
    "arrivals": 742746,
    "finished_requests": 99692,
    "scheduler_time": 215.64973081526557
}
#Debug simulation 
Total elapsed time: 91.58228616416454. Arrivals time: 0.6808417649008334 Scheduler time: 90.66313481563702 Scheduler overhead time: 0.09399919677525759 Adapter cache time: 0.017355446238070726 Engine time: 0.09222284099087119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 88.88686113990843,
    "estimated_duration": 3600.037063025047,
    "input_throughput": 6610.349722345187,
    "output_throughput": 5744.467525737837,
    "total_throughput": 12354.817248083024,
    "itl": 90.79754331253369,
    "ttft": 1980252.5172533223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1119867457449442,
    "arrivals": 742746,
    "finished_requests": 96182,
    "scheduler_time": 221.20617412821673
}
#Debug simulation 
Total elapsed time: 88.88705546082929. Arrivals time: 0.6650730590336025 Scheduler time: 87.97637917939574 Scheduler overhead time: 0.09725326579064131 Adapter cache time: 0.01723521715030074 Engine time: 0.09558917349204421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 103.40046676294878,
    "estimated_duration": 3600.050550087284,
    "input_throughput": 6855.850121160547,
    "output_throughput": 5970.911158311051,
    "total_throughput": 12826.761279471597,
    "itl": 99.83385972174626,
    "ttft": 1936001.5354475088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4348927599331347,
    "arrivals": 679344,
    "finished_requests": 99927,
    "scheduler_time": 211.94652315806238
}
#Debug simulation 
Total elapsed time: 103.40064466279. Arrivals time: 0.8091895994730294 Scheduler time: 102.34018478402868 Scheduler overhead time: 0.10068421671167016 Adapter cache time: 0.018053857143968344 Engine time: 0.09759216802194715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.71121207764372,
    "estimated_duration": 3600.02758909928,
    "input_throughput": 6775.589740994988,
    "output_throughput": 5928.4634552869875,
    "total_throughput": 12704.053196281975,
    "itl": 97.72065198203791,
    "ttft": 1940853.9086355004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6757039252808337,
    "arrivals": 679344,
    "finished_requests": 98840,
    "scheduler_time": 213.81025370497005
}
#Debug simulation 
Total elapsed time: 78.71139340102673. Arrivals time: 0.6749392696656287 Scheduler time: 77.80434130877256 Scheduler overhead time: 0.09196861926466227 Adapter cache time: 0.017121916636824608 Engine time: 0.08987362822517753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 68.30307273194194,
    "estimated_duration": 3600.047959984324,
    "input_throughput": 6604.069519147052,
    "output_throughput": 5753.635571035614,
    "total_throughput": 12357.705090182666,
    "itl": 91.17188242750854,
    "ttft": 1962974.2485010782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.195876817624093,
    "arrivals": 679344,
    "finished_requests": 96228,
    "scheduler_time": 220.18432542344055
}
#Debug simulation 
Total elapsed time: 68.30327137792483. Arrivals time: 0.7017538789659739 Scheduler time: 67.37549699423835 Scheduler overhead time: 0.08803706848993897 Adapter cache time: 0.017402491997927427 Engine time: 0.0873727505095303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 84.83105517597869,
    "estimated_duration": 3600.038574483897,
    "input_throughput": 6819.155820717668,
    "output_throughput": 5948.690425649158,
    "total_throughput": 12767.846246366826,
    "itl": 97.69152478997748,
    "ttft": 1942754.03919687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7669957902980948,
    "arrivals": 679344,
    "finished_requests": 99457,
    "scheduler_time": 213.9281663788878
}
#Debug simulation 
Total elapsed time: 84.83123224182054. Arrivals time: 0.764805088751018 Scheduler time: 83.82588765816763 Scheduler overhead time: 0.0951523301191628 Adapter cache time: 0.017775921616703272 Engine time: 0.0930110877379775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 66.77143177622929,
    "estimated_duration": 3600.0920720813415,
    "input_throughput": 6608.5887592995,
    "output_throughput": 5768.524133326882,
    "total_throughput": 12377.112892626383,
    "itl": 91.25717592348319,
    "ttft": 1959840.980072218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.093784869960517,
    "arrivals": 679344,
    "finished_requests": 96383,
    "scheduler_time": 220.46473660218516
}
#Debug simulation 
Total elapsed time: 66.7716066702269. Arrivals time: 0.7176346136257052 Scheduler time: 65.82376201590523 Scheduler overhead time: 0.09034663019701838 Adapter cache time: 0.017272724770009518 Engine time: 0.08899939712136984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 89.99968467326835,
    "estimated_duration": 3600.1017459173972,
    "input_throughput": 6776.507088352651,
    "output_throughput": 5910.048799073076,
    "total_throughput": 12686.555887425728,
    "itl": 97.53710203957367,
    "ttft": 1943108.5947785114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3853108441038016,
    "arrivals": 679344,
    "finished_requests": 98867,
    "scheduler_time": 214.0847992784733
}
#Debug simulation 
Total elapsed time: 89.99987496528774. Arrivals time: 0.7038694187067449 Scheduler time: 89.06088484171778 Scheduler overhead time: 0.09306317055597901 Adapter cache time: 0.01750428369268775 Engine time: 0.09075461141765118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 68.8770529772155,
    "estimated_duration": 3600.0660257268046,
    "input_throughput": 6598.123153923116,
    "output_throughput": 5757.598569547165,
    "total_throughput": 12355.721723470282,
    "itl": 91.42343137759302,
    "ttft": 1959371.0020898944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7822900629229892,
    "arrivals": 679344,
    "finished_requests": 96240,
    "scheduler_time": 219.78625854884672
}
#Debug simulation 
Total elapsed time: 68.87723919237033. Arrivals time: 0.7218579198233783 Scheduler time: 67.92350294487551 Scheduler overhead time: 0.09152963245287538 Adapter cache time: 0.016876437701284885 Engine time: 0.08950436534360051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 80.32613389892504,
    "estimated_duration": 3600.060544559768,
    "input_throughput": 6876.77573573346,
    "output_throughput": 5988.993166401468,
    "total_throughput": 12865.768902134929,
    "itl": 100.06506155097348,
    "ttft": 1928696.180285053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4613423960609344,
    "arrivals": 634153,
    "finished_requests": 100205,
    "scheduler_time": 211.15202899319758
}
#Debug simulation 
Total elapsed time: 80.3263158542104. Arrivals time: 0.6413674517534673 Scheduler time: 79.45233014784753 Scheduler overhead time: 0.09175698552280664 Adapter cache time: 0.017220973502844572 Engine time: 0.09024466061964631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.63068238319829,
    "estimated_duration": 3600.019649524588,
    "input_throughput": 6798.0362282833285,
    "output_throughput": 5924.887382994357,
    "total_throughput": 12722.923611277685,
    "itl": 97.47346565266807,
    "ttft": 1934276.551116108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6360121786640975,
    "arrivals": 634153,
    "finished_requests": 99090,
    "scheduler_time": 213.5096227568871
}
#Debug simulation 
Total elapsed time: 74.63087273435667. Arrivals time: 0.6397332618944347 Scheduler time: 73.76838268619031 Scheduler overhead time: 0.08727703290060163 Adapter cache time: 0.01634574867784977 Engine time: 0.08656927151605487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.08302881382406,
    "estimated_duration": 3600.0406409347147,
    "input_throughput": 6597.821905097416,
    "output_throughput": 5753.768933736778,
    "total_throughput": 12351.590838834194,
    "itl": 91.18998002044805,
    "ttft": 1947752.1394977618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.074735717019072,
    "arrivals": 634153,
    "finished_requests": 96147,
    "scheduler_time": 219.76105742321286
}
#Debug simulation 
Total elapsed time: 75.08320358581841. Arrivals time: 0.6162896142341197 Scheduler time: 74.22969850664958 Scheduler overhead time: 0.09337677294388413 Adapter cache time: 0.017833294346928596 Engine time: 0.09163695340976119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 89.36971387919039,
    "estimated_duration": 3600.0596966117696,
    "input_throughput": 6798.657817545475,
    "output_throughput": 5928.65557759713,
    "total_throughput": 12727.313395142606,
    "itl": 97.47368239214676,
    "ttft": 1928190.164506606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2265722906868897,
    "arrivals": 634153,
    "finished_requests": 99191,
    "scheduler_time": 213.56861825021767
}
#Debug simulation 
Total elapsed time: 89.36991050699726. Arrivals time: 0.6908177933655679 Scheduler time: 88.43644406273961 Scheduler overhead time: 0.09623169060796499 Adapter cache time: 0.017398401629179716 Engine time: 0.09481705259531736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 106.92425693012774,
    "estimated_duration": 3600.08901575372,
    "input_throughput": 6566.633740596716,
    "output_throughput": 5723.276538395885,
    "total_throughput": 12289.9102789926,
    "itl": 90.02203720018515,
    "ttft": 1947960.9313051112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1698171831807134,
    "arrivals": 634153,
    "finished_requests": 95802,
    "scheduler_time": 220.94205998946043
}
#Debug simulation 
Total elapsed time: 106.92444745730609. Arrivals time: 0.6950123771093786 Scheduler time: 105.96107651758939 Scheduler overhead time: 0.10733596328645945 Adapter cache time: 0.019786435179412365 Engine time: 0.10398377059027553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 89.36864286009222,
    "estimated_duration": 3600.0474791138545,
    "input_throughput": 6784.175525932721,
    "output_throughput": 5915.888088576637,
    "total_throughput": 12700.063614509358,
    "itl": 97.1763984109408,
    "ttft": 1926738.1665084208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1363379274215522,
    "arrivals": 634153,
    "finished_requests": 98979,
    "scheduler_time": 213.83456201645353
}
#Debug simulation 
Total elapsed time: 89.36882479581982. Arrivals time: 0.6668180618435144 Scheduler time: 88.46685892250389 Scheduler overhead time: 0.09368589799851179 Adapter cache time: 0.017059538513422012 Engine time: 0.09137119213119149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.68841066304594,
    "estimated_duration": 3600.013844214748,
    "input_throughput": 6603.374883738902,
    "output_throughput": 5761.508676787946,
    "total_throughput": 12364.883560526849,
    "itl": 91.41469783458093,
    "ttft": 1945118.9821561163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4806865078955922,
    "arrivals": 634153,
    "finished_requests": 96379,
    "scheduler_time": 219.50548238021415
}
#Debug simulation 
Total elapsed time: 77.68859437527135. Arrivals time: 0.7478806134313345 Scheduler time: 76.69925790512934 Scheduler overhead time: 0.09494014969095588 Adapter cache time: 0.017891888041049242 Engine time: 0.09436161676421762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 88.4140927628614,
    "estimated_duration": 3600.035493234075,
    "input_throughput": 6871.096422935086,
    "output_throughput": 5997.995308819034,
    "total_throughput": 12869.091731754119,
    "itl": 100.71190150295718,
    "ttft": 1916125.534704234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1770088076870875,
    "arrivals": 626696,
    "finished_requests": 100339,
    "scheduler_time": 210.32736753803277
}
#Debug simulation 
Total elapsed time: 88.4142938121222. Arrivals time: 0.7268752758391201 Scheduler time: 87.44891546666622 Scheduler overhead time: 0.09531774744391441 Adapter cache time: 0.017739221919327974 Engine time: 0.09292490687221289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 81.88823304418474,
    "estimated_duration": 3600.077515013361,
    "input_throughput": 6802.161313993018,
    "output_throughput": 5935.438031789351,
    "total_throughput": 12737.599345782368,
    "itl": 97.97078181966855,
    "ttft": 1923644.0868145374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3084773281123503,
    "arrivals": 626696,
    "finished_requests": 99299,
    "scheduler_time": 212.7327305926104
}
#Debug simulation 
Total elapsed time: 81.88840467482805. Arrivals time: 0.5163518493063748 Scheduler time: 81.15876854769886 Scheduler overhead time: 0.08326153317466378 Adapter cache time: 0.015422413125634193 Engine time: 0.08285369910299778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 72.92212943034247,
    "estimated_duration": 3600.0523324336846,
    "input_throughput": 6623.477882578597,
    "output_throughput": 5772.097481136486,
    "total_throughput": 12395.575363715083,
    "itl": 91.73390926132562,
    "ttft": 1943086.775056095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.482090803957548,
    "arrivals": 626696,
    "finished_requests": 96650,
    "scheduler_time": 218.8606169474155
}
#Debug simulation 
Total elapsed time: 72.92230762727559. Arrivals time: 0.5764370476827025 Scheduler time: 72.13203783193603 Scheduler overhead time: 0.0834860042668879 Adapter cache time: 0.0155954216606915 Engine time: 0.08238636003807187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 77.25903305783868,
    "estimated_duration": 3600.082398942067,
    "input_throughput": 6794.403652313076,
    "output_throughput": 5932.646432280645,
    "total_throughput": 12727.050084593722,
    "itl": 97.90496829243826,
    "ttft": 1922714.7117136957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.258491895389742,
    "arrivals": 626696,
    "finished_requests": 99194,
    "scheduler_time": 212.89302729403167
}
#Debug simulation 
Total elapsed time: 77.25921324687079. Arrivals time: 0.6594950119033456 Scheduler time: 76.39319298882037 Scheduler overhead time: 0.08047358551993966 Adapter cache time: 0.01478846650570631 Engine time: 0.08022670075297356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 65.43965574167669,
    "estimated_duration": 3600.076017961408,
    "input_throughput": 6619.298004016606,
    "output_throughput": 5768.615411560074,
    "total_throughput": 12387.91341557668,
    "itl": 91.70034681026948,
    "ttft": 1943413.206522221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4244696977129243,
    "arrivals": 626696,
    "finished_requests": 96570,
    "scheduler_time": 219.02995264703364
}
#Debug simulation 
Total elapsed time: 65.4398266589269. Arrivals time: 0.4855275978334248 Scheduler time: 64.75077716866508 Scheduler overhead time: 0.07900427049025893 Adapter cache time: 0.014698028098791838 Engine time: 0.07772591011598706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 76.58421845175326,
    "estimated_duration": 3600.019541836072,
    "input_throughput": 6771.617408378517,
    "output_throughput": 5913.49095542476,
    "total_throughput": 12685.108363803276,
    "itl": 97.60559076035781,
    "ttft": 1923669.7949169828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1682575321244046,
    "arrivals": 626696,
    "finished_requests": 98868,
    "scheduler_time": 213.61901124593055
}
#Debug simulation 
Total elapsed time: 76.5844037369825. Arrivals time: 0.5235312040895224 Scheduler time: 75.85685363318771 Scheduler overhead time: 0.07977601233869791 Adapter cache time: 0.014773660339415073 Engine time: 0.07853476470336318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.90703166509047,
    "estimated_duration": 3600.0448662815393,
    "input_throughput": 6621.776640418045,
    "output_throughput": 5771.781400452974,
    "total_throughput": 12393.558040871018,
    "itl": 91.7280202860751,
    "ttft": 1943045.0148238621,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4252433044090904,
    "arrivals": 626696,
    "finished_requests": 96638,
    "scheduler_time": 218.93229553741708
}
#Debug simulation 
Total elapsed time: 71.90720499912277. Arrivals time: 0.5605881456285715 Scheduler time: 71.13213578332216 Scheduler overhead time: 0.08426511334255338 Adapter cache time: 0.015247794333845377 Engine time: 0.08236709469929338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.50577652873471,
    "estimated_duration": 3600.0332697084327,
    "input_throughput": 6856.482190787955,
    "output_throughput": 5990.770191339569,
    "total_throughput": 12847.252382127524,
    "itl": 100.59040918929331,
    "ttft": 1917902.3620136816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.130721944463438,
    "arrivals": 623022,
    "finished_requests": 100173,
    "scheduler_time": 210.6805205433773
}
#Debug simulation 
Total elapsed time: 81.5059311138466. Arrivals time: 0.8265182133764029 Scheduler time: 80.47545237094164 Scheduler overhead time: 0.08063470665365458 Adapter cache time: 0.01465381495654583 Engine time: 0.07760219043120742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 66.02301434800029,
    "estimated_duration": 3600.1071283816723,
    "input_throughput": 6781.81818744244,
    "output_throughput": 5932.758175893823,
    "total_throughput": 12714.576363336264,
    "itl": 98.06281572603491,
    "ttft": 1929597.8560970926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5591217965725819,
    "arrivals": 623022,
    "finished_requests": 99103,
    "scheduler_time": 212.91676428228524
}
#Debug simulation 
Total elapsed time: 66.0231752526015. Arrivals time: 0.42558660404756665 Scheduler time: 65.41114445030689 Scheduler overhead time: 0.07233011396601796 Adapter cache time: 0.013225688599050045 Engine time: 0.07114301202818751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 59.72075746022165,
    "estimated_duration": 3600.044036287161,
    "input_throughput": 6607.388343097095,
    "output_throughput": 5762.953394702616,
    "total_throughput": 12370.34173779971,
    "itl": 91.46716916975703,
    "ttft": 1941757.7298604643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0505074968980685,
    "arrivals": 623022,
    "finished_requests": 96367,
    "scheduler_time": 219.31104666625268
}
#Debug simulation 
Total elapsed time: 59.720914548262954. Arrivals time: 0.4942961484193802 Scheduler time: 59.0330095468089 Scheduler overhead time: 0.07487191446125507 Adapter cache time: 0.014390397816896439 Engine time: 0.07310418179258704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 71.48617515526712,
    "estimated_duration": 3600.0621981511526,
    "input_throughput": 6768.148342690655,
    "output_throughput": 5924.029871192955,
    "total_throughput": 12692.17821388361,
    "itl": 98.03264294316412,
    "ttft": 1928363.6921542108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.381174614233894,
    "arrivals": 623022,
    "finished_requests": 98936,
    "scheduler_time": 213.2249323212867
}
#Debug simulation 
Total elapsed time: 71.48633947316557. Arrivals time: 0.46442363457754254 Scheduler time: 70.82441900111735 Scheduler overhead time: 0.07779674278572202 Adapter cache time: 0.01412966474890709 Engine time: 0.07511187018826604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 61.928987262770534,
    "estimated_duration": 3600.0586765373655,
    "input_throughput": 6603.220429413817,
    "output_throughput": 5759.758621474453,
    "total_throughput": 12362.97905088827,
    "itl": 91.30385910879099,
    "ttft": 1943369.5226928904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.613941928693097,
    "arrivals": 623022,
    "finished_requests": 96303,
    "scheduler_time": 219.57585134389643
}
#Debug simulation 
Total elapsed time: 61.92915000766516. Arrivals time: 0.4588780477643013 Scheduler time: 61.268649488221854 Scheduler overhead time: 0.0781914503313601 Adapter cache time: 0.014621667098253965 Engine time: 0.07685478497296572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 70.8888622210361,
    "estimated_duration": 3600.048794107401,
    "input_throughput": 6767.6263276983655,
    "output_throughput": 5914.275949495589,
    "total_throughput": 12681.902277193954,
    "itl": 97.57651614250459,
    "ttft": 1933209.293910176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.417230448806654,
    "arrivals": 623022,
    "finished_requests": 98862,
    "scheduler_time": 213.58788666922786
}
#Debug simulation 
Total elapsed time: 70.88899892568588. Arrivals time: 0.49736539786681533 Scheduler time: 70.18830514047295 Scheduler overhead time: 0.078948806039989 Adapter cache time: 0.015257813036441803 Engine time: 0.07750551449134946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 59.58392419340089,
    "estimated_duration": 3600.0953727132223,
    "input_throughput": 6595.42443791014,
    "output_throughput": 5751.650402637665,
    "total_throughput": 12347.074840547804,
    "itl": 91.24770689898288,
    "ttft": 1941301.5739048584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2562299852818346,
    "arrivals": 623022,
    "finished_requests": 96212,
    "scheduler_time": 219.68992516459957
}
#Debug simulation 
Total elapsed time: 59.584046591073275. Arrivals time: 0.4598486158065498 Scheduler time: 58.92555748159066 Scheduler overhead time: 0.07688507111743093 Adapter cache time: 0.01542798150330782 Engine time: 0.0749084590934217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.77477451693267,
    "estimated_duration": 3600.1072443567973,
    "input_throughput": 6864.899938394897,
    "output_throughput": 5981.4284793194465,
    "total_throughput": 12846.328417714343,
    "itl": 100.14637120792167,
    "ttft": 1912486.400875434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5473037134762835,
    "arrivals": 621138,
    "finished_requests": 100096,
    "scheduler_time": 210.901876339635
}
#Debug simulation 
Total elapsed time: 79.77491997601464. Arrivals time: 0.5002731666900218 Scheduler time: 79.07739433972165 Scheduler overhead time: 0.0772740887477994 Adapter cache time: 0.014421730302274227 Engine time: 0.07510105473920703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.65654708910733,
    "estimated_duration": 3600.0636680349357,
    "input_throughput": 6827.040093270231,
    "output_throughput": 5944.250428126687,
    "total_throughput": 12771.290521396919,
    "itl": 97.40946309876436,
    "ttft": 1926765.2460900026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.097335515846501,
    "arrivals": 621138,
    "finished_requests": 99487,
    "scheduler_time": 213.71219345866984
}
#Debug simulation 
Total elapsed time: 79.65668261796236. Arrivals time: 0.5846673524938524 Scheduler time: 78.85972497612238 Scheduler overhead time: 0.08389163110405207 Adapter cache time: 0.01576773775741458 Engine time: 0.081224475055933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 59.36438332591206,
    "estimated_duration": 3600.0869244495602,
    "input_throughput": 6619.696551811383,
    "output_throughput": 5764.956078990582,
    "total_throughput": 12384.652630801964,
    "itl": 91.40414447090066,
    "ttft": 1938114.297562667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.45927455676255,
    "arrivals": 621138,
    "finished_requests": 96539,
    "scheduler_time": 219.08805113123088
}
#Debug simulation 
Total elapsed time: 59.36451490689069. Arrivals time: 0.5165061745792627 Scheduler time: 58.65049464441836 Scheduler overhead time: 0.07724452205002308 Adapter cache time: 0.015135899186134338 Engine time: 0.07409908110275865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 74.91633548028767,
    "estimated_duration": 3600.1061138366804,
    "input_throughput": 6830.970594306107,
    "output_throughput": 5948.701322355394,
    "total_throughput": 12779.671916661502,
    "itl": 97.56221327554923,
    "ttft": 1927113.1653124418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9762769603636086,
    "arrivals": 621138,
    "finished_requests": 99609,
    "scheduler_time": 213.46332584946384
}
#Debug simulation 
Total elapsed time: 74.91648463532329. Arrivals time: 0.4853376066312194 Scheduler time: 74.22787261148915 Scheduler overhead time: 0.07916512619704008 Adapter cache time: 0.015237963292747736 Engine time: 0.0780667900107801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 54.36568967299536,
    "estimated_duration": 3600.0176481284852,
    "input_throughput": 6599.022927665415,
    "output_throughput": 5755.660950932228,
    "total_throughput": 12354.683878597643,
    "itl": 91.12175229982451,
    "ttft": 1941677.718198525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5256871292879923,
    "arrivals": 621138,
    "finished_requests": 96322,
    "scheduler_time": 219.4064044653426
}
#Debug simulation 
Total elapsed time: 54.36582449590787. Arrivals time: 0.561779675539583 Scheduler time: 53.603121515363455 Scheduler overhead time: 0.07810223242267966 Adapter cache time: 0.01571175642311573 Engine time: 0.07553817704319954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 76.7067288029939,
    "estimated_duration": 3600.06612096878,
    "input_throughput": 6834.581414126581,
    "output_throughput": 5950.625705239867,
    "total_throughput": 12785.207119366449,
    "itl": 97.39638205846218,
    "ttft": 1926317.538628586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9024084402900119,
    "arrivals": 621138,
    "finished_requests": 99592,
    "scheduler_time": 213.79502628865328
}
#Debug simulation 
Total elapsed time: 76.70686453813687. Arrivals time: 0.530753480270505 Scheduler time: 75.97650482179597 Scheduler overhead time: 0.07860542833805084 Adapter cache time: 0.014720141422003508 Engine time: 0.07552652154117823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 72.03748477064073,
    "estimated_duration": 3600.0126603952262,
    "input_throughput": 6614.354238794769,
    "output_throughput": 5758.378915735296,
    "total_throughput": 12372.733154530066,
    "itl": 90.98175406488839,
    "ttft": 1931403.9902155614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 333,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.447668153550487,
    "arrivals": 621138,
    "finished_requests": 96350,
    "scheduler_time": 219.71594683060397
}
#Debug simulation 
Total elapsed time: 72.03762112371624. Arrivals time: 0.5895622912794352 Scheduler time: 71.23522682022303 Scheduler overhead time: 0.08296619402244687 Adapter cache time: 0.015972843393683434 Engine time: 0.08120735501870513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 70.09234642283991,
    "estimated_duration": 3600.087648262453,
    "input_throughput": 6916.444107136571,
    "output_throughput": 6017.716821549071,
    "total_throughput": 12934.160928685642,
    "itl": 99.68744185094474,
    "ttft": 1921312.416994804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8514745289459802,
    "arrivals": 620208,
    "finished_requests": 100888,
    "scheduler_time": 211.68858392402407
}
#Debug simulation 
Total elapsed time: 70.09248943068087. Arrivals time: 0.5072966897860169 Scheduler time: 69.38484912458807 Scheduler overhead time: 0.07857549656182528 Adapter cache time: 0.014734266325831413 Engine time: 0.07624000636860728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.10092323366553,
    "estimated_duration": 3600.044668763722,
    "input_throughput": 6828.832490137501,
    "output_throughput": 5939.730466550894,
    "total_throughput": 12768.562956688394,
    "itl": 96.902090220521,
    "ttft": 1926764.8618480612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7598092653602384,
    "arrivals": 620208,
    "finished_requests": 99615,
    "scheduler_time": 214.2193035160358
}
#Debug simulation 
Total elapsed time: 74.10105497669429. Arrivals time: 0.8091248595155776 Scheduler time: 73.08348789764568 Scheduler overhead time: 0.08256581053137779 Adapter cache time: 0.015547478571534157 Engine time: 0.07916249753907323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.12958353105932,
    "estimated_duration": 3600.0210794448894,
    "input_throughput": 6644.853036162952,
    "output_throughput": 5786.442784721726,
    "total_throughput": 12431.295820884678,
    "itl": 90.88920890520535,
    "ttft": 1938137.878071772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6749806477269193,
    "arrivals": 620208,
    "finished_requests": 96903,
    "scheduler_time": 220.3369815694969
}
#Debug simulation 
Total elapsed time: 75.1297295242548. Arrivals time: 0.5076162405312061 Scheduler time: 74.40735479164869 Scheduler overhead time: 0.0846634590998292 Adapter cache time: 0.015342871192842722 Engine time: 0.08208642387762666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 71.5161058451049,
    "estimated_duration": 3600.000653431982,
    "input_throughput": 6805.262098117805,
    "output_throughput": 5913.139482268208,
    "total_throughput": 12718.401580386013,
    "itl": 96.97162879228418,
    "ttft": 1925294.0766301076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5901819311780834,
    "arrivals": 620208,
    "finished_requests": 99222,
    "scheduler_time": 213.59686458618847
}
#Debug simulation 
Total elapsed time: 71.5162616148591. Arrivals time: 0.4872365454211831 Scheduler time: 70.82590687507764 Scheduler overhead time: 0.07989751687273383 Adapter cache time: 0.015233941376209259 Engine time: 0.07690424844622612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 65.7885710191913,
    "estimated_duration": 3600.0643309679126,
    "input_throughput": 6638.081379388828,
    "output_throughput": 5770.257720482149,
    "total_throughput": 12408.339099870976,
    "itl": 90.81112266607646,
    "ttft": 1945661.9427209133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.074681213758892,
    "arrivals": 620208,
    "finished_requests": 96783,
    "scheduler_time": 220.2735125963189
}
#Debug simulation 
Total elapsed time: 65.7887069042772. Arrivals time: 0.4979135701432824 Scheduler time: 65.07927924394608 Scheduler overhead time: 0.08268291084095836 Adapter cache time: 0.01604624092578888 Engine time: 0.08023755112662911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.52215823112056,
    "estimated_duration": 3600.070867902272,
    "input_throughput": 6807.944870896727,
    "output_throughput": 5933.257922904824,
    "total_throughput": 12741.20279380155,
    "itl": 97.20123493832332,
    "ttft": 1921838.7828631382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3853108441038016,
    "arrivals": 620208,
    "finished_requests": 99373,
    "scheduler_time": 213.8835020849617
}
#Debug simulation 
Total elapsed time: 79.52230327017605. Arrivals time: 0.8119836309924722 Scheduler time: 78.49827241664752 Scheduler overhead time: 0.08392049511894584 Adapter cache time: 0.015200276859104633 Engine time: 0.08067750092595816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 65.57335743820295,
    "estimated_duration": 3600.0301719577665,
    "input_throughput": 6642.814325912975,
    "output_throughput": 5773.328279828603,
    "total_throughput": 12416.142605741577,
    "itl": 90.84643008091203,
    "ttft": 1945479.182181186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0509161039441883,
    "arrivals": 620208,
    "finished_requests": 96833,
    "scheduler_time": 220.27892556775956
}
#Debug simulation 
Total elapsed time: 65.57349370187148. Arrivals time: 0.49615235859528184 Scheduler time: 64.86864451272413 Scheduler overhead time: 0.08194530801847577 Adapter cache time: 0.015254277735948563 Engine time: 0.07956421468406916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 84.57978819217533,
    "estimated_duration": 3600.0210009028406,
    "input_throughput": 6931.295399038546,
    "output_throughput": 6009.655219948506,
    "total_throughput": 12940.950618987052,
    "itl": 99.23983360510104,
    "ttft": 1912996.8015488156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3687686696136354,
    "arrivals": 619792,
    "finished_requests": 101090,
    "scheduler_time": 212.6041706065648
}
#Debug simulation 
Total elapsed time: 84.5799217200838. Arrivals time: 0.5704886019229889 Scheduler time: 83.79746001632884 Scheduler overhead time: 0.0843031732365489 Adapter cache time: 0.015435938723385334 Engine time: 0.08073167270049453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 76.46813171636313,
    "estimated_duration": 3600.0183672458475,
    "input_throughput": 6866.746076885176,
    "output_throughput": 5960.55153363461,
    "total_throughput": 12827.297610519787,
    "itl": 96.91823212755962,
    "ttft": 1917396.50104981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5066622080747052,
    "arrivals": 619792,
    "finished_requests": 100235,
    "scheduler_time": 214.58884918659282
}
#Debug simulation 
Total elapsed time: 76.46827676333487. Arrivals time: 0.5246258769184351 Scheduler time: 75.74150342168286 Scheduler overhead time: 0.07953861122950912 Adapter cache time: 0.014654642902314663 Engine time: 0.0770065519027412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.39836492715403,
    "estimated_duration": 3600.080173398195,
    "input_throughput": 6662.958557769553,
    "output_throughput": 5778.593252928369,
    "total_throughput": 12441.551810697922,
    "itl": 90.8928444781705,
    "ttft": 1937250.838078576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.534269787585365,
    "arrivals": 619792,
    "finished_requests": 97168,
    "scheduler_time": 220.14392761541146
}
#Debug simulation 
Total elapsed time: 77.39850567793474. Arrivals time: 0.5251597934402525 Scheduler time: 76.65792116476223 Scheduler overhead time: 0.08583733718842268 Adapter cache time: 0.015354806557297707 Engine time: 0.0815219315700233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 80.28266123775393,
    "estimated_duration": 3600.0045946262526,
    "input_throughput": 6857.032359583085,
    "output_throughput": 5948.841852026404,
    "total_throughput": 12805.874211609489,
    "itl": 97.13114954561954,
    "ttft": 1918397.147481575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3972760819876548,
    "arrivals": 619792,
    "finished_requests": 100077,
    "scheduler_time": 213.99543254560456
}
#Debug simulation 
Total elapsed time: 80.28280221298337. Arrivals time: 0.5498495846986771 Scheduler time: 79.5217100912705 Scheduler overhead time: 0.08310872595757246 Adapter cache time: 0.015366056468337774 Engine time: 0.08061404805630445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 77.11604074109346,
    "estimated_duration": 3600.0201309494482,
    "input_throughput": 6643.523960987495,
    "output_throughput": 5760.413621501273,
    "total_throughput": 12403.937582488768,
    "itl": 90.99519205314449,
    "ttft": 1936186.9531736271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.506985182282517,
    "arrivals": 619792,
    "finished_requests": 96917,
    "scheduler_time": 219.75374996315787
}
#Debug simulation 
Total elapsed time: 77.11618675477803. Arrivals time: 0.5243557211942971 Scheduler time: 76.37098118430004 Scheduler overhead time: 0.0875728027895093 Adapter cache time: 0.015753514133393764 Engine time: 0.08405754249542952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 73.81273242924362,
    "estimated_duration": 3600.005236032075,
    "input_throughput": 6851.138646449526,
    "output_throughput": 5951.326621850811,
    "total_throughput": 12802.465268300337,
    "itl": 97.11932077610622,
    "ttft": 1922335.8515243863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.62151591890491,
    "arrivals": 619792,
    "finished_requests": 99998,
    "scheduler_time": 214.09081517711314
}
#Debug simulation 
Total elapsed time: 73.81287914700806. Arrivals time: 0.5105329286307096 Scheduler time: 73.09429093729705 Scheduler overhead time: 0.08195948787033558 Adapter cache time: 0.015357889700680971 Engine time: 0.07947531435638666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.3732699570246,
    "estimated_duration": 3600.0299481404004,
    "input_throughput": 6648.255804750484,
    "output_throughput": 5769.872834177297,
    "total_throughput": 12418.128638927781,
    "itl": 90.98753045105236,
    "ttft": 1928407.3943613425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6024452495016193,
    "arrivals": 619792,
    "finished_requests": 96888,
    "scheduler_time": 219.73928763120375
}
#Debug simulation 
Total elapsed time: 79.3733901870437. Arrivals time: 0.531583555508405 Scheduler time: 78.62465962348506 Scheduler overhead time: 0.0867348862811923 Adapter cache time: 0.015763301867991686 Engine time: 0.08220703806728125 
