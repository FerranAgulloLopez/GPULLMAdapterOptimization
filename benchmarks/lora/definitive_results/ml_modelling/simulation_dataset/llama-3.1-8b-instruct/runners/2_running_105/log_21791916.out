INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 34560, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209385 . Total input tokens: 46707459 . Total output tokens: 41046600
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.8428536099381745,
    "estimated_duration": 3599.994765358837,
    "input_throughput": 4860.457345208359,
    "output_throughput": 4214.218349978012,
    "total_throughput": 9074.675695186372,
    "itl": 34.73560686302092,
    "ttft": 8325.5769612595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 70101,
    "finished_requests": 69940,
    "scheduler_time": 42.89128504918285
}
#Debug simulation 
Total elapsed time: 4.842940315138549. Arrivals time: 0.16087736980989575 Scheduler time: 4.400354134850204 Scheduler overhead time: 0.1066752695478499 Adapter cache time: 0.01840684749186039 Engine time: 0.10669014975428581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 34560, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209385 . Total input tokens: 46707459 . Total output tokens: 41046600
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.811787072103471,
    "estimated_duration": 3599.9753261259307,
    "input_throughput": 4860.483590822232,
    "output_throughput": 4214.241106015096,
    "total_throughput": 9074.72469683733,
    "itl": 34.73567651296364,
    "ttft": 8325.5380014522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 70101,
    "finished_requests": 69940,
    "scheduler_time": 42.89111112243636
}
#Debug simulation 
Total elapsed time: 4.8119039428420365. Arrivals time: 0.1596341673284769 Scheduler time: 4.3736641919240355 Scheduler overhead time: 0.10656624427065253 Adapter cache time: 0.01817080518230796 Engine time: 0.10436500236392021 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 34560, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209385 . Total input tokens: 46707459 . Total output tokens: 41046600
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.850131673272699,
    "estimated_duration": 3599.981583428796,
    "input_throughput": 4860.475142579597,
    "output_throughput": 4214.233781037916,
    "total_throughput": 9074.708923617512,
    "itl": 34.73574374777976,
    "ttft": 8325.549376388373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 70101,
    "finished_requests": 69940,
    "scheduler_time": 42.89122124818982
}
#Debug simulation 
Total elapsed time: 4.850221760105342. Arrivals time: 0.1610890575684607 Scheduler time: 4.404708402231336 Scheduler overhead time: 0.10726168425753713 Adapter cache time: 0.018327608238905668 Engine time: 0.10734247462823987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 34560, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209385 . Total input tokens: 46707459 . Total output tokens: 41046600
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.85507200891152,
    "estimated_duration": 3600.0020869265422,
    "input_throughput": 4860.447460167552,
    "output_throughput": 4214.209779237155,
    "total_throughput": 9074.657239404707,
    "itl": 34.73564640687821,
    "ttft": 8325.53947092402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 70101,
    "finished_requests": 69940,
    "scheduler_time": 42.89137074218748
}
#Debug simulation 
Total elapsed time: 4.855141947977245. Arrivals time: 0.15875481208786368 Scheduler time: 4.413537884131074 Scheduler overhead time: 0.10754535300657153 Adapter cache time: 0.018264138139784336 Engine time: 0.10700746206566691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 34560, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209385 . Total input tokens: 46707459 . Total output tokens: 41046600
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.814943881239742,
    "estimated_duration": 3599.979865462388,
    "input_throughput": 4860.4774620739645,
    "output_throughput": 4214.235792135851,
    "total_throughput": 9074.713254209815,
    "itl": 34.7357032018477,
    "ttft": 8325.548582282188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 70101,
    "finished_requests": 69940,
    "scheduler_time": 42.89119606340544
}
#Debug simulation 
Total elapsed time: 4.81501891836524. Arrivals time: 0.15728713432326913 Scheduler time: 4.377435406669974 Scheduler overhead time: 0.10572604415938258 Adapter cache time: 0.018218636512756348 Engine time: 0.10692177433520555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 34560, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209385 . Total input tokens: 46707459 . Total output tokens: 41046600
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.822447684593499,
    "estimated_duration": 3599.9900977677867,
    "input_throughput": 4860.463647066583,
    "output_throughput": 4214.223813950779,
    "total_throughput": 9074.687461017362,
    "itl": 34.73559856189534,
    "ttft": 8325.55044619734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 70101,
    "finished_requests": 69940,
    "scheduler_time": 42.891232507653235
}
#Debug simulation 
Total elapsed time: 4.82252340670675. Arrivals time: 0.15894600609317422 Scheduler time: 4.381245796568692 Scheduler overhead time: 0.10616642935201526 Adapter cache time: 0.018159426748752594 Engine time: 0.10794836189597845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 34560, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209385 . Total input tokens: 46707459 . Total output tokens: 41046600
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.841046196874231,
    "estimated_duration": 3599.9759563769617,
    "input_throughput": 4860.482739892995,
    "output_throughput": 4214.240368224113,
    "total_throughput": 9074.723108117108,
    "itl": 34.73571399714644,
    "ttft": 8325.540296985835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 70101,
    "finished_requests": 69940,
    "scheduler_time": 42.89112730166862
}
#Debug simulation 
Total elapsed time: 4.8411476588808. Arrivals time: 0.15994847333058715 Scheduler time: 4.399686593096703 Scheduler overhead time: 0.10702543845400214 Adapter cache time: 0.01814536191523075 Engine time: 0.106563082896173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 34560, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209040 . Total input tokens: 46625088 . Total output tokens: 40980577
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.806214196141809,
    "estimated_duration": 3600.0328309580173,
    "input_throughput": 4867.684774790421,
    "output_throughput": 4169.680584831049,
    "total_throughput": 9037.36535962147,
    "itl": 34.329385131997235,
    "ttft": 7822.7421162096225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 69999,
    "finished_requests": 69848,
    "scheduler_time": 42.06775792679326
}
#Debug simulation 
Total elapsed time: 4.806314294226468. Arrivals time: 0.15888206660747528 Scheduler time: 4.363011870533228 Scheduler overhead time: 0.10847634775564075 Adapter cache time: 0.018061079550534487 Engine time: 0.10754935862496495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 34560, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209040 . Total input tokens: 46625088 . Total output tokens: 40980577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.813811535015702,
    "estimated_duration": 3600.0156073083,
    "input_throughput": 4867.708063383206,
    "output_throughput": 4169.700533943958,
    "total_throughput": 9037.408597327163,
    "itl": 34.32948663024627,
    "ttft": 7771.355406797465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 69999,
    "finished_requests": 69848,
    "scheduler_time": 42.067627740899276
}
#Debug simulation 
Total elapsed time: 4.813881169073284. Arrivals time: 0.16144522512331605 Scheduler time: 4.368894427549094 Scheduler overhead time: 0.10802631080150604 Adapter cache time: 0.018394576385617256 Engine time: 0.10676953243091702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 34560, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209040 . Total input tokens: 46625088 . Total output tokens: 40980577
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.786809724289924,
    "estimated_duration": 3600.020035154942,
    "input_throughput": 4867.702076342969,
    "output_throughput": 4169.69540541847,
    "total_throughput": 9037.39748176144,
    "itl": 34.32932947951259,
    "ttft": 7822.698205856057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 69999,
    "finished_requests": 69848,
    "scheduler_time": 42.06769729564723
}
#Debug simulation 
Total elapsed time: 4.786881581414491. Arrivals time: 0.1595268314704299 Scheduler time: 4.344259374309331 Scheduler overhead time: 0.10799154499545693 Adapter cache time: 0.018183817621320486 Engine time: 0.10664704954251647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 34560, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209040 . Total input tokens: 46625088 . Total output tokens: 40980577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.858585648238659,
    "estimated_duration": 3600.001353213001,
    "input_throughput": 4867.727336924468,
    "output_throughput": 4169.717043745746,
    "total_throughput": 9037.444380670213,
    "itl": 34.329361033341364,
    "ttft": 7771.322580538605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 69999,
    "finished_requests": 69848,
    "scheduler_time": 42.06738113053376
}
#Debug simulation 
Total elapsed time: 4.858663787133992. Arrivals time: 0.16122539108619094 Scheduler time: 4.413688232656568 Scheduler overhead time: 0.10792346205562353 Adapter cache time: 0.01814110018312931 Engine time: 0.10733372764661908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 34560, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209040 . Total input tokens: 46625088 . Total output tokens: 40980577
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.837821740657091,
    "estimated_duration": 3600.018843970293,
    "input_throughput": 4867.703686982313,
    "output_throughput": 4169.696785099348,
    "total_throughput": 9037.400472081661,
    "itl": 34.329471499129326,
    "ttft": 7822.6807502313295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 69999,
    "finished_requests": 69848,
    "scheduler_time": 42.06767703063214
}
#Debug simulation 
Total elapsed time: 4.8378991996869445. Arrivals time: 0.1604141853749752 Scheduler time: 4.3941934746690094 Scheduler overhead time: 0.1079372800886631 Adapter cache time: 0.018201854545623064 Engine time: 0.10703146178275347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 34560, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209040 . Total input tokens: 46625088 . Total output tokens: 40980577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.823732085060328,
    "estimated_duration": 3600.0279765073187,
    "input_throughput": 4867.691338610455,
    "output_throughput": 4169.686207428695,
    "total_throughput": 9037.37754603915,
    "itl": 34.32926055701885,
    "ttft": 7822.741252459088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 69999,
    "finished_requests": 69848,
    "scheduler_time": 42.067693250839305
}
#Debug simulation 
Total elapsed time: 4.8238058728165925. Arrivals time: 0.16161403618752956 Scheduler time: 4.377706108149141 Scheduler overhead time: 0.10798581037670374 Adapter cache time: 0.018196395598351955 Engine time: 0.10791438957676291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 34560, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 209040 . Total input tokens: 46625088 . Total output tokens: 40980577
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.834754216019064,
    "estimated_duration": 3600.0183785989657,
    "input_throughput": 4867.704316226247,
    "output_throughput": 4169.69732411252,
    "total_throughput": 9037.401640338767,
    "itl": 34.32949556537146,
    "ttft": 7822.693096237271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 69999,
    "finished_requests": 69848,
    "scheduler_time": 42.06768845399494
}
#Debug simulation 
Total elapsed time: 4.834827336948365. Arrivals time: 0.16065235109999776 Scheduler time: 4.388583187479526 Scheduler overhead time: 0.10781631292775273 Adapter cache time: 0.018046617973595858 Engine time: 0.10919326217845082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 34560, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 208875 . Total input tokens: 46593849 . Total output tokens: 40948133
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.865377084817737,
    "estimated_duration": 3600.033758954716,
    "input_throughput": 4842.149315027929,
    "output_throughput": 4190.903755408306,
    "total_throughput": 9033.053070436235,
    "itl": 34.38212539802585,
    "ttft": 9113.29823573812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 69963,
    "finished_requests": 69786,
    "scheduler_time": 42.40439591589286
}
#Debug simulation 
Total elapsed time: 4.865459553897381. Arrivals time: 0.1600531553849578 Scheduler time: 4.422820421867073 Scheduler overhead time: 0.10746871354058385 Adapter cache time: 0.017986047081649303 Engine time: 0.10670601530000567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 34560, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 208875 . Total input tokens: 46593849 . Total output tokens: 40948133
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.865632982924581,
    "estimated_duration": 3600.0125191626835,
    "input_throughput": 4842.177883329816,
    "output_throughput": 4190.928481412374,
    "total_throughput": 9033.106364742189,
    "itl": 34.382270372182944,
    "ttft": 9113.189833517612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 69963,
    "finished_requests": 69786,
    "scheduler_time": 42.4041646917486
}
#Debug simulation 
Total elapsed time: 4.865712559781969. Arrivals time: 0.16069777496159077 Scheduler time: 4.422883161343634 Scheduler overhead time: 0.10736169386655092 Adapter cache time: 0.017969471868127584 Engine time: 0.10677771922200918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 34560, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 208875 . Total input tokens: 46593849 . Total output tokens: 40948133
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.831656150054187,
    "estimated_duration": 3600.020940798224,
    "input_throughput": 4842.166555879774,
    "output_throughput": 4190.918677449335,
    "total_throughput": 9033.085233329108,
    "itl": 34.38232627188041,
    "ttft": 9113.226979867648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 69963,
    "finished_requests": 69786,
    "scheduler_time": 42.40434266330202
}
#Debug simulation 
Total elapsed time: 4.831730458885431. Arrivals time: 0.1659981505945325 Scheduler time: 4.374591405969113 Scheduler overhead time: 0.10851291008293629 Adapter cache time: 0.020148510579019785 Engine time: 0.11198437632992864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 34560, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 208875 . Total input tokens: 46593849 . Total output tokens: 40948133
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.8457009070552886,
    "estimated_duration": 3600.0394744115074,
    "input_throughput": 4842.266348440426,
    "output_throughput": 4191.047100245033,
    "total_throughput": 9033.313448685458,
    "itl": 34.3822351823711,
    "ttft": 9113.123292114125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 69963,
    "finished_requests": 69787,
    "scheduler_time": 42.40446480055456
}
#Debug simulation 
Total elapsed time: 4.845776709262282. Arrivals time: 0.1604524338617921 Scheduler time: 4.400227768346667 Scheduler overhead time: 0.10693952115252614 Adapter cache time: 0.017945569939911366 Engine time: 0.10990867018699646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 34560, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 208875 . Total input tokens: 46593849 . Total output tokens: 40948133
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.902393457945436,
    "estimated_duration": 3600.019927487876,
    "input_throughput": 4842.167918821529,
    "output_throughput": 4190.9198570820445,
    "total_throughput": 9033.087775903574,
    "itl": 34.38230572609382,
    "ttft": 9113.19169937079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 69963,
    "finished_requests": 69786,
    "scheduler_time": 42.40432248023675
}
#Debug simulation 
Total elapsed time: 4.90246535371989. Arrivals time: 0.16098053753376007 Scheduler time: 4.458098220638931 Scheduler overhead time: 0.10765057196840644 Adapter cache time: 0.018063316587358713 Engine time: 0.10688917711377144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 34560, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 208875 . Total input tokens: 46593849 . Total output tokens: 40948133
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.842703232076019,
    "estimated_duration": 3600.0248732638843,
    "input_throughput": 4842.161266568069,
    "output_throughput": 4190.914099524357,
    "total_throughput": 9033.075366092426,
    "itl": 34.382155177050244,
    "ttft": 9113.218546319737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 69963,
    "finished_requests": 69786,
    "scheduler_time": 42.40425447053639
}
#Debug simulation 
Total elapsed time: 4.842803196981549. Arrivals time: 0.16022554086521268 Scheduler time: 4.401146825402975 Scheduler overhead time: 0.10687589179724455 Adapter cache time: 0.01777674164623022 Engine time: 0.10674794064834714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 34560, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 34560]
Prompts retrieved: 208875 . Total input tokens: 46593849 . Total output tokens: 40948133
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.8355106161907315,
    "estimated_duration": 3600.017598099902,
    "input_throughput": 4842.17105194169,
    "output_throughput": 4190.922568812764,
    "total_throughput": 9033.093620754455,
    "itl": 34.38226743717296,
    "ttft": 9113.203261042176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 69963,
    "finished_requests": 69786,
    "scheduler_time": 42.40428599501463
}
#Debug simulation 
Total elapsed time: 4.835593075025827. Arrivals time: 0.15812879847362638 Scheduler time: 4.396835646126419 Scheduler overhead time: 0.10656113550066948 Adapter cache time: 0.017833591904491186 Engine time: 0.10612600762397051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 34560, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208365 . Total input tokens: 46483353 . Total output tokens: 40849567
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.834033126011491,
    "estimated_duration": 3600.013335076466,
    "input_throughput": 4849.505369854187,
    "output_throughput": 4179.542573744497,
    "total_throughput": 9029.047943598684,
    "itl": 34.154229994547144,
    "ttft": 9134.778024188365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 69793,
    "finished_requests": 69617,
    "scheduler_time": 42.12728265352572
}
#Debug simulation 
Total elapsed time: 4.834136049263179. Arrivals time: 0.1600146903656423 Scheduler time: 4.391378667671233 Scheduler overhead time: 0.10748708387836814 Adapter cache time: 0.017743638716638088 Engine time: 0.10713235288858414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 34560, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208365 . Total input tokens: 46483353 . Total output tokens: 40849567
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.859197341836989,
    "estimated_duration": 3599.9936710543634,
    "input_throughput": 4849.5315812282615,
    "output_throughput": 4179.549014482916,
    "total_throughput": 9029.080595711177,
    "itl": 34.154228237293694,
    "ttft": 9186.470336952572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 69793,
    "finished_requests": 69616,
    "scheduler_time": 42.12700669064317
}
#Debug simulation 
Total elapsed time: 4.859291542787105. Arrivals time: 0.16098291147500277 Scheduler time: 4.414327842183411 Scheduler overhead time: 0.10917074885219336 Adapter cache time: 0.01764556998386979 Engine time: 0.10671096667647362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 34560, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208365 . Total input tokens: 46483353 . Total output tokens: 40849567
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.8515777718275785,
    "estimated_duration": 3600.000283641831,
    "input_throughput": 4849.522673464586,
    "output_throughput": 4179.541337363123,
    "total_throughput": 9029.064010827708,
    "itl": 34.15430862476277,
    "ttft": 9186.36375246859,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 69793,
    "finished_requests": 69616,
    "scheduler_time": 42.12715230373248
}
#Debug simulation 
Total elapsed time: 4.851651447825134. Arrivals time: 0.16066979337483644 Scheduler time: 4.407587357796729 Scheduler overhead time: 0.10772380046546459 Adapter cache time: 0.017678597010672092 Engine time: 0.10768274683505297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 34560, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208365 . Total input tokens: 46483353 . Total output tokens: 40849567
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.848837727680802,
    "estimated_duration": 3599.9825692345416,
    "input_throughput": 4849.546536474516,
    "output_throughput": 4179.561903600906,
    "total_throughput": 9029.10844007542,
    "itl": 34.15421169779525,
    "ttft": 9186.462890967012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 69793,
    "finished_requests": 69616,
    "scheduler_time": 42.12686900326982
}
#Debug simulation 
Total elapsed time: 4.8489158558659256. Arrivals time: 0.1655339701101184 Scheduler time: 4.3974551828578115 Scheduler overhead time: 0.11038782028481364 Adapter cache time: 0.017730592750012875 Engine time: 0.10682475101202726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 34560, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208365 . Total input tokens: 46483353 . Total output tokens: 40849567
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.840904931072146,
    "estimated_duration": 3599.9990972560718,
    "input_throughput": 4849.524271632942,
    "output_throughput": 4179.542714738002,
    "total_throughput": 9029.066986370945,
    "itl": 34.15429766425963,
    "ttft": 9186.415813503123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 69793,
    "finished_requests": 69616,
    "scheduler_time": 42.12713207969221
}
#Debug simulation 
Total elapsed time: 4.840979930013418. Arrivals time: 0.1604148456826806 Scheduler time: 4.395579053554684 Scheduler overhead time: 0.10786054423078895 Adapter cache time: 0.01762016210705042 Engine time: 0.10874983388930559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 34560, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208365 . Total input tokens: 46483353 . Total output tokens: 40849567
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.866508174221963,
    "estimated_duration": 3600.0086081809077,
    "input_throughput": 4849.511737368236,
    "output_throughput": 4179.548061581715,
    "total_throughput": 9029.059798949951,
    "itl": 34.154201422244306,
    "ttft": 9134.690226447243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 69793,
    "finished_requests": 69617,
    "scheduler_time": 42.12723003004614
}
#Debug simulation 
Total elapsed time: 4.866579781286418. Arrivals time: 0.16059834603220224 Scheduler time: 4.421624190174043 Scheduler overhead time: 0.10834208270534873 Adapter cache time: 0.01766995806246996 Engine time: 0.107496646232903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 34560, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208365 . Total input tokens: 46483353 . Total output tokens: 40849567
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.851128787267953,
    "estimated_duration": 3599.9943373344704,
    "input_throughput": 4849.530683686177,
    "output_throughput": 4179.548240939931,
    "total_throughput": 9029.078924626108,
    "itl": 34.15425620015623,
    "ttft": 9186.478301578414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 69793,
    "finished_requests": 69616,
    "scheduler_time": 42.127002645835155
}
#Debug simulation 
Total elapsed time: 4.851226035039872. Arrivals time: 0.1584595968015492 Scheduler time: 4.410367663018405 Scheduler overhead time: 0.10777633963152766 Adapter cache time: 0.017499756067991257 Engine time: 0.10661260038614273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 34560, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208200 . Total input tokens: 46443765 . Total output tokens: 40819543
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.844607190228999,
    "estimated_duration": 3600.0004387883464,
    "input_throughput": 4839.310521268594,
    "output_throughput": 4180.299490481472,
    "total_throughput": 9019.610011750066,
    "itl": 34.05669831645564,
    "ttft": 6872.837538239851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 69715,
    "finished_requests": 69582,
    "scheduler_time": 42.04989000986535
}
#Debug simulation 
Total elapsed time: 4.844683306291699. Arrivals time: 0.16100154491141438 Scheduler time: 4.399117981549352 Scheduler overhead time: 0.10840159608051181 Adapter cache time: 0.017416411079466343 Engine time: 0.10831682663410902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 34560, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208200 . Total input tokens: 46443765 . Total output tokens: 40819543
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.843020518776029,
    "estimated_duration": 3600.0183707200554,
    "input_throughput": 4839.286416339994,
    "output_throughput": 4180.278668130787,
    "total_throughput": 9019.565084470782,
    "itl": 34.056627019983125,
    "ttft": 6924.37175516142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 69715,
    "finished_requests": 69582,
    "scheduler_time": 42.050101337768744
}
#Debug simulation 
Total elapsed time: 4.843095317017287. Arrivals time: 0.15867243614047766 Scheduler time: 4.401497007813305 Scheduler overhead time: 0.10743086179718375 Adapter cache time: 0.01757654221728444 Engine time: 0.10740227485075593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 34560, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208200 . Total input tokens: 46443765 . Total output tokens: 40819543
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.8756009242497385,
    "estimated_duration": 3600.022957859373,
    "input_throughput": 4839.280250134598,
    "output_throughput": 4180.273341631245,
    "total_throughput": 9019.553591765842,
    "itl": 34.056737081236555,
    "ttft": 6924.3952796294625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 69715,
    "finished_requests": 69582,
    "scheduler_time": 42.0501457487075
}
#Debug simulation 
Total elapsed time: 4.875692380126566. Arrivals time: 0.16035056114196777 Scheduler time: 4.431123874150217 Scheduler overhead time: 0.10874156327918172 Adapter cache time: 0.01749203633517027 Engine time: 0.10688452608883381 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 34560, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208200 . Total input tokens: 46443765 . Total output tokens: 40819543
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.9100462342612445,
    "estimated_duration": 3600.0050520363675,
    "input_throughput": 4839.304319905162,
    "output_throughput": 4180.294133611669,
    "total_throughput": 9019.598453516832,
    "itl": 34.05660391660836,
    "ttft": 6872.853233356226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 69715,
    "finished_requests": 69582,
    "scheduler_time": 42.04989005084017
}
#Debug simulation 
Total elapsed time: 4.910123826004565. Arrivals time: 0.16178973205387592 Scheduler time: 4.461199225392193 Scheduler overhead time: 0.10976767307147384 Adapter cache time: 0.017482201103121042 Engine time: 0.10927799344062805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 34560, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208200 . Total input tokens: 46443765 . Total output tokens: 40819543
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.905113053973764,
    "estimated_duration": 3600.021069329034,
    "input_throughput": 4839.282788766287,
    "output_throughput": 4180.275534555363,
    "total_throughput": 9019.558323321651,
    "itl": 34.056711876112914,
    "ttft": 6924.3475640495635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 69715,
    "finished_requests": 69582,
    "scheduler_time": 42.05012552466728
}
#Debug simulation 
Total elapsed time: 4.905199280008674. Arrivals time: 0.16308118076995015 Scheduler time: 4.455668301787227 Scheduler overhead time: 0.10907931486144662 Adapter cache time: 0.01780516328290105 Engine time: 0.10851787868887186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 34560, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208200 . Total input tokens: 46443765 . Total output tokens: 40819543
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.875360514968634,
    "estimated_duration": 3600.0279941050903,
    "input_throughput": 4839.273480241564,
    "output_throughput": 4180.267493653466,
    "total_throughput": 9019.540973895031,
    "itl": 34.056497647688566,
    "ttft": 6924.414611953753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 69715,
    "finished_requests": 69582,
    "scheduler_time": 42.050105218677366
}
#Debug simulation 
Total elapsed time: 4.8754585068672895. Arrivals time: 0.15845121815800667 Scheduler time: 4.43321348214522 Scheduler overhead time: 0.10806305008009076 Adapter cache time: 0.017597821075469255 Engine time: 0.10737403109669685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 34560, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 34560]
Prompts retrieved: 208200 . Total input tokens: 46443765 . Total output tokens: 40819543
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.8714177599176764,
    "estimated_duration": 3600.019934886979,
    "input_throughput": 4839.284313726152,
    "output_throughput": 4180.276851848172,
    "total_throughput": 9019.561165574323,
    "itl": 34.05665285364413,
    "ttft": 6924.313666281628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 69715,
    "finished_requests": 69582,
    "scheduler_time": 42.050121520834125
}
#Debug simulation 
Total elapsed time: 4.871511755045503. Arrivals time: 0.1583781074732542 Scheduler time: 4.430149838794023 Scheduler overhead time: 0.10783882392570376 Adapter cache time: 0.01746244402602315 Engine time: 0.10659924522042274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 34560, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 34560]
Prompts retrieved: 207855 . Total input tokens: 46367400 . Total output tokens: 40758625
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.868559850845486,
    "estimated_duration": 3600.0361646036795,
    "input_throughput": 4806.662824706646,
    "output_throughput": 4189.928742468773,
    "total_throughput": 8996.59156717542,
    "itl": 34.01530639504609,
    "ttft": 8640.690558131042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 69614,
    "finished_requests": 69448,
    "scheduler_time": 42.16122252167961
}
#Debug simulation 
Total elapsed time: 4.868637674022466. Arrivals time: 0.15949476789683104 Scheduler time: 4.424325691070408 Scheduler overhead time: 0.10839784471318126 Adapter cache time: 0.01733028097078204 Engine time: 0.10830943798646331 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 34560, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 34560]
Prompts retrieved: 207855 . Total input tokens: 46367400 . Total output tokens: 40758625
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.855437628924847,
    "estimated_duration": 3600.0187923999124,
    "input_throughput": 4806.59074239571,
    "output_throughput": 4189.873406173158,
    "total_throughput": 8996.464148568868,
    "itl": 34.01556416182177,
    "ttft": 8692.586198761106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 69614,
    "finished_requests": 69446,
    "scheduler_time": 42.1610960937699
}
#Debug simulation 
Total elapsed time: 4.855518118944019. Arrivals time: 0.16043836949393153 Scheduler time: 4.407392478082329 Scheduler overhead time: 0.10905228508636355 Adapter cache time: 0.017387876752763987 Engine time: 0.10779298655688763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 34560, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 34560]
Prompts retrieved: 207855 . Total input tokens: 46367400 . Total output tokens: 40758625
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.867698869667947,
    "estimated_duration": 3600.022718320951,
    "input_throughput": 4806.5855006799775,
    "output_throughput": 4189.868837004172,
    "total_throughput": 8996.454337684148,
    "itl": 34.015577803518475,
    "ttft": 8692.655398299114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 69614,
    "finished_requests": 69446,
    "scheduler_time": 42.161128657108826
}
#Debug simulation 
Total elapsed time: 4.8678048946894705. Arrivals time: 0.16489985026419163 Scheduler time: 4.417039922904223 Scheduler overhead time: 0.10905420174822211 Adapter cache time: 0.01733384281396866 Engine time: 0.10839880723506212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 34560, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 34560]
Prompts retrieved: 207855 . Total input tokens: 46367400 . Total output tokens: 40758625
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.883209237828851,
    "estimated_duration": 3600.007181815733,
    "input_throughput": 4806.4645780158535,
    "output_throughput": 4189.761919417203,
    "total_throughput": 8996.226497433056,
    "itl": 34.01558079217078,
    "ttft": 8744.242737619406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 69614,
    "finished_requests": 69445,
    "scheduler_time": 42.160918245141126
}
#Debug simulation 
Total elapsed time: 4.8833092506974936. Arrivals time: 0.16292801685631275 Scheduler time: 4.424229602329433 Scheduler overhead time: 0.11073776101693511 Adapter cache time: 0.019809975288808346 Engine time: 0.11267566727474332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 34560, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 34560]
Prompts retrieved: 207855 . Total input tokens: 46367400 . Total output tokens: 40758625
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.887665403075516,
    "estimated_duration": 3600.0216987972835,
    "input_throughput": 4806.586861901683,
    "output_throughput": 4189.870023572143,
    "total_throughput": 8996.456885473826,
    "itl": 34.01558079563548,
    "ttft": 8692.636837443872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 69614,
    "finished_requests": 69446,
    "scheduler_time": 42.16112865710879
}
#Debug simulation 
Total elapsed time: 4.887767283245921. Arrivals time: 0.1574688060209155 Scheduler time: 4.448395020328462 Scheduler overhead time: 0.10725956922397017 Adapter cache time: 0.017189248465001583 Engine time: 0.1071467469446361 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 34560, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 34560]
Prompts retrieved: 207855 . Total input tokens: 46367400 . Total output tokens: 40758625
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.868882312905043,
    "estimated_duration": 3600.0312010204743,
    "input_throughput": 4806.602230307056,
    "output_throughput": 4189.859520029814,
    "total_throughput": 8996.46175033687,
    "itl": 34.01522738565761,
    "ttft": 8640.95741256535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 69614,
    "finished_requests": 69447,
    "scheduler_time": 42.16119008126585
}
#Debug simulation 
Total elapsed time: 4.868969224859029. Arrivals time: 0.1643494786694646 Scheduler time: 4.406647273804992 Scheduler overhead time: 0.11020992882549763 Adapter cache time: 0.020067380741238594 Engine time: 0.1148059037514031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 34560, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 34560]
Prompts retrieved: 207855 . Total input tokens: 46367400 . Total output tokens: 40758625
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.825831674039364,
    "estimated_duration": 3600.0191161199978,
    "input_throughput": 4806.59031017857,
    "output_throughput": 4189.873029412332,
    "total_throughput": 8996.463339590902,
    "itl": 34.015534894743865,
    "ttft": 8692.604216728376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 69614,
    "finished_requests": 69446,
    "scheduler_time": 42.161076867615336
}
#Debug simulation 
Total elapsed time: 4.825908526778221. Arrivals time: 0.16054000006988645 Scheduler time: 4.382680393289775 Scheduler overhead time: 0.1080613024532795 Adapter cache time: 0.01737205870449543 Engine time: 0.10652671055868268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 168480 . Total input tokens: 37615104 . Total output tokens: 33016056
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.06676312815398,
    "estimated_duration": 3600.0054279181654,
    "input_throughput": 3911.4446580551353,
    "output_throughput": 3404.205422847652,
    "total_throughput": 7315.6500809027875,
    "itl": 34.18791250025008,
    "ttft": 8721.894683145198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 56500,
    "finished_requests": 56364,
    "scheduler_time": 30.763899689133346
}
#Debug simulation 
Total elapsed time: 4.066838447004557. Arrivals time: 0.13540002750232816 Scheduler time: 3.654887930955738 Scheduler overhead time: 0.10542901558801532 Adapter cache time: 0.017647891771048307 Engine time: 0.1037850365974009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 168480 . Total input tokens: 37615104 . Total output tokens: 33016056
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.067508906126022,
    "estimated_duration": 3600.021740663228,
    "input_throughput": 3911.426934162301,
    "output_throughput": 3404.189997403251,
    "total_throughput": 7315.6169315655525,
    "itl": 34.187976447243294,
    "ttft": 8721.867816927816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 56500,
    "finished_requests": 56364,
    "scheduler_time": 30.76402908201575
}
#Debug simulation 
Total elapsed time: 4.0676307999528944. Arrivals time: 0.13549945037811995 Scheduler time: 3.652574814390391 Scheduler overhead time: 0.10540052317082882 Adapter cache time: 0.01743410201743245 Engine time: 0.10743847955018282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 168480 . Total input tokens: 37615104 . Total output tokens: 33016056
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.094712120015174,
    "estimated_duration": 3600.026706269817,
    "input_throughput": 3911.421539033614,
    "output_throughput": 3404.1853019191167,
    "total_throughput": 7315.606840952731,
    "itl": 34.18801628469877,
    "ttft": 8721.936508583643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 56500,
    "finished_requests": 56364,
    "scheduler_time": 30.764097843752307
}
#Debug simulation 
Total elapsed time: 4.094787009060383. Arrivals time: 0.13632485317066312 Scheduler time: 3.680365815758705 Scheduler overhead time: 0.10579955764114857 Adapter cache time: 0.017549396492540836 Engine time: 0.10501727228984237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 168480 . Total input tokens: 37615104 . Total output tokens: 33016056
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 4.059557960834354,
    "estimated_duration": 3600.010352748095,
    "input_throughput": 3911.439307181712,
    "output_throughput": 3404.200765879724,
    "total_throughput": 7315.640073061436,
    "itl": 34.18787884624256,
    "ttft": 8721.902886940725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 56500,
    "finished_requests": 56364,
    "scheduler_time": 30.76393284060877
}
#Debug simulation 
Total elapsed time: 4.059655830729753. Arrivals time: 0.13534752186387777 Scheduler time: 3.6486379289999604 Scheduler overhead time: 0.10498131718486547 Adapter cache time: 0.017488731537014246 Engine time: 0.10393507964909077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 168480 . Total input tokens: 37615104 . Total output tokens: 33016056
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 4.065725974738598,
    "estimated_duration": 3600.025854552892,
    "input_throughput": 3911.4224644225033,
    "output_throughput": 3404.186107302843,
    "total_throughput": 7315.608571725346,
    "itl": 34.18800588240033,
    "ttft": 8721.935021957264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 56500,
    "finished_requests": 56364,
    "scheduler_time": 30.764085709328267
}
#Debug simulation 
Total elapsed time: 4.065799816977233. Arrivals time: 0.13582786172628403 Scheduler time: 3.6501239840872586 Scheduler overhead time: 0.10569813195616007 Adapter cache time: 0.017556548584252596 Engine time: 0.1069308826699853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 168480 . Total input tokens: 37615104 . Total output tokens: 33016056
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 4.032052699010819,
    "estimated_duration": 3600.0377923561305,
    "input_throughput": 3911.4094940609525,
    "output_throughput": 3404.1748189480313,
    "total_throughput": 7315.584313008983,
    "itl": 34.18783655097747,
    "ttft": 8721.966009671994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 56500,
    "finished_requests": 56364,
    "scheduler_time": 30.76419475524549
}
#Debug simulation 
Total elapsed time: 4.032131047919393. Arrivals time: 0.13438135897740722 Scheduler time: 3.6218937537632883 Scheduler overhead time: 0.10509866150096059 Adapter cache time: 0.017536459490656853 Engine time: 0.10395601810887456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 4320, 4320, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 168480 . Total input tokens: 37615104 . Total output tokens: 33016056
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 4.065934199374169,
    "estimated_duration": 3600.0245440639965,
    "input_throughput": 3911.423888267158,
    "output_throughput": 3404.1873465021977,
    "total_throughput": 7315.611234769356,
    "itl": 34.187990250228594,
    "ttft": 8721.939795111655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 56500,
    "finished_requests": 56364,
    "scheduler_time": 30.76407357490413
}
#Debug simulation 
Total elapsed time: 4.066059369128197. Arrivals time: 0.13561142701655626 Scheduler time: 3.6537447166629136 Scheduler overhead time: 0.10555714275687933 Adapter cache time: 0.017606580164283514 Engine time: 0.10360303753986955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 152280 . Total input tokens: 33967274 . Total output tokens: 29831511
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.710901060141623,
    "estimated_duration": 3600.027775163011,
    "input_throughput": 3504.97073579624,
    "output_throughput": 3069.3785409751895,
    "total_throughput": 6574.34927677143,
    "itl": 31.503409508646428,
    "ttft": 7676.056183616256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 50997,
    "finished_requests": 50889,
    "scheduler_time": 24.298517721420207
}
#Debug simulation 
Total elapsed time: 3.7109827040694654. Arrivals time: 0.1242299322038889 Scheduler time: 3.2899795034900308 Scheduler overhead time: 0.1110347080975771 Adapter cache time: 0.020377470180392265 Engine time: 0.11292080162093043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 152280 . Total input tokens: 33967274 . Total output tokens: 29831511
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.6765176579356194,
    "estimated_duration": 3600.0086901358013,
    "input_throughput": 3504.9893169907928,
    "output_throughput": 3069.394812928402,
    "total_throughput": 6574.3841299191945,
    "itl": 31.503161871636188,
    "ttft": 7605.580658514423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 50997,
    "finished_requests": 50889,
    "scheduler_time": 24.29842790165724
}
#Debug simulation 
Total elapsed time: 3.6765922429040074. Arrivals time: 0.12161725014448166 Scheduler time: 3.262431665789336 Scheduler overhead time: 0.11022837879136205 Adapter cache time: 0.02011700253933668 Engine time: 0.11006935266777873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 152280 . Total input tokens: 33967274 . Total output tokens: 29831511
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.6771211731247604,
    "estimated_duration": 3600.008689300014,
    "input_throughput": 3504.98931780452,
    "output_throughput": 3069.394813641001,
    "total_throughput": 6574.384131445521,
    "itl": 31.503188222449936,
    "ttft": 7605.58837971067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 50997,
    "finished_requests": 50889,
    "scheduler_time": 24.29840363280895
}
#Debug simulation 
Total elapsed time: 3.6772019611671567. Arrivals time: 0.12326956912875175 Scheduler time: 3.2626242986880243 Scheduler overhead time: 0.11162492213770747 Adapter cache time: 0.02013474376872182 Engine time: 0.10759127419441938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 152280 . Total input tokens: 33967274 . Total output tokens: 29831511
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.7359572150744498,
    "estimated_duration": 3600.0072017362754,
    "input_throughput": 3504.9907661057928,
    "output_throughput": 3069.3960819496924,
    "total_throughput": 6574.386848055486,
    "itl": 31.50326799406122,
    "ttft": 7605.602168622194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 50997,
    "finished_requests": 50889,
    "scheduler_time": 24.298448125697355
}
#Debug simulation 
Total elapsed time: 3.736035828012973. Arrivals time: 0.12445027567446232 Scheduler time: 3.314418140798807 Scheduler overhead time: 0.11203658441081643 Adapter cache time: 0.02046061120927334 Engine time: 0.11201522219926119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 152280 . Total input tokens: 33967274 . Total output tokens: 29831511
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.694635236170143,
    "estimated_duration": 3600.0092071925114,
    "input_throughput": 3504.9888135814563,
    "output_throughput": 3069.3943720819784,
    "total_throughput": 6574.383185663435,
    "itl": 31.503200088712,
    "ttft": 7605.587530333359,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 50997,
    "finished_requests": 50889,
    "scheduler_time": 24.298411722425012
}
#Debug simulation 
Total elapsed time: 3.694706865120679. Arrivals time: 0.12315557431429625 Scheduler time: 3.2799828709103167 Scheduler overhead time: 0.11010152567178011 Adapter cache time: 0.020244276616722345 Engine time: 0.10920696286484599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 152280 . Total input tokens: 33967274 . Total output tokens: 29831511
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.685330281034112,
    "estimated_duration": 3600.0188628881524,
    "input_throughput": 3504.9794127681557,
    "output_throughput": 3069.3861395868203,
    "total_throughput": 6574.365552354976,
    "itl": 31.50328298217714,
    "ttft": 7605.657512704591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 50997,
    "finished_requests": 50889,
    "scheduler_time": 24.29841255641099
}
#Debug simulation 
Total elapsed time: 3.685401182156056. Arrivals time: 0.12173136323690414 Scheduler time: 3.2729545375332236 Scheduler overhead time: 0.11067484458908439 Adapter cache time: 0.020105374976992607 Engine time: 0.10745141375809908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [5 5 6]
Adapter prompts. [8640, 1080, 1080, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 152280 . Total input tokens: 33967274 . Total output tokens: 29831511
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.6855284040793777,
    "estimated_duration": 3600.0082979753947,
    "input_throughput": 3504.989698800478,
    "output_throughput": 3069.3951472873864,
    "total_throughput": 6574.384846087864,
    "itl": 31.503173856203805,
    "ttft": 7605.60660883023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 50997,
    "finished_requests": 50889,
    "scheduler_time": 24.298411722425087
}
#Debug simulation 
Total elapsed time: 3.6856037210673094. Arrivals time: 0.12294171331450343 Scheduler time: 3.2687115338630974 Scheduler overhead time: 0.11019404465332627 Adapter cache time: 0.020197051111608744 Engine time: 0.11172249680384994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 17280, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 149580 . Total input tokens: 33344384 . Total output tokens: 29305645
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.656203345861286,
    "estimated_duration": 3599.9276097906186,
    "input_throughput": 3449.841315204206,
    "output_throughput": 3005.7107177856115,
    "total_throughput": 6455.552032989817,
    "itl": 30.71457139099616,
    "ttft": 7806.737390865359,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 50129,
    "finished_requests": 50021,
    "scheduler_time": 22.853070179009308
}
#Debug simulation 
Total elapsed time: 3.6562802167609334. Arrivals time: 0.12174071511253715 Scheduler time: 3.235712439753115 Scheduler overhead time: 0.112651901319623 Adapter cache time: 0.020036389585584402 Engine time: 0.11244323663413525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 17280, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 149580 . Total input tokens: 33344384 . Total output tokens: 29305645
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.621714513283223,
    "estimated_duration": 3599.9111586878216,
    "input_throughput": 3449.8570805082945,
    "output_throughput": 3005.7244534734705,
    "total_throughput": 6455.581533981765,
    "itl": 30.714648047495917,
    "ttft": 7806.813132504066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 50129,
    "finished_requests": 50021,
    "scheduler_time": 22.853012758685587
}
#Debug simulation 
Total elapsed time: 3.621789818163961. Arrivals time: 0.11990797193720937 Scheduler time: 3.2045451155863702 Scheduler overhead time: 0.11311675747856498 Adapter cache time: 0.019949831999838352 Engine time: 0.110905887093395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 17280, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 149580 . Total input tokens: 33344384 . Total output tokens: 29305645
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.6236748970113695,
    "estimated_duration": 3599.918800694085,
    "input_throughput": 3449.849757057162,
    "output_throughput": 3005.7180728392477,
    "total_throughput": 6455.567829896409,
    "itl": 30.714671882524406,
    "ttft": 7806.824509889415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 50129,
    "finished_requests": 50021,
    "scheduler_time": 22.85308961003854
}
#Debug simulation 
Total elapsed time: 3.6237514256499708. Arrivals time: 0.12072743941098452 Scheduler time: 3.204314154572785 Scheduler overhead time: 0.11252611503005028 Adapter cache time: 0.020044852048158646 Engine time: 0.11293990444391966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 17280, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 149580 . Total input tokens: 33344384 . Total output tokens: 29305645
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.6157367611303926,
    "estimated_duration": 3599.9309111415796,
    "input_throughput": 3449.8381514943394,
    "output_throughput": 3005.7079613699434,
    "total_throughput": 6455.546112864283,
    "itl": 30.714497800135312,
    "ttft": 7806.712741139437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 50129,
    "finished_requests": 50021,
    "scheduler_time": 22.85307830960031
}
#Debug simulation 
Total elapsed time: 3.6158121242187917. Arrivals time: 0.12079950654879212 Scheduler time: 3.2001356431283057 Scheduler overhead time: 0.11177599104121327 Adapter cache time: 0.019908249843865633 Engine time: 0.11041782097890973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 17280, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 149580 . Total input tokens: 33344384 . Total output tokens: 29305645
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.657673155888915,
    "estimated_duration": 3599.9158523354654,
    "input_throughput": 3449.852582510502,
    "output_throughput": 3005.7205345453403,
    "total_throughput": 6455.573117055842,
    "itl": 30.714714639708685,
    "ttft": 7806.87242208028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 50129,
    "finished_requests": 50021,
    "scheduler_time": 22.85306129638209
}
#Debug simulation 
Total elapsed time: 3.6577502922154963. Arrivals time: 0.1211148202419281 Scheduler time: 3.240138549823314 Scheduler overhead time: 0.11227425560355186 Adapter cache time: 0.020008176565170288 Engine time: 0.11082765134051442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 17280, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 149580 . Total input tokens: 33344384 . Total output tokens: 29305645
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.6359359808266163,
    "estimated_duration": 3599.9222719335576,
    "input_throughput": 3449.8464305257135,
    "output_throughput": 3005.7151745635542,
    "total_throughput": 6455.561605089268,
    "itl": 30.71454724833482,
    "ttft": 7806.84984673076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 50129,
    "finished_requests": 50021,
    "scheduler_time": 22.853029730928963
}
#Debug simulation 
Total elapsed time: 3.636021918617189. Arrivals time: 0.12200491456314921 Scheduler time: 3.2140973014757037 Scheduler overhead time: 0.11259344266727567 Adapter cache time: 0.0200089062564075 Engine time: 0.11396056832745671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [5 5 6]
Adapter prompts. [8640, 540, 540, 8640, 17280, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 149580 . Total input tokens: 33344384 . Total output tokens: 29305645
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.63346280856058,
    "estimated_duration": 3599.914896986638,
    "input_throughput": 3449.8534980356503,
    "output_throughput": 3005.721332206305,
    "total_throughput": 6455.574830241955,
    "itl": 30.714707824385698,
    "ttft": 7806.863744344408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 50129,
    "finished_requests": 50021,
    "scheduler_time": 22.853053206766038
}
#Debug simulation 
Total elapsed time: 3.633541357703507. Arrivals time: 0.12050390150398016 Scheduler time: 3.214764440432191 Scheduler overhead time: 0.11257015960291028 Adapter cache time: 0.019901536870747805 Engine time: 0.11257980624213815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 17280, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 148230 . Total input tokens: 33048826 . Total output tokens: 29044209
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.595671113114804,
    "estimated_duration": 3599.960622104203,
    "input_throughput": 3440.7190245208926,
    "output_throughput": 3001.2792177945266,
    "total_throughput": 6441.99824231542,
    "itl": 30.4787163215268,
    "ttft": 7438.723655855887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 49700,
    "finished_requests": 49598,
    "scheduler_time": 22.608192293131086
}
#Debug simulation 
Total elapsed time: 3.595742458011955. Arrivals time: 0.11757272342219949 Scheduler time: 3.1833799523301423 Scheduler overhead time: 0.11230720579624176 Adapter cache time: 0.019042522180825472 Engine time: 0.11048290552571416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 17280, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 148230 . Total input tokens: 33048826 . Total output tokens: 29044209
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.652834525797516,
    "estimated_duration": 3599.97410274337,
    "input_throughput": 3440.7061402360837,
    "output_throughput": 3001.267979057519,
    "total_throughput": 6441.974119293603,
    "itl": 30.47852916057711,
    "ttft": 7438.732956072934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 49700,
    "finished_requests": 49598,
    "scheduler_time": 22.608273230266718
}
#Debug simulation 
Total elapsed time: 3.6529308040626347. Arrivals time: 0.12033696286380291 Scheduler time: 3.2294329232536256 Scheduler overhead time: 0.11446300568059087 Adapter cache time: 0.019351505674421787 Engine time: 0.11544533539563417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 17280, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 148230 . Total input tokens: 33048826 . Total output tokens: 29044209
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.6492604268714786,
    "estimated_duration": 3599.9806066499327,
    "input_throughput": 3440.699924082807,
    "output_throughput": 3001.2625568153912,
    "total_throughput": 6441.962480898198,
    "itl": 30.47850645821398,
    "ttft": 7438.707191937457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 49700,
    "finished_requests": 49598,
    "scheduler_time": 22.608346036811504
}
#Debug simulation 
Total elapsed time: 3.6493348870426416. Arrivals time: 0.11969597917050123 Scheduler time: 3.2290909439325333 Scheduler overhead time: 0.11329996818676591 Adapter cache time: 0.019158847630023956 Engine time: 0.11455977708101273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 17280, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 148230 . Total input tokens: 33048826 . Total output tokens: 29044209
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.617214996367693,
    "estimated_duration": 3599.9679351712894,
    "input_throughput": 3440.7120349561233,
    "output_throughput": 3001.273120919038,
    "total_throughput": 6441.985155875161,
    "itl": 30.478663979615117,
    "ttft": 7438.754660907533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 49700,
    "finished_requests": 49598,
    "scheduler_time": 22.608252965251555
}
#Debug simulation 
Total elapsed time: 3.6172844041138887. Arrivals time: 0.1188659486360848 Scheduler time: 3.199406689964235 Scheduler overhead time: 0.11370871914550662 Adapter cache time: 0.019085823092609644 Engine time: 0.11267159041017294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 17280, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 148230 . Total input tokens: 33048826 . Total output tokens: 29044209
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.607302878983319,
    "estimated_duration": 3599.978099565895,
    "input_throughput": 3440.7023202429,
    "output_throughput": 3001.2646469440647,
    "total_throughput": 6441.966967186964,
    "itl": 30.478469134986753,
    "ttft": 7438.73536167893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 49700,
    "finished_requests": 49598,
    "scheduler_time": 22.60829749911496
}
#Debug simulation 
Total elapsed time: 3.607374347280711. Arrivals time: 0.11864669062197208 Scheduler time: 3.1934253526851535 Scheduler overhead time: 0.11272345623001456 Adapter cache time: 0.019162646029144526 Engine time: 0.11007959768176079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 17280, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 148230 . Total input tokens: 33048826 . Total output tokens: 29044209
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.630130562931299,
    "estimated_duration": 3599.9879857996134,
    "input_throughput": 3440.6928714371184,
    "output_throughput": 3001.2564049155167,
    "total_throughput": 6441.949276352635,
    "itl": 30.478678561460313,
    "ttft": 7438.647551442356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 49700,
    "finished_requests": 49598,
    "scheduler_time": 22.608346870797455
}
#Debug simulation 
Total elapsed time: 3.6302011609077454. Arrivals time: 0.1203665086068213 Scheduler time: 3.2104312609881163 Scheduler overhead time: 0.11393090523779392 Adapter cache time: 0.019370530266314745 Engine time: 0.11227669520303607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [5 5 6]
Adapter prompts. [8640, 270, 270, 8640, 17280, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 148230 . Total input tokens: 33048826 . Total output tokens: 29044209
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.6291781989857554,
    "estimated_duration": 3599.975501996568,
    "input_throughput": 3440.704802888353,
    "output_throughput": 3001.2668125124096,
    "total_throughput": 6441.971615400762,
    "itl": 30.4784690359342,
    "ttft": 7438.774310772466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 49700,
    "finished_requests": 49598,
    "scheduler_time": 22.608273230266686
}
#Debug simulation 
Total elapsed time: 3.629255810752511. Arrivals time: 0.11985633429139853 Scheduler time: 3.2099718526005745 Scheduler overhead time: 0.11280319653451443 Adapter cache time: 0.01918837195262313 Engine time: 0.1139605469070375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 17280, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147555 . Total input tokens: 32900141 . Total output tokens: 28921586
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.578312342055142,
    "estimated_duration": 3599.9122425634214,
    "input_throughput": 3405.225231621472,
    "output_throughput": 2952.96420682475,
    "total_throughput": 6358.189438446222,
    "itl": 30.11001045004706,
    "ttft": 7470.452926288008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 49483,
    "finished_requests": 49381,
    "scheduler_time": 21.659449796319855
}
#Debug simulation 
Total elapsed time: 3.57840464822948. Arrivals time: 0.11858970019966364 Scheduler time: 3.1598812374286354 Scheduler overhead time: 0.1141848904080689 Adapter cache time: 0.018783627543598413 Engine time: 0.11286353133618832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 17280, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147555 . Total input tokens: 32900141 . Total output tokens: 28921586
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.5666002347134054,
    "estimated_duration": 3599.932409915246,
    "input_throughput": 3405.206155047951,
    "output_throughput": 2952.9476638841325,
    "total_throughput": 6358.153818932084,
    "itl": 30.110074221982064,
    "ttft": 7470.564828360477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 49483,
    "finished_requests": 49381,
    "scheduler_time": 21.659664130171134
}
#Debug simulation 
Total elapsed time: 3.5666730320081115. Arrivals time: 0.11825092043727636 Scheduler time: 3.150217621587217 Scheduler overhead time: 0.1129211662337184 Adapter cache time: 0.018737674225121737 Engine time: 0.11271356884390116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 17280, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147555 . Total input tokens: 32900141 . Total output tokens: 28921586
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.5806669113226235,
    "estimated_duration": 3599.9393277835684,
    "input_throughput": 3405.19961139106,
    "output_throughput": 2952.9419893154127,
    "total_throughput": 6358.141600706473,
    "itl": 30.110098301133657,
    "ttft": 7470.575685615866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 49483,
    "finished_requests": 49381,
    "scheduler_time": 21.6596973226215
}
#Debug simulation 
Total elapsed time: 3.5807413212023675. Arrivals time: 0.11894130054861307 Scheduler time: 3.1620403337292373 Scheduler overhead time: 0.11431728955358267 Adapter cache time: 0.01873632101342082 Engine time: 0.11254119873046875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 17280, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147555 . Total input tokens: 32900141 . Total output tokens: 28921586
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.5841067121364176,
    "estimated_duration": 3599.922493814622,
    "input_throughput": 3405.2155347962475,
    "output_throughput": 2952.9557978720786,
    "total_throughput": 6358.171332668326,
    "itl": 30.110030388377083,
    "ttft": 7470.503619301361,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 49483,
    "finished_requests": 49381,
    "scheduler_time": 21.659546871712674
}
#Debug simulation 
Total elapsed time: 3.584181172773242. Arrivals time: 0.11933225113898516 Scheduler time: 3.1628046347759664 Scheduler overhead time: 0.11404411448165774 Adapter cache time: 0.018712923862040043 Engine time: 0.11532477848231792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 17280, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147555 . Total input tokens: 32900141 . Total output tokens: 28921586
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.5529705127701163,
    "estimated_duration": 3599.937819030187,
    "input_throughput": 3405.2010385286067,
    "output_throughput": 2952.943226909348,
    "total_throughput": 6358.144265437954,
    "itl": 30.110095113696385,
    "ttft": 7470.538907383517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 49483,
    "finished_requests": 49381,
    "scheduler_time": 21.659689233005466
}
#Debug simulation 
Total elapsed time: 3.553046455141157. Arrivals time: 0.11783632542937994 Scheduler time: 3.138401508331299 Scheduler overhead time: 0.11301254667341709 Adapter cache time: 0.01865829061716795 Engine time: 0.11134316585958004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 17280, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147555 . Total input tokens: 32900141 . Total output tokens: 28921586
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.565117435064167,
    "estimated_duration": 3599.9435578054317,
    "input_throughput": 3405.1956101981036,
    "output_throughput": 2952.938519536241,
    "total_throughput": 6358.134129734345,
    "itl": 30.10998695458118,
    "ttft": 7470.508021656598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 49483,
    "finished_requests": 49381,
    "scheduler_time": 21.65966500513205
}
#Debug simulation 
Total elapsed time: 3.565193022135645. Arrivals time: 0.11892394069582224 Scheduler time: 3.146166756749153 Scheduler overhead time: 0.11358406022191048 Adapter cache time: 0.018673283513635397 Engine time: 0.11394442757591605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [5 5 6]
Adapter prompts. [8640, 135, 135, 8640, 17280, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147555 . Total input tokens: 32900141 . Total output tokens: 28921586
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.5992216151207685,
    "estimated_duration": 3599.933939683099,
    "input_throughput": 3405.204708028368,
    "output_throughput": 2952.9464090487704,
    "total_throughput": 6358.151117077138,
    "itl": 30.11011065996669,
    "ttft": 7470.526370824139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 49483,
    "finished_requests": 49381,
    "scheduler_time": 21.659673053773247
}
#Debug simulation 
Total elapsed time: 3.5993199590593576. Arrivals time: 0.11937706219032407 Scheduler time: 3.1777645186521113 Scheduler overhead time: 0.11528403963893652 Adapter cache time: 0.018811247777193785 Engine time: 0.11380735132843256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 17280, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147210 . Total input tokens: 32814214 . Total output tokens: 28855485
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.6001636991277337,
    "estimated_duration": 3599.999580592999,
    "input_throughput": 3387.431505753469,
    "output_throughput": 2972.185346265373,
    "total_throughput": 6359.616852018842,
    "itl": 30.1414937817841,
    "ttft": 6685.106079335877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 49372,
    "finished_requests": 49281,
    "scheduler_time": 21.911521277022622
}
#Debug simulation 
Total elapsed time: 3.600247808266431. Arrivals time: 0.11937390780076385 Scheduler time: 3.1808158420026302 Scheduler overhead time: 0.1140026687644422 Adapter cache time: 0.018422742374241352 Engine time: 0.11376868840306997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 17280, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147210 . Total input tokens: 32814214 . Total output tokens: 28855485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.614726275205612,
    "estimated_duration": 3600.018249497231,
    "input_throughput": 3387.4553279564925,
    "output_throughput": 2972.2399328098822,
    "total_throughput": 6359.695260766374,
    "itl": 30.14161950983686,
    "ttft": 6612.110171974517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 49372,
    "finished_requests": 49282,
    "scheduler_time": 21.911696160679327
}
#Debug simulation 
Total elapsed time: 3.614805866032839. Arrivals time: 0.12101702624931931 Scheduler time: 3.1803495534695685 Scheduler overhead time: 0.11753161810338497 Adapter cache time: 0.021289606112986803 Engine time: 0.1199273862875998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 17280, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147210 . Total input tokens: 32814214 . Total output tokens: 28855485
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.578234849963337,
    "estimated_duration": 3600.0248972965956,
    "input_throughput": 3387.4490726877043,
    "output_throughput": 2972.2344442770805,
    "total_throughput": 6359.683516964785,
    "itl": 30.14167372676792,
    "ttft": 6612.188219599216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 49372,
    "finished_requests": 49282,
    "scheduler_time": 21.911773012031976
}
#Debug simulation 
Total elapsed time: 3.5783083979040384. Arrivals time: 0.11793411010876298 Scheduler time: 3.1621231185272336 Scheduler overhead time: 0.11417131079360843 Adapter cache time: 0.01835513487458229 Engine time: 0.11213499074801803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 17280, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147210 . Total input tokens: 32814214 . Total output tokens: 28855485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.588760763872415,
    "estimated_duration": 3600.0057573005906,
    "input_throughput": 3387.4256937700147,
    "output_throughput": 2972.1802467402526,
    "total_throughput": 6359.605940510267,
    "itl": 30.141556396358013,
    "ttft": 6685.168593802858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 49372,
    "finished_requests": 49281,
    "scheduler_time": 21.911553635487007
}
#Debug simulation 
Total elapsed time: 3.588856115937233. Arrivals time: 0.11937459418550134 Scheduler time: 3.1675225645303726 Scheduler overhead time: 0.11478581838309765 Adapter cache time: 0.01861752150580287 Engine time: 0.11431925091892481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 17280, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147210 . Total input tokens: 32814214 . Total output tokens: 28855485
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.604807250201702,
    "estimated_duration": 3600.0236183888355,
    "input_throughput": 3387.450276078394,
    "output_throughput": 2972.235500162846,
    "total_throughput": 6359.68577624124,
    "itl": 30.141698606850365,
    "ttft": 6612.108613063864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 49372,
    "finished_requests": 49282,
    "scheduler_time": 21.91176492241597
}
#Debug simulation 
Total elapsed time: 3.6048799538984895. Arrivals time: 0.11948669981211424 Scheduler time: 3.184970511123538 Scheduler overhead time: 0.11403846135362983 Adapter cache time: 0.018531143199652433 Engine time: 0.1140661658719182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 17280, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147210 . Total input tokens: 32814214 . Total output tokens: 28855485
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.6070109447464347,
    "estimated_duration": 3599.9955662902307,
    "input_throughput": 3387.435283029141,
    "output_throughput": 2972.1886605060836,
    "total_throughput": 6359.6239435352245,
    "itl": 30.14151292579639,
    "ttft": 6685.093701894291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 49372,
    "finished_requests": 49281,
    "scheduler_time": 21.911533493396643
}
#Debug simulation 
Total elapsed time: 3.607082469854504. Arrivals time: 0.12065927963703871 Scheduler time: 3.171355758793652 Scheduler overhead time: 0.11584526859223843 Adapter cache time: 0.021898258943110704 Engine time: 0.12109494535252452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [5 5 6]
Adapter prompts. [8640, 66, 66, 8640, 17280, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147210 . Total input tokens: 32814214 . Total output tokens: 28855485
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.618093370925635,
    "estimated_duration": 3600.0202764250967,
    "input_throughput": 3387.4534207095685,
    "output_throughput": 2972.238259342657,
    "total_throughput": 6359.691680052226,
    "itl": 30.14164372547458,
    "ttft": 6612.099607058113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 49372,
    "finished_requests": 49282,
    "scheduler_time": 21.91172042952749
}
#Debug simulation 
Total elapsed time: 3.6181679680012167. Arrivals time: 0.11985056940466166 Scheduler time: 3.19580937968567 Scheduler overhead time: 0.11440940480679274 Adapter cache time: 0.01851471047848463 Engine time: 0.11550311092287302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 17280, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147045 . Total input tokens: 32776383 . Total output tokens: 28826211
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.595426064915955,
    "estimated_duration": 3599.9995553408253,
    "input_throughput": 3378.4056950664135,
    "output_throughput": 2977.440645540639,
    "total_throughput": 6355.846340607052,
    "itl": 30.119879442533755,
    "ttft": 7424.385008245863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 49304,
    "finished_requests": 49203,
    "scheduler_time": 22.006937087165912
}
#Debug simulation 
Total elapsed time: 3.5955001767724752. Arrivals time: 0.11854244582355022 Scheduler time: 3.176832966506481 Scheduler overhead time: 0.11465282645076513 Adapter cache time: 0.01839698664844036 Engine time: 0.11271203216165304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 17280, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147045 . Total input tokens: 32776383 . Total output tokens: 28826211
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.601530690677464,
    "estimated_duration": 3600.017758155768,
    "input_throughput": 3378.444723625651,
    "output_throughput": 2977.4261462229742,
    "total_throughput": 6355.870869848625,
    "itl": 30.11965686700214,
    "ttft": 7351.361762247113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 49304,
    "finished_requests": 49204,
    "scheduler_time": 22.00707052485598
}
#Debug simulation 
Total elapsed time: 3.60160462372005. Arrivals time: 0.11785239726305008 Scheduler time: 3.1831089798361063 Scheduler overhead time: 0.11438528262078762 Adapter cache time: 0.018322660122066736 Engine time: 0.1138178464025259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 17280, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147045 . Total input tokens: 32776383 . Total output tokens: 28826211
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.590793971903622,
    "estimated_duration": 3600.0198776732846,
    "input_throughput": 3378.442734560864,
    "output_throughput": 2977.4243932585227,
    "total_throughput": 6355.867127819387,
    "itl": 30.119683294072622,
    "ttft": 7351.399733432588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 49304,
    "finished_requests": 49204,
    "scheduler_time": 22.00706247621495
}
#Debug simulation 
Total elapsed time: 3.590867409016937. Arrivals time: 0.1175278308801353 Scheduler time: 3.1740855798125267 Scheduler overhead time: 0.11381123773753643 Adapter cache time: 0.01826649671420455 Engine time: 0.11310868943110108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 17280, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147045 . Total input tokens: 32776383 . Total output tokens: 28826211
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.6014302200637758,
    "estimated_duration": 3600.0089237754323,
    "input_throughput": 3378.4530142899976,
    "output_throughput": 2977.433452792362,
    "total_throughput": 6355.88646708236,
    "itl": 30.119590818797835,
    "ttft": 7351.41212837035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 49304,
    "finished_requests": 49204,
    "scheduler_time": 22.00698966967032
}
#Debug simulation 
Total elapsed time: 3.601506487932056. Arrivals time: 0.11956895468756557 Scheduler time: 3.181579678785056 Scheduler overhead time: 0.11359800538048148 Adapter cache time: 0.018257672432810068 Engine time: 0.1147118047811091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 17280, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147045 . Total input tokens: 32776383 . Total output tokens: 28826211
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.6108844205737114,
    "estimated_duration": 3600.0195593240524,
    "input_throughput": 3378.4430333160885,
    "output_throughput": 2977.42465655175,
    "total_throughput": 6355.867689867839,
    "itl": 30.119652221418285,
    "ttft": 7351.396746579917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 49304,
    "finished_requests": 49204,
    "scheduler_time": 22.007062476214923
}
#Debug simulation 
Total elapsed time: 3.6109580085612833. Arrivals time: 0.12265085056424141 Scheduler time: 3.1878960938192904 Scheduler overhead time: 0.11480809794738889 Adapter cache time: 0.018417429644614458 Engine time: 0.11302120424807072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 17280, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147045 . Total input tokens: 32776383 . Total output tokens: 28826211
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.592024803161621,
    "estimated_duration": 3599.9932670964804,
    "input_throughput": 3378.4115962553687,
    "output_throughput": 2977.4458463487827,
    "total_throughput": 6355.857442604151,
    "itl": 30.11986163427537,
    "ttft": 7424.423405899134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 49304,
    "finished_requests": 49203,
    "scheduler_time": 22.00687237023718
}
#Debug simulation 
Total elapsed time: 3.5920970221050084. Arrivals time: 0.11875952547416091 Scheduler time: 3.173073250800371 Scheduler overhead time: 0.11425976268947124 Adapter cache time: 0.018243534956127405 Engine time: 0.11393111478537321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [5 5 6]
Adapter prompts. [8640, 33, 33, 8640, 17280, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 17280]
Prompts retrieved: 147045 . Total input tokens: 32776383 . Total output tokens: 28826211
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.597514323890209,
    "estimated_duration": 3600.0187038968797,
    "input_throughput": 3378.443836093021,
    "output_throughput": 2977.4253640397287,
    "total_throughput": 6355.86920013275,
    "itl": 30.119672900756576,
    "ttft": 7351.361161224361,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 49304,
    "finished_requests": 49204,
    "scheduler_time": 22.007062476214838
}
#Debug simulation 
Total elapsed time: 3.597589109092951. Arrivals time: 0.11703886976465583 Scheduler time: 3.1822119895368814 Scheduler overhead time: 0.11398179410025477 Adapter cache time: 0.01827859692275524 Engine time: 0.11218546703457832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 130680 . Total input tokens: 29122604 . Total output tokens: 25622227
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.220560245681554,
    "estimated_duration": 3599.9542416569943,
    "input_throughput": 3018.7125364676886,
    "output_throughput": 2612.4912620197965,
    "total_throughput": 5631.203798487485,
    "itl": 29.24070658959338,
    "ttft": 5887.119127256197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 43777,
    "finished_requests": 43706,
    "scheduler_time": 16.041628641904197
}
#Debug simulation 
Total elapsed time: 3.2206335309892893. Arrivals time: 0.10751987248659134 Scheduler time: 2.8008151780813932 Scheduler overhead time: 0.11537108244374394 Adapter cache time: 0.022563752252608538 Engine time: 0.12000408675521612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 130680 . Total input tokens: 29122604 . Total output tokens: 25622227
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.214188660029322,
    "estimated_duration": 3599.9496661009684,
    "input_throughput": 3018.716373268094,
    "output_throughput": 2612.4945825107047,
    "total_throughput": 5631.210955778798,
    "itl": 29.240830832646854,
    "ttft": 5887.155440142156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 43777,
    "finished_requests": 43706,
    "scheduler_time": 16.041716752720212
}
#Debug simulation 
Total elapsed time: 3.2142655020579696. Arrivals time: 0.10823793616145849 Scheduler time: 2.7983127175830305 Scheduler overhead time: 0.1157591724768281 Adapter cache time: 0.022719344589859247 Engine time: 0.11458009947091341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 130680 . Total input tokens: 29122604 . Total output tokens: 25622227
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.2128758491016924,
    "estimated_duration": 3599.951785622361,
    "input_throughput": 3018.714595957087,
    "output_throughput": 2612.4930443683947,
    "total_throughput": 5631.207640325481,
    "itl": 29.240697152716972,
    "ttft": 5887.14613292586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 43777,
    "finished_requests": 43706,
    "scheduler_time": 16.041700614462982
}
#Debug simulation 
Total elapsed time: 3.212946889922023. Arrivals time: 0.10739431111142039 Scheduler time: 2.799320725724101 Scheduler overhead time: 0.11611854331567883 Adapter cache time: 0.022698954213410616 Engine time: 0.11264168377965689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 130680 . Total input tokens: 29122604 . Total output tokens: 25622227
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.2290175021626055,
    "estimated_duration": 3599.940231267298,
    "input_throughput": 3018.724284812467,
    "output_throughput": 2612.5014294165608,
    "total_throughput": 5631.225714229027,
    "itl": 29.240623268448733,
    "ttft": 5887.1290388875905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 43777,
    "finished_requests": 43706,
    "scheduler_time": 16.041643987150312
}
#Debug simulation 
Total elapsed time: 3.2290910612791777. Arrivals time: 0.10831498354673386 Scheduler time: 2.809194419067353 Scheduler overhead time: 0.11627519084140658 Adapter cache time: 0.022635304369032383 Engine time: 0.11758579639717937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 130680 . Total input tokens: 29122604 . Total output tokens: 25622227
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.1975845210254192,
    "estimated_duration": 3599.951873318072,
    "input_throughput": 3018.7145224204587,
    "output_throughput": 2612.492980727423,
    "total_throughput": 5631.207503147882,
    "itl": 29.24071710529257,
    "ttft": 5887.141185606415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 43777,
    "finished_requests": 43706,
    "scheduler_time": 16.041700614462997
}
#Debug simulation 
Total elapsed time: 3.1976570608094335. Arrivals time: 0.1079106661491096 Scheduler time: 2.7834245078265667 Scheduler overhead time: 0.11471399897709489 Adapter cache time: 0.022558012045919895 Engine time: 0.11469755740836263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 130680 . Total input tokens: 29122604 . Total output tokens: 25622227
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.2320358850993216,
    "estimated_duration": 3599.9518340495297,
    "input_throughput": 3018.7145553488212,
    "output_throughput": 2612.4930092246905,
    "total_throughput": 5631.207564573512,
    "itl": 29.2406227525482,
    "ttft": 5887.121132869848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 43777,
    "finished_requests": 43706,
    "scheduler_time": 16.041612462672035
}
#Debug simulation 
Total elapsed time: 3.2321100058034062. Arrivals time: 0.10827256459742785 Scheduler time: 2.814329106360674 Scheduler overhead time: 0.11658376082777977 Adapter cache time: 0.022577479481697083 Engine time: 0.11521375691518188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [5 5 6]
Adapter prompts. [4320, 1080, 1080, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 130680 . Total input tokens: 29122604 . Total output tokens: 25622227
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.2057967809960246,
    "estimated_duration": 3599.951122358794,
    "input_throughput": 3018.7151521322526,
    "output_throughput": 2612.493525700334,
    "total_throughput": 5631.208677832587,
    "itl": 29.240821240676677,
    "ttft": 5887.130047283887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 43777,
    "finished_requests": 43706,
    "scheduler_time": 16.041708663104128
}
#Debug simulation 
Total elapsed time: 3.2058688611723483. Arrivals time: 0.1073683756403625 Scheduler time: 2.7904003360308707 Scheduler overhead time: 0.11606264114379883 Adapter cache time: 0.022558207157999277 Engine time: 0.11487839929759502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 17280, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 127980 . Total input tokens: 28518830 . Total output tokens: 25089071
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.1473638392053545,
    "estimated_duration": 3600.0052251978573,
    "input_throughput": 2941.934341058607,
    "output_throughput": 2559.71489582867,
    "total_throughput": 5501.649236887277,
    "itl": 28.73219952392497,
    "ttft": 7261.345370319498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 42918,
    "finished_requests": 42831,
    "scheduler_time": 14.867308082129592
}
#Debug simulation 
Total elapsed time: 3.147436353377998. Arrivals time: 0.10479059955105186 Scheduler time: 2.7316523492336273 Scheduler overhead time: 0.11712042381986976 Adapter cache time: 0.021603308152407408 Engine time: 0.11666078772395849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 17280, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 127980 . Total input tokens: 28518830 . Total output tokens: 25089071
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.1640241290442646,
    "estimated_duration": 3600.0268575598575,
    "input_throughput": 2941.91666313809,
    "output_throughput": 2559.69951464363,
    "total_throughput": 5501.61617778172,
    "itl": 28.732199535876482,
    "ttft": 7261.362853554431,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 42918,
    "finished_requests": 42831,
    "scheduler_time": 14.86748196790033
}
#Debug simulation 
Total elapsed time: 3.164096785709262. Arrivals time: 0.1063081193715334 Scheduler time: 2.745110764168203 Scheduler overhead time: 0.11715212091803551 Adapter cache time: 0.021646915934979916 Engine time: 0.1181816766038537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 17280, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 127980 . Total input tokens: 28518830 . Total output tokens: 25089071
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.1846763812936842,
    "estimated_duration": 3600.031151710675,
    "input_throughput": 2941.9131539923874,
    "output_throughput": 2559.696461410116,
    "total_throughput": 5501.609615402504,
    "itl": 28.73215947635177,
    "ttft": 7261.2624141603155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 42918,
    "finished_requests": 42831,
    "scheduler_time": 14.86749009849136
}
#Debug simulation 
Total elapsed time: 3.1847661579959095. Arrivals time: 0.10799969686195254 Scheduler time: 2.7641758620738983 Scheduler overhead time: 0.11791087361052632 Adapter cache time: 0.02173634711652994 Engine time: 0.11726186005398631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 17280, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 127980 . Total input tokens: 28518830 . Total output tokens: 25089071
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.1755869989283383,
    "estimated_duration": 3600.0160045506414,
    "input_throughput": 2941.9255321677324,
    "output_throughput": 2559.7072313988856,
    "total_throughput": 5501.632763566618,
    "itl": 28.73208507383881,
    "ttft": 7261.4549680217115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 42918,
    "finished_requests": 42831,
    "scheduler_time": 14.867412413152522
}
#Debug simulation 
Total elapsed time: 3.1756733688525856. Arrivals time: 0.10807397169992328 Scheduler time: 2.754740848671645 Scheduler overhead time: 0.11776577681303024 Adapter cache time: 0.02169477567076683 Engine time: 0.11769256228581071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 17280, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 127980 . Total input tokens: 28518830 . Total output tokens: 25089071
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.139165828935802,
    "estimated_duration": 3600.0303089173576,
    "input_throughput": 2941.9138427156854,
    "output_throughput": 2559.6970606537025,
    "total_throughput": 5501.610903369387,
    "itl": 28.73218516036543,
    "ttft": 7261.256632401239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 42918,
    "finished_requests": 42831,
    "scheduler_time": 14.867490098491372
}
#Debug simulation 
Total elapsed time: 3.139240142889321. Arrivals time: 0.1044354597106576 Scheduler time: 2.7268640506081283 Scheduler overhead time: 0.11692571127787232 Adapter cache time: 0.02151051117107272 Engine time: 0.11412361497059464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 17280, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 127980 . Total input tokens: 28518830 . Total output tokens: 25089071
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.140916450880468,
    "estimated_duration": 3600.0024133186917,
    "input_throughput": 2941.936638935922,
    "output_throughput": 2559.716895163159,
    "total_throughput": 5501.653534099081,
    "itl": 28.732216935137743,
    "ttft": 7261.2820239746425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 42918,
    "finished_requests": 42831,
    "scheduler_time": 14.867307248143529
}
#Debug simulation 
Total elapsed time: 3.1409950680099428. Arrivals time: 0.10431686555966735 Scheduler time: 2.7250860785134137 Scheduler overhead time: 0.11767225665971637 Adapter cache time: 0.0217730519361794 Engine time: 0.11671936232596636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [5 5 6]
Adapter prompts. [4320, 540, 540, 4320, 17280, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 127980 . Total input tokens: 28518830 . Total output tokens: 25089071
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.153426024131477,
    "estimated_duration": 3600.026799176989,
    "input_throughput": 2941.916710848161,
    "output_throughput": 2559.6995561551544,
    "total_throughput": 5501.616267003315,
    "itl": 28.732184988644686,
    "ttft": 7261.35902126493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 42918,
    "finished_requests": 42831,
    "scheduler_time": 14.867473878284256
}
#Debug simulation 
Total elapsed time: 3.15352145396173. Arrivals time: 0.10546656651422381 Scheduler time: 2.7381812226958573 Scheduler overhead time: 0.11569004319608212 Adapter cache time: 0.02122998284175992 Engine time: 0.11754118744283915 
