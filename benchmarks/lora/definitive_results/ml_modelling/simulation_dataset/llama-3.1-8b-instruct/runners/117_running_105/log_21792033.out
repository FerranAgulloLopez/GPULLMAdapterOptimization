INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 621175011 . Total output tokens: 547344786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 85.74083725642413,
    "estimated_duration": 3600.023053386261,
    "input_throughput": 7395.70541776287,
    "output_throughput": 6444.84372903559,
    "total_throughput": 13840.54914679846,
    "itl": 88.29783757571444,
    "ttft": 1966152.8986452213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9818298442568607,
    "arrivals": 927414,
    "finished_requests": 108058,
    "scheduler_time": 282.83477708338535
}
#Debug simulation 
Total elapsed time: 85.74104508804157. Arrivals time: 0.599448021966964 Scheduler time: 84.9255974511616 Scheduler overhead time: 0.08310058852657676 Adapter cache time: 0.016629548277705908 Engine time: 0.08328764932230115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 621175011 . Total output tokens: 547344786
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 85.77047572797164,
    "estimated_duration": 3600.0092852357716,
    "input_throughput": 7372.15126328816,
    "output_throughput": 6417.086226622617,
    "total_throughput": 13789.237489910776,
    "itl": 86.39395978181992,
    "ttft": 1965695.4805937558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0717852838151223,
    "arrivals": 927414,
    "finished_requests": 107622,
    "scheduler_time": 284.261555262447
}
#Debug simulation 
Total elapsed time: 85.77066660206765. Arrivals time: 0.5020748502574861 Scheduler time: 85.05026996647939 Scheduler overhead time: 0.08498655213043094 Adapter cache time: 0.016873043030500412 Engine time: 0.08288435405120254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 621175011 . Total output tokens: 547344786
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 85.03953897580504,
    "estimated_duration": 3600.00255914747,
    "input_throughput": 7451.604702845743,
    "output_throughput": 6488.238165456031,
    "total_throughput": 13939.842868301776,
    "itl": 88.41074477923907,
    "ttft": 1959819.519939688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9024084402900119,
    "arrivals": 927414,
    "finished_requests": 108807,
    "scheduler_time": 280.83681403656806
}
#Debug simulation 
Total elapsed time: 85.03970814682543. Arrivals time: 0.5169338919222355 Scheduler time: 84.30877665476874 Scheduler overhead time: 0.08318752981722355 Adapter cache time: 0.01653974363580346 Engine time: 0.08171841874718666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 621175011 . Total output tokens: 547344786
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 90.32560141803697,
    "estimated_duration": 3600.051108172182,
    "input_throughput": 7266.1801774478845,
    "output_throughput": 6337.983910340839,
    "total_throughput": 13604.164087788724,
    "itl": 85.32004680898758,
    "ttft": 1970111.9320071572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8722124696336728,
    "arrivals": 927414,
    "finished_requests": 106161,
    "scheduler_time": 288.3929039537541
}
#Debug simulation 
Total elapsed time: 90.32576394174248. Arrivals time: 0.5131301409564912 Scheduler time: 89.59101629490033 Scheduler overhead time: 0.08572574378922582 Adapter cache time: 0.016770565882325172 Engine time: 0.0856249388307333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 90.02832106687129,
    "estimated_duration": 3600.0912414568493,
    "input_throughput": 7394.160373898697,
    "output_throughput": 6447.42047998958,
    "total_throughput": 13841.580853888276,
    "itl": 88.88715670809404,
    "ttft": 1960306.5679736754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6663270760513822,
    "arrivals": 926784,
    "finished_requests": 107870,
    "scheduler_time": 282.5106959206186
}
#Debug simulation 
Total elapsed time: 90.02849694993347. Arrivals time: 0.5089201238006353 Scheduler time: 89.3034577909857 Scheduler overhead time: 0.08434798521921039 Adapter cache time: 0.01661268575116992 Engine time: 0.08258504513651133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 86.846677234862,
    "estimated_duration": 3600.0633476819453,
    "input_throughput": 7412.51762060871,
    "output_throughput": 6466.2537160600605,
    "total_throughput": 13878.771336668771,
    "itl": 88.03490623535662,
    "ttft": 1964274.3491134022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9174713616585382,
    "arrivals": 926784,
    "finished_requests": 108180,
    "scheduler_time": 281.74677129821737
}
#Debug simulation 
Total elapsed time: 86.84684098185971. Arrivals time: 0.5385225391946733 Scheduler time: 86.09224477317184 Scheduler overhead time: 0.0844559078104794 Adapter cache time: 0.016288158483803272 Engine time: 0.08239085739478469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 81.14004114409909,
    "estimated_duration": 3600.0209782949705,
    "input_throughput": 7357.791290578882,
    "output_throughput": 6417.618713694893,
    "total_throughput": 13775.410004273775,
    "itl": 86.49609702524342,
    "ttft": 1966170.6652703164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.042126492927789,
    "arrivals": 926784,
    "finished_requests": 107410,
    "scheduler_time": 284.05877493529897
}
#Debug simulation 
Total elapsed time: 81.14020190481097. Arrivals time: 0.5029896730557084 Scheduler time: 80.42083743773401 Scheduler overhead time: 0.08425799803808331 Adapter cache time: 0.016543595120310783 Engine time: 0.08298767544329166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 86.40242610685527,
    "estimated_duration": 3600.005649658048,
    "input_throughput": 7386.120908595158,
    "output_throughput": 6442.688500281407,
    "total_throughput": 13828.809408876565,
    "itl": 87.80767876113579,
    "ttft": 1960041.4307584343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8008607999002548,
    "arrivals": 926784,
    "finished_requests": 107764,
    "scheduler_time": 283.27521529386587
}
#Debug simulation 
Total elapsed time: 86.40267773298547. Arrivals time: 0.612993145827204 Scheduler time: 85.57361816847697 Scheduler overhead time: 0.0840572384186089 Adapter cache time: 0.01643243432044983 Engine time: 0.08238324476405978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 83.74605718115345,
    "estimated_duration": 3600.022055206464,
    "input_throughput": 7313.471583298407,
    "output_throughput": 6379.395916973871,
    "total_throughput": 13692.867500272278,
    "itl": 85.93368428135425,
    "ttft": 1967825.9593146811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9672142748395043,
    "arrivals": 926784,
    "finished_requests": 106789,
    "scheduler_time": 286.0072393606639
}
#Debug simulation 
Total elapsed time: 83.74621800798923. Arrivals time: 0.5104593522846699 Scheduler time: 83.0189023851417 Scheduler overhead time: 0.08470821427181363 Adapter cache time: 0.01642467873170972 Engine time: 0.0822842107154429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 86.15033985907212,
    "estimated_duration": 3600.022126315771,
    "input_throughput": 7386.883209858204,
    "output_throughput": 6437.789043179657,
    "total_throughput": 13824.672253037861,
    "itl": 87.77538105547916,
    "ttft": 1968759.7246052756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6406676817266215,
    "arrivals": 926784,
    "finished_requests": 107795,
    "scheduler_time": 283.1230160827876
}
#Debug simulation 
Total elapsed time: 86.15051357122138. Arrivals time: 0.5256531182676554 Scheduler time: 85.40836195694283 Scheduler overhead time: 0.08419298473745584 Adapter cache time: 0.01650429703295231 Engine time: 0.08299235068261623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 85.2608214886859,
    "estimated_duration": 3600.0577934195517,
    "input_throughput": 7303.209700704915,
    "output_throughput": 6361.766481044631,
    "total_throughput": 13664.976181749546,
    "itl": 85.71194535063371,
    "ttft": 1970124.805991399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8609796248935218,
    "arrivals": 926784,
    "finished_requests": 106541,
    "scheduler_time": 286.84854688462116
}
#Debug simulation 
Total elapsed time: 85.26098758075386. Arrivals time: 0.5242384825833142 Scheduler time: 84.51688347477466 Scheduler overhead time: 0.08547015441581607 Adapter cache time: 0.016751985531300306 Engine time: 0.08393116481602192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 84.62613663310185,
    "estimated_duration": 3600.0184397496105,
    "input_throughput": 7357.919811611395,
    "output_throughput": 6434.673429509204,
    "total_throughput": 13792.593241120598,
    "itl": 88.18473060061416,
    "ttft": 1947056.0607796854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2548314798949343,
    "arrivals": 850231,
    "finished_requests": 107477,
    "scheduler_time": 283.098024703302
}
#Debug simulation 
Total elapsed time: 84.62630686024204. Arrivals time: 0.5854554795660079 Scheduler time: 83.82549159228802 Scheduler overhead time: 0.08336079306900501 Adapter cache time: 0.017114276066422462 Engine time: 0.08201877819374204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 83.70411759382114,
    "estimated_duration": 3600.081977456314,
    "input_throughput": 7359.807406030526,
    "output_throughput": 6424.9461942372345,
    "total_throughput": 13784.75360026776,
    "itl": 87.33114317402486,
    "ttft": 1954411.1280757424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5195242903381616,
    "arrivals": 850231,
    "finished_requests": 107427,
    "scheduler_time": 283.54174165148385
}
#Debug simulation 
Total elapsed time: 83.7042722068727. Arrivals time: 0.5160022159107029 Scheduler time: 82.97002449631691 Scheduler overhead time: 0.08483736263588071 Adapter cache time: 0.017229844350367785 Engine time: 0.08340562554076314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 81.43380312994123,
    "estimated_duration": 3600.0940492231557,
    "input_throughput": 7305.02193565606,
    "output_throughput": 6380.351648023346,
    "total_throughput": 13685.373583679406,
    "itl": 85.4998634997795,
    "ttft": 1955399.9498426563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.594858997720308,
    "arrivals": 850231,
    "finished_requests": 106645,
    "scheduler_time": 285.69158007399284
}
#Debug simulation 
Total elapsed time: 81.43396623292938. Arrivals time: 0.5836267094127834 Scheduler time: 80.63351484574378 Scheduler overhead time: 0.08416866045445204 Adapter cache time: 0.017123485915362835 Engine time: 0.08261255593970418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 83.66963866399601,
    "estimated_duration": 3600.041403858454,
    "input_throughput": 7356.767333735176,
    "output_throughput": 6427.519965520316,
    "total_throughput": 13784.287299255493,
    "itl": 87.08799770781174,
    "ttft": 1947509.8822409774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 353,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4187223878456248,
    "arrivals": 850231,
    "finished_requests": 107360,
    "scheduler_time": 283.4118622700151
}
#Debug simulation 
Total elapsed time: 83.66980059864. Arrivals time: 0.5052011422812939 Scheduler time: 82.9486420894973 Scheduler overhead time: 0.08428119448944926 Adapter cache time: 0.016794202383607626 Engine time: 0.08184281084686518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 82.00790084572509,
    "estimated_duration": 3600.075482856273,
    "input_throughput": 7305.059609232069,
    "output_throughput": 6380.3845528749525,
    "total_throughput": 13685.444162107022,
    "itl": 85.49947010853657,
    "ttft": 1955394.9430291655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.570419170879773,
    "arrivals": 850231,
    "finished_requests": 106645,
    "scheduler_time": 285.69179018553154
}
#Debug simulation 
Total elapsed time: 82.00815057102591. Arrivals time: 0.5212354557588696 Scheduler time: 81.26951198931783 Scheduler overhead time: 0.0846374649554491 Adapter cache time: 0.017111417837440968 Engine time: 0.08257064083591104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 83.60620197094977,
    "estimated_duration": 3600.080324637743,
    "input_throughput": 7360.372716868851,
    "output_throughput": 6425.494131808761,
    "total_throughput": 13785.866848677613,
    "itl": 87.32405828574525,
    "ttft": 1954301.451614456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.196068803556255,
    "arrivals": 850231,
    "finished_requests": 107436,
    "scheduler_time": 283.56594244621743
}
#Debug simulation 
Total elapsed time: 83.60636670095846. Arrivals time: 0.5958051802590489 Scheduler time: 82.79585863742977 Scheduler overhead time: 0.08356582513079047 Adapter cache time: 0.01700709853321314 Engine time: 0.08184888772666454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 81.47053219983354,
    "estimated_duration": 3600.0431885929675,
    "input_throughput": 7305.125139423271,
    "output_throughput": 6380.441788249071,
    "total_throughput": 13685.566927672342,
    "itl": 85.49842005193429,
    "ttft": 1955378.6012664943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5466006955690816,
    "arrivals": 850231,
    "finished_requests": 106645,
    "scheduler_time": 285.6908992391681
}
#Debug simulation 
Total elapsed time: 81.47069802787155. Arrivals time: 0.5268595782108605 Scheduler time: 80.72629307722673 Scheduler overhead time: 0.08429465815424919 Adapter cache time: 0.01712416484951973 Engine time: 0.08317932020872831 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 86.06387993693352,
    "estimated_duration": 3600.0164449615954,
    "input_throughput": 7403.122015539658,
    "output_throughput": 6424.172876312157,
    "total_throughput": 13827.294891851816,
    "itl": 88.20412308453623,
    "ttft": 1924304.866459984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.287893525054685,
    "arrivals": 793158,
    "finished_requests": 107756,
    "scheduler_time": 284.08053148525573
}
#Debug simulation 
Total elapsed time: 86.06404215469956. Arrivals time: 0.5245249215513468 Scheduler time: 85.32350489171222 Scheduler overhead time: 0.0842077904380858 Adapter cache time: 0.017366566695272923 Engine time: 0.08209374826401472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 87.47206145012751,
    "estimated_duration": 3600.0078880453643,
    "input_throughput": 7367.28857958152,
    "output_throughput": 6390.5793307834265,
    "total_throughput": 13757.867910364947,
    "itl": 86.74931731741738,
    "ttft": 1926535.09879677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.143968367329801,
    "arrivals": 793158,
    "finished_requests": 107112,
    "scheduler_time": 285.7833853221444
}
#Debug simulation 
Total elapsed time: 87.47222364414483. Arrivals time: 0.5327788684517145 Scheduler time: 86.72119577042758 Scheduler overhead time: 0.08494853414595127 Adapter cache time: 0.016787916887551546 Engine time: 0.08318598615005612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.5676119280979,
    "estimated_duration": 3600.060514348581,
    "input_throughput": 7303.972779123672,
    "output_throughput": 6323.9873077854545,
    "total_throughput": 13627.960086909126,
    "itl": 85.08718621289141,
    "ttft": 1928034.2096842758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6687845058785906,
    "arrivals": 793158,
    "finished_requests": 106199,
    "scheduler_time": 288.14857971526993
}
#Debug simulation 
Total elapsed time: 82.56777056492865. Arrivals time: 0.502420109231025 Scheduler time: 81.84389759367332 Scheduler overhead time: 0.085639757104218 Adapter cache time: 0.01748791988939047 Engine time: 0.08466765563935041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 87.9232936669141,
    "estimated_duration": 3600.053849162262,
    "input_throughput": 7367.23507793413,
    "output_throughput": 6390.5277431762825,
    "total_throughput": 13757.762821110413,
    "itl": 86.74627096924537,
    "ttft": 1926447.0860324944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.007922711945137,
    "arrivals": 793158,
    "finished_requests": 107114,
    "scheduler_time": 285.79939262912427
}
#Debug simulation 
Total elapsed time: 87.92344462079927. Arrivals time: 0.5144689860753715 Scheduler time: 87.19038752559572 Scheduler overhead time: 0.08512051310390234 Adapter cache time: 0.01707519544288516 Engine time: 0.083309066016227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 84.4614993412979,
    "estimated_duration": 3600.0488103102048,
    "input_throughput": 7298.504099375244,
    "output_throughput": 6329.910009758016,
    "total_throughput": 13628.41410913326,
    "itl": 84.97613620614993,
    "ttft": 1925453.6253735933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.555041768592796,
    "arrivals": 793158,
    "finished_requests": 106208,
    "scheduler_time": 288.9094684496205
}
#Debug simulation 
Total elapsed time: 84.4616568852216. Arrivals time: 0.5091581656597555 Scheduler time: 83.72994261747226 Scheduler overhead time: 0.08653240511193871 Adapter cache time: 0.017452277708798647 Engine time: 0.08465871447697282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 82.21948728198186,
    "estimated_duration": 3600.053110093902,
    "input_throughput": 7363.512756429461,
    "output_throughput": 6383.36666077701,
    "total_throughput": 13746.87941720647,
    "itl": 87.32645403990148,
    "ttft": 1925982.8896338358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3748185898922287,
    "arrivals": 793158,
    "finished_requests": 107180,
    "scheduler_time": 285.31380228017787
}
#Debug simulation 
Total elapsed time: 82.21972473990172. Arrivals time: 0.5120170270092785 Scheduler time: 81.48898810008541 Scheduler overhead time: 0.0847068247385323 Adapter cache time: 0.01742974016815424 Engine time: 0.08326962823048234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 83.70786122977734,
    "estimated_duration": 3600.0567065003343,
    "input_throughput": 7356.672174685241,
    "output_throughput": 6372.712673824064,
    "total_throughput": 13729.384848509306,
    "itl": 85.48423405991478,
    "ttft": 1932886.5199868067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6260112777538747,
    "arrivals": 793158,
    "finished_requests": 107087,
    "scheduler_time": 285.95689484200057
}
#Debug simulation 
Total elapsed time: 83.70801461907104. Arrivals time: 0.8338375766761601 Scheduler time: 82.65714953234419 Scheduler overhead time: 0.084312638733536 Adapter cache time: 0.017256914172321558 Engine time: 0.08268489968031645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 92.16979368077591,
    "estimated_duration": 3600.0316119723684,
    "input_throughput": 7384.5759886077485,
    "output_throughput": 6404.348484975738,
    "total_throughput": 13788.924473583487,
    "itl": 87.92267316547553,
    "ttft": 1933333.1889562956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7589008024986812,
    "arrivals": 783512,
    "finished_requests": 107534,
    "scheduler_time": 284.24247530971206
}
#Debug simulation 
Total elapsed time: 92.16996132209897. Arrivals time: 0.5232205246575177 Scheduler time: 91.42784205451608 Scheduler overhead time: 0.08535565389320254 Adapter cache time: 0.016954977065324783 Engine time: 0.08403550647199154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 81.5095721706748,
    "estimated_duration": 3600.0945314381775,
    "input_throughput": 7369.599539210213,
    "output_throughput": 6397.106464534926,
    "total_throughput": 13766.70600374514,
    "itl": 87.68586580057199,
    "ttft": 1922659.4726504483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5761485417559777,
    "arrivals": 783512,
    "finished_requests": 107352,
    "scheduler_time": 284.56168191206194
}
#Debug simulation 
Total elapsed time: 81.50973207084462. Arrivals time: 0.6006032773293555 Scheduler time: 80.69147989712656 Scheduler overhead time: 0.08421923127025366 Adapter cache time: 0.017353748437017202 Engine time: 0.0832348489202559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.67074816767126,
    "estimated_duration": 3600.0081183045822,
    "input_throughput": 7305.410747902903,
    "output_throughput": 6363.321205727861,
    "total_throughput": 13668.731953630762,
    "itl": 85.72778728339954,
    "ttft": 1923477.6738074867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3997923889570054,
    "arrivals": 783512,
    "finished_requests": 106466,
    "scheduler_time": 287.5617256189894
}
#Debug simulation 
Total elapsed time: 82.67091227276251. Arrivals time: 0.5163768613711 Scheduler time: 81.9333857302554 Scheduler overhead time: 0.08620395418256521 Adapter cache time: 0.017270877957344055 Engine time: 0.08451951388269663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 87.54803650826216,
    "estimated_duration": 3600.0211591553666,
    "input_throughput": 7328.955257082511,
    "output_throughput": 6379.471671046599,
    "total_throughput": 13708.42692812911,
    "itl": 86.75841201518976,
    "ttft": 1916197.7754768291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.059551263395695,
    "arrivals": 783512,
    "finished_requests": 106734,
    "scheduler_time": 286.8736076079782
}
#Debug simulation 
Total elapsed time: 87.54819572204724. Arrivals time: 0.520428579300642 Scheduler time: 86.80716736335307 Scheduler overhead time: 0.0853675794787705 Adapter cache time: 0.017606277484446764 Engine time: 0.0839700042270124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 81.58920330228284,
    "estimated_duration": 3600.088225847573,
    "input_throughput": 7298.668907985955,
    "output_throughput": 6334.527814142951,
    "total_throughput": 13633.196722128905,
    "itl": 85.77975283401732,
    "ttft": 1929857.7680451702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5620841113478097,
    "arrivals": 783512,
    "finished_requests": 106347,
    "scheduler_time": 287.78016252139287
}
#Debug simulation 
Total elapsed time: 81.58936306927353. Arrivals time: 0.5177080659195781 Scheduler time: 80.85015921015292 Scheduler overhead time: 0.08623414812609553 Adapter cache time: 0.017433887347579002 Engine time: 0.08442481560632586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 86.53827595664188,
    "estimated_duration": 3600.0244190193534,
    "input_throughput": 7334.860247196019,
    "output_throughput": 6383.834753615445,
    "total_throughput": 13718.695000811464,
    "itl": 86.98917004741591,
    "ttft": 1914261.0573798814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.042854700982563,
    "arrivals": 783512,
    "finished_requests": 106850,
    "scheduler_time": 286.6774603331375
}
#Debug simulation 
Total elapsed time: 86.53843147680163. Arrivals time: 0.5986088239587843 Scheduler time: 85.72132491599768 Scheduler overhead time: 0.08491354063153267 Adapter cache time: 0.017081673722714186 Engine time: 0.08342516981065273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 84.66507477592677,
    "estimated_duration": 3600.0003024645293,
    "input_throughput": 7299.806331129865,
    "output_throughput": 6332.497523512255,
    "total_throughput": 13632.30385464212,
    "itl": 85.4172209778916,
    "ttft": 1924866.4887572678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3319161688163987,
    "arrivals": 783512,
    "finished_requests": 106334,
    "scheduler_time": 287.71331035795254
}
#Debug simulation 
Total elapsed time: 84.66523897415027. Arrivals time: 0.5181308151222765 Scheduler time: 83.92400603182614 Scheduler overhead time: 0.08629891648888588 Adapter cache time: 0.017518511973321438 Engine time: 0.08563695941120386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 89.53024583822116,
    "estimated_duration": 3600.074905584855,
    "input_throughput": 7359.091600816566,
    "output_throughput": 6424.636877449226,
    "total_throughput": 13783.728478265792,
    "itl": 88.48289341419832,
    "ttft": 1939434.9446505227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8184124837862305,
    "arrivals": 778725,
    "finished_requests": 107504,
    "scheduler_time": 283.1627113757372
}
#Debug simulation 
Total elapsed time: 89.53039909014478. Arrivals time: 0.5740510835312307 Scheduler time: 88.7395090139471 Scheduler overhead time: 0.08394305128604174 Adapter cache time: 0.0168059803545475 Engine time: 0.08296672441065311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 87.87551898276433,
    "estimated_duration": 3600.026034221257,
    "input_throughput": 7303.438294630972,
    "output_throughput": 6403.670912615168,
    "total_throughput": 13707.109207246142,
    "itl": 87.6773279443053,
    "ttft": 1916985.351530618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1786644139792792,
    "arrivals": 778725,
    "finished_requests": 106926,
    "scheduler_time": 285.8458850974949
}
#Debug simulation 
Total elapsed time: 87.87568083778024. Arrivals time: 0.522049565333873 Scheduler time: 87.13214083062485 Scheduler overhead time: 0.08562898216769099 Adapter cache time: 0.017246967647224665 Engine time: 0.08565049199387431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 84.59142660396174,
    "estimated_duration": 3600.075723299455,
    "input_throughput": 7256.923744941705,
    "output_throughput": 6331.299603640048,
    "total_throughput": 13588.223348581754,
    "itl": 85.6891418033827,
    "ttft": 1936362.3159390627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3454366403352584,
    "arrivals": 778725,
    "finished_requests": 106291,
    "scheduler_time": 287.81411836529344
}
#Debug simulation 
Total elapsed time: 84.59159078588709. Arrivals time: 0.513632282614708 Scheduler time: 83.85989168053493 Scheduler overhead time: 0.08455529063940048 Adapter cache time: 0.01693322043865919 Engine time: 0.0831065634265542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 88.27924567135051,
    "estimated_duration": 3600.0368263031637,
    "input_throughput": 7313.9993479007435,
    "output_throughput": 6388.255484490789,
    "total_throughput": 13702.254832391533,
    "itl": 87.46444541914802,
    "ttft": 1931604.332090876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0553866004757557,
    "arrivals": 778725,
    "finished_requests": 107043,
    "scheduler_time": 285.4911485635426
}
#Debug simulation 
Total elapsed time: 88.27940717013553. Arrivals time: 0.5192570672370493 Scheduler time: 87.54140257043764 Scheduler overhead time: 0.08536232449114323 Adapter cache time: 0.016931785736232996 Engine time: 0.08325102040544152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 86.29090472590178,
    "estimated_duration": 3600.0108351166145,
    "input_throughput": 7245.6301368757,
    "output_throughput": 6330.130392305282,
    "total_throughput": 13575.760529180981,
    "itl": 85.2322522986905,
    "ttft": 1943407.557667278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.065312423608273,
    "arrivals": 778725,
    "finished_requests": 106014,
    "scheduler_time": 288.453605440824
}
#Debug simulation 
Total elapsed time: 86.2910698321648. Arrivals time: 0.5066999136470258 Scheduler time: 85.56475330889225 Scheduler overhead time: 0.0850572120398283 Adapter cache time: 0.016775580123066902 Engine time: 0.08420841256156564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 86.32493084808812,
    "estimated_duration": 3600.0861797379557,
    "input_throughput": 7362.226257019555,
    "output_throughput": 6436.628136964955,
    "total_throughput": 13798.854393984511,
    "itl": 88.09259322583365,
    "ttft": 1931986.7526681712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7938817843003134,
    "arrivals": 778725,
    "finished_requests": 107697,
    "scheduler_time": 283.60471611792764
}
#Debug simulation 
Total elapsed time: 86.32509042602032. Arrivals time: 0.5093141533434391 Scheduler time: 85.60040000639856 Scheduler overhead time: 0.08351009804755449 Adapter cache time: 0.016850754152983427 Engine time: 0.0821508914232254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 86.12239951081574,
    "estimated_duration": 3600.05069133571,
    "input_throughput": 7234.703684224106,
    "output_throughput": 6314.637750716081,
    "total_throughput": 13549.341434940186,
    "itl": 85.53262136474697,
    "ttft": 1930634.6829518685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.229259228147576,
    "arrivals": 778725,
    "finished_requests": 105927,
    "scheduler_time": 288.68513253858845
}
#Debug simulation 
Total elapsed time: 86.12256360892206. Arrivals time: 0.509948197286576 Scheduler time: 85.3911155811511 Scheduler overhead time: 0.08600877551361918 Adapter cache time: 0.017189543694257736 Engine time: 0.08422806579619646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 86.62711605709046,
    "estimated_duration": 3600.0856424079834,
    "input_throughput": 7337.067121084504,
    "output_throughput": 6440.343453743994,
    "total_throughput": 13777.410574828498,
    "itl": 88.82623481008415,
    "ttft": 1923915.4026909368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1291957082878814,
    "arrivals": 776317,
    "finished_requests": 107102,
    "scheduler_time": 282.48756715713915
}
#Debug simulation 
Total elapsed time: 86.62728189211339. Arrivals time: 0.5134616293944418 Scheduler time: 85.89918293524534 Scheduler overhead time: 0.08327320078387856 Adapter cache time: 0.016964702866971493 Engine time: 0.08168567158281803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 88.71926104230806,
    "estimated_duration": 3600.036077950965,
    "input_throughput": 7309.584801432767,
    "output_throughput": 6400.39169082845,
    "total_throughput": 13709.976492261216,
    "itl": 87.43901630323342,
    "ttft": 1933105.611803626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1178754996415248,
    "arrivals": 776317,
    "finished_requests": 106599,
    "scheduler_time": 284.3436025136471
}
#Debug simulation 
Total elapsed time: 88.7194224819541. Arrivals time: 0.5183440460823476 Scheduler time: 87.98299344209954 Scheduler overhead time: 0.08441023528575897 Adapter cache time: 0.017100729048252106 Engine time: 0.08341156970709562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 85.71954870503396,
    "estimated_duration": 3600.0431971226003,
    "input_throughput": 7270.119431044339,
    "output_throughput": 6390.926091772802,
    "total_throughput": 13661.04552281714,
    "itl": 86.08832579270653,
    "ttft": 1936531.719403369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0658867544494637,
    "arrivals": 776317,
    "finished_requests": 106100,
    "scheduler_time": 285.97950810344946
}
#Debug simulation 
Total elapsed time: 85.71971063688397. Arrivals time: 0.5141685279086232 Scheduler time: 84.9861045894213 Scheduler overhead time: 0.08523204922676086 Adapter cache time: 0.016893458552658558 Engine time: 0.08431683527305722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 82.68196887616068,
    "estimated_duration": 3600.0182141228374,
    "input_throughput": 7315.362154748643,
    "output_throughput": 6437.518540624037,
    "total_throughput": 13752.88069537268,
    "itl": 88.17942327412209,
    "ttft": 1924079.1190903704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2027739660348717,
    "arrivals": 776317,
    "finished_requests": 106800,
    "scheduler_time": 283.89853788555143
}
#Debug simulation 
Total elapsed time: 82.68213025992736. Arrivals time: 0.5154781150631607 Scheduler time: 81.94831933500245 Scheduler overhead time: 0.0846395748667419 Adapter cache time: 0.017204292118549347 Engine time: 0.08328386768698692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 84.43608623603359,
    "estimated_duration": 3600.0476683074844,
    "input_throughput": 7243.783528083176,
    "output_throughput": 6360.407169487588,
    "total_throughput": 13604.190697570764,
    "itl": 85.77086447875624,
    "ttft": 1938993.7537436527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1896084403153595,
    "arrivals": 776317,
    "finished_requests": 105820,
    "scheduler_time": 286.4522311995601
}
#Debug simulation 
Total elapsed time: 84.43625255580992. Arrivals time: 0.510914537589997 Scheduler time: 83.7057218523696 Scheduler overhead time: 0.08555768989026546 Adapter cache time: 0.016927748452872038 Engine time: 0.08358761901035905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 85.67480420321226,
    "estimated_duration": 3600.0257269983113,
    "input_throughput": 7330.125949406133,
    "output_throughput": 6434.232351809764,
    "total_throughput": 13764.358301215896,
    "itl": 88.16212707421853,
    "ttft": 1933921.360165063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.787497863359743,
    "arrivals": 776317,
    "finished_requests": 107043,
    "scheduler_time": 283.0981177384569
}
#Debug simulation 
Total elapsed time: 85.67495957808569. Arrivals time: 0.5170352179557085 Scheduler time: 84.94010849343613 Scheduler overhead time: 0.08479793090373278 Adapter cache time: 0.016811866778880358 Engine time: 0.08338515739887953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.45058019086719,
    "estimated_duration": 3600.0773762103713,
    "input_throughput": 7281.8137669016905,
    "output_throughput": 6390.039045276264,
    "total_throughput": 13671.852812177955,
    "itl": 86.41887255799256,
    "ttft": 1931299.2676992037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.473917890470488,
    "arrivals": 776317,
    "finished_requests": 106260,
    "scheduler_time": 285.24748315873626
}
#Debug simulation 
Total elapsed time: 82.45073879417032. Arrivals time: 0.5129085150547326 Scheduler time: 81.71948722982779 Scheduler overhead time: 0.08442844776436687 Adapter cache time: 0.01709914719685912 Engine time: 0.08395615126937628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 88.47554450901225,
    "estimated_duration": 3600.0422485412155,
    "input_throughput": 7473.914510559655,
    "output_throughput": 6492.529638914988,
    "total_throughput": 13966.444149474644,
    "itl": 89.30455517050996,
    "ttft": 1922939.2553353126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.785350438626481,
    "arrivals": 775108,
    "finished_requests": 108576,
    "scheduler_time": 280.75524906424204
}
#Debug simulation 
Total elapsed time: 88.47570918686688. Arrivals time: 0.5192516981624067 Scheduler time: 87.74077161774039 Scheduler overhead time: 0.08345024194568396 Adapter cache time: 0.016564457211643457 Engine time: 0.08313678950071335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.53773634089157,
    "estimated_duration": 3600.0137421593095,
    "input_throughput": 7511.395771445187,
    "output_throughput": 6516.207625899086,
    "total_throughput": 14027.603397344274,
    "itl": 88.66391492036098,
    "ttft": 1924379.1664508393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3016304636281006,
    "arrivals": 775108,
    "finished_requests": 109155,
    "scheduler_time": 278.97152400008633
}
#Debug simulation 
Total elapsed time: 82.53790152911097. Arrivals time: 0.6078704395331442 Scheduler time: 81.71402964694425 Scheduler overhead time: 0.0831987438723445 Adapter cache time: 0.016688036266714334 Engine time: 0.08319635037332773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 86.30025980109349,
    "estimated_duration": 3600.007582925959,
    "input_throughput": 7335.908714540938,
    "output_throughput": 6348.751071636303,
    "total_throughput": 13684.65978617724,
    "itl": 85.88439952148441,
    "ttft": 1929131.1062517785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1495971423760136,
    "arrivals": 775108,
    "finished_requests": 106473,
    "scheduler_time": 287.03613174657596
}
#Debug simulation 
Total elapsed time: 86.30041686212644. Arrivals time: 0.5937484628520906 Scheduler time: 85.48617144441232 Scheduler overhead time: 0.08530456572771072 Adapter cache time: 0.01693713804706931 Engine time: 0.08422574819996953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 86.1025275667198,
    "estimated_duration": 3600.016745557963,
    "input_throughput": 7444.320372417141,
    "output_throughput": 6446.356681155707,
    "total_throughput": 13890.677053572848,
    "itl": 88.20872346039863,
    "ttft": 1928407.6463609925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.030408100639468,
    "arrivals": 775108,
    "finished_requests": 108093,
    "scheduler_time": 282.3220689497499
}
#Debug simulation 
Total elapsed time: 86.1026904466562. Arrivals time: 0.5924188825301826 Scheduler time: 85.29143506335095 Scheduler overhead time: 0.0848893471993506 Adapter cache time: 0.01733444072306156 Engine time: 0.08385864784941077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 84.65187737485394,
    "estimated_duration": 3600.018791564604,
    "input_throughput": 7386.5442209102985,
    "output_throughput": 6408.7176583800265,
    "total_throughput": 13795.261879290325,
    "itl": 86.45103834754295,
    "ttft": 1926750.5568113385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1430697789089836,
    "arrivals": 775108,
    "finished_requests": 107227,
    "scheduler_time": 284.8177205028235
}
#Debug simulation 
Total elapsed time: 84.65203634789214. Arrivals time: 0.5882056578993797 Scheduler time: 83.84457171708345 Scheduler overhead time: 0.08476572902873158 Adapter cache time: 0.016747592948377132 Engine time: 0.0845194854773581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 82.47855770913884,
    "estimated_duration": 3600.006215816715,
    "input_throughput": 7512.138418312348,
    "output_throughput": 6516.754303624908,
    "total_throughput": 14028.892721937256,
    "itl": 88.65722913668856,
    "ttft": 1924310.254246449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.00455117533914,
    "arrivals": 775108,
    "finished_requests": 109164,
    "scheduler_time": 278.99034386644206
}
#Debug simulation 
Total elapsed time: 82.47872364101931. Arrivals time: 0.5891328603029251 Scheduler time: 81.6759134507738 Scheduler overhead time: 0.08322112588211894 Adapter cache time: 0.016709848307073116 Engine time: 0.08140146918594837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 84.23774744290859,
    "estimated_duration": 3600.0234381326327,
    "input_throughput": 7404.256793902005,
    "output_throughput": 6425.037058091461,
    "total_throughput": 13829.293851993467,
    "itl": 86.3771942130193,
    "ttft": 1925903.8091699127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1075004865415465,
    "arrivals": 775108,
    "finished_requests": 107450,
    "scheduler_time": 283.9203117537278
}
#Debug simulation 
Total elapsed time: 84.23790687462315. Arrivals time: 0.5888714860193431 Scheduler time: 83.4306787061505 Scheduler overhead time: 0.08440895611420274 Adapter cache time: 0.01655273186042905 Engine time: 0.08411363884806633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 84.5784726049751,
    "estimated_duration": 3600.0481547764225,
    "input_throughput": 7499.5105174299315,
    "output_throughput": 6523.036356845178,
    "total_throughput": 14022.546874275109,
    "itl": 89.67498314399604,
    "ttft": 1922719.7906825147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9242110282974294,
    "arrivals": 774530,
    "finished_requests": 109182,
    "scheduler_time": 278.3046830039636
}
#Debug simulation 
Total elapsed time: 84.57862947974354. Arrivals time: 0.5145679819397628 Scheduler time: 83.84995169378817 Scheduler overhead time: 0.08305427711457014 Adapter cache time: 0.016198490746319294 Engine time: 0.08246708614751697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 90.95365740498528,
    "estimated_duration": 3600.045466352965,
    "input_throughput": 7439.618818795794,
    "output_throughput": 6482.6464604744115,
    "total_throughput": 13922.265279270207,
    "itl": 88.21730685057179,
    "ttft": 1922700.7442896224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8361424635257595,
    "arrivals": 774530,
    "finished_requests": 108357,
    "scheduler_time": 280.93233966244094
}
#Debug simulation 
Total elapsed time: 90.95381789188832. Arrivals time: 0.5190114439465106 Scheduler time: 90.21539092995226 Scheduler overhead time: 0.08504026010632515 Adapter cache time: 0.016522512305527925 Engine time: 0.08465139102190733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 91.30941201001406,
    "estimated_duration": 3600.053401389028,
    "input_throughput": 7344.454109985799,
    "output_throughput": 6380.857014825617,
    "total_throughput": 13725.311124811415,
    "itl": 85.81816868292084,
    "ttft": 1935318.4076246682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8118059364799457,
    "arrivals": 774530,
    "finished_requests": 106831,
    "scheduler_time": 285.4098609330729
}
#Debug simulation 
Total elapsed time: 91.30957205826417. Arrivals time: 0.5214365208521485 Scheduler time: 90.56567132240161 Scheduler overhead time: 0.08660114323720336 Adapter cache time: 0.017068056855350733 Engine time: 0.08504323707893491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 81.12215357087553,
    "estimated_duration": 3600.0006789893405,
    "input_throughput": 7486.579976858889,
    "output_throughput": 6517.805992910891,
    "total_throughput": 14004.385969769779,
    "itl": 88.7531855619825,
    "ttft": 1927969.900459592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1558672614302465,
    "arrivals": 774530,
    "finished_requests": 109019,
    "scheduler_time": 278.6413547205002
}
#Debug simulation 
Total elapsed time: 81.12231034692377. Arrivals time: 0.5027113067917526 Scheduler time: 80.40617292141542 Scheduler overhead time: 0.08214311581104994 Adapter cache time: 0.016683021560311317 Engine time: 0.08200468914583325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 91.0790645936504,
    "estimated_duration": 3600.0405935735603,
    "input_throughput": 7339.50779531956,
    "output_throughput": 6380.045836427784,
    "total_throughput": 13719.553631747343,
    "itl": 85.21043333003685,
    "ttft": 1935640.3562019307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7916118323849557,
    "arrivals": 774530,
    "finished_requests": 106811,
    "scheduler_time": 285.6149926488809
}
#Debug simulation 
Total elapsed time: 91.07922186981887. Arrivals time: 0.5082591199316084 Scheduler time: 90.3502459935844 Scheduler overhead time: 0.08503940841183066 Adapter cache time: 0.01701460499316454 Engine time: 0.08535482129082084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 87.39706827094778,
    "estimated_duration": 3600.0491993765554,
    "input_throughput": 7436.44086992928,
    "output_throughput": 6457.23175228431,
    "total_throughput": 13893.67262221359,
    "itl": 87.86170836189973,
    "ttft": 1925325.0007236083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.710890812072897,
    "arrivals": 774530,
    "finished_requests": 108128,
    "scheduler_time": 281.6454781674023
}
#Debug simulation 
Total elapsed time: 87.39723238395527. Arrivals time: 0.5070728282444179 Scheduler time: 86.6738392249681 Scheduler overhead time: 0.08409106032922864 Adapter cache time: 0.016823286190629005 Engine time: 0.08275078376755118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 88.7513181110844,
    "estimated_duration": 3600.0748350733006,
    "input_throughput": 7348.687516786394,
    "output_throughput": 6382.396492469626,
    "total_throughput": 13731.08400925602,
    "itl": 85.6394902321031,
    "ttft": 1928942.9581175935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9224295852519626,
    "arrivals": 774530,
    "finished_requests": 106954,
    "scheduler_time": 285.19083258579303
}
#Debug simulation 
Total elapsed time: 88.75147436093539. Arrivals time: 0.5091441706754267 Scheduler time: 88.02220037020743 Scheduler overhead time: 0.08594067068770528 Adapter cache time: 0.016621500719338655 Engine time: 0.08394898474216461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 87.24842326017097,
    "estimated_duration": 3600.035462337518,
    "input_throughput": 7357.945297239009,
    "output_throughput": 6426.692248463989,
    "total_throughput": 13784.637545702997,
    "itl": 87.97500557129632,
    "ttft": 1910083.182670875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.87792416507378,
    "arrivals": 716829,
    "finished_requests": 107302,
    "scheduler_time": 283.600321646153
}
#Debug simulation 
Total elapsed time: 87.24858019687235. Arrivals time: 0.5801318809390068 Scheduler time: 86.4527246481739 Scheduler overhead time: 0.08349218498915434 Adapter cache time: 0.01657998003065586 Engine time: 0.08268802519887686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 86.38579325424507,
    "estimated_duration": 3600.0578405702345,
    "input_throughput": 7293.836144543945,
    "output_throughput": 6365.550225818079,
    "total_throughput": 13659.386370362025,
    "itl": 86.5228261511844,
    "ttft": 1916401.9579354555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.037934822482059,
    "arrivals": 716829,
    "finished_requests": 106303,
    "scheduler_time": 286.04165991993284
}
#Debug simulation 
Total elapsed time: 86.3859564368613. Arrivals time: 0.49607971543446183 Scheduler time: 85.6717349565588 Scheduler overhead time: 0.08557098265737295 Adapter cache time: 0.016690673772245646 Engine time: 0.08300534822046757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 85.22112352401018,
    "estimated_duration": 3600.010484060867,
    "input_throughput": 7298.147634937772,
    "output_throughput": 6376.9180955562615,
    "total_throughput": 13675.065730494034,
    "itl": 85.13840094075648,
    "ttft": 1919967.386920245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.110457623302944,
    "arrivals": 716829,
    "finished_requests": 106343,
    "scheduler_time": 286.25639909278243
}
#Debug simulation 
Total elapsed time: 85.22127815894783. Arrivals time: 0.5012168153189123 Scheduler time: 84.49994415789843 Scheduler overhead time: 0.08592184679582715 Adapter cache time: 0.016854414716362953 Engine time: 0.08422095654532313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 86.16867065895349,
    "estimated_duration": 3600.0414947463355,
    "input_throughput": 7294.287034836053,
    "output_throughput": 6365.897457972163,
    "total_throughput": 13660.184492808216,
    "itl": 86.52108319259918,
    "ttft": 1916339.340210276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.910218492937272,
    "arrivals": 716829,
    "finished_requests": 106308,
    "scheduler_time": 286.0485939843444
}
#Debug simulation 
Total elapsed time: 86.16892668604851. Arrivals time: 0.496829180046916 Scheduler time: 85.45542073110119 Scheduler overhead time: 0.08418187079951167 Adapter cache time: 0.016650245524942875 Engine time: 0.08312193490564823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 85.44972695689648,
    "estimated_duration": 3600.0267062065773,
    "input_throughput": 7272.140496864898,
    "output_throughput": 6342.497671096383,
    "total_throughput": 13614.63816796128,
    "itl": 85.36078525207665,
    "ttft": 1920152.4894324134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9039999285759455,
    "arrivals": 716829,
    "finished_requests": 105919,
    "scheduler_time": 287.2771016581407
}
#Debug simulation 
Total elapsed time: 85.44989888183773. Arrivals time: 0.5625287024304271 Scheduler time: 84.66648949775845 Scheduler overhead time: 0.08612189488485456 Adapter cache time: 0.016628445591777563 Engine time: 0.08485470293089747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 86.43600831739604,
    "estimated_duration": 3600.0164091475744,
    "input_throughput": 7294.506473157444,
    "output_throughput": 6365.950983380795,
    "total_throughput": 13660.457456538239,
    "itl": 86.51806328250872,
    "ttft": 1916287.8571910644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7811139424191724,
    "arrivals": 716829,
    "finished_requests": 106309,
    "scheduler_time": 286.05475002428204
}
#Debug simulation 
Total elapsed time: 86.43617813801393. Arrivals time: 0.5878748488612473 Scheduler time: 85.63103588437662 Scheduler overhead time: 0.08451385656371713 Adapter cache time: 0.016749711707234383 Engine time: 0.08333361707627773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 84.07699669571593,
    "estimated_duration": 3600.02655724835,
    "input_throughput": 7190.701120767923,
    "output_throughput": 6278.230629852768,
    "total_throughput": 13468.931750620692,
    "itl": 84.16632145065145,
    "ttft": 1911421.0075586047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2135213157534688,
    "arrivals": 716829,
    "finished_requests": 104828,
    "scheduler_time": 290.3034343161131
}
#Debug simulation 
Total elapsed time: 84.07715838588774. Arrivals time: 0.4837995837442577 Scheduler time: 83.37293435074389 Scheduler overhead time: 0.08573691407218575 Adapter cache time: 0.01689941855147481 Engine time: 0.08474111463874578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 91.74274568678811,
    "estimated_duration": 3600.0464637954437,
    "input_throughput": 7341.869963571066,
    "output_throughput": 6434.508341200181,
    "total_throughput": 13776.378304771248,
    "itl": 88.3863356295293,
    "ttft": 1917959.2769159097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6134278037957828,
    "arrivals": 707393,
    "finished_requests": 107335,
    "scheduler_time": 283.64660835361366
}
#Debug simulation 
Total elapsed time: 91.74291376397014. Arrivals time: 0.5698990607634187 Scheduler time: 90.9560918211937 Scheduler overhead time: 0.085105343721807 Adapter cache time: 0.016463405918329954 Engine time: 0.08284379728138447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 84.78514407062903,
    "estimated_duration": 3600.0283250153734,
    "input_throughput": 7372.4033823780155,
    "output_throughput": 6451.330351657947,
    "total_throughput": 13823.733734035963,
    "itl": 88.00148065257412,
    "ttft": 1908161.7056715149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1403608883358562,
    "arrivals": 707393,
    "finished_requests": 107644,
    "scheduler_time": 282.24195403913217
}
#Debug simulation 
Total elapsed time: 84.78531014593318. Arrivals time: 0.57442945567891 Scheduler time: 83.99393835943192 Scheduler overhead time: 0.08431860618293285 Adapter cache time: 0.016552353277802467 Engine time: 0.08302460936829448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.93429794209078,
    "estimated_duration": 3600.0198771793857,
    "input_throughput": 7279.587306204795,
    "output_throughput": 6358.774612637875,
    "total_throughput": 13638.36191884267,
    "itl": 85.63406511484375,
    "ttft": 1912103.631401951,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4066276145354024,
    "arrivals": 707393,
    "finished_requests": 106314,
    "scheduler_time": 286.4027655573642
}
#Debug simulation 
Total elapsed time: 82.93446289515123. Arrivals time: 0.5641852752305567 Scheduler time: 82.15260381856933 Scheduler overhead time: 0.08460540743544698 Adapter cache time: 0.01662295125424862 Engine time: 0.08323398884385824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 88.18532105488703,
    "estimated_duration": 3600.06057538467,
    "input_throughput": 7344.156701356316,
    "output_throughput": 6419.011434975869,
    "total_throughput": 13763.168136332184,
    "itl": 87.62615361429413,
    "ttft": 1913798.0172680449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7175864968681689,
    "arrivals": 707393,
    "finished_requests": 107285,
    "scheduler_time": 283.363677957521
}
#Debug simulation 
Total elapsed time: 88.18548745894805. Arrivals time: 0.5069774859584868 Scheduler time: 87.45860365824774 Scheduler overhead time: 0.08508497988805175 Adapter cache time: 0.0180382770486176 Engine time: 0.08383536478504539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 83.09054792206734,
    "estimated_duration": 3600.066755175263,
    "input_throughput": 7254.776585032659,
    "output_throughput": 6338.513575393157,
    "total_throughput": 13593.290160425817,
    "itl": 85.33830204729499,
    "ttft": 1910501.260349804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.269281719187284,
    "arrivals": 707393,
    "finished_requests": 105991,
    "scheduler_time": 287.2743168229367
}
#Debug simulation 
Total elapsed time: 83.09071367187425. Arrivals time: 0.48939974373206496 Scheduler time: 82.38356891460717 Scheduler overhead time: 0.0846635871566832 Adapter cache time: 0.016729488968849182 Engine time: 0.08347489964216948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 92.60257227811962,
    "estimated_duration": 3600.0223079104067,
    "input_throughput": 7322.920733594545,
    "output_throughput": 6406.260858251871,
    "total_throughput": 13729.181591846414,
    "itl": 87.60886201400838,
    "ttft": 1916895.632118577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6023641560831985,
    "arrivals": 707393,
    "finished_requests": 107063,
    "scheduler_time": 284.49199601507325
}
#Debug simulation 
Total elapsed time: 92.60274204192683. Arrivals time: 0.5018604104407132 Scheduler time: 91.88258441630751 Scheduler overhead time: 0.08513770764693618 Adapter cache time: 0.016495862044394016 Engine time: 0.08369364077225327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 84.11562084918842,
    "estimated_duration": 3600.0319339069083,
    "input_throughput": 7262.861685680845,
    "output_throughput": 6350.481445643525,
    "total_throughput": 13613.343131324371,
    "itl": 85.41094954003772,
    "ttft": 1911050.0880090483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1929178255982777,
    "arrivals": 707393,
    "finished_requests": 106137,
    "scheduler_time": 286.9575890508224
}
#Debug simulation 
Total elapsed time: 84.11578677687794. Arrivals time: 0.5057692928239703 Scheduler time: 83.38882827758789 Scheduler overhead time: 0.08530102437362075 Adapter cache time: 0.017128434497863054 Engine time: 0.08546446682885289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 91.32040876382962,
    "estimated_duration": 3600.0775987642983,
    "input_throughput": 7371.923874393568,
    "output_throughput": 6455.88695309722,
    "total_throughput": 13827.810827490788,
    "itl": 88.76857976083629,
    "ttft": 1911479.5527094058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.593590576699933,
    "arrivals": 702770,
    "finished_requests": 107494,
    "scheduler_time": 281.6037831672938
}
#Debug simulation 
Total elapsed time: 91.32057126285508. Arrivals time: 0.5110379955731332 Scheduler time: 90.5929243224673 Scheduler overhead time: 0.08458007173612714 Adapter cache time: 0.016437624115496874 Engine time: 0.08291961625218391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 87.45374591089785,
    "estimated_duration": 3600.0521396243607,
    "input_throughput": 7321.809234338022,
    "output_throughput": 6428.6468924349665,
    "total_throughput": 13750.45612677299,
    "itl": 87.51526002248039,
    "ttft": 1902718.9859156255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.799501011976974,
    "arrivals": 702770,
    "finished_requests": 106761,
    "scheduler_time": 283.6498976322745
}
#Debug simulation 
Total elapsed time: 87.45391638111323. Arrivals time: 0.501185086555779 Scheduler time: 86.7354025025852 Scheduler overhead time: 0.08464785385876894 Adapter cache time: 0.01632287446409464 Engine time: 0.0833120490424335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 89.4768274789676,
    "estimated_duration": 3600.0175642824056,
    "input_throughput": 7274.8100619948955,
    "output_throughput": 6381.282754818773,
    "total_throughput": 13656.092816813669,
    "itl": 85.6973374142918,
    "ttft": 1914503.0742988936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7790336843859451,
    "arrivals": 702770,
    "finished_requests": 106022,
    "scheduler_time": 285.3443864687847
}
#Debug simulation 
Total elapsed time: 89.47699521901086. Arrivals time: 0.5011912588961422 Scheduler time: 88.75491753406823 Scheduler overhead time: 0.08608801569789648 Adapter cache time: 0.016474741511046886 Engine time: 0.08447699900716543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 88.7100546839647,
    "estimated_duration": 3600.005662438004,
    "input_throughput": 7333.000132594824,
    "output_throughput": 6439.63764887529,
    "total_throughput": 13772.637781470114,
    "itl": 87.4894758639137,
    "ttft": 1909132.1079454648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7161982758948562,
    "arrivals": 702770,
    "finished_requests": 106767,
    "scheduler_time": 283.55896325901216
}
#Debug simulation 
Total elapsed time: 88.71022557094693. Arrivals time: 0.5020297626033425 Scheduler time: 87.98932660045102 Scheduler overhead time: 0.08459795219823718 Adapter cache time: 0.016943827271461487 Engine time: 0.08416335051879287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 86.67616043519229,
    "estimated_duration": 3600.0550512984787,
    "input_throughput": 7310.687093661867,
    "output_throughput": 6401.597662148989,
    "total_throughput": 13712.284755810857,
    "itl": 85.94793191959052,
    "ttft": 1915101.642033465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.870247670109389,
    "arrivals": 702770,
    "finished_requests": 106556,
    "scheduler_time": 284.21278409269297
}
#Debug simulation 
Total elapsed time: 86.6763240462169. Arrivals time: 0.5131777813658118 Scheduler time: 85.94375692913309 Scheduler overhead time: 0.08512395108118653 Adapter cache time: 0.01666966313496232 Engine time: 0.0837275916710496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 89.6209128559567,
    "estimated_duration": 3600.0460782281393,
    "input_throughput": 7363.680470736479,
    "output_throughput": 6454.549884938296,
    "total_throughput": 13818.230355674776,
    "itl": 87.80665376664537,
    "ttft": 1901308.1776027072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5257571047963525,
    "arrivals": 702770,
    "finished_requests": 107182,
    "scheduler_time": 282.3560366401104
}
#Debug simulation 
Total elapsed time: 89.62108429707587. Arrivals time: 0.8109177891165018 Scheduler time: 88.59454409126192 Scheduler overhead time: 0.08392632147297263 Adapter cache time: 0.01631107274442911 Engine time: 0.08253799146041274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 87.04772056173533,
    "estimated_duration": 3600.0473498575175,
    "input_throughput": 7270.075211937382,
    "output_throughput": 6373.4181165445225,
    "total_throughput": 13643.493328481904,
    "itl": 85.6610815545965,
    "ttft": 1913351.3624631856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8362315771915072,
    "arrivals": 702770,
    "finished_requests": 105918,
    "scheduler_time": 286.00149078097473
}
#Debug simulation 
Total elapsed time: 87.047881484963. Arrivals time: 0.4931624224409461 Scheduler time: 86.33476469712332 Scheduler overhead time: 0.08556358236819506 Adapter cache time: 0.01661965763196349 Engine time: 0.08400303311645985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 84.50750338984653,
    "estimated_duration": 3600.035100785063,
    "input_throughput": 7395.538725217994,
    "output_throughput": 6430.6670218164045,
    "total_throughput": 13826.205747034399,
    "itl": 88.70352297899034,
    "ttft": 1901717.457281222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9242110282974294,
    "arrivals": 700393,
    "finished_requests": 107547,
    "scheduler_time": 282.68732175702576
}
#Debug simulation 
Total elapsed time: 84.50767083885148. Arrivals time: 0.49436812242493033 Scheduler time: 83.79927548533306 Scheduler overhead time: 0.08308302517980337 Adapter cache time: 0.016445342916995287 Engine time: 0.08217005943879485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 91.31191531196237,
    "estimated_duration": 3600.0027031936997,
    "input_throughput": 7377.8305711930225,
    "output_throughput": 6419.018513374888,
    "total_throughput": 13796.84908456791,
    "itl": 87.69219079590083,
    "ttft": 1911733.2540936037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8228174377186244,
    "arrivals": 700393,
    "finished_requests": 107371,
    "scheduler_time": 283.26621071593183
}
#Debug simulation 
Total elapsed time: 91.3120812731795. Arrivals time: 0.5737824738025665 Scheduler time: 90.5175839047879 Scheduler overhead time: 0.08550261659547687 Adapter cache time: 0.017044573556631804 Engine time: 0.08504137117415667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 87.349177822005,
    "estimated_duration": 3600.0159907549555,
    "input_throughput": 7290.8431705315115,
    "output_throughput": 6357.926758875326,
    "total_throughput": 13648.769929406837,
    "itl": 85.74915597191188,
    "ttft": 1911044.8369251483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8656937265023619,
    "arrivals": 700393,
    "finished_requests": 106203,
    "scheduler_time": 286.2555335926648
}
#Debug simulation 
Total elapsed time: 87.34934332501143. Arrivals time: 0.5699774390086532 Scheduler time: 86.55884463619441 Scheduler overhead time: 0.08594996761530638 Adapter cache time: 0.016690817661583424 Engine time: 0.08448201743885875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 87.53687663003802,
    "estimated_duration": 3600.052583044522,
    "input_throughput": 7416.379728937367,
    "output_throughput": 6459.806478807866,
    "total_throughput": 13876.186207745233,
    "itl": 88.35964448228756,
    "ttft": 1900713.2453071293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.769772232244721,
    "arrivals": 700393,
    "finished_requests": 107870,
    "scheduler_time": 281.19928957852414
}
#Debug simulation 
Total elapsed time: 87.53704345598817. Arrivals time: 0.5048426752910018 Scheduler time: 86.8144897161983 Scheduler overhead time: 0.0841794447042048 Adapter cache time: 0.01686623925343156 Engine time: 0.08374637458473444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 85.84095707675442,
    "estimated_duration": 3600.033490261387,
    "input_throughput": 7251.025044798868,
    "output_throughput": 6316.914290247017,
    "total_throughput": 13567.939335045885,
    "itl": 84.91388698687257,
    "ttft": 1913250.5407761626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.949245873205368,
    "arrivals": 700393,
    "finished_requests": 105590,
    "scheduler_time": 288.3051155905039
}
#Debug simulation 
Total elapsed time: 85.84112683404237. Arrivals time: 0.4944557510316372 Scheduler time: 85.12452405504882 Scheduler overhead time: 0.08617177698761225 Adapter cache time: 0.01719725178554654 Engine time: 0.08521039737388492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 83.83203894365579,
    "estimated_duration": 3600.0062049313724,
    "input_throughput": 7396.0389189128,
    "output_throughput": 6449.502217022599,
    "total_throughput": 13845.5411359354,
    "itl": 88.2047237510761,
    "ttft": 1900393.7749864198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7555782586568904,
    "arrivals": 700393,
    "finished_requests": 107633,
    "scheduler_time": 282.082338707803
}
#Debug simulation 
Total elapsed time: 83.83220179891214. Arrivals time: 0.5675172419287264 Scheduler time: 83.04856302356347 Scheduler overhead time: 0.08394326828420162 Adapter cache time: 0.016524729318916798 Engine time: 0.08251375099644065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 88.68206397304311,
    "estimated_duration": 3600.05714158436,
    "input_throughput": 7308.7276021458765,
    "output_throughput": 6385.675586772156,
    "total_throughput": 13694.403188918033,
    "itl": 85.99957609290549,
    "ttft": 1908479.0495206038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7369385250657827,
    "arrivals": 700393,
    "finished_requests": 106467,
    "scheduler_time": 285.3041836000854
}
#Debug simulation 
Total elapsed time: 88.68231866601855. Arrivals time: 0.5016557229682803 Scheduler time: 87.95917394338176 Scheduler overhead time: 0.08652037614956498 Adapter cache time: 0.017062681261450052 Engine time: 0.08433770947158337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 87.73932225210592,
    "estimated_duration": 3600.08389099033,
    "input_throughput": 7486.941086971576,
    "output_throughput": 6512.406852149928,
    "total_throughput": 13999.347939121504,
    "itl": 89.45466713815232,
    "ttft": 1891457.0512855106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8051876657223307,
    "arrivals": 699127,
    "finished_requests": 109183,
    "scheduler_time": 278.60342411472266
}
#Debug simulation 
Total elapsed time: 87.73949227342382. Arrivals time: 0.505408032797277 Scheduler time: 87.02031462965533 Scheduler overhead time: 0.08295182976871729 Adapter cache time: 0.0162130082026124 Engine time: 0.08156055677682161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 88.73127593612298,
    "estimated_duration": 3600.072555976136,
    "input_throughput": 7452.5453536936575,
    "output_throughput": 6476.781408556301,
    "total_throughput": 13929.326762249959,
    "itl": 88.43789772167486,
    "ttft": 1899681.026468453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.011010917746464,
    "arrivals": 699127,
    "finished_requests": 108625,
    "scheduler_time": 280.3141317282596
}
#Debug simulation 
Total elapsed time: 88.73144796630368. Arrivals time: 0.5109905879944563 Scheduler time: 88.00202652951702 Scheduler overhead time: 0.08485230151563883 Adapter cache time: 0.01712653087452054 Engine time: 0.08293890953063965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 83.03257280867547,
    "estimated_duration": 3600.0352668282826,
    "input_throughput": 7357.952363432635,
    "output_throughput": 6403.168939039045,
    "total_throughput": 13761.12130247168,
    "itl": 85.93414866694981,
    "ttft": 1896672.9451359084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1346858434239473,
    "arrivals": 699127,
    "finished_requests": 107295,
    "scheduler_time": 284.6121803531123
}
#Debug simulation 
Total elapsed time: 83.03274155687541. Arrivals time: 0.5014015068300068 Scheduler time: 82.31270450912416 Scheduler overhead time: 0.08482774998992682 Adapter cache time: 0.01680820109322667 Engine time: 0.08352814987301826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 90.13534727180377,
    "estimated_duration": 3600.023064202686,
    "input_throughput": 7390.105431419573,
    "output_throughput": 6434.885717914317,
    "total_throughput": 13824.99114933389,
    "itl": 87.39948013654629,
    "ttft": 1887050.7743735062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7883668111404385,
    "arrivals": 699127,
    "finished_requests": 107698,
    "scheduler_time": 283.3652450541781
}
#Debug simulation 
Total elapsed time: 90.13551292708144. Arrivals time: 0.5065321018919349 Scheduler time: 89.41150442464277 Scheduler overhead time: 0.0846473965793848 Adapter cache time: 0.016346427146345377 Engine time: 0.08338059112429619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 84.17735425336286,
    "estimated_duration": 3600.0533283368386,
    "input_throughput": 7386.728632790926,
    "output_throughput": 6430.630295883749,
    "total_throughput": 13817.358928674676,
    "itl": 86.30310903415457,
    "ttft": 1894940.6720870205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.992161659910348,
    "arrivals": 699127,
    "finished_requests": 107655,
    "scheduler_time": 283.59880426328647
}
#Debug simulation 
Total elapsed time: 84.17751949094236. Arrivals time: 0.5028894408605993 Scheduler time: 83.45584404561669 Scheduler overhead time: 0.08461418189108372 Adapter cache time: 0.016851764172315598 Engine time: 0.08393113780766726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 90.21567409019917,
    "estimated_duration": 3600.045129051373,
    "input_throughput": 7442.655311118365,
    "output_throughput": 6469.868339160929,
    "total_throughput": 13912.523650279294,
    "itl": 88.23041827100975,
    "ttft": 1900944.8371966442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.583212393261487,
    "arrivals": 699127,
    "finished_requests": 108419,
    "scheduler_time": 280.76639965477045
}
#Debug simulation 
Total elapsed time: 90.2158450470306. Arrivals time: 0.5117047931998968 Scheduler time: 89.48779451986775 Scheduler overhead time: 0.08397344313561916 Adapter cache time: 0.016679576132446527 Engine time: 0.08298082929104567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 84.06530076172203,
    "estimated_duration": 3600.0296471060274,
    "input_throughput": 7335.289036085613,
    "output_throughput": 6376.523876255709,
    "total_throughput": 13711.812912341322,
    "itl": 85.7138327857518,
    "ttft": 1896770.9505664583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1378351322189038,
    "arrivals": 699127,
    "finished_requests": 106977,
    "scheduler_time": 285.17080611859114
}
#Debug simulation 
Total elapsed time: 84.06546827871352. Arrivals time: 0.50040312577039 Scheduler time: 83.34636195283383 Scheduler overhead time: 0.08432406326755881 Adapter cache time: 0.016615491826087236 Engine time: 0.08389713522046804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 88.65562993474305,
    "estimated_duration": 3600.07458472958,
    "input_throughput": 7501.452640606484,
    "output_throughput": 6542.611950293595,
    "total_throughput": 14044.06459090008,
    "itl": 89.31895982315895,
    "ttft": 1896934.2362394014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7060015302430818,
    "arrivals": 698509,
    "finished_requests": 109389,
    "scheduler_time": 277.3287797104746
}
#Debug simulation 
Total elapsed time: 88.65579464007169. Arrivals time: 0.5121659892611206 Scheduler time: 87.93069729814306 Scheduler overhead time: 0.08251449558883905 Adapter cache time: 0.016261618584394455 Engine time: 0.08167819352820516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 91.84393267100677,
    "estimated_duration": 3600.085977650452,
    "input_throughput": 7435.84491209037,
    "output_throughput": 6481.6260347284515,
    "total_throughput": 13917.470946818821,
    "itl": 87.9280296834599,
    "ttft": 1901025.6831033048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6743157043075203,
    "arrivals": 698509,
    "finished_requests": 108358,
    "scheduler_time": 280.16712467868837
}
#Debug simulation 
Total elapsed time: 91.84409227175638. Arrivals time: 0.5135914636775851 Scheduler time: 91.1135974638164 Scheduler overhead time: 0.08446177700534463 Adapter cache time: 0.01665420550853014 Engine time: 0.08346785372123122 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 86.88391605811194,
    "estimated_duration": 3600.0894162238733,
    "input_throughput": 7322.703119871799,
    "output_throughput": 6375.929413463379,
    "total_throughput": 13698.632533335178,
    "itl": 85.11490560829868,
    "ttft": 1906587.6515612511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9205174337234394,
    "arrivals": 698509,
    "finished_requests": 106773,
    "scheduler_time": 285.3556444578962
}
#Debug simulation 
Total elapsed time: 86.88408425310627. Arrivals time: 0.4849259750917554 Scheduler time: 86.1811479353346 Scheduler overhead time: 0.08426234731450677 Adapter cache time: 0.016675601713359356 Engine time: 0.08355510467663407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 91.78332422720268,
    "estimated_duration": 3600.046263130376,
    "input_throughput": 7443.50628891614,
    "output_throughput": 6487.706349554253,
    "total_throughput": 13931.212638470393,
    "itl": 88.00040752907343,
    "ttft": 1902042.4578099276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.570199131309053,
    "arrivals": 698509,
    "finished_requests": 108499,
    "scheduler_time": 280.0308515687554
}
#Debug simulation 
Total elapsed time: 91.7834863238968. Arrivals time: 0.5086697856895626 Scheduler time: 91.05686554452404 Scheduler overhead time: 0.08536440832540393 Adapter cache time: 0.01635949593037367 Engine time: 0.08349563553929329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 86.9383498779498,
    "estimated_duration": 3600.0796306132743,
    "input_throughput": 7333.6000058150075,
    "output_throughput": 6388.958123151797,
    "total_throughput": 13722.558128966804,
    "itl": 85.5723951343287,
    "ttft": 1916251.887101329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.605190779543485,
    "arrivals": 698509,
    "finished_requests": 106876,
    "scheduler_time": 284.76046180046
}
#Debug simulation 
Total elapsed time: 86.93851756723598. Arrivals time: 0.495885725133121 Scheduler time: 86.2236980451271 Scheduler overhead time: 0.08563657710328698 Adapter cache time: 0.01634505484253168 Engine time: 0.08374753827229142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 89.73379069101065,
    "estimated_duration": 3600.0842915932017,
    "input_throughput": 7371.2596291061,
    "output_throughput": 6423.159606012061,
    "total_throughput": 13794.419235118161,
    "itl": 87.47505964213923,
    "ttft": 1908430.5148110376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.378926923163231,
    "arrivals": 698509,
    "finished_requests": 107485,
    "scheduler_time": 283.21210711979455
}
#Debug simulation 
Total elapsed time: 89.73396198172122. Arrivals time: 0.4952738396823406 Scheduler time: 89.0217273873277 Scheduler overhead time: 0.08408383093774319 Adapter cache time: 0.01614708872511983 Engine time: 0.08310717577114701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 87.18482866883278,
    "estimated_duration": 3600.007303371603,
    "input_throughput": 7333.74067749072,
    "output_throughput": 6389.060927309404,
    "total_throughput": 13722.801604800125,
    "itl": 85.56944509008319,
    "ttft": 1916248.2913382992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5900712256506118,
    "arrivals": 698509,
    "finished_requests": 106876,
    "scheduler_time": 284.75476801099217
}
#Debug simulation 
Total elapsed time: 87.18499550502747. Arrivals time: 0.4840687080286443 Scheduler time: 86.48157333163545 Scheduler overhead time: 0.08542846702039242 Adapter cache time: 0.01649887254461646 Engine time: 0.08430942101404071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 435376779 . Total output tokens: 383506976
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 87.79830094473436,
    "estimated_duration": 3600.0421697974994,
    "input_throughput": 7351.099446006934,
    "output_throughput": 6460.142382528865,
    "total_throughput": 13811.241828535798,
    "itl": 89.51761094473696,
    "ttft": 1886760.05449695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.593590576699933,
    "arrivals": 650482,
    "finished_requests": 107660,
    "scheduler_time": 281.35646274290247
}
#Debug simulation 
Total elapsed time: 87.79847466899082. Arrivals time: 0.5137367211282253 Scheduler time: 87.06702928151935 Scheduler overhead time: 0.08470893744379282 Adapter cache time: 0.016352596692740917 Engine time: 0.08374473033472896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 435376779 . Total output tokens: 383506976
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 78.57897245278582,
    "estimated_duration": 3600.058756101638,
    "input_throughput": 7352.247780717257,
    "output_throughput": 6467.101393981442,
    "total_throughput": 13819.3491746987,
    "itl": 88.78253749197458,
    "ttft": 1891408.1170334264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7237249977374471,
    "arrivals": 650482,
    "finished_requests": 107707,
    "scheduler_time": 280.6882503395243
}
#Debug simulation 
Total elapsed time: 78.57913889596239. Arrivals time: 0.48809538036584854 Scheduler time: 77.87773433886468 Scheduler overhead time: 0.08278178097680211 Adapter cache time: 0.01596052199602127 Engine time: 0.08216131012886763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 435376779 . Total output tokens: 383506976
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.6955626369454,
    "estimated_duration": 3600.086395648529,
    "input_throughput": 7268.970553493025,
    "output_throughput": 6390.503024540769,
    "total_throughput": 13659.473578033794,
    "itl": 86.47494299479034,
    "ttft": 1894815.7873846397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7471973494906026,
    "arrivals": 650482,
    "finished_requests": 106438,
    "scheduler_time": 284.45473258871334
}
#Debug simulation 
Total elapsed time: 82.69573088921607. Arrivals time: 0.4847328122705221 Scheduler time: 81.99352809553966 Scheduler overhead time: 0.08542649587616324 Adapter cache time: 0.016269220039248466 Engine time: 0.08260874589905143 
