INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [8640, 8640, 8640, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 17280, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 8640, 17280]
Prompts retrieved: 1935360 . Total input tokens: 431747992 . Total output tokens: 380047635
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 12.323408198077232,
    "estimated_duration": 3600.1367800852204,
    "input_throughput": 5768.063901035573,
    "output_throughput": 4983.879529037025,
    "total_throughput": 10751.943430072597,
    "itl": 116.17099061269333,
    "ttft": 2033185.4799091634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.127669472112354,
    "arrivals": 644954,
    "finished_requests": 84077,
    "scheduler_time": 101.33546255921125
}
#Debug simulation 
Total elapsed time: 12.323526097927243. Arrivals time: 0.36442565778270364 Scheduler time: 11.826728034298867 Scheduler overhead time: 0.04818661091849208 Adapter cache time: 0.013147018849849701 Engine time: 0.049126076977699995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [8640, 8640, 8640, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 17280, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 8640, 17280]
Prompts retrieved: 1935360 . Total input tokens: 431747992 . Total output tokens: 380047635
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.384338318835944,
    "estimated_duration": 3600.025419806081,
    "input_throughput": 5586.040834423675,
    "output_throughput": 4833.779201741681,
    "total_throughput": 10419.820036165356,
    "itl": 108.63381214409873,
    "ttft": 2047192.2445300345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6370316964760483,
    "arrivals": 644954,
    "finished_requests": 81441,
    "scheduler_time": 101.09833533697844
}
#Debug simulation 
Total elapsed time: 10.3844586587511. Arrivals time: 0.294307854026556 Scheduler time: 9.951332414057106 Scheduler overhead time: 0.050848803482949734 Adapter cache time: 0.014035868924111128 Engine time: 0.050773841328918934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [8640, 8640, 8640, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 17280, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 8640, 17280]
Prompts retrieved: 1935360 . Total input tokens: 431747992 . Total output tokens: 380047635
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.387414649128914,
    "estimated_duration": 3600.0674117695416,
    "input_throughput": 5274.8826141191275,
    "output_throughput": 4567.897797201779,
    "total_throughput": 9842.780411320906,
    "itl": 96.48041761348765,
    "ttft": 2078269.360488332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.786708341361992,
    "arrivals": 644954,
    "finished_requests": 76883,
    "scheduler_time": 100.9968895658722
}
#Debug simulation 
Total elapsed time: 8.38756568590179. Arrivals time: 0.2767210532911122 Scheduler time: 7.959733046125621 Scheduler overhead time: 0.054780729580670595 Adapter cache time: 0.016296688001602888 Engine time: 0.054832934867590666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [8640, 8640, 8640, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 17280, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 8640, 17280]
Prompts retrieved: 1935360 . Total input tokens: 431747992 . Total output tokens: 380047635
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 10.830342434812337,
    "estimated_duration": 3600.1062713574065,
    "input_throughput": 5605.223145924366,
    "output_throughput": 4849.7604470483875,
    "total_throughput": 10454.983592972754,
    "itl": 108.35559957866361,
    "ttft": 2048535.2736222716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.329422779693264,
    "arrivals": 644954,
    "finished_requests": 81734,
    "scheduler_time": 101.3968510235521
}
#Debug simulation 
Total elapsed time: 10.83046019077301. Arrivals time: 0.36584092769771814 Scheduler time: 10.32520752446726 Scheduler overhead time: 0.051132338121533394 Adapter cache time: 0.013739239890128374 Engine time: 0.05127472151070833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [8640, 8640, 8640, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 17280, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 8640, 17280]
Prompts retrieved: 1935360 . Total input tokens: 431747992 . Total output tokens: 380047635
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 8.428078867029399,
    "estimated_duration": 3600.0186746765885,
    "input_throughput": 5274.954025538765,
    "output_throughput": 4567.959637453084,
    "total_throughput": 9842.913662991848,
    "itl": 96.47919022814064,
    "ttft": 2078249.8776520046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.738035804857537,
    "arrivals": 644954,
    "finished_requests": 76883,
    "scheduler_time": 100.99682500942338
}
#Debug simulation 
Total elapsed time: 8.428220979869366. Arrivals time: 0.31044327560812235 Scheduler time: 7.966065596789122 Scheduler overhead time: 0.05517576867714524 Adapter cache time: 0.016611934639513493 Engine time: 0.054487768560647964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [8640, 8640, 8640, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 17280, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 8640, 17280]
Prompts retrieved: 1935360 . Total input tokens: 431747992 . Total output tokens: 380047635
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 10.421977314632386,
    "estimated_duration": 3600.0893944411227,
    "input_throughput": 5590.550065528146,
    "output_throughput": 4834.797998870819,
    "total_throughput": 10425.348064398964,
    "itl": 108.58560310086762,
    "ttft": 2046763.5886212513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7218259083526037,
    "arrivals": 644954,
    "finished_requests": 81511,
    "scheduler_time": 101.11659751997213
}
#Debug simulation 
Total elapsed time: 10.422131000086665. Arrivals time: 0.6669925949536264 Scheduler time: 9.61627186043188 Scheduler overhead time: 0.050539218820631504 Adapter cache time: 0.014538178220391273 Engine time: 0.050669487565755844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [8640, 8640, 8640, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 17280, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 8640, 17280]
Prompts retrieved: 1935360 . Total input tokens: 431747992 . Total output tokens: 380047635
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.395849345251918,
    "estimated_duration": 3600.0807844209357,
    "input_throughput": 5274.86357588903,
    "output_throughput": 4568.04915910941,
    "total_throughput": 9842.91273499844,
    "itl": 96.47833148888503,
    "ttft": 2078222.7791563233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.693505611885377,
    "arrivals": 644954,
    "finished_requests": 76885,
    "scheduler_time": 100.99977861883839
}
#Debug simulation 
Total elapsed time: 8.395949221216142. Arrivals time: 0.2689719442278147 Scheduler time: 7.9759635156951845 Scheduler overhead time: 0.054796521086245775 Adapter cache time: 0.01653417246416211 Engine time: 0.0544193428941071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 17280, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 17280]
Prompts retrieved: 1797120 . Total input tokens: 400794013 . Total output tokens: 353045135
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 10.623283797875047,
    "estimated_duration": 3600.0308385768863,
    "input_throughput": 5687.853776301112,
    "output_throughput": 4949.131493285535,
    "total_throughput": 10636.985269586647,
    "itl": 116.65591933487087,
    "ttft": 2026037.811102991,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5737533496040832,
    "arrivals": 599169,
    "finished_requests": 82875,
    "scheduler_time": 100.70146973367152
}
#Debug simulation 
Total elapsed time: 10.623389948159456. Arrivals time: 0.35264277225360274 Scheduler time: 10.14130304241553 Scheduler overhead time: 0.048319358844310045 Adapter cache time: 0.010632164776325226 Engine time: 0.04846253897994757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 17280, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 17280]
Prompts retrieved: 1797120 . Total input tokens: 400794013 . Total output tokens: 353045135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.026148490142077,
    "estimated_duration": 3600.0816819329143,
    "input_throughput": 5533.706665595381,
    "output_throughput": 4817.452361438721,
    "total_throughput": 10351.159027034102,
    "itl": 108.93620164415711,
    "ttft": 2042213.7897125182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2996945364121393,
    "arrivals": 599169,
    "finished_requests": 80655,
    "scheduler_time": 100.76009880919254
}
#Debug simulation 
Total elapsed time: 9.026314829941839. Arrivals time: 0.3547317204065621 Scheduler time: 8.53575290972367 Scheduler overhead time: 0.05031959246844053 Adapter cache time: 0.011939858086407185 Engine time: 0.05049473186954856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 17280, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 17280]
Prompts retrieved: 1797120 . Total input tokens: 400794013 . Total output tokens: 353045135
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.30553919589147,
    "estimated_duration": 3600.0523483081975,
    "input_throughput": 5229.0442412168,
    "output_throughput": 4557.541783443916,
    "total_throughput": 9786.586024660717,
    "itl": 96.31432239367415,
    "ttft": 2074397.1961228717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 537,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.106162538388781,
    "arrivals": 599169,
    "finished_requests": 76205,
    "scheduler_time": 100.86148186759833
}
#Debug simulation 
Total elapsed time: 7.305639693979174. Arrivals time: 0.29223766876384616 Scheduler time: 6.86296246945858 Scheduler overhead time: 0.05494112055748701 Adapter cache time: 0.015650713350623846 Engine time: 0.054524580016732216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 17280, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 17280]
Prompts retrieved: 1797120 . Total input tokens: 400794013 . Total output tokens: 353045135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 8.998868584632874,
    "estimated_duration": 3600.042116642539,
    "input_throughput": 5533.821092787544,
    "output_throughput": 4817.556416860689,
    "total_throughput": 10351.377509648233,
    "itl": 108.93177214251664,
    "ttft": 2042120.9201095204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1358844615612163,
    "arrivals": 599169,
    "finished_requests": 80656,
    "scheduler_time": 100.7632270024642
}
#Debug simulation 
Total elapsed time: 8.998971364926547. Arrivals time: 0.2793283644132316 Scheduler time: 8.58467230759561 Scheduler overhead time: 0.05028397450223565 Adapter cache time: 0.011886956635862589 Engine time: 0.0497866990044713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 17280, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 17280]
Prompts retrieved: 1797120 . Total input tokens: 400794013 . Total output tokens: 353045135
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 7.254309359937906,
    "estimated_duration": 3600.00982492426,
    "input_throughput": 5229.106006785982,
    "output_throughput": 4557.595617213404,
    "total_throughput": 9786.701623999386,
    "itl": 96.31321380389888,
    "ttft": 2074379.7454380684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 537,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.063703517182767,
    "arrivals": 599169,
    "finished_requests": 76205,
    "scheduler_time": 100.86141750486729
}
#Debug simulation 
Total elapsed time: 7.254482968244702. Arrivals time: 0.32481775991618633 Scheduler time: 6.778647489380091 Scheduler overhead time: 0.05575221078470349 Adapter cache time: 0.015500395558774471 Engine time: 0.054437143728137016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 17280, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 17280]
Prompts retrieved: 1797120 . Total input tokens: 400794013 . Total output tokens: 353045135
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.040107283741236,
    "estimated_duration": 3600.0089066794894,
    "input_throughput": 5534.001586228216,
    "output_throughput": 4818.175857236643,
    "total_throughput": 10352.17744346486,
    "itl": 108.92796634102385,
    "ttft": 2042175.1540191546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9790154915768579,
    "arrivals": 599169,
    "finished_requests": 80660,
    "scheduler_time": 100.7663362560254
}
#Debug simulation 
Total elapsed time: 9.040205508936197. Arrivals time: 0.3369657206349075 Scheduler time: 8.567282860167325 Scheduler overhead time: 0.05048298882320523 Adapter cache time: 0.011926416773349047 Engine time: 0.05038596224039793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 17280, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 17280]
Prompts retrieved: 1797120 . Total input tokens: 400794013 . Total output tokens: 353045135
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.324378906283528,
    "estimated_duration": 3600.0759573373725,
    "input_throughput": 5229.14827994998,
    "output_throughput": 4557.590504877893,
    "total_throughput": 9786.738784827874,
    "itl": 96.31174520348091,
    "ttft": 2074353.8054280072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 537,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.022901433389672,
    "arrivals": 599169,
    "finished_requests": 76207,
    "scheduler_time": 100.86438225495334
}
#Debug simulation 
Total elapsed time: 7.324482654221356. Arrivals time: 0.29908381309360266 Scheduler time: 6.874256811570376 Scheduler overhead time: 0.054988243617117405 Adapter cache time: 0.01568873366340995 Engine time: 0.05516955256462097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 17280]
Prompts retrieved: 1693440 . Total input tokens: 377699300 . Total output tokens: 332726037
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.643318125978112,
    "estimated_duration": 3600.0554452547917,
    "input_throughput": 5723.5357380840815,
    "output_throughput": 4951.782901982035,
    "total_throughput": 10675.318640066116,
    "itl": 116.75828402409192,
    "ttft": 2015128.8497872278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.023397163776679,
    "arrivals": 564644,
    "finished_requests": 83030,
    "scheduler_time": 100.62267766108084
}
#Debug simulation 
Total elapsed time: 7.6434919480234385. Arrivals time: 0.3073100345209241 Scheduler time: 7.208699476439506 Scheduler overhead time: 0.0473762471228838 Adapter cache time: 0.011255999095737934 Engine time: 0.04704767093062401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 17280]
Prompts retrieved: 1693440 . Total input tokens: 377699300 . Total output tokens: 332726037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.110569037962705,
    "estimated_duration": 3600.0545771899733,
    "input_throughput": 5579.130140765117,
    "output_throughput": 4819.639988220211,
    "total_throughput": 10398.770128985328,
    "itl": 108.97612262344059,
    "ttft": 2031688.8121136897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6883016321063105,
    "arrivals": 564644,
    "finished_requests": 80849,
    "scheduler_time": 100.67954962176182
}
#Debug simulation 
Total elapsed time: 7.11066679796204. Arrivals time: 0.3211569176055491 Scheduler time: 6.654696702957153 Scheduler overhead time: 0.049790071323513985 Adapter cache time: 0.012387442402541637 Engine time: 0.04964572098106146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 17280]
Prompts retrieved: 1693440 . Total input tokens: 377699300 . Total output tokens: 332726037
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.327016156632453,
    "estimated_duration": 3600.0926079405776,
    "input_throughput": 5268.405028850041,
    "output_throughput": 4563.890374308726,
    "total_throughput": 9832.295403158767,
    "itl": 96.59137325448258,
    "ttft": 2062788.2963668962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.497699631713369,
    "arrivals": 564644,
    "finished_requests": 76362,
    "scheduler_time": 100.77382089495289
}
#Debug simulation 
Total elapsed time: 6.327115692663938. Arrivals time: 0.30388222681358457 Scheduler time: 5.872353862971067 Scheduler overhead time: 0.05460262391716242 Adapter cache time: 0.016030735801905394 Engine time: 0.05479940539225936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 17280]
Prompts retrieved: 1693440 . Total input tokens: 377699300 . Total output tokens: 332726037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 7.122346850112081,
    "estimated_duration": 3600.08874561803,
    "input_throughput": 5579.455513270057,
    "output_throughput": 4820.0656223048345,
    "total_throughput": 10399.52113557489,
    "itl": 108.97117873876984,
    "ttft": 2031583.0824568013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.478680265136061,
    "arrivals": 564644,
    "finished_requests": 80855,
    "scheduler_time": 100.68606606540258
}
#Debug simulation 
Total elapsed time: 7.122519268188626. Arrivals time: 0.35025127325206995 Scheduler time: 6.636934705078602 Scheduler overhead time: 0.049869513139128685 Adapter cache time: 0.012393550481647253 Engine time: 0.04998725559562445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 17280]
Prompts retrieved: 1693440 . Total input tokens: 377699300 . Total output tokens: 332726037
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.32362983096391,
    "estimated_duration": 3600.043656175455,
    "input_throughput": 5268.476666238411,
    "output_throughput": 4563.952432025517,
    "total_throughput": 9832.429098263927,
    "itl": 96.59012495764061,
    "ttft": 2062768.5294666453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.448819978032299,
    "arrivals": 564644,
    "finished_requests": 76362,
    "scheduler_time": 100.77374878351081
}
#Debug simulation 
Total elapsed time: 6.32373316725716. Arrivals time: 0.30369996884837747 Scheduler time: 5.8698814609088 Scheduler overhead time: 0.054515749681741 Adapter cache time: 0.015812836587429047 Engine time: 0.05448993621394038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 17280]
Prompts retrieved: 1693440 . Total input tokens: 377699300 . Total output tokens: 332726037
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.064568517263979,
    "estimated_duration": 3600.029751812203,
    "input_throughput": 5579.773608784293,
    "output_throughput": 4820.249608010803,
    "total_throughput": 10400.023216795096,
    "itl": 108.96659038267396,
    "ttft": 2031487.365822972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2982115386053827,
    "arrivals": 564644,
    "finished_requests": 80858,
    "scheduler_time": 100.68911687459756
}
#Debug simulation 
Total elapsed time: 7.064674220047891. Arrivals time: 0.35134895239025354 Scheduler time: 6.578857083339244 Scheduler overhead time: 0.04956665262579918 Adapter cache time: 0.012522116303443909 Engine time: 0.04949263343587518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 17280]
Prompts retrieved: 1693440 . Total input tokens: 377699300 . Total output tokens: 332726037
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.289436682127416,
    "estimated_duration": 3600.106480305267,
    "input_throughput": 5268.384727996084,
    "output_throughput": 4563.872788175643,
    "total_throughput": 9832.257516171727,
    "itl": 96.58894908409344,
    "ttft": 2062704.7459707556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.403254199177065,
    "arrivals": 564644,
    "finished_requests": 76362,
    "scheduler_time": 100.77675185078175
}
#Debug simulation 
Total elapsed time: 6.289798735175282. Arrivals time: 0.30447484739124775 Scheduler time: 5.834657702129334 Scheduler overhead time: 0.05431379470974207 Adapter cache time: 0.015932345297187567 Engine time: 0.05487548839300871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 17280, 17280, 540, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 34560, 540, 540, 17280, 540, 34560, 540, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 540, 540, 17280]
Prompts retrieved: 1676160 . Total input tokens: 373826914 . Total output tokens: 329331925
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.984067385084927,
    "estimated_duration": 3600.0452132277446,
    "input_throughput": 5710.821887585941,
    "output_throughput": 4953.13945904933,
    "total_throughput": 10663.961346635271,
    "itl": 117.01876544921643,
    "ttft": 2011348.3077404436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7985752566903808,
    "arrivals": 558915,
    "finished_requests": 82901,
    "scheduler_time": 100.61368511258317
}
#Debug simulation 
Total elapsed time: 6.9841698170639575. Arrivals time: 0.27245746832340956 Scheduler time: 6.5851832227781415 Scheduler overhead time: 0.04694770323112607 Adapter cache time: 0.010644334368407726 Engine time: 0.047267158050090075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 17280, 17280, 540, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 34560, 540, 540, 17280, 540, 34560, 540, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 540, 540, 17280]
Prompts retrieved: 1676160 . Total input tokens: 373826914 . Total output tokens: 329331925
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.472201582044363,
    "estimated_duration": 3600.0277743185793,
    "input_throughput": 5558.821002092934,
    "output_throughput": 4821.373358233433,
    "total_throughput": 10380.194360326366,
    "itl": 109.21254767561334,
    "ttft": 2028250.2612608853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6641541693173414,
    "arrivals": 558915,
    "finished_requests": 80626,
    "scheduler_time": 100.66429279357511
}
#Debug simulation 
Total elapsed time: 6.4723033220507205. Arrivals time: 0.25869428366422653 Scheduler time: 6.079886939842254 Scheduler overhead time: 0.0493473862297833 Adapter cache time: 0.012120116036385298 Engine time: 0.049444661010056734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 17280, 17280, 540, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 34560, 540, 540, 17280, 540, 34560, 540, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 540, 540, 17280]
Prompts retrieved: 1676160 . Total input tokens: 373826914 . Total output tokens: 329331925
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.834425528999418,
    "estimated_duration": 3600.0114247215834,
    "input_throughput": 5252.751941325423,
    "output_throughput": 4561.746634254103,
    "total_throughput": 9814.498575579526,
    "itl": 96.7566020353423,
    "ttft": 2058355.6572006654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2575941954972585,
    "arrivals": 558915,
    "finished_requests": 76153,
    "scheduler_time": 100.76314439729117
}
#Debug simulation 
Total elapsed time: 5.834581280127168. Arrivals time: 0.28338687494397163 Scheduler time: 5.402800481766462 Scheduler overhead time: 0.053785239811986685 Adapter cache time: 0.015273256227374077 Engine time: 0.054247917141765356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 17280, 17280, 540, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 34560, 540, 540, 17280, 540, 34560, 540, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 540, 540, 17280]
Prompts retrieved: 1676160 . Total input tokens: 373826914 . Total output tokens: 329331925
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.45258832257241,
    "estimated_duration": 3600.067706854738,
    "input_throughput": 5559.515439637698,
    "output_throughput": 4821.825147051443,
    "total_throughput": 10381.340586689143,
    "itl": 109.20726534605132,
    "ttft": 2028174.9429568537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4686888652015457,
    "arrivals": 558915,
    "finished_requests": 80630,
    "scheduler_time": 100.67040459005624
}
#Debug simulation 
Total elapsed time: 6.4526871857233346. Arrivals time: 0.26059846486896276 Scheduler time: 6.05837413109839 Scheduler overhead time: 0.04940855270251632 Adapter cache time: 0.012033332139253616 Engine time: 0.04951579216867685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 17280, 17280, 540, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 34560, 540, 540, 17280, 540, 34560, 540, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 540, 540, 17280]
Prompts retrieved: 1676160 . Total input tokens: 373826914 . Total output tokens: 329331925
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.80413270322606,
    "estimated_duration": 3600.0690796600134,
    "input_throughput": 5252.829482312571,
    "output_throughput": 4562.00607171433,
    "total_throughput": 9814.8355540269,
    "itl": 96.75593554049179,
    "ttft": 2058317.587584552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.210785713582337,
    "arrivals": 558915,
    "finished_requests": 76157,
    "scheduler_time": 100.76603360551837
}
#Debug simulation 
Total elapsed time: 5.804237415082753. Arrivals time: 0.24590342584997416 Scheduler time: 5.409011743962765 Scheduler overhead time: 0.05418786033987999 Adapter cache time: 0.01524870889261365 Engine time: 0.05466336011886597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 17280, 17280, 540, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 34560, 540, 540, 17280, 540, 34560, 540, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 540, 540, 17280]
Prompts retrieved: 1676160 . Total input tokens: 373826914 . Total output tokens: 329331925
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.608662941958755,
    "estimated_duration": 3600.095448624743,
    "input_throughput": 5559.825922850515,
    "output_throughput": 4821.278004345829,
    "total_throughput": 10381.103927196344,
    "itl": 109.19597101752522,
    "ttft": 2028137.0360859262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 353,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2535240920213893,
    "arrivals": 558915,
    "finished_requests": 80632,
    "scheduler_time": 100.67665969511238
}
#Debug simulation 
Total elapsed time: 6.608819760847837. Arrivals time: 0.30913310404866934 Scheduler time: 6.164915209636092 Scheduler overhead time: 0.04998964024707675 Adapter cache time: 0.012103782501071692 Engine time: 0.049621089827269316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 17280, 17280, 540, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 34560, 540, 540, 17280, 540, 34560, 540, 540, 34560, 540, 17280, 17280, 540, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 540, 540, 17280]
Prompts retrieved: 1676160 . Total input tokens: 373826914 . Total output tokens: 329331925
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.848756197839975,
    "estimated_duration": 3600.0962459508924,
    "input_throughput": 5254.117031252841,
    "output_throughput": 4561.9852576085905,
    "total_throughput": 9816.102288861432,
    "itl": 96.74253926745094,
    "ttft": 2058564.329482481,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 536,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.037557815462388,
    "arrivals": 558915,
    "finished_requests": 76173,
    "scheduler_time": 100.77186479610553
}
#Debug simulation 
Total elapsed time: 5.848881842102855. Arrivals time: 0.2494271914474666 Scheduler time: 5.449390346184373 Scheduler overhead time: 0.05458376882597804 Adapter cache time: 0.015332590322941542 Engine time: 0.0548143214546144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 17280, 17280, 270, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 34560, 270, 270, 17280, 270, 34560, 270, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 270, 270, 17280]
Prompts retrieved: 1667520 . Total input tokens: 371857167 . Total output tokens: 327615916
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.455768366809934,
    "estimated_duration": 3600.0267601461524,
    "input_throughput": 5679.346116629964,
    "output_throughput": 4953.564011639743,
    "total_throughput": 10632.910128269707,
    "itl": 117.15990097820534,
    "ttft": 2011747.5848522917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5274664863804337,
    "arrivals": 556029,
    "finished_requests": 82695,
    "scheduler_time": 100.6082527987576
}
#Debug simulation 
Total elapsed time: 6.4558695941232145. Arrivals time: 0.26127482764422894 Scheduler time: 6.068773837760091 Scheduler overhead time: 0.046798184979707 Adapter cache time: 0.010202770121395588 Engine time: 0.047228818759322166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 17280, 17280, 270, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 34560, 270, 270, 17280, 270, 34560, 270, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 270, 270, 17280]
Prompts retrieved: 1667520 . Total input tokens: 371857167 . Total output tokens: 327615916
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.1383607648313046,
    "estimated_duration": 3600.0416148262984,
    "input_throughput": 5521.867280125637,
    "output_throughput": 4822.582585850939,
    "total_throughput": 10344.449865976576,
    "itl": 109.4192695111714,
    "ttft": 2027569.7945867227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.404897044212563,
    "arrivals": 556029,
    "finished_requests": 80448,
    "scheduler_time": 100.6581940396492
}
#Debug simulation 
Total elapsed time: 6.138510857708752. Arrivals time: 0.26351710548624396 Scheduler time: 5.7410670863464475 Scheduler overhead time: 0.049477207474410534 Adapter cache time: 0.011955644004046917 Engine time: 0.049484421499073505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 17280, 17280, 270, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 34560, 270, 270, 17280, 270, 34560, 270, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 270, 270, 17280]
Prompts retrieved: 1667520 . Total input tokens: 371857167 . Total output tokens: 327615916
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.6504307352006435,
    "estimated_duration": 3600.007388817145,
    "input_throughput": 5233.674258149419,
    "output_throughput": 4567.8664580194445,
    "total_throughput": 9801.540716168864,
    "itl": 96.90515191445647,
    "ttft": 2060772.6433971399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3684532351978396,
    "arrivals": 556029,
    "finished_requests": 76175,
    "scheduler_time": 100.7731763304957
}
#Debug simulation 
Total elapsed time: 5.650531947147101. Arrivals time: 0.2483494756743312 Scheduler time: 5.254908979404718 Scheduler overhead time: 0.05394099047407508 Adapter cache time: 0.014279634226113558 Engine time: 0.053984108846634626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 17280, 17280, 270, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 34560, 270, 270, 17280, 270, 34560, 270, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 270, 270, 17280]
Prompts retrieved: 1667520 . Total input tokens: 371857167 . Total output tokens: 327615916
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.127916722558439,
    "estimated_duration": 3600.1118772344375,
    "input_throughput": 5522.935863669332,
    "output_throughput": 4822.192085134886,
    "total_throughput": 10345.127948804218,
    "itl": 109.40002879158446,
    "ttft": 2027478.806554458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2194326177146273,
    "arrivals": 556029,
    "finished_requests": 80462,
    "scheduler_time": 100.66530613748836
}
#Debug simulation 
Total elapsed time: 6.128022614866495. Arrivals time: 0.2603522981517017 Scheduler time: 5.734071362297982 Scheduler overhead time: 0.049389162100851536 Adapter cache time: 0.011910370085388422 Engine time: 0.04941110033541918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 17280, 17280, 270, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 34560, 270, 270, 17280, 270, 34560, 270, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 270, 270, 17280]
Prompts retrieved: 1667520 . Total input tokens: 371857167 . Total output tokens: 327615916
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.650351368822157,
    "estimated_duration": 3600.009886626857,
    "input_throughput": 5234.1764587917905,
    "output_throughput": 4568.284954186467,
    "total_throughput": 9802.461412978259,
    "itl": 96.90734558895124,
    "ttft": 2060802.7406723336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.33453788721471,
    "arrivals": 556029,
    "finished_requests": 76182,
    "scheduler_time": 100.77443567546908
}
#Debug simulation 
Total elapsed time: 5.650508018676192. Arrivals time: 0.24105406505987048 Scheduler time: 5.260807333979756 Scheduler overhead time: 0.054081066977232695 Adapter cache time: 0.0143762300722301 Engine time: 0.05499723134562373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 17280, 17280, 270, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 34560, 270, 270, 17280, 270, 34560, 270, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 270, 270, 17280]
Prompts retrieved: 1667520 . Total input tokens: 371857167 . Total output tokens: 327615916
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.133111161179841,
    "estimated_duration": 3600.0800217868064,
    "input_throughput": 5522.181695876007,
    "output_throughput": 4822.7133549611335,
    "total_throughput": 10344.895050837142,
    "itl": 109.40966340983087,
    "ttft": 2027492.684458523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.144997436031691,
    "arrivals": 556029,
    "finished_requests": 80453,
    "scheduler_time": 100.66578409830004
}
#Debug simulation 
Total elapsed time: 6.133210028056055. Arrivals time: 0.24830011930316687 Scheduler time: 5.750390623696148 Scheduler overhead time: 0.04959970572963357 Adapter cache time: 0.012124334461987019 Engine time: 0.04988401522859931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 17280, 17280, 270, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 34560, 270, 270, 17280, 270, 34560, 270, 270, 34560, 270, 17280, 17280, 270, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 270, 270, 17280]
Prompts retrieved: 1667520 . Total input tokens: 371857167 . Total output tokens: 327615916
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.662485790904611,
    "estimated_duration": 3600.077476196257,
    "input_throughput": 5234.078189869705,
    "output_throughput": 4568.199187028679,
    "total_throughput": 9802.277376898384,
    "itl": 96.90695342642078,
    "ttft": 2060781.93306301,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3098894466832545,
    "arrivals": 556029,
    "finished_requests": 76182,
    "scheduler_time": 100.77693968540645
}
#Debug simulation 
Total elapsed time: 5.662605392746627. Arrivals time: 0.23827254120260477 Scheduler time: 5.276482172310352 Scheduler overhead time: 0.054275933653116226 Adapter cache time: 0.014194945804774761 Engine time: 0.054193584248423576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 17280, 17280, 135, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 34560, 135, 135, 17280, 135, 34560, 135, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 135, 135, 17280]
Prompts retrieved: 1663200 . Total input tokens: 370889297 . Total output tokens: 326771370
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.178881207015365,
    "estimated_duration": 3600.027954887641,
    "input_throughput": 5694.601613347705,
    "output_throughput": 4954.910412787816,
    "total_throughput": 10649.512026135522,
    "itl": 116.85580851779342,
    "ttft": 2005800.060894953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5142416683165338,
    "arrivals": 554568,
    "finished_requests": 82866,
    "scheduler_time": 100.62644095113325
}
#Debug simulation 
Total elapsed time: 6.179021747782826. Arrivals time: 0.2524109701626003 Scheduler time: 5.8008553655818105 Scheduler overhead time: 0.046841784846037626 Adapter cache time: 0.009977234993129969 Engine time: 0.047278506215661764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 17280, 17280, 135, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 34560, 135, 135, 17280, 135, 34560, 135, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 135, 135, 17280]
Prompts retrieved: 1663200 . Total input tokens: 370889297 . Total output tokens: 326771370
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.986165920738131,
    "estimated_duration": 3600.0487914949185,
    "input_throughput": 5538.216328373932,
    "output_throughput": 4823.9279537065995,
    "total_throughput": 10362.144282080531,
    "itl": 109.1644963267883,
    "ttft": 2021317.7922061633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0310031952988394,
    "arrivals": 554568,
    "finished_requests": 80657,
    "scheduler_time": 100.68895608945662
}
#Debug simulation 
Total elapsed time: 5.986266911961138. Arrivals time: 0.3260115599259734 Scheduler time: 5.527734462171793 Scheduler overhead time: 0.04910248564556241 Adapter cache time: 0.011083307210355997 Engine time: 0.04960018116980791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 17280, 17280, 135, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 34560, 135, 135, 17280, 135, 34560, 135, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 135, 135, 17280]
Prompts retrieved: 1663200 . Total input tokens: 370889297 . Total output tokens: 326771370
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.566596217919141,
    "estimated_duration": 3600.08778874519,
    "input_throughput": 5251.867484762113,
    "output_throughput": 4572.068228852018,
    "total_throughput": 9823.935713614132,
    "itl": 96.7408789384463,
    "ttft": 2054843.8835046291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.657453847718442,
    "arrivals": 554568,
    "finished_requests": 76447,
    "scheduler_time": 100.8227545988421
}
#Debug simulation 
Total elapsed time: 5.566692462190986. Arrivals time: 0.23531965725123882 Scheduler time: 5.185513756703585 Scheduler overhead time: 0.053818251471966505 Adapter cache time: 0.012861079536378384 Engine time: 0.05410393327474594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 17280, 17280, 135, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 34560, 135, 135, 17280, 135, 34560, 135, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 135, 135, 17280]
Prompts retrieved: 1663200 . Total input tokens: 370889297 . Total output tokens: 326771370
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.977403433993459,
    "estimated_duration": 3600.0816403417834,
    "input_throughput": 5538.108018602369,
    "output_throughput": 4823.530334815239,
    "total_throughput": 10361.63835341761,
    "itl": 109.1601598033826,
    "ttft": 2021208.9359319971,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8927382818935432,
    "arrivals": 554568,
    "finished_requests": 80656,
    "scheduler_time": 100.69323821606575
}
#Debug simulation 
Total elapsed time: 5.97754145367071. Arrivals time: 0.29663843661546707 Scheduler time: 5.5476745860651135 Scheduler overhead time: 0.04945644037798047 Adapter cache time: 0.011190461926162243 Engine time: 0.049648099578917027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 17280, 17280, 135, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 34560, 135, 135, 17280, 135, 34560, 135, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 135, 135, 17280]
Prompts retrieved: 1663200 . Total input tokens: 370889297 . Total output tokens: 326771370
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.597525900695473,
    "estimated_duration": 3600.036806176243,
    "input_throughput": 5251.877694017774,
    "output_throughput": 4572.011867145954,
    "total_throughput": 9823.889561163729,
    "itl": 96.73875575166595,
    "ttft": 2054878.0658473384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6334282552311365,
    "arrivals": 554568,
    "finished_requests": 76445,
    "scheduler_time": 100.82228419179475
}
#Debug simulation 
Total elapsed time: 5.597620919812471. Arrivals time: 0.23431110428646207 Scheduler time: 5.217426792252809 Scheduler overhead time: 0.05386725906282663 Adapter cache time: 0.012843491975218058 Engine time: 0.05410044966265559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 17280, 17280, 135, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 34560, 135, 135, 17280, 135, 34560, 135, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 135, 135, 17280]
Prompts retrieved: 1663200 . Total input tokens: 370889297 . Total output tokens: 326771370
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.92207847116515,
    "estimated_duration": 3600.0206990703905,
    "input_throughput": 5538.816764344976,
    "output_throughput": 4824.581704345472,
    "total_throughput": 10363.398468690448,
    "itl": 109.15611781669497,
    "ttft": 2021168.19702465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.74919433771632,
    "arrivals": 554568,
    "finished_requests": 80665,
    "scheduler_time": 100.69498190415541
}
#Debug simulation 
Total elapsed time: 5.9221763289533556. Arrivals time: 0.29397730994969606 Scheduler time: 5.4958295822143555 Scheduler overhead time: 0.049066624604165554 Adapter cache time: 0.011177350301295519 Engine time: 0.04937307629734278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 17280, 17280, 135, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 34560, 135, 135, 17280, 135, 34560, 135, 135, 34560, 135, 17280, 17280, 135, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 135, 135, 17280]
Prompts retrieved: 1663200 . Total input tokens: 370889297 . Total output tokens: 326771370
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.595055294688791,
    "estimated_duration": 3600.095157847247,
    "input_throughput": 5251.863678877301,
    "output_throughput": 4572.151923296026,
    "total_throughput": 9824.015602173327,
    "itl": 96.74032267201653,
    "ttft": 2055074.17828734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.599100169409083,
    "arrivals": 554568,
    "finished_requests": 76449,
    "scheduler_time": 100.82431590708254
}
#Debug simulation 
Total elapsed time: 5.595199787989259. Arrivals time: 0.23405936127528548 Scheduler time: 5.214677210431546 Scheduler overhead time: 0.05402875738218427 Adapter cache time: 0.012902212794870138 Engine time: 0.05437183612957597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 17280, 17280, 66, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 34560, 66, 66, 17280, 66, 34560, 66, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 66, 66, 17280]
Prompts retrieved: 1660992 . Total input tokens: 370385274 . Total output tokens: 326338748
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.88611862808466,
    "estimated_duration": 3600.1001707549417,
    "input_throughput": 5538.271729761043,
    "output_throughput": 4837.0187422724775,
    "total_throughput": 10375.290472033521,
    "itl": 111.73030823582337,
    "ttft": 1995290.0930104111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2828073521982863,
    "arrivals": 553859,
    "finished_requests": 80562,
    "scheduler_time": 100.73211941920282
}
#Debug simulation 
Total elapsed time: 5.886214768979698. Arrivals time: 0.2928940299898386 Scheduler time: 5.462530908174813 Scheduler overhead time: 0.04888021294027567 Adapter cache time: 0.010064736474305391 Engine time: 0.04914144519716501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 17280, 17280, 66, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 34560, 66, 66, 17280, 66, 34560, 66, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 66, 66, 17280]
Prompts retrieved: 1660992 . Total input tokens: 370385274 . Total output tokens: 326338748
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.7486388380639255,
    "estimated_duration": 3600.0653694440216,
    "input_throughput": 5454.240127598694,
    "output_throughput": 4765.498189453575,
    "total_throughput": 10219.738317052268,
    "itl": 106.52975657750181,
    "ttft": 2016935.950890895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5469111386174357,
    "arrivals": 553859,
    "finished_requests": 79369,
    "scheduler_time": 100.74236405068753
}
#Debug simulation 
Total elapsed time: 5.748738782014698. Arrivals time: 0.29543188912793994 Scheduler time: 5.319173266645521 Scheduler overhead time: 0.050099944695830345 Adapter cache time: 0.01038896944373846 Engine time: 0.050390149001032114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 17280, 17280, 66, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 34560, 66, 66, 17280, 66, 34560, 66, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 66, 66, 17280]
Prompts retrieved: 1660992 . Total input tokens: 370385274 . Total output tokens: 326338748
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.544475704897195,
    "estimated_duration": 3600.069172363323,
    "input_throughput": 5229.182023644772,
    "output_throughput": 4568.005005638358,
    "total_throughput": 9797.18702928313,
    "itl": 96.7530966246279,
    "ttft": 2056492.2972240564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0458490363135997,
    "arrivals": 553859,
    "finished_requests": 76053,
    "scheduler_time": 100.82258175404809
}
#Debug simulation 
Total elapsed time: 5.54463769774884. Arrivals time: 0.23402309650555253 Scheduler time: 5.165277354419231 Scheduler overhead time: 0.053543427493423223 Adapter cache time: 0.012206227984279394 Engine time: 0.054550235625356436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 17280, 17280, 66, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 34560, 66, 66, 17280, 66, 34560, 66, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 66, 66, 17280]
Prompts retrieved: 1660992 . Total input tokens: 370385274 . Total output tokens: 326338748
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.823236084077507,
    "estimated_duration": 3600.072972219388,
    "input_throughput": 5529.799021747948,
    "output_throughput": 4824.031383256546,
    "total_throughput": 10353.830405004495,
    "itl": 109.23686870203552,
    "ttft": 2025357.465578658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5041406638827162,
    "arrivals": 553859,
    "finished_requests": 80432,
    "scheduler_time": 100.68245225522884
}
#Debug simulation 
Total elapsed time: 5.823331417981535. Arrivals time: 0.2965221661143005 Scheduler time: 5.395470389164984 Scheduler overhead time: 0.04907890548929572 Adapter cache time: 0.010424118023365736 Engine time: 0.04915152583271265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 17280, 17280, 66, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 34560, 66, 66, 17280, 66, 34560, 66, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 66, 66, 17280]
Prompts retrieved: 1660992 . Total input tokens: 370385274 . Total output tokens: 326338748
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.5488989050500095,
    "estimated_duration": 3600.0348825458814,
    "input_throughput": 5229.455995350363,
    "output_throughput": 4568.357123353623,
    "total_throughput": 9797.813118703985,
    "itl": 96.7744926817932,
    "ttft": 2056450.5301290273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0528865307755835,
    "arrivals": 553859,
    "finished_requests": 76057,
    "scheduler_time": 100.81825137101579
}
#Debug simulation 
Total elapsed time: 5.54900737432763. Arrivals time: 0.24176481273025274 Scheduler time: 5.161946718581021 Scheduler overhead time: 0.05364896450191736 Adapter cache time: 0.0124641009606421 Engine time: 0.054209379479289055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 17280, 17280, 66, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 34560, 66, 66, 17280, 66, 34560, 66, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 66, 66, 17280]
Prompts retrieved: 1660992 . Total input tokens: 370385274 . Total output tokens: 326338748
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.84016162995249,
    "estimated_duration": 3600.0903003866592,
    "input_throughput": 5529.931290296191,
    "output_throughput": 4824.037885420469,
    "total_throughput": 10353.96917571666,
    "itl": 109.23909879774799,
    "ttft": 2025396.3314005632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.391694765044372,
    "arrivals": 553859,
    "finished_requests": 80434,
    "scheduler_time": 100.6850689710996
}
#Debug simulation 
Total elapsed time: 5.840294090099633. Arrivals time: 0.2955111558549106 Scheduler time: 5.41285413922742 Scheduler overhead time: 0.04912990378215909 Adapter cache time: 0.010436889249831438 Engine time: 0.04952936293557286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 17280, 17280, 66, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 34560, 66, 66, 17280, 66, 34560, 66, 66, 34560, 66, 17280, 17280, 66, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 66, 66, 17280]
Prompts retrieved: 1660992 . Total input tokens: 370385274 . Total output tokens: 326338748
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.496567361056805,
    "estimated_duration": 3600.086686706685,
    "input_throughput": 5170.178837284339,
    "output_throughput": 4517.558718809003,
    "total_throughput": 9687.737556093341,
    "itl": 94.67331092715055,
    "ttft": 2056034.468150419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.014935211502022,
    "arrivals": 553859,
    "finished_requests": 75204,
    "scheduler_time": 100.87309287482574
}
#Debug simulation 
Total elapsed time: 5.496686962433159. Arrivals time: 0.28655800642445683 Scheduler time: 5.062358841765672 Scheduler overhead time: 0.05454439437016845 Adapter cache time: 0.012552771717309952 Engine time: 0.05519900796934962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 17280, 17280, 33, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 34560, 33, 33, 17280, 33, 34560, 33, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 33, 33, 17280]
Prompts retrieved: 1659936 . Total input tokens: 370154558 . Total output tokens: 326122796
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.871930422727019,
    "estimated_duration": 3600.0698141307507,
    "input_throughput": 5685.065583913333,
    "output_throughput": 4955.965278775564,
    "total_throughput": 10641.030862688896,
    "itl": 116.7368111292641,
    "ttft": 2010136.0026064382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8397759470576411,
    "arrivals": 553508,
    "finished_requests": 82958,
    "scheduler_time": 100.63001542209753
}
#Debug simulation 
Total elapsed time: 5.872028157114983. Arrivals time: 0.2968593859113753 Scheduler time: 5.452838181518018 Scheduler overhead time: 0.04600794147700071 Adapter cache time: 0.008624767884612083 Engine time: 0.04642979335039854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 17280, 17280, 33, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 34560, 33, 33, 17280, 33, 34560, 33, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 33, 33, 17280]
Prompts retrieved: 1659936 . Total input tokens: 370154558 . Total output tokens: 326122796
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.784963245969266,
    "estimated_duration": 3600.112752952089,
    "input_throughput": 5533.832234466444,
    "output_throughput": 4825.114987233616,
    "total_throughput": 10358.94722170006,
    "itl": 108.99053676057858,
    "ttft": 2026807.9276639074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0378405820997438,
    "arrivals": 553508,
    "finished_requests": 80747,
    "scheduler_time": 100.70466713024135
}
#Debug simulation 
Total elapsed time: 5.78512238105759. Arrivals time: 0.29537116922438145 Scheduler time: 5.359673015307635 Scheduler overhead time: 0.04885951895266771 Adapter cache time: 0.00943699898198247 Engine time: 0.049086613580584526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 17280, 17280, 33, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 34560, 33, 33, 17280, 33, 34560, 33, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 33, 33, 17280]
Prompts retrieved: 1659936 . Total input tokens: 370154558 . Total output tokens: 326122796
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.485972533933818,
    "estimated_duration": 3600.0043224036144,
    "input_throughput": 5219.826232723396,
    "output_throughput": 4559.654803147666,
    "total_throughput": 9779.481035871062,
    "itl": 95.93943689545075,
    "ttft": 2059203.11055511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2724178268387931,
    "arrivals": 553508,
    "finished_requests": 76172,
    "scheduler_time": 100.8931458500585
}
#Debug simulation 
Total elapsed time: 5.486066957004368. Arrivals time: 0.23214113898575306 Scheduler time: 5.111469212453812 Scheduler overhead time: 0.052729396149516106 Adapter cache time: 0.01122388755902648 Engine time: 0.05384069215506315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 17280, 17280, 33, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 34560, 33, 33, 17280, 33, 34560, 33, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 33, 33, 17280]
Prompts retrieved: 1659936 . Total input tokens: 370154558 . Total output tokens: 326122796
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.802175916265696,
    "estimated_duration": 3600.0527580738085,
    "input_throughput": 5533.924455779197,
    "output_throughput": 4825.195397773628,
    "total_throughput": 10359.119853552826,
    "itl": 108.98921879385577,
    "ttft": 2026777.650264078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9781470802472897,
    "arrivals": 553508,
    "finished_requests": 80747,
    "scheduler_time": 100.70436575381152
}
#Debug simulation 
Total elapsed time: 5.80229513393715. Arrivals time: 0.29934555431827903 Scheduler time: 5.372588298283517 Scheduler overhead time: 0.048849019687622786 Adapter cache time: 0.00940802413970232 Engine time: 0.04948172997683287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 17280, 17280, 33, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 34560, 33, 33, 17280, 33, 34560, 33, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 33, 33, 17280]
Prompts retrieved: 1659936 . Total input tokens: 370154558 . Total output tokens: 326122796
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.520979898050427,
    "estimated_duration": 3600.104492569588,
    "input_throughput": 5219.859323190676,
    "output_throughput": 4559.539045013626,
    "total_throughput": 9779.398368204302,
    "itl": 95.93829582527988,
    "ttft": 2059304.2634702884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2633046710677462,
    "arrivals": 553508,
    "finished_requests": 76175,
    "scheduler_time": 100.89619207366083
}
#Debug simulation 
Total elapsed time: 5.521109314169735. Arrivals time: 0.2367671518586576 Scheduler time: 5.140071136876941 Scheduler overhead time: 0.05350244278088212 Adapter cache time: 0.011385492514818907 Engine time: 0.05436221184208989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 17280, 17280, 33, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 34560, 33, 33, 17280, 33, 34560, 33, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 33, 33, 17280]
Prompts retrieved: 1659936 . Total input tokens: 370154558 . Total output tokens: 326122796
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.7905391324311495,
    "estimated_duration": 3600.114143508205,
    "input_throughput": 5533.857318364382,
    "output_throughput": 4825.13645611037,
    "total_throughput": 10358.993774474753,
    "itl": 108.98800728627089,
    "ttft": 2026811.3301111753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9129006945015846,
    "arrivals": 553508,
    "finished_requests": 80748,
    "scheduler_time": 100.70761363143076
}
#Debug simulation 
Total elapsed time: 5.790634448174387. Arrivals time: 0.2455266690813005 Scheduler time: 5.415370144881308 Scheduler overhead time: 0.04877227824181318 Adapter cache time: 0.009390647057443857 Engine time: 0.04894693801179528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 17280, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 17280, 17280, 33, 34560, 34560, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 34560, 33, 33, 17280, 33, 34560, 33, 33, 34560, 33, 17280, 17280, 33, 17280, 34560, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 33, 33, 17280]
Prompts retrieved: 1659936 . Total input tokens: 370154558 . Total output tokens: 326122796
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.516887009609491,
    "estimated_duration": 3600.092835747076,
    "input_throughput": 5219.876224691954,
    "output_throughput": 4559.55380844885,
    "total_throughput": 9779.430033140805,
    "itl": 95.938049433111,
    "ttft": 2059298.211276256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2517061091773227,
    "arrivals": 553508,
    "finished_requests": 76175,
    "scheduler_time": 100.89613381303974
}
#Debug simulation 
Total elapsed time: 5.516985507681966. Arrivals time: 0.23610648093745112 Scheduler time: 5.1369039146229625 Scheduler overhead time: 0.053334586787968874 Adapter cache time: 0.011384055018424988 Engine time: 0.054296988528221846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 8640]
Prompts retrieved: 1520640 . Total input tokens: 339077557 . Total output tokens: 298769094
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.256701646838337,
    "estimated_duration": 3600.103755945904,
    "input_throughput": 5688.317723115009,
    "output_throughput": 4948.291273710635,
    "total_throughput": 10636.608996825646,
    "itl": 116.78879319335658,
    "ttft": 1987806.7518668682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 489,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2334680166235565,
    "arrivals": 507133,
    "finished_requests": 82866,
    "scheduler_time": 100.47916768328066
}
#Debug simulation 
Total elapsed time: 8.256837111897767. Arrivals time: 0.5959334736689925 Scheduler time: 7.5312754032202065 Scheduler overhead time: 0.047205436043441296 Adapter cache time: 0.01354170124977827 Engine time: 0.04714456619694829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 8640]
Prompts retrieved: 1520640 . Total input tokens: 339077557 . Total output tokens: 298769094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.331347504165024,
    "estimated_duration": 3600.0510161824805,
    "input_throughput": 5530.738567453331,
    "output_throughput": 4815.559257930598,
    "total_throughput": 10346.297825383928,
    "itl": 108.95376881191292,
    "ttft": 2005440.8709240153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.510561885898942,
    "arrivals": 507133,
    "finished_requests": 80599,
    "scheduler_time": 100.51426233275785
}
#Debug simulation 
Total elapsed time: 7.331440384034067. Arrivals time: 0.25293960282579064 Scheduler time: 6.940376674756408 Scheduler overhead time: 0.04991913214325905 Adapter cache time: 0.015418909024447203 Engine time: 0.04976861132308841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 8640]
Prompts retrieved: 1520640 . Total input tokens: 339077557 . Total output tokens: 298769094
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.549866266082972,
    "estimated_duration": 3600.029683372714,
    "input_throughput": 5253.721125510471,
    "output_throughput": 4563.317651483705,
    "total_throughput": 9817.038776994177,
    "itl": 96.79240947612253,
    "ttft": 2039795.4608848803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 870,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.621196719571049,
    "arrivals": 507133,
    "finished_requests": 76487,
    "scheduler_time": 100.58140634095813
}
#Debug simulation 
Total elapsed time: 6.5499636689201. Arrivals time: 0.23940625973045826 Scheduler time: 6.156066669151187 Scheduler overhead time: 0.05504553020000458 Adapter cache time: 0.019367840606719255 Engine time: 0.0546213467605412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 8640]
Prompts retrieved: 1520640 . Total input tokens: 339077557 . Total output tokens: 298769094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 7.333055220078677,
    "estimated_duration": 3600.0533875694528,
    "input_throughput": 5533.130444337254,
    "output_throughput": 4817.688832028577,
    "total_throughput": 10350.81927636583,
    "itl": 109.04839978469735,
    "ttft": 2005335.40986805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.131313184746536,
    "arrivals": 507133,
    "finished_requests": 80628,
    "scheduler_time": 100.52111861961207
}
#Debug simulation 
Total elapsed time: 7.333205724135041. Arrivals time: 0.2535402551293373 Scheduler time: 6.941714991815388 Scheduler overhead time: 0.049807684030383825 Adapter cache time: 0.015365441795438528 Engine time: 0.04984448431059718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 8640]
Prompts retrieved: 1520640 . Total input tokens: 339077557 . Total output tokens: 298769094
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.508476549759507,
    "estimated_duration": 3600.068347753642,
    "input_throughput": 5253.778865560113,
    "output_throughput": 4563.419750141987,
    "total_throughput": 9817.1986157021,
    "itl": 96.7908289910399,
    "ttft": 2039786.4443911572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 870,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.552226699758355,
    "arrivals": 507133,
    "finished_requests": 76489,
    "scheduler_time": 100.58435145765199
}
#Debug simulation 
Total elapsed time: 6.508570448961109. Arrivals time: 0.243872603867203 Scheduler time: 6.11134867509827 Scheduler overhead time: 0.05436345376074314 Adapter cache time: 0.019348236732184887 Engine time: 0.05451518204063177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 8640]
Prompts retrieved: 1520640 . Total input tokens: 339077557 . Total output tokens: 298769094
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.611816606018692,
    "estimated_duration": 3600.038232511241,
    "input_throughput": 5534.394835052016,
    "output_throughput": 4818.7613240704195,
    "total_throughput": 10353.156159122434,
    "itl": 109.1069522359056,
    "ttft": 2004679.4682651535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9388792203320007,
    "arrivals": 507133,
    "finished_requests": 80655,
    "scheduler_time": 100.52477519158283
}
#Debug simulation 
Total elapsed time: 7.611944345291704. Arrivals time: 0.3668045988306403 Scheduler time: 7.101496229879558 Scheduler overhead time: 0.0512542650103569 Adapter cache time: 0.017859731800854206 Engine time: 0.05087765445932746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 4320, 8640]
Prompts retrieved: 1520640 . Total input tokens: 339077557 . Total output tokens: 298769094
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.879746831022203,
    "estimated_duration": 3600.0061316750566,
    "input_throughput": 5253.869662494011,
    "output_throughput": 4563.49861614149,
    "total_throughput": 9817.368278635502,
    "itl": 96.78922924901971,
    "ttft": 2039761.4609412178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 870,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.490091546773945,
    "arrivals": 507133,
    "finished_requests": 76489,
    "scheduler_time": 100.58427053205112
}
#Debug simulation 
Total elapsed time: 6.879927415866405. Arrivals time: 0.30573748983442783 Scheduler time: 6.412540087010711 Scheduler overhead time: 0.057008895091712475 Adapter cache time: 0.021491379477083683 Engine time: 0.056571145076304674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 8640]
Prompts retrieved: 1416960 . Total input tokens: 315854953 . Total output tokens: 278377465
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.09839485399425,
    "estimated_duration": 3600.0315708207145,
    "input_throughput": 5689.959267587803,
    "output_throughput": 4947.946052578408,
    "total_throughput": 10637.905320166212,
    "itl": 116.92518866493786,
    "ttft": 1977790.2675086462,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4832133236621345,
    "arrivals": 472666,
    "finished_requests": 82748,
    "scheduler_time": 100.33651876962657
}
#Debug simulation 
Total elapsed time: 7.098520049825311. Arrivals time: 0.39254930475726724 Scheduler time: 6.561893243342638 Scheduler overhead time: 0.05064967181533575 Adapter cache time: 0.01920885918661952 Engine time: 0.05094285123050213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 8640]
Prompts retrieved: 1416960 . Total input tokens: 315854953 . Total output tokens: 278377465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.459034519735724,
    "estimated_duration": 3600.0993380183777,
    "input_throughput": 5530.407394524613,
    "output_throughput": 4812.875805126339,
    "total_throughput": 10343.283199650952,
    "itl": 108.94165383940738,
    "ttft": 1995939.797272931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.154355684160269,
    "arrivals": 472666,
    "finished_requests": 80457,
    "scheduler_time": 100.36088925539573
}
#Debug simulation 
Total elapsed time: 6.459147865884006. Arrivals time: 0.38859992334619164 Scheduler time: 5.926269342657179 Scheduler overhead time: 0.05047079501673579 Adapter cache time: 0.020014481619000435 Engine time: 0.05040699476376176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 8640]
Prompts retrieved: 1416960 . Total input tokens: 315854953 . Total output tokens: 278377465
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.965918800793588,
    "estimated_duration": 3600.0714302202646,
    "input_throughput": 5240.063528085552,
    "output_throughput": 4556.660143545077,
    "total_throughput": 9796.723671630629,
    "itl": 96.76375713760096,
    "ttft": 2030727.0549023652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.899869133331798,
    "arrivals": 472666,
    "finished_requests": 76153,
    "scheduler_time": 100.40091782346616
}
#Debug simulation 
Total elapsed time: 5.966165603604168. Arrivals time: 0.2976341634057462 Scheduler time: 5.5084551512263715 Scheduler overhead time: 0.05523174908012152 Adapter cache time: 0.023877933621406555 Engine time: 0.055172188207507133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 8640]
Prompts retrieved: 1416960 . Total input tokens: 315854953 . Total output tokens: 278377465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.617299871984869,
    "estimated_duration": 3600.00809929061,
    "input_throughput": 5533.393939842895,
    "output_throughput": 4816.57805253739,
    "total_throughput": 10349.971992380286,
    "itl": 109.04019921532836,
    "ttft": 1995966.0083294369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 819,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.628238890641362,
    "arrivals": 472666,
    "finished_requests": 80513,
    "scheduler_time": 100.3697353648658
}
#Debug simulation 
Total elapsed time: 6.617421732284129. Arrivals time: 0.30891886493191123 Scheduler time: 6.159551402553916 Scheduler overhead time: 0.05218600109219551 Adapter cache time: 0.020477714482694864 Engine time: 0.05224984372034669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 8640]
Prompts retrieved: 1416960 . Total input tokens: 315854953 . Total output tokens: 278377465
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.086204451974481,
    "estimated_duration": 3600.079892181167,
    "input_throughput": 5228.765628474758,
    "output_throughput": 4545.775785571498,
    "total_throughput": 9774.541414046256,
    "itl": 96.26887481204123,
    "ttft": 2032984.8813322017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.304264705036726,
    "arrivals": 472666,
    "finished_requests": 75982,
    "scheduler_time": 100.39998304082133
}
#Debug simulation 
Total elapsed time: 6.086317799054086. Arrivals time: 0.38191952044144273 Scheduler time: 5.539521901868284 Scheduler overhead time: 0.05629053944721818 Adapter cache time: 0.02564905397593975 Engine time: 0.05669742776080966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 8640]
Prompts retrieved: 1416960 . Total input tokens: 315854953 . Total output tokens: 278377465
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.735266609117389,
    "estimated_duration": 3600.0974791518966,
    "input_throughput": 5537.861714982418,
    "output_throughput": 4819.851990254368,
    "total_throughput": 10357.713705236785,
    "itl": 109.17699985172287,
    "ttft": 1995570.748767982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 810,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.170975961862107,
    "arrivals": 472666,
    "finished_requests": 80573,
    "scheduler_time": 100.38189515787867
}
#Debug simulation 
Total elapsed time: 6.73546407232061. Arrivals time: 0.40391756501048803 Scheduler time: 6.181417787447572 Scheduler overhead time: 0.05230087973177433 Adapter cache time: 0.020890450570732355 Engine time: 0.052582002710551023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 8640]
Prompts retrieved: 1416960 . Total input tokens: 315854953 . Total output tokens: 278377465
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.108218082226813,
    "estimated_duration": 3600.0973820582267,
    "input_throughput": 5228.889388885853,
    "output_throughput": 4545.938974196144,
    "total_throughput": 9774.828363081997,
    "itl": 96.26557760166592,
    "ttft": 2033025.452084447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.21251179579642,
    "arrivals": 472666,
    "finished_requests": 75985,
    "scheduler_time": 100.40298872897907
}
#Debug simulation 
Total elapsed time: 6.108345326967537. Arrivals time: 0.3802934312261641 Scheduler time: 5.56173056922853 Scheduler overhead time: 0.05691734189167619 Adapter cache time: 0.02594243222847581 Engine time: 0.05702171195298433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 8640, 8640, 540, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 34560, 540, 540, 8640, 540, 34560, 540, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 540, 540, 8640]
Prompts retrieved: 1399680 . Total input tokens: 311955384 . Total output tokens: 274989602
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.911708650179207,
    "estimated_duration": 3600.0076676197104,
    "input_throughput": 5691.043989788592,
    "output_throughput": 4951.067788081956,
    "total_throughput": 10642.111777870548,
    "itl": 117.07598908377854,
    "ttft": 1965977.3983286733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 728,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.813833775259642,
    "arrivals": 466828,
    "finished_requests": 83098,
    "scheduler_time": 100.32216093723206
}
#Debug simulation 
Total elapsed time: 6.9117926973849535. Arrivals time: 0.4091015011072159 Scheduler time: 6.363186770584434 Scheduler overhead time: 0.04884803481400013 Adapter cache time: 0.01938567729666829 Engine time: 0.04876599786803126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 8640, 8640, 540, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 34560, 540, 540, 8640, 540, 34560, 540, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 540, 540, 8640]
Prompts retrieved: 1399680 . Total input tokens: 311955384 . Total output tokens: 274989602
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.292031181976199,
    "estimated_duration": 3600.007573925383,
    "input_throughput": 5535.879464350558,
    "output_throughput": 4821.225134555716,
    "total_throughput": 10357.104598906273,
    "itl": 109.33241912150332,
    "ttft": 1983212.7375182554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 842,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.258169970987379,
    "arrivals": 466828,
    "finished_requests": 80835,
    "scheduler_time": 100.34381682681878
}
#Debug simulation 
Total elapsed time: 6.292234784923494. Arrivals time: 0.4366543898358941 Scheduler time: 5.712042497936636 Scheduler overhead time: 0.050238133408129215 Adapter cache time: 0.019325651694089174 Engine time: 0.050462333019822836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 8640, 8640, 540, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 34560, 540, 540, 8640, 540, 34560, 540, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 540, 540, 8640]
Prompts retrieved: 1399680 . Total input tokens: 311955384 . Total output tokens: 274989602
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.0142276477999985,
    "estimated_duration": 3600.0163846456535,
    "input_throughput": 5241.930309119262,
    "output_throughput": 4564.285615499313,
    "total_throughput": 9806.215924618577,
    "itl": 96.93390761043509,
    "ttft": 2017736.9419260134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.49962498003151,
    "arrivals": 466828,
    "finished_requests": 76497,
    "scheduler_time": 100.4059271515776
}
#Debug simulation 
Total elapsed time: 6.014344616793096. Arrivals time: 0.39038012037053704 Scheduler time: 5.459249137435108 Scheduler overhead time: 0.05650056805461645 Adapter cache time: 0.024557075928896666 Engine time: 0.05737575050443411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 8640, 8640, 540, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 34560, 540, 540, 8640, 540, 34560, 540, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 540, 540, 8640]
Prompts retrieved: 1399680 . Total input tokens: 311955384 . Total output tokens: 274989602
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.4102220251224935,
    "estimated_duration": 3600.0109907309966,
    "input_throughput": 5536.899762617821,
    "output_throughput": 4822.028611772408,
    "total_throughput": 10358.928374390229,
    "itl": 109.31893423338728,
    "ttft": 1983076.8543758723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.789234612812281,
    "arrivals": 466828,
    "finished_requests": 80847,
    "scheduler_time": 100.35704885549578
}
#Debug simulation 
Total elapsed time: 6.410360595211387. Arrivals time: 0.32115990156307817 Scheduler time: 5.939174375962466 Scheduler overhead time: 0.05223300028592348 Adapter cache time: 0.0211389041505754 Engine time: 0.052494462579488754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 8640, 8640, 540, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 34560, 540, 540, 8640, 540, 34560, 540, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 540, 540, 8640]
Prompts retrieved: 1399680 . Total input tokens: 311955384 . Total output tokens: 274989602
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.107754033058882,
    "estimated_duration": 3600.0254282429078,
    "input_throughput": 5242.146028177066,
    "output_throughput": 4564.671369007677,
    "total_throughput": 9806.817397184743,
    "itl": 96.92816176945382,
    "ttft": 2017646.9712183068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.38364720244893,
    "arrivals": 466828,
    "finished_requests": 76501,
    "scheduler_time": 100.41189644318563
}
#Debug simulation 
Total elapsed time: 6.107973644975573. Arrivals time: 0.37697818502783775 Scheduler time: 5.562615414150059 Scheduler overhead time: 0.058203212916851044 Adapter cache time: 0.02438326645642519 Engine time: 0.05858904169872403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 8640, 8640, 540, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 34560, 540, 540, 8640, 540, 34560, 540, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 540, 540, 8640]
Prompts retrieved: 1399680 . Total input tokens: 311955384 . Total output tokens: 274989602
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.2325931950472295,
    "estimated_duration": 3600.0423055622045,
    "input_throughput": 5537.626590998275,
    "output_throughput": 4822.573882861281,
    "total_throughput": 10360.200473859555,
    "itl": 109.30869298779558,
    "ttft": 1983159.4825879522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.356109669138651,
    "arrivals": 466828,
    "finished_requests": 80859,
    "scheduler_time": 100.36991849424027
}
#Debug simulation 
Total elapsed time: 6.23271912522614. Arrivals time: 0.3894274798221886 Scheduler time: 5.697664400562644 Scheduler overhead time: 0.050402607303112745 Adapter cache time: 0.020855289418250322 Engine time: 0.05089888256043196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 8640, 8640, 540, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 34560, 540, 540, 8640, 540, 34560, 540, 540, 34560, 540, 8640, 8640, 540, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 540, 540, 8640]
Prompts retrieved: 1399680 . Total input tokens: 311955384 . Total output tokens: 274989602
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.231099501717836,
    "estimated_duration": 3600.053762754083,
    "input_throughput": 5242.46948622284,
    "output_throughput": 4565.020436646912,
    "total_throughput": 9807.489922869752,
    "itl": 96.92245449552344,
    "ttft": 2017813.1980567768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.23003345027568,
    "arrivals": 466828,
    "finished_requests": 76508,
    "scheduler_time": 100.4170616749948
}
#Debug simulation 
Total elapsed time: 6.231209788005799. Arrivals time: 0.7513595023192465 Scheduler time: 5.319950777571648 Scheduler overhead time: 0.054998097475618124 Adapter cache time: 0.023429570253938437 Engine time: 0.05572554795071483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 8640, 8640, 270, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 34560, 270, 270, 8640, 270, 34560, 270, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 270, 270, 8640]
Prompts retrieved: 1391040 . Total input tokens: 310039748 . Total output tokens: 273276434
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.277621271088719,
    "estimated_duration": 3600.0956080215515,
    "input_throughput": 5653.615130289661,
    "output_throughput": 4956.883633934083,
    "total_throughput": 10610.498764223745,
    "itl": 117.38506545965609,
    "ttft": 1972756.1011090525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.093081190777076,
    "arrivals": 463915,
    "finished_requests": 82777,
    "scheduler_time": 100.32536159308559
}
#Debug simulation 
Total elapsed time: 6.277803989127278. Arrivals time: 0.3952979240566492 Scheduler time: 5.747401054017246 Scheduler overhead time: 0.04770490247756243 Adapter cache time: 0.017545454669743776 Engine time: 0.047802402172237635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 8640, 8640, 270, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 34560, 270, 270, 8640, 270, 34560, 270, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 270, 270, 8640]
Prompts retrieved: 1391040 . Total input tokens: 310039748 . Total output tokens: 273276434
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.08854303508997,
    "estimated_duration": 3600.010191388809,
    "input_throughput": 5496.531106309554,
    "output_throughput": 4822.813291342944,
    "total_throughput": 10319.344397652498,
    "itl": 109.63655027500425,
    "ttft": 1989985.1767956659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.366035724985423,
    "arrivals": 463915,
    "finished_requests": 80483,
    "scheduler_time": 100.3531844428589
}
#Debug simulation 
Total elapsed time: 6.088687734212726. Arrivals time: 0.3855729908682406 Scheduler time: 5.560316626448184 Scheduler overhead time: 0.05035784328356385 Adapter cache time: 0.018693032674491405 Engine time: 0.05041236290708184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 8640, 8640, 270, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 34560, 270, 270, 8640, 270, 34560, 270, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 270, 270, 8640]
Prompts retrieved: 1391040 . Total input tokens: 310039748 . Total output tokens: 273276434
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.800745232962072,
    "estimated_duration": 3600.0936161609807,
    "input_throughput": 5206.363222295127,
    "output_throughput": 4570.034214150367,
    "total_throughput": 9776.397436445493,
    "itl": 97.16437728499487,
    "ttft": 2024920.6252150626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 982,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.383886006670097,
    "arrivals": 463915,
    "finished_requests": 76257,
    "scheduler_time": 100.46087973448434
}
#Debug simulation 
Total elapsed time: 5.800903627183288. Arrivals time: 0.28266775282099843 Scheduler time: 5.35852747829631 Scheduler overhead time: 0.055135217029601336 Adapter cache time: 0.022505166940391064 Engine time: 0.05618120729923248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 8640, 8640, 270, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 34560, 270, 270, 8640, 270, 34560, 270, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 270, 270, 8640]
Prompts retrieved: 1391040 . Total input tokens: 310039748 . Total output tokens: 273276434
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.25605707289651,
    "estimated_duration": 3600.0384819931637,
    "input_throughput": 5497.106238998692,
    "output_throughput": 4823.427884689198,
    "total_throughput": 10320.53412368789,
    "itl": 109.62761301369937,
    "ttft": 1989830.9789924563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.003983904072076,
    "arrivals": 463915,
    "finished_requests": 80493,
    "scheduler_time": 100.36433799855574
}
#Debug simulation 
Total elapsed time: 6.256251906044781. Arrivals time: 0.39027756033465266 Scheduler time: 5.718691346235573 Scheduler overhead time: 0.051597509533166885 Adapter cache time: 0.01957659889012575 Engine time: 0.05208833422511816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 8640, 8640, 270, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 34560, 270, 270, 8640, 270, 34560, 270, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 270, 270, 8640]
Prompts retrieved: 1391040 . Total input tokens: 310039748 . Total output tokens: 273276434
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.855391677003354,
    "estimated_duration": 3600.0859721638153,
    "input_throughput": 5206.387609886532,
    "output_throughput": 4570.072250277736,
    "total_throughput": 9776.459860164268,
    "itl": 97.16522578578886,
    "ttft": 2024922.7301965563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.331739498851847,
    "arrivals": 463915,
    "finished_requests": 76258,
    "scheduler_time": 100.4618132243285
}
#Debug simulation 
Total elapsed time: 5.8555140336975455. Arrivals time: 0.2960661705583334 Scheduler time: 5.397555774543434 Scheduler overhead time: 0.055852781515568495 Adapter cache time: 0.02322025829926133 Engine time: 0.05673670722171664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 8640, 8640, 270, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 34560, 270, 270, 8640, 270, 34560, 270, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 270, 270, 8640]
Prompts retrieved: 1391040 . Total input tokens: 310039748 . Total output tokens: 273276434
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.169243052136153,
    "estimated_duration": 3600.0750459939877,
    "input_throughput": 5497.378456602611,
    "output_throughput": 4823.847774874692,
    "total_throughput": 10321.226231477302,
    "itl": 109.6180836072597,
    "ttft": 1989841.9898577589,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.666646207557037,
    "arrivals": 463915,
    "finished_requests": 80502,
    "scheduler_time": 100.37318832518054
}
#Debug simulation 
Total elapsed time: 6.169367391150445. Arrivals time: 0.31033067172393203 Scheduler time: 5.713703884743154 Scheduler overhead time: 0.051285690162330866 Adapter cache time: 0.019029272720217705 Engine time: 0.051199003122746944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 8640, 8640, 270, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 34560, 270, 270, 8640, 270, 34560, 270, 270, 34560, 270, 8640, 8640, 270, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 270, 270, 8640]
Prompts retrieved: 1391040 . Total input tokens: 310039748 . Total output tokens: 273276434
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.031514936126769,
    "estimated_duration": 3600.048715350591,
    "input_throughput": 5206.540378211142,
    "output_throughput": 4570.180378350168,
    "total_throughput": 9776.72075656131,
    "itl": 97.16137424225886,
    "ttft": 2024885.7926130139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 981,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.240407168772098,
    "arrivals": 463915,
    "finished_requests": 76260,
    "scheduler_time": 100.46295320476686
}
#Debug simulation 
Total elapsed time: 6.031710240058601. Arrivals time: 0.30950194550678134 Scheduler time: 5.555293545592576 Scheduler overhead time: 0.057664941530674696 Adapter cache time: 0.023845679592341185 Engine time: 0.05835009412840009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 8640, 8640, 135, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 34560, 135, 135, 8640, 135, 34560, 135, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 135, 135, 8640]
Prompts retrieved: 1386720 . Total input tokens: 309082126 . Total output tokens: 272412582
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.200787967070937,
    "estimated_duration": 3600.0411682532854,
    "input_throughput": 5690.157985037455,
    "output_throughput": 4960.599939101461,
    "total_throughput": 10650.757924138916,
    "itl": 117.44924903178247,
    "ttft": 1964249.830177013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 497,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2863672888791577,
    "arrivals": 462529,
    "finished_requests": 82955,
    "scheduler_time": 100.36992080113448
}
#Debug simulation 
Total elapsed time: 6.200967521872371. Arrivals time: 0.3195867068134248 Scheduler time: 5.747404686175287 Scheduler overhead time: 0.047540238592773676 Adapter cache time: 0.016202492639422417 Engine time: 0.04803492082282901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 8640, 8640, 135, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 34560, 135, 135, 8640, 135, 34560, 135, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 135, 135, 8640]
Prompts retrieved: 1386720 . Total input tokens: 309082126 . Total output tokens: 272412582
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.127653056755662,
    "estimated_duration": 3600.033219820681,
    "input_throughput": 5536.402244919502,
    "output_throughput": 4828.434333410354,
    "total_throughput": 10364.836578329856,
    "itl": 109.62854558950777,
    "ttft": 1981166.7411821838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.202169320485561,
    "arrivals": 462529,
    "finished_requests": 80720,
    "scheduler_time": 100.4489430683567
}
#Debug simulation 
Total elapsed time: 6.127787198871374. Arrivals time: 0.3733632881194353 Scheduler time: 5.610342711210251 Scheduler overhead time: 0.05089136213064194 Adapter cache time: 0.01783681893721223 Engine time: 0.05171944573521614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 8640, 8640, 135, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 34560, 135, 135, 8640, 135, 34560, 135, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 135, 135, 8640]
Prompts retrieved: 1386720 . Total input tokens: 309082126 . Total output tokens: 272412582
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.8786440449766815,
    "estimated_duration": 3600.0164075032694,
    "input_throughput": 5241.218612413467,
    "output_throughput": 4577.9180243875135,
    "total_throughput": 9819.13663680098,
    "itl": 96.73414153621141,
    "ttft": 2014790.1981990675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 734,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.507471483200815,
    "arrivals": 462529,
    "finished_requests": 76486,
    "scheduler_time": 100.71385810271828
}
#Debug simulation 
Total elapsed time: 5.87882524728775. Arrivals time: 0.42793173529207706 Scheduler time: 5.292671914212406 Scheduler overhead time: 0.05507580190896988 Adapter cache time: 0.02127533871680498 Engine time: 0.05592522770166397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 8640, 8640, 135, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 34560, 135, 135, 8640, 135, 34560, 135, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 135, 135, 8640]
Prompts retrieved: 1386720 . Total input tokens: 309082126 . Total output tokens: 272412582
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.162206616718322,
    "estimated_duration": 3600.048276599982,
    "input_throughput": 5536.746029090765,
    "output_throughput": 4828.723579345371,
    "total_throughput": 10365.469608436137,
    "itl": 109.62327881190396,
    "ttft": 1981089.6077684262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.932854451662856,
    "arrivals": 462529,
    "finished_requests": 80726,
    "scheduler_time": 100.4533322620156
}
#Debug simulation 
Total elapsed time: 6.162334071006626. Arrivals time: 0.3930856455117464 Scheduler time: 5.624420447740704 Scheduler overhead time: 0.05149441212415695 Adapter cache time: 0.018034854903817177 Engine time: 0.05158330500125885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 8640, 8640, 135, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 34560, 135, 135, 8640, 135, 34560, 135, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 135, 135, 8640]
Prompts retrieved: 1386720 . Total input tokens: 309082126 . Total output tokens: 272412582
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.855666269082576,
    "estimated_duration": 3600.054024910759,
    "input_throughput": 5242.18799757257,
    "output_throughput": 4578.81934158158,
    "total_throughput": 9821.00733915415,
    "itl": 96.72887024062716,
    "ttft": 2014600.6154120765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.483437690641757,
    "arrivals": 462529,
    "finished_requests": 76499,
    "scheduler_time": 100.73021520586335
}
#Debug simulation 
Total elapsed time: 5.8558036969043314. Arrivals time: 0.30574480909854174 Scheduler time: 5.38984059728682 Scheduler overhead time: 0.055588699877262115 Adapter cache time: 0.021690852474421263 Engine time: 0.0568148517049849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 8640, 8640, 135, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 34560, 135, 135, 8640, 135, 34560, 135, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 135, 135, 8640]
Prompts retrieved: 1386720 . Total input tokens: 309082126 . Total output tokens: 272412582
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.097881560213864,
    "estimated_duration": 3600.0992763586746,
    "input_throughput": 5536.835089770474,
    "output_throughput": 4829.162660643224,
    "total_throughput": 10365.997750413697,
    "itl": 109.6201098437009,
    "ttft": 1981048.7933690958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6579866989468988,
    "arrivals": 462529,
    "finished_requests": 80730,
    "scheduler_time": 100.46262587397031
}
#Debug simulation 
Total elapsed time: 6.098101384937763. Arrivals time: 0.3806317704729736 Scheduler time: 5.574622557964176 Scheduler overhead time: 0.050753103103488684 Adapter cache time: 0.01740401005372405 Engine time: 0.051022442523390055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 8640, 8640, 135, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 34560, 135, 135, 8640, 135, 34560, 135, 135, 34560, 135, 8640, 8640, 135, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 135, 135, 8640]
Prompts retrieved: 1386720 . Total input tokens: 309082126 . Total output tokens: 272412582
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.915487504098564,
    "estimated_duration": 3600.0025467351215,
    "input_throughput": 5243.152957521726,
    "output_throughput": 4579.720371295912,
    "total_throughput": 9822.873328817639,
    "itl": 96.79968319920572,
    "ttft": 2014569.4054430558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 733,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.4015327232517585,
    "arrivals": 462529,
    "finished_requests": 76510,
    "scheduler_time": 100.71763284325428
}
#Debug simulation 
Total elapsed time: 5.915631996933371. Arrivals time: 0.3838517991825938 Scheduler time: 5.37129085091874 Scheduler overhead time: 0.05567051097750664 Adapter cache time: 0.02208589669317007 Engine time: 0.05665679834783077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 8640, 8640, 66, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 34560, 66, 66, 8640, 66, 34560, 66, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 66, 66, 8640]
Prompts retrieved: 1384512 . Total input tokens: 308590362 . Total output tokens: 271978574
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.325270731933415,
    "estimated_duration": 3600.103113231191,
    "input_throughput": 5709.248694699838,
    "output_throughput": 4957.689110182391,
    "total_throughput": 10666.93780488223,
    "itl": 116.62250630984438,
    "ttft": 1961432.218324726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 383,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5325526592368406,
    "arrivals": 461810,
    "finished_requests": 83032,
    "scheduler_time": 100.46800897561785
}
#Debug simulation 
Total elapsed time: 6.325434086844325. Arrivals time: 0.38425335427746177 Scheduler time: 5.804321548435837 Scheduler overhead time: 0.04894147999584675 Adapter cache time: 0.01566362101584673 Engine time: 0.04956489196047187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 8640, 8640, 66, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 34560, 66, 66, 8640, 66, 34560, 66, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 66, 66, 8640]
Prompts retrieved: 1384512 . Total input tokens: 308590362 . Total output tokens: 271978574
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.195707994047552,
    "estimated_duration": 3600.0853165663343,
    "input_throughput": 5565.94003697434,
    "output_throughput": 4831.312724718733,
    "total_throughput": 10397.252761693073,
    "itl": 108.73933061830292,
    "ttft": 1978323.2406044004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0421842481009693,
    "arrivals": 461810,
    "finished_requests": 80924,
    "scheduler_time": 100.63380321971106
}
#Debug simulation 
Total elapsed time: 6.195906894747168. Arrivals time: 0.37967408122494817 Scheduler time: 5.6726199723780155 Scheduler overhead time: 0.051052368711680174 Adapter cache time: 0.016662203706800938 Engine time: 0.05200017802417278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 8640, 8640, 66, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 34560, 66, 66, 8640, 66, 34560, 66, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 66, 66, 8640]
Prompts retrieved: 1384512 . Total input tokens: 308590362 . Total output tokens: 271978574
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.833485913928598,
    "estimated_duration": 3600.0852886367425,
    "input_throughput": 5278.95271258881,
    "output_throughput": 4594.759477563333,
    "total_throughput": 9873.712190152142,
    "itl": 96.06116609202743,
    "ttft": 2010847.3252921856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6367932401085548,
    "arrivals": 461810,
    "finished_requests": 76931,
    "scheduler_time": 101.10647198716453
}
#Debug simulation 
Total elapsed time: 5.8336026556789875. Arrivals time: 0.3620839403010905 Scheduler time: 5.311747005674988 Scheduler overhead time: 0.05656367214396596 Adapter cache time: 0.02100470894947648 Engine time: 0.056360858492553234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 8640, 8640, 66, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 34560, 66, 66, 8640, 66, 34560, 66, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 66, 66, 8640]
Prompts retrieved: 1384512 . Total input tokens: 308590362 . Total output tokens: 271978574
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.93116344185546,
    "estimated_duration": 3600.1053784049814,
    "input_throughput": 5566.343729882268,
    "output_throughput": 4831.7219002369,
    "total_throughput": 10398.065630119168,
    "itl": 108.73511399035698,
    "ttft": 1978433.5270394948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8644919635169166,
    "arrivals": 461810,
    "finished_requests": 80931,
    "scheduler_time": 100.63920175939198
}
#Debug simulation 
Total elapsed time: 5.931283097714186. Arrivals time: 0.30580686079338193 Scheduler time: 5.487051394302398 Scheduler overhead time: 0.049101154319942 Adapter cache time: 0.01653003692626953 Engine time: 0.04997030971571803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 8640, 8640, 66, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 34560, 66, 66, 8640, 66, 34560, 66, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 66, 66, 8640]
Prompts retrieved: 1384512 . Total input tokens: 308590362 . Total output tokens: 271978574
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.740029016975313,
    "estimated_duration": 3600.0450926882295,
    "input_throughput": 5278.970265844341,
    "output_throughput": 4594.807446605649,
    "total_throughput": 9873.77771244999,
    "itl": 96.05764561327933,
    "ttft": 2010858.509500522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6307823524438123,
    "arrivals": 461810,
    "finished_requests": 76930,
    "scheduler_time": 101.10846236630447
}
#Debug simulation 
Total elapsed time: 5.740231996867806. Arrivals time: 0.386090240906924 Scheduler time: 5.198509473353624 Scheduler overhead time: 0.05417430866509676 Adapter cache time: 0.020728930365294218 Engine time: 0.05534438509494066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 8640, 8640, 66, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 34560, 66, 66, 8640, 66, 34560, 66, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 66, 66, 8640]
Prompts retrieved: 1384512 . Total input tokens: 308590362 . Total output tokens: 271978574
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.936496538110077,
    "estimated_duration": 3600.0560758761935,
    "input_throughput": 5566.533569931307,
    "output_throughput": 4832.091676729272,
    "total_throughput": 10398.62524666058,
    "itl": 108.72791447976108,
    "ttft": 1978372.8336586282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6812467950396126,
    "arrivals": 461810,
    "finished_requests": 80933,
    "scheduler_time": 100.64191210202603
}
#Debug simulation 
Total elapsed time: 5.936627184972167. Arrivals time: 0.3005939591675997 Scheduler time: 5.497530501335859 Scheduler overhead time: 0.04906028090044856 Adapter cache time: 0.016895172651857138 Engine time: 0.04973248764872551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 8640, 8640, 66, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 34560, 66, 66, 8640, 66, 34560, 66, 66, 34560, 66, 8640, 8640, 66, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 66, 66, 8640]
Prompts retrieved: 1384512 . Total input tokens: 308590362 . Total output tokens: 271978574
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.516127462033182,
    "estimated_duration": 3600.0376270261013,
    "input_throughput": 5278.822326004043,
    "output_throughput": 4594.576144378748,
    "total_throughput": 9873.39847038279,
    "itl": 96.06097083307479,
    "ttft": 2010889.4937894451,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.65774712352085,
    "arrivals": 461810,
    "finished_requests": 76928,
    "scheduler_time": 101.10576989793292
}
#Debug simulation 
Total elapsed time: 6.516233691945672. Arrivals time: 0.88335184706375 Scheduler time: 5.46848765341565 Scheduler overhead time: 0.056653729639947414 Adapter cache time: 0.02273728186264634 Engine time: 0.058497617952525616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 8640, 8640, 33, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 34560, 33, 33, 8640, 33, 34560, 33, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 33, 33, 8640]
Prompts retrieved: 1383456 . Total input tokens: 308361283 . Total output tokens: 271776137
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.385060410015285,
    "estimated_duration": 3600.114075807077,
    "input_throughput": 5705.331988791316,
    "output_throughput": 4964.089088203017,
    "total_throughput": 10669.421076994333,
    "itl": 116.49591667123686,
    "ttft": 1966250.6874401765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5803657586360331,
    "arrivals": 461464,
    "finished_requests": 83096,
    "scheduler_time": 100.57788353998615
}
#Debug simulation 
Total elapsed time: 6.385239300783724. Arrivals time: 0.32406244426965714 Scheduler time: 5.92486584931612 Scheduler overhead time: 0.04917802894487977 Adapter cache time: 0.014624497387558222 Engine time: 0.04976883390918374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 8640, 8640, 33, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 34560, 33, 33, 8640, 33, 34560, 33, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 33, 33, 8640]
Prompts retrieved: 1383456 . Total input tokens: 308361283 . Total output tokens: 271776137
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.099878714885563,
    "estimated_duration": 3600.085668876199,
    "input_throughput": 5556.8702636581465,
    "output_throughput": 4834.428011107007,
    "total_throughput": 10391.298274765153,
    "itl": 108.59384110959282,
    "ttft": 1982195.5733301595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9843608661321943,
    "arrivals": 461464,
    "finished_requests": 80886,
    "scheduler_time": 100.75232754741319
}
#Debug simulation 
Total elapsed time: 6.1000036098994315. Arrivals time: 0.30179966846480966 Scheduler time: 5.6576560558751225 Scheduler overhead time: 0.04999218601733446 Adapter cache time: 0.016082569025456905 Engine time: 0.05110018281266093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 8640, 8640, 33, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 34560, 33, 33, 8640, 33, 34560, 33, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 33, 33, 8640]
Prompts retrieved: 1383456 . Total input tokens: 308361283 . Total output tokens: 271776137
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.717668977100402,
    "estimated_duration": 3600.0421660714555,
    "input_throughput": 5282.8903447967305,
    "output_throughput": 4600.396116491285,
    "total_throughput": 9883.286461288017,
    "itl": 95.86360881032877,
    "ttft": 2014903.784532848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.184688062057838,
    "arrivals": 461464,
    "finished_requests": 76951,
    "scheduler_time": 101.27902230078408
}
#Debug simulation 
Total elapsed time: 5.717812344897538. Arrivals time: 0.28997796727344394 Scheduler time: 5.273325839545578 Scheduler overhead time: 0.05430070590227842 Adapter cache time: 0.01953817717730999 Engine time: 0.055317637510597706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 8640, 8640, 33, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 34560, 33, 33, 8640, 33, 34560, 33, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 33, 33, 8640]
Prompts retrieved: 1383456 . Total input tokens: 308361283 . Total output tokens: 271776137
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.882129425182939,
    "estimated_duration": 3600.0253095494854,
    "input_throughput": 5557.182319505301,
    "output_throughput": 4834.684065645665,
    "total_throughput": 10391.866385150966,
    "itl": 108.59117842259596,
    "ttft": 1982077.6207979145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.897450651056131,
    "arrivals": 461464,
    "finished_requests": 80889,
    "scheduler_time": 100.75401994348985
}
#Debug simulation 
Total elapsed time: 5.8823279067873955. Arrivals time: 0.3601283482275903 Scheduler time: 5.3866718844510615 Scheduler overhead time: 0.048367887269705534 Adapter cache time: 0.015134910587221384 Engine time: 0.04927315888926387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 8640, 8640, 33, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 34560, 33, 33, 8640, 33, 34560, 33, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 33, 33, 8640]
Prompts retrieved: 1383456 . Total input tokens: 308361283 . Total output tokens: 271776137
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.93066704692319,
    "estimated_duration": 3600.0363997787995,
    "input_throughput": 5282.898806570006,
    "output_throughput": 4600.403485091876,
    "total_throughput": 9883.302291661881,
    "itl": 95.86636849473614,
    "ttft": 2014980.2913978123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.153934333734222,
    "arrivals": 461464,
    "finished_requests": 76951,
    "scheduler_time": 101.2766813105154
}
#Debug simulation 
Total elapsed time: 5.930803935974836. Arrivals time: 0.31263664178550243 Scheduler time: 5.458541976753622 Scheduler overhead time: 0.05580489756539464 Adapter cache time: 0.020327369682490826 Engine time: 0.057284584268927574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 8640, 8640, 33, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 34560, 33, 33, 8640, 33, 34560, 33, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 33, 33, 8640]
Prompts retrieved: 1383456 . Total input tokens: 308361283 . Total output tokens: 271776137
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.232141243293881,
    "estimated_duration": 3600.0495328010647,
    "input_throughput": 5556.967707728892,
    "output_throughput": 4834.665145970308,
    "total_throughput": 10391.6328536992,
    "itl": 108.59038195334041,
    "ttft": 1982129.499789478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7428104167757494,
    "arrivals": 461464,
    "finished_requests": 80887,
    "scheduler_time": 100.75571187331953
}
#Debug simulation 
Total elapsed time: 6.2322711972519755. Arrivals time: 0.39509507920593023 Scheduler time: 5.692840141244233 Scheduler overhead time: 0.05153231415897608 Adapter cache time: 0.016434321179986 Engine time: 0.05242635076865554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_96_slots_64_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 8640, 8640, 33, 34560, 34560, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 34560, 33, 33, 8640, 33, 34560, 33, 33, 34560, 33, 8640, 8640, 33, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 33, 33, 8640]
Prompts retrieved: 1383456 . Total input tokens: 308361283 . Total output tokens: 271776137
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.0732761337421834,
    "estimated_duration": 3600.0828919276487,
    "input_throughput": 5282.830582219333,
    "output_throughput": 4600.3440746143915,
    "total_throughput": 9883.174656833724,
    "itl": 95.86406728312367,
    "ttft": 2014894.8368095737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1198148616030887,
    "arrivals": 461464,
    "finished_requests": 76951,
    "scheduler_time": 101.28012393194636
}
#Debug simulation 
Total elapsed time: 6.073466786649078. Arrivals time: 0.30313626723363996 Scheduler time: 5.605559422168881 Scheduler overhead time: 0.05785011034458876 Adapter cache time: 0.020842707250267267 Engine time: 0.05905536375939846 
