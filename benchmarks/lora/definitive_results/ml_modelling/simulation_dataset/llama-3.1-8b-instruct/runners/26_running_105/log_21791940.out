INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 108.87672794563696,
    "estimated_duration": 3600.0047573579322,
    "input_throughput": 7802.809688678176,
    "output_throughput": 6778.377709119733,
    "total_throughput": 14581.187397797908,
    "itl": 90.03722246244743,
    "ttft": 1470931.589148402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5473037134762835,
    "arrivals": 256011,
    "finished_requests": 113041,
    "scheduler_time": 226.1512171219013
}
#Debug simulation 
Total elapsed time: 108.8769375118427. Arrivals time: 0.6721450365148485 Scheduler time: 107.92850051028654 Scheduler overhead time: 0.1092203427106142 Adapter cache time: 0.02090441109612584 Engine time: 0.10965070268139243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.18116684397683,
    "estimated_duration": 3600.0448411958364,
    "input_throughput": 7774.352052432532,
    "output_throughput": 6750.321752083432,
    "total_throughput": 14524.673804515964,
    "itl": 88.98105658252156,
    "ttft": 1478857.8956360095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.786733170095833,
    "arrivals": 256011,
    "finished_requests": 112628,
    "scheduler_time": 227.21417434114144
}
#Debug simulation 
Total elapsed time: 106.18138841493055. Arrivals time: 0.6401148131117225 Scheduler time: 105.26721879048273 Scheduler overhead time: 0.108948755543679 Adapter cache time: 0.02204481652006507 Engine time: 0.10675028525292873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 103.40400210488588,
    "estimated_duration": 3600.0806401487866,
    "input_throughput": 7695.568730053503,
    "output_throughput": 6678.738729309767,
    "total_throughput": 14374.30745936327,
    "itl": 86.62452224611084,
    "ttft": 1490390.7594667876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8823138318490287,
    "arrivals": 256011,
    "finished_requests": 111460,
    "scheduler_time": 229.74274942399822
}
#Debug simulation 
Total elapsed time: 103.40420561283827. Arrivals time: 0.645250319968909 Scheduler time: 102.48532695462927 Scheduler overhead time: 0.10893850168213248 Adapter cache time: 0.02102194679901004 Engine time: 0.10688748210668564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 107.23808526434004,
    "estimated_duration": 3600.0093314122687,
    "input_throughput": 7768.090142430095,
    "output_throughput": 6748.714729155717,
    "total_throughput": 14516.804871585811,
    "itl": 88.94706021709563,
    "ttft": 1476579.3822354677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5810215682908875,
    "arrivals": 256011,
    "finished_requests": 112566,
    "scheduler_time": 227.27227736481188
}
#Debug simulation 
Total elapsed time: 107.23826611135155. Arrivals time: 0.6499517671763897 Scheduler time: 106.30954391648993 Scheduler overhead time: 0.11026288149878383 Adapter cache time: 0.021721784956753254 Engine time: 0.10987019259482622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 104.02150555001572,
    "estimated_duration": 3600.060757602308,
    "input_throughput": 7689.431057946057,
    "output_throughput": 6678.304511730662,
    "total_throughput": 14367.735569676719,
    "itl": 86.6456962537868,
    "ttft": 1489376.4539001163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9280747557664346,
    "arrivals": 256011,
    "finished_requests": 111414,
    "scheduler_time": 229.73158891260493
}
#Debug simulation 
Total elapsed time: 104.02169974334538. Arrivals time: 0.6510488712228835 Scheduler time: 103.09437391813844 Scheduler overhead time: 0.10838088812306523 Adapter cache time: 0.02169259963557124 Engine time: 0.10915976529940963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 106.03254731418565,
    "estimated_duration": 3600.0778455741,
    "input_throughput": 7766.908439042661,
    "output_throughput": 6748.1410241910735,
    "total_throughput": 14515.049463233734,
    "itl": 88.95583454313169,
    "ttft": 1477258.2587546222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.570444551380346,
    "arrivals": 256011,
    "finished_requests": 112537,
    "scheduler_time": 227.26224094879737
}
#Debug simulation 
Total elapsed time: 106.03273839410394. Arrivals time: 0.6537076001986861 Scheduler time: 105.10469809453934 Scheduler overhead time: 0.10892361495643854 Adapter cache time: 0.02201719954609871 Engine time: 0.10709855705499649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171673222 . Total output tokens: 150877213
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 103.9785425439477,
    "estimated_duration": 3600.075628217435,
    "input_throughput": 7695.688607996844,
    "output_throughput": 6679.221906209272,
    "total_throughput": 14374.910514206116,
    "itl": 86.62277608019389,
    "ttft": 1490133.126606682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7777849952690343,
    "arrivals": 256011,
    "finished_requests": 111458,
    "scheduler_time": 229.74960006935677
}
#Debug simulation 
Total elapsed time: 103.97873194888234. Arrivals time: 0.6470223520882428 Scheduler time: 103.05522911297157 Scheduler overhead time: 0.1090990724042058 Adapter cache time: 0.02161699067801237 Engine time: 0.1086742328479886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 112.44668958103284,
    "estimated_duration": 3600.052691967219,
    "input_throughput": 7786.307145599758,
    "output_throughput": 6782.230453037583,
    "total_throughput": 14568.53759863734,
    "itl": 89.92957726336469,
    "ttft": 1464592.9652960608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.600202985731883,
    "arrivals": 255515,
    "finished_requests": 112871,
    "scheduler_time": 226.04228475350303
}
#Debug simulation 
Total elapsed time: 112.44687889609486. Arrivals time: 0.6789769912138581 Scheduler time: 111.48592177312821 Scheduler overhead time: 0.11121747642755508 Adapter cache time: 0.02252073446288705 Engine time: 0.11181952571496367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 109.49847920518368,
    "estimated_duration": 3600.0352189877876,
    "input_throughput": 7740.3214982532445,
    "output_throughput": 6751.9256122262,
    "total_throughput": 14492.247110479444,
    "itl": 88.92386207745012,
    "ttft": 1472856.0932183564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8453028264129556,
    "arrivals": 255515,
    "finished_requests": 112274,
    "scheduler_time": 227.0841534181255
}
#Debug simulation 
Total elapsed time: 109.49867983302101. Arrivals time: 0.6911629298701882 Scheduler time: 108.52706759469584 Scheduler overhead time: 0.11105561349540949 Adapter cache time: 0.022576724644750357 Engine time: 0.11022455990314484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 109.58595092408359,
    "estimated_duration": 3600.0132066638575,
    "input_throughput": 7654.43580845535,
    "output_throughput": 6675.393566755846,
    "total_throughput": 14329.829375211197,
    "itl": 86.32439853698683,
    "ttft": 1484053.8657488523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6495115798106457,
    "arrivals": 255515,
    "finished_requests": 111065,
    "scheduler_time": 229.91306391343483
}
#Debug simulation 
Total elapsed time: 109.58629031386226. Arrivals time: 0.6608432689681649 Scheduler time: 108.63891414972022 Scheduler overhead time: 0.11408523237332702 Adapter cache time: 0.022477682679891586 Engine time: 0.11234382446855307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 110.7730890228413,
    "estimated_duration": 3600.024273061011,
    "input_throughput": 7735.742008295074,
    "output_throughput": 6750.055876522744,
    "total_throughput": 14485.797884817819,
    "itl": 88.90120578037956,
    "ttft": 1472851.5748678911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7686578643927329,
    "arrivals": 255515,
    "finished_requests": 112219,
    "scheduler_time": 227.12615621037793
}
#Debug simulation 
Total elapsed time: 110.77329737599939. Arrivals time: 0.6532128392718732 Scheduler time: 109.83648903807625 Scheduler overhead time: 0.11299688601866364 Adapter cache time: 0.021655444521456957 Engine time: 0.11072484450414777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 107.20348099479452,
    "estimated_duration": 3600.0751009348905,
    "input_throughput": 7663.37235376995,
    "output_throughput": 6680.843406226204,
    "total_throughput": 14344.215759996154,
    "itl": 86.52872038984239,
    "ttft": 1484181.9184042832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0509746764833174,
    "arrivals": 255515,
    "finished_requests": 111137,
    "scheduler_time": 229.66175203450092
}
#Debug simulation 
Total elapsed time: 107.20379598997533. Arrivals time: 0.7452547601424158 Scheduler time: 106.1727600498125 Scheduler overhead time: 0.11365230055525899 Adapter cache time: 0.022419016808271408 Engine time: 0.1116109318099916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 109.87848938582465,
    "estimated_duration": 3600.0151656811554,
    "input_throughput": 7746.286533968969,
    "output_throughput": 6751.069059844217,
    "total_throughput": 14497.355593813185,
    "itl": 88.8456413367766,
    "ttft": 1473019.8315784184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5512927885586345,
    "arrivals": 255515,
    "finished_requests": 112294,
    "scheduler_time": 227.16996600943142
}
#Debug simulation 
Total elapsed time: 109.87868384923786. Arrivals time: 0.6807904005981982 Scheduler time: 108.91348846210167 Scheduler overhead time: 0.11293133487924933 Adapter cache time: 0.022219352424144745 Engine time: 0.1123754121363163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171342922 . Total output tokens: 150598976
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 107.99594473419711,
    "estimated_duration": 3600.071743836429,
    "input_throughput": 7660.4626136175775,
    "output_throughput": 6679.647437907448,
    "total_throughput": 14340.110051525025,
    "itl": 86.43624334571142,
    "ttft": 1484900.3210274742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.845241712499415,
    "arrivals": 255515,
    "finished_requests": 111078,
    "scheduler_time": 229.78295180737956
}
#Debug simulation 
Total elapsed time: 107.99613363388926. Arrivals time: 0.6590715264901519 Scheduler time: 107.05365238757804 Scheduler overhead time: 0.11214331490918994 Adapter cache time: 0.022821866907179356 Engine time: 0.11079815030097961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 113.16317381802946,
    "estimated_duration": 3600.0190643310575,
    "input_throughput": 7765.3374886209285,
    "output_throughput": 6786.725726559431,
    "total_throughput": 14552.06321518036,
    "itl": 90.15218290044596,
    "ttft": 1466976.0806893846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6464898489555324,
    "arrivals": 255262,
    "finished_requests": 112943,
    "scheduler_time": 225.83549491165573
}
#Debug simulation 
Total elapsed time: 113.16333844233304. Arrivals time: 0.6855770191177726 Scheduler time: 112.1929510096088 Scheduler overhead time: 0.11303519969806075 Adapter cache time: 0.022138309199362993 Engine time: 0.11286431457847357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 84.89271221216768,
    "estimated_duration": 3600.00259122583,
    "input_throughput": 7736.8413755824395,
    "output_throughput": 6751.2284739006645,
    "total_throughput": 14488.069849483105,
    "itl": 88.9286537407662,
    "ttft": 1472469.6021510111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8761080632638203,
    "arrivals": 255262,
    "finished_requests": 112345,
    "scheduler_time": 227.09880433576654
}
#Debug simulation 
Total elapsed time: 84.89290158031508. Arrivals time: 0.4861512486822903 Scheduler time: 84.19059246778488 Scheduler overhead time: 0.08443471090868115 Adapter cache time: 0.016476719174534082 Engine time: 0.08310090843588114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 111.43222283106297,
    "estimated_duration": 3600.0445785143556,
    "input_throughput": 7645.76421199731,
    "output_throughput": 6680.895604333165,
    "total_throughput": 14326.659816330475,
    "itl": 86.64315757129556,
    "ttft": 1483395.171017963,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.968037956766788,
    "arrivals": 255262,
    "finished_requests": 111149,
    "scheduler_time": 229.57551827897362
}
#Debug simulation 
Total elapsed time: 111.43240999709815. Arrivals time: 0.6932476838119328 Scheduler time: 110.44666833430529 Scheduler overhead time: 0.1159446737729013 Adapter cache time: 0.023423756007105112 Engine time: 0.11512271827086806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 113.52076235227287,
    "estimated_duration": 3600.092671749204,
    "input_throughput": 7730.970710394784,
    "output_throughput": 6750.804275322021,
    "total_throughput": 14481.774985716806,
    "itl": 88.90872358331302,
    "ttft": 1472131.2362523337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.679830677467396,
    "arrivals": 255262,
    "finished_requests": 112359,
    "scheduler_time": 227.12492657492368
}
#Debug simulation 
Total elapsed time: 113.52094389218837. Arrivals time: 0.6888860226608813 Scheduler time: 112.5410815118812 Scheduler overhead time: 0.11524204211309552 Adapter cache time: 0.022897296585142612 Engine time: 0.11505160899832845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 108.89868488907814,
    "estimated_duration": 3600.013953557765,
    "input_throughput": 7648.13591147049,
    "output_throughput": 6680.176329937139,
    "total_throughput": 14328.31224140763,
    "itl": 86.64437625117775,
    "ttft": 1484774.1853561832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.029172572442335,
    "arrivals": 255262,
    "finished_requests": 111107,
    "scheduler_time": 229.56082988154267
}
#Debug simulation 
Total elapsed time: 108.89887895109132. Arrivals time: 0.6773326811380684 Scheduler time: 107.93163440516219 Scheduler overhead time: 0.11521812016144395 Adapter cache time: 0.02173662232235074 Engine time: 0.11554535292088985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 114.3979283128865,
    "estimated_duration": 3600.0871116132384,
    "input_throughput": 7728.194662359872,
    "output_throughput": 6750.75609187396,
    "total_throughput": 14478.950754233832,
    "itl": 88.90652842966209,
    "ttft": 1471923.0489516377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5895963142020575,
    "arrivals": 255262,
    "finished_requests": 112350,
    "scheduler_time": 227.12893395573334
}
#Debug simulation 
Total elapsed time: 114.39810590166599. Arrivals time: 0.6696577551774681 Scheduler time: 113.43746361648664 Scheduler overhead time: 0.11544042825698853 Adapter cache time: 0.022181398700922728 Engine time: 0.11521972948685288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 171186662 . Total output tokens: 150463709
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 109.77310571307316,
    "estimated_duration": 3600.085088198839,
    "input_throughput": 7648.294505665645,
    "output_throughput": 6680.400993531094,
    "total_throughput": 14328.69549919674,
    "itl": 86.64190692353425,
    "ttft": 1484676.6518100295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.001059498433028,
    "arrivals": 255262,
    "finished_requests": 111111,
    "scheduler_time": 229.5689685967507
}
#Debug simulation 
Total elapsed time: 109.77330682100728. Arrivals time: 0.6709752506576478 Scheduler time: 108.81468571163714 Scheduler overhead time: 0.11355632869526744 Adapter cache time: 0.02218644041568041 Engine time: 0.11388755636289716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 119.5907147959806,
    "estimated_duration": 3600.0243209484825,
    "input_throughput": 7788.763769410451,
    "output_throughput": 6793.195495289536,
    "total_throughput": 14581.959264699986,
    "itl": 90.21936467125504,
    "ttft": 1458345.7555036398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8316373018501304,
    "arrivals": 254534,
    "finished_requests": 113364,
    "scheduler_time": 225.74201203346564
}
#Debug simulation 
Total elapsed time: 119.59088948229328. Arrivals time: 0.6977930860593915 Scheduler time: 118.59853624319658 Scheduler overhead time: 0.11717082327231765 Adapter cache time: 0.0241018058732152 Engine time: 0.11556559288874269 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 120.16481456300244,
    "estimated_duration": 3600.04476598405,
    "input_throughput": 7734.619931146535,
    "output_throughput": 6747.811646547627,
    "total_throughput": 14482.431577694162,
    "itl": 88.81005409825411,
    "ttft": 1463313.463279246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1015001787664396,
    "arrivals": 254534,
    "finished_requests": 112579,
    "scheduler_time": 227.23743333435334
}
#Debug simulation 
Total elapsed time: 120.16513158008456. Arrivals time: 0.7049597604200244 Scheduler time: 119.16179170040414 Scheduler overhead time: 0.11725606070831418 Adapter cache time: 0.0241294139996171 Engine time: 0.11825526598840952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 116.36672400636598,
    "estimated_duration": 3600.0840620380595,
    "input_throughput": 7661.904145756136,
    "output_throughput": 6683.452270939119,
    "total_throughput": 14345.356416695256,
    "itl": 86.67201990902628,
    "ttft": 1474905.2825775403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.206902545187631,
    "arrivals": 254534,
    "finished_requests": 111515,
    "scheduler_time": 229.52474670520994
}
#Debug simulation 
Total elapsed time: 116.36689581209794. Arrivals time: 0.6863061636686325 Scheduler time: 115.38469086959958 Scheduler overhead time: 0.11594608332961798 Adapter cache time: 0.02330533927306533 Engine time: 0.11752365669235587 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 120.98539097467437,
    "estimated_duration": 3600.0393505908005,
    "input_throughput": 7741.280382234279,
    "output_throughput": 6754.82172049293,
    "total_throughput": 14496.102102727209,
    "itl": 88.97600098073545,
    "ttft": 1463283.0840162213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9232601879397377,
    "arrivals": 254534,
    "finished_requests": 112699,
    "scheduler_time": 227.05274280935092
}
#Debug simulation 
Total elapsed time: 120.9855746650137. Arrivals time: 0.6931290817447007 Scheduler time: 119.99237884767354 Scheduler overhead time: 0.11943257553502917 Adapter cache time: 0.023680135142058134 Engine time: 0.11846184777095914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 116.094129499048,
    "estimated_duration": 3600.0765830569158,
    "input_throughput": 7658.938181972582,
    "output_throughput": 6681.2256475877675,
    "total_throughput": 14340.16382956035,
    "itl": 86.59571917220386,
    "ttft": 1474716.675790134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 305,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2682961470214744,
    "arrivals": 254534,
    "finished_requests": 111472,
    "scheduler_time": 229.60994729094614
}
#Debug simulation 
Total elapsed time: 116.09431490069255. Arrivals time: 0.7014938923530281 Scheduler time: 115.09406793070957 Scheduler overhead time: 0.1187271811068058 Adapter cache time: 0.023988523054867983 Engine time: 0.1170878685079515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 121.14853724977002,
    "estimated_duration": 3600.016712096919,
    "input_throughput": 7744.723213731956,
    "output_throughput": 6756.932521524674,
    "total_throughput": 14501.655735256629,
    "itl": 89.04577115707986,
    "ttft": 1463341.0901762047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8066496261814544,
    "arrivals": 254534,
    "finished_requests": 112744,
    "scheduler_time": 226.97164902640745
}
#Debug simulation 
Total elapsed time: 121.14871680503711. Arrivals time: 0.7082563773728907 Scheduler time: 120.13771880045533 Scheduler overhead time: 0.12006263714283705 Adapter cache time: 0.02402662904933095 Engine time: 0.11941247759386897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170709327 . Total output tokens: 150058867
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 116.39606433035806,
    "estimated_duration": 3600.027030663532,
    "input_throughput": 7658.908881836387,
    "output_throughput": 6681.198722990369,
    "total_throughput": 14340.107604826755,
    "itl": 86.59790162597515,
    "ttft": 1474770.7059785433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 305,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.249862718302766,
    "arrivals": 254534,
    "finished_requests": 111470,
    "scheduler_time": 229.6056448043176
}
#Debug simulation 
Total elapsed time: 116.39623103896156. Arrivals time: 0.6863081781193614 Scheduler time: 115.40986346779391 Scheduler overhead time: 0.11891059391200542 Adapter cache time: 0.022787689231336117 Engine time: 0.11745197186246514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 115.5191419380717,
    "estimated_duration": 3600.066345912248,
    "input_throughput": 7822.761664372522,
    "output_throughput": 6803.710167122858,
    "total_throughput": 14626.47183149538,
    "itl": 90.13749872574039,
    "ttft": 1455399.9771193748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.86469934700988,
    "arrivals": 254308,
    "finished_requests": 113781,
    "scheduler_time": 226.24858576969615
}
#Debug simulation 
Total elapsed time: 115.51932959398255. Arrivals time: 0.7065486167557538 Scheduler time: 114.5272049116902 Scheduler overhead time: 0.11370945861563087 Adapter cache time: 0.02118071774020791 Engine time: 0.11235977848991752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 113.784694456961,
    "estimated_duration": 3600.019610963781,
    "input_throughput": 7784.609537862307,
    "output_throughput": 6770.337007546155,
    "total_throughput": 14554.946545408462,
    "itl": 88.94198037189173,
    "ttft": 1461766.3254109405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0845676739653594,
    "arrivals": 254308,
    "finished_requests": 113215,
    "scheduler_time": 227.49515366510286
}
#Debug simulation 
Total elapsed time: 113.78488057618961. Arrivals time: 0.6866204142570496 Scheduler time: 112.81127637717873 Scheduler overhead time: 0.11393195623531938 Adapter cache time: 0.021706902887672186 Engine time: 0.11431921506300569 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 111.95050762919709,
    "estimated_duration": 3600.0451929154333,
    "input_throughput": 7698.098361247712,
    "output_throughput": 6701.531982842173,
    "total_throughput": 14399.630344089886,
    "itl": 86.56494839421218,
    "ttft": 1472973.3135216422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.155496450755754,
    "arrivals": 254308,
    "finished_requests": 111990,
    "scheduler_time": 230.13915788015845
}
#Debug simulation 
Total elapsed time: 111.9506866671145. Arrivals time: 0.662198725156486 Scheduler time: 110.9991574883461 Scheduler overhead time: 0.11634113313630223 Adapter cache time: 0.020767721347510815 Engine time: 0.11501008830964565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 113.61180691421032,
    "estimated_duration": 3600.07593586681,
    "input_throughput": 7784.2941369103355,
    "output_throughput": 6771.597720238174,
    "total_throughput": 14555.891857148508,
    "itl": 88.94451337884873,
    "ttft": 1461559.9882670944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9157618991471792,
    "arrivals": 254308,
    "finished_requests": 113216,
    "scheduler_time": 227.50523470922366
}
#Debug simulation 
Total elapsed time: 113.6119878361933. Arrivals time: 0.5869386419653893 Scheduler time: 112.7560849795118 Scheduler overhead time: 0.10800720052793622 Adapter cache time: 0.019957532174885273 Engine time: 0.10456055169925094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 107.67067816201597,
    "estimated_duration": 3600.023531843422,
    "input_throughput": 7698.144402353134,
    "output_throughput": 6701.46508393693,
    "total_throughput": 14399.609486290065,
    "itl": 86.56299211884736,
    "ttft": 1472974.754942472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1312637410918334,
    "arrivals": 254308,
    "finished_requests": 111989,
    "scheduler_time": 230.1390169196805
}
#Debug simulation 
Total elapsed time: 107.67082893988118. Arrivals time: 0.5602692761458457 Scheduler time: 106.84664091654122 Scheduler overhead time: 0.1049295705743134 Adapter cache time: 0.019142945762723684 Engine time: 0.10362634295597672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 127.48707983689383,
    "estimated_duration": 3600.0858388228207,
    "input_throughput": 7784.541051155547,
    "output_throughput": 6771.849642332884,
    "total_throughput": 14556.39069348843,
    "itl": 88.9393292508136,
    "ttft": 1461492.0012173816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8194174680625954,
    "arrivals": 254308,
    "finished_requests": 113222,
    "scheduler_time": 227.5158621754346
}
#Debug simulation 
Total elapsed time: 127.48725373484194. Arrivals time: 0.6845186231657863 Scheduler time: 126.48997094063088 Scheduler overhead time: 0.12586946273222566 Adapter cache time: 0.022896329406648874 Engine time: 0.12425477989017963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170548709 . Total output tokens: 149920004
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 111.16943375021219,
    "estimated_duration": 3600.074022323795,
    "input_throughput": 7698.195878236685,
    "output_throughput": 6701.580259293363,
    "total_throughput": 14399.776137530049,
    "itl": 86.56347342862615,
    "ttft": 1472958.7943593436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1157299528457303,
    "arrivals": 254308,
    "finished_requests": 111992,
    "scheduler_time": 230.1406613721781
}
#Debug simulation 
Total elapsed time: 111.16969563625753. Arrivals time: 0.616400558501482 Scheduler time: 110.276461513713 Scheduler overhead time: 0.11079948209226131 Adapter cache time: 0.02090047812089324 Engine time: 0.10775983752682805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 117.53880610177293,
    "estimated_duration": 3600.0326852938633,
    "input_throughput": 7855.440622948559,
    "output_throughput": 6826.388021528192,
    "total_throughput": 14681.828644476751,
    "itl": 89.52388417948397,
    "ttft": 1454255.2298134994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5208540773484838,
    "arrivals": 253785,
    "finished_requests": 113811,
    "scheduler_time": 227.4221644214391
}
#Debug simulation 
Total elapsed time: 117.53904467495158. Arrivals time: 0.6288621788844466 Scheduler time: 116.63900180580094 Scheduler overhead time: 0.10984640382230282 Adapter cache time: 0.019410934299230576 Engine time: 0.10493176802992821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 114.99906461685896,
    "estimated_duration": 3600.045984655712,
    "input_throughput": 7816.87376215314,
    "output_throughput": 6794.322934833069,
    "total_throughput": 14611.196696986208,
    "itl": 88.36711828853113,
    "ttft": 1459605.9706685909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6687628204142693,
    "arrivals": 253785,
    "finished_requests": 113250,
    "scheduler_time": 228.66740699498678
}
#Debug simulation 
Total elapsed time: 114.99923522491008. Arrivals time: 0.6274800216779113 Scheduler time: 114.09861181396991 Scheduler overhead time: 0.10992584750056267 Adapter cache time: 0.019769547507166862 Engine time: 0.10695886192843318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 111.26021269662306,
    "estimated_duration": 3600.0919756711946,
    "input_throughput": 7735.3893145488455,
    "output_throughput": 6723.28211711518,
    "total_throughput": 14458.671431664025,
    "itl": 86.02204466051934,
    "ttft": 1470881.0040256623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7190835579810697,
    "arrivals": 253785,
    "finished_requests": 112080,
    "scheduler_time": 231.29265831724445
}
#Debug simulation 
Total elapsed time: 111.26044169487432. Arrivals time: 0.6191377574577928 Scheduler time: 110.36284399731085 Scheduler overhead time: 0.11356377834454179 Adapter cache time: 0.020521339029073715 Engine time: 0.10728668002411723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 110.67312511289492,
    "estimated_duration": 3600.0581095710277,
    "input_throughput": 7813.875816396735,
    "output_throughput": 6791.554540466401,
    "total_throughput": 14605.430356863135,
    "itl": 88.25011856128381,
    "ttft": 1459963.0503542707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5632580264424887,
    "arrivals": 253785,
    "finished_requests": 113205,
    "scheduler_time": 228.81800992536733
}
#Debug simulation 
Total elapsed time: 110.67330488469452. Arrivals time: 0.6372470436617732 Scheduler time: 109.76685510203242 Scheduler overhead time: 0.10885708360001445 Adapter cache time: 0.020539688877761364 Engine time: 0.10345815168693662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 111.30819250969216,
    "estimated_duration": 3600.06024491404,
    "input_throughput": 7732.034773397145,
    "output_throughput": 6723.167767593268,
    "total_throughput": 14455.202540990413,
    "itl": 86.0089494059173,
    "ttft": 1470986.3016474433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7114187259320204,
    "arrivals": 253785,
    "finished_requests": 112018,
    "scheduler_time": 231.30815780550822
}
#Debug simulation 
Total elapsed time: 111.30840947199613. Arrivals time: 0.6276639783754945 Scheduler time: 110.40015649190173 Scheduler overhead time: 0.11377970315515995 Adapter cache time: 0.02076605334877968 Engine time: 0.10770267667248845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 116.6871003229171,
    "estimated_duration": 3600.046237074446,
    "input_throughput": 7814.5143554772185,
    "output_throughput": 6792.369983523169,
    "total_throughput": 14606.884339000388,
    "itl": 88.29292730542112,
    "ttft": 1459799.1038897596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4619178953906475,
    "arrivals": 253785,
    "finished_requests": 113216,
    "scheduler_time": 228.76674055105096
}
#Debug simulation 
Total elapsed time: 116.68722904799506. Arrivals time: 0.6252354332245886 Scheduler time: 115.78580480255187 Scheduler overhead time: 0.11195079423487186 Adapter cache time: 0.020208251662552357 Engine time: 0.10698124207556248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 170237020 . Total output tokens: 149630276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 110.56775364559144,
    "estimated_duration": 3600.0092007197018,
    "input_throughput": 7735.210786248256,
    "output_throughput": 6723.270316965093,
    "total_throughput": 14458.48110321335,
    "itl": 86.01913697944666,
    "ttft": 1470910.9404685479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6882230986654791,
    "arrivals": 253785,
    "finished_requests": 112076,
    "scheduler_time": 231.29339015650643
}
#Debug simulation 
Total elapsed time: 110.56790904095396. Arrivals time: 0.6137037728913128 Scheduler time: 109.68166732694954 Scheduler overhead time: 0.10949876392260194 Adapter cache time: 0.02020105952396989 Engine time: 0.10577241145074368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 147.14307543076575,
    "estimated_duration": 3600.0820130627276,
    "input_throughput": 7212.286805074958,
    "output_throughput": 6303.045018881526,
    "total_throughput": 13515.331823956483,
    "itl": 83.50319903973065,
    "ttft": 1498328.15594792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8199387199617914,
    "arrivals": 216813,
    "finished_requests": 104938,
    "scheduler_time": 234.5244832948909
}
#Debug simulation 
Total elapsed time: 147.1432133181952. Arrivals time: 0.7343779010698199 Scheduler time: 146.07654717983678 Scheduler overhead time: 0.1372273825109005 Adapter cache time: 0.023186521604657173 Engine time: 0.12957628862932324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 146.76307161571458,
    "estimated_duration": 3600.03917626724,
    "input_throughput": 7177.991609189576,
    "output_throughput": 6259.0977199764,
    "total_throughput": 13437.089329165976,
    "itl": 82.58184089153472,
    "ttft": 1502000.780691637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9118242373829709,
    "arrivals": 216813,
    "finished_requests": 104370,
    "scheduler_time": 235.3066790516055
}
#Debug simulation 
Total elapsed time: 146.76321143703535. Arrivals time: 0.7184753557667136 Scheduler time: 145.7198496311903 Scheduler overhead time: 0.1332384953275323 Adapter cache time: 0.023248013108968735 Engine time: 0.12676800787448883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 143.21725095715374,
    "estimated_duration": 3600.0657042701923,
    "input_throughput": 7112.882125908598,
    "output_throughput": 6209.677777125979,
    "total_throughput": 13322.559903034577,
    "itl": 80.57733776996577,
    "ttft": 1508070.575297002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8234334281086924,
    "arrivals": 216813,
    "finished_requests": 103455,
    "scheduler_time": 237.39441944461717
}
#Debug simulation 
Total elapsed time: 143.21747256815434. Arrivals time: 0.7428272725082934 Scheduler time: 142.1443678853102 Scheduler overhead time: 0.13555703591555357 Adapter cache time: 0.023118298035115004 Engine time: 0.12917392421513796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 148.70516136381775,
    "estimated_duration": 3600.0903609338434,
    "input_throughput": 7178.54676105862,
    "output_throughput": 6259.724823727591,
    "total_throughput": 13438.271584786211,
    "itl": 82.5853634127802,
    "ttft": 1501987.5756981124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.856295398450456,
    "arrivals": 216813,
    "finished_requests": 104375,
    "scheduler_time": 235.3093571886436
}
#Debug simulation 
Total elapsed time: 148.70534428022802. Arrivals time: 0.7468059794045985 Scheduler time: 147.622416973114 Scheduler overhead time: 0.13686529221013188 Adapter cache time: 0.023689329624176025 Engine time: 0.1323498347774148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 144.4629814173095,
    "estimated_duration": 3600.058057761178,
    "input_throughput": 7112.897233641979,
    "output_throughput": 6209.690966456911,
    "total_throughput": 13322.58820009889,
    "itl": 80.57717836134495,
    "ttft": 1508066.9641705863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8161843269271778,
    "arrivals": 216813,
    "finished_requests": 103455,
    "scheduler_time": 237.39422429922777
}
#Debug simulation 
Total elapsed time: 144.46321370312944. Arrivals time: 0.7199300434440374 Scheduler time: 143.4135959725827 Scheduler overhead time: 0.13566458271816373 Adapter cache time: 0.022758321836590767 Engine time: 0.1285225716419518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 147.46752434130758,
    "estimated_duration": 3600.0348556342387,
    "input_throughput": 7203.746919120066,
    "output_throughput": 6273.986476728374,
    "total_throughput": 13477.733395848441,
    "itl": 82.36061742580169,
    "ttft": 1504384.879671899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7533026709873222,
    "arrivals": 216813,
    "finished_requests": 104592,
    "scheduler_time": 235.5448214270893
}
#Debug simulation 
Total elapsed time: 147.46767881931737. Arrivals time: 0.7396085159853101 Scheduler time: 146.39275233680382 Scheduler overhead time: 0.137993183452636 Adapter cache time: 0.02347341924905777 Engine time: 0.13103507924824953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145588967 . Total output tokens: 128017914
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 145.957817456685,
    "estimated_duration": 3600.0553528768583,
    "input_throughput": 7116.523077770671,
    "output_throughput": 6211.608380446174,
    "total_throughput": 13328.131458216845,
    "itl": 80.57477319505844,
    "ttft": 1508844.5601369154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8148882583715024,
    "arrivals": 216813,
    "finished_requests": 103466,
    "scheduler_time": 237.4143990930363
}
#Debug simulation 
Total elapsed time: 145.9580336306244. Arrivals time: 0.7753172870725393 Scheduler time: 144.84227068489417 Scheduler overhead time: 0.13832306815311313 Adapter cache time: 0.02367431716993451 Engine time: 0.13411397207528353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 145.4793247533962,
    "estimated_duration": 3600.0622670313924,
    "input_throughput": 7126.3581285651935,
    "output_throughput": 6193.63426132806,
    "total_throughput": 13319.992389893254,
    "itl": 82.59358843538423,
    "ttft": 1458665.789670758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6612409031949931,
    "arrivals": 194341,
    "finished_requests": 103208,
    "scheduler_time": 233.2829809108887
}
#Debug simulation 
Total elapsed time: 145.47946945903823. Arrivals time: 0.7284603621810675 Scheduler time: 144.41773869795725 Scheduler overhead time: 0.13654586439952254 Adapter cache time: 0.02320419577881694 Engine time: 0.13059375621378422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.68286005174741,
    "estimated_duration": 3600.095221071106,
    "input_throughput": 7098.844733444685,
    "output_throughput": 6170.775670036429,
    "total_throughput": 13269.620403481113,
    "itl": 81.51548593491621,
    "ttft": 1467776.0723559253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7930228506540877,
    "arrivals": 194341,
    "finished_requests": 102827,
    "scheduler_time": 234.19588605318495
}
#Debug simulation 
Total elapsed time: 104.68303147656843. Arrivals time: 0.4246280468069017 Scheduler time: 104.02306606946513 Scheduler overhead time: 0.09539654292166233 Adapter cache time: 0.015599546488374472 Engine time: 0.0877758814021945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 139.48726002685726,
    "estimated_duration": 3600.040457948374,
    "input_throughput": 7038.778673737723,
    "output_throughput": 6116.126820571346,
    "total_throughput": 13154.905494309069,
    "itl": 79.54613619632508,
    "ttft": 1475166.9711745654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8072812813613566,
    "arrivals": 194341,
    "finished_requests": 101940,
    "scheduler_time": 236.24602728585003
}
#Debug simulation 
Total elapsed time: 139.48741600802168. Arrivals time: 0.6814837916754186 Scheduler time: 138.4728049263358 Scheduler overhead time: 0.1374614303931594 Adapter cache time: 0.024345184210687876 Engine time: 0.12836981005966663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 150.0875993547961,
    "estimated_duration": 3600.034496880685,
    "input_throughput": 7117.121244865999,
    "output_throughput": 6175.87054214744,
    "total_throughput": 13292.991787013438,
    "itl": 81.79868513913965,
    "ttft": 1458068.4285071362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.675043023289181,
    "arrivals": 194341,
    "finished_requests": 103019,
    "scheduler_time": 233.9565723283509
}
#Debug simulation 
Total elapsed time: 150.08774832775816. Arrivals time: 0.7178272427991033 Scheduler time: 149.019939978607 Scheduler overhead time: 0.14445095276460052 Adapter cache time: 0.02597441803663969 Engine time: 0.13450934831053019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 139.47291668504477,
    "estimated_duration": 3600.034964407622,
    "input_throughput": 7035.548612836238,
    "output_throughput": 6116.59726022226,
    "total_throughput": 13152.145873058498,
    "itl": 79.5729887909506,
    "ttft": 1475326.2396048424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8000321801798419,
    "arrivals": 194341,
    "finished_requests": 101954,
    "scheduler_time": 236.2425982684845
}
#Debug simulation 
Total elapsed time: 139.47304595820606. Arrivals time: 0.6954768747091293 Scheduler time: 138.4416109402664 Scheduler overhead time: 0.13934815116226673 Adapter cache time: 0.023753280751407146 Engine time: 0.12958086840808392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 145.22897646017373,
    "estimated_duration": 3600.0358733790267,
    "input_throughput": 7102.100895456303,
    "output_throughput": 6168.985749343335,
    "total_throughput": 13271.086644799638,
    "itl": 81.71581199479677,
    "ttft": 1461876.9533482317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6575438568787647,
    "arrivals": 194341,
    "finished_requests": 102843,
    "scheduler_time": 234.1273016074976
}
#Debug simulation 
Total elapsed time: 145.22924244729802. Arrivals time: 0.7137474180199206 Scheduler time: 144.1748680700548 Scheduler overhead time: 0.1406304594129324 Adapter cache time: 0.024207741022109985 Engine time: 0.1323566734790802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130293646 . Total output tokens: 114727610
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 138.17067080177367,
    "estimated_duration": 3600.049895613466,
    "input_throughput": 7035.65220883803,
    "output_throughput": 6116.7607779079335,
    "total_throughput": 13152.412986745963,
    "itl": 79.54663711248644,
    "ttft": 1474501.0221258535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7927830789983273,
    "arrivals": 194341,
    "finished_requests": 101916,
    "scheduler_time": 236.2520628683086
}
#Debug simulation 
Total elapsed time: 138.1708152438514. Arrivals time: 0.697608086746186 Scheduler time: 137.14868610724807 Scheduler overhead time: 0.13186769420281053 Adapter cache time: 0.02244998747482896 Engine time: 0.1261694161221385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 136.47113191289827,
    "estimated_duration": 3600.08509213419,
    "input_throughput": 7118.831456510675,
    "output_throughput": 6200.373165837374,
    "total_throughput": 13319.204622348048,
    "itl": 82.66668591905281,
    "ttft": 1450656.6973107345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7339774025464423,
    "arrivals": 190487,
    "finished_requests": 103242,
    "scheduler_time": 231.83927069170642
}
#Debug simulation 
Total elapsed time: 136.47127325600013. Arrivals time: 0.6970259449444711 Scheduler time: 135.44915966689587 Scheduler overhead time: 0.1335829682648182 Adapter cache time: 0.02310907607898116 Engine time: 0.12598134204745293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 132.8175447108224,
    "estimated_duration": 3600.0634194476147,
    "input_throughput": 7095.186674216778,
    "output_throughput": 6183.308016116996,
    "total_throughput": 13278.494690333773,
    "itl": 81.69291831475964,
    "ttft": 1451303.3340515343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0786466965684678,
    "arrivals": 190487,
    "finished_requests": 102848,
    "scheduler_time": 232.28190940160766
}
#Debug simulation 
Total elapsed time: 132.81772697577253. Arrivals time: 0.7073959871195257 Scheduler time: 131.78155232034624 Scheduler overhead time: 0.1344603463076055 Adapter cache time: 0.023415294010192156 Engine time: 0.12679120060056448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 135.31070925015956,
    "estimated_duration": 3600.0678562394755,
    "input_throughput": 7039.087320556969,
    "output_throughput": 6127.15173181244,
    "total_throughput": 13166.239052369408,
    "itl": 79.80115638514577,
    "ttft": 1467033.2375960404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9670939424401149,
    "arrivals": 190487,
    "finished_requests": 102055,
    "scheduler_time": 234.38059679524525
}
#Debug simulation 
Total elapsed time: 135.3108581271954. Arrivals time: 0.7075288235209882 Scheduler time: 134.2738036364317 Scheduler overhead time: 0.13570414390414953 Adapter cache time: 0.023422472178936005 Engine time: 0.1269559715874493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 142.06272643618286,
    "estimated_duration": 3600.092668365618,
    "input_throughput": 7102.645224854291,
    "output_throughput": 6184.020538034387,
    "total_throughput": 13286.665762888679,
    "itl": 81.73936326101527,
    "ttft": 1450155.4823490381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7810765681369227,
    "arrivals": 190487,
    "finished_requests": 103009,
    "scheduler_time": 232.223094159787
}
#Debug simulation 
Total elapsed time: 142.0628792862408. Arrivals time: 0.7165703331120312 Scheduler time: 141.00887636374682 Scheduler overhead time: 0.13951839786022902 Adapter cache time: 0.023708056658506393 Engine time: 0.13080386398360133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 132.954910825938,
    "estimated_duration": 3600.059424222493,
    "input_throughput": 7039.103807424777,
    "output_throughput": 6127.166082755402,
    "total_throughput": 13166.26989018018,
    "itl": 79.80093711258255,
    "ttft": 1467029.846451599,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.958809255375527,
    "arrivals": 190487,
    "finished_requests": 102055,
    "scheduler_time": 234.3804494653262
}
#Debug simulation 
Total elapsed time: 132.95506886392832. Arrivals time: 0.704026139806956 Scheduler time: 131.92276939889416 Scheduler overhead time: 0.13506967248395085 Adapter cache time: 0.023661434650421143 Engine time: 0.12729186890646815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 135.81430315272883,
    "estimated_duration": 3600.0009507987334,
    "input_throughput": 7090.944516094149,
    "output_throughput": 6177.871979468652,
    "total_throughput": 13268.8164955628,
    "itl": 81.68369851555471,
    "ttft": 1454235.8661879948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7213830662844697,
    "arrivals": 190487,
    "finished_requests": 102855,
    "scheduler_time": 232.78947186277784
}
#Debug simulation 
Total elapsed time: 135.81467920495197. Arrivals time: 0.7092718067578971 Scheduler time: 134.78118456201628 Scheduler overhead time: 0.13377447705715895 Adapter cache time: 0.022371469996869564 Engine time: 0.12520127091556787 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127777611 . Total output tokens: 112482248
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 137.45680969534442,
    "estimated_duration": 3600.005329735168,
    "input_throughput": 7038.449301924786,
    "output_throughput": 6130.293979765718,
    "total_throughput": 13168.743281690504,
    "itl": 79.76884962859046,
    "ttft": 1465095.9263664854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9895069839246571,
    "arrivals": 190487,
    "finished_requests": 102105,
    "scheduler_time": 234.30230505918814
}
#Debug simulation 
Total elapsed time: 137.45703773805872. Arrivals time: 0.7042466360144317 Scheduler time: 136.422987699043 Scheduler overhead time: 0.13650979660451412 Adapter cache time: 0.023631424643099308 Engine time: 0.12636859994381666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 121.82363532297313,
    "estimated_duration": 3600.003152670304,
    "input_throughput": 7182.918987395726,
    "output_throughput": 6229.607044473018,
    "total_throughput": 13412.526031868743,
    "itl": 82.63288907959775,
    "ttft": 1420876.9087865457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8331635380256912,
    "arrivals": 188561,
    "finished_requests": 104680,
    "scheduler_time": 229.93961095227465
}
#Debug simulation 
Total elapsed time: 121.82375384867191. Arrivals time: 0.5582946641370654 Scheduler time: 120.98217925382778 Scheduler overhead time: 0.11506275041028857 Adapter cache time: 0.019255324732512236 Engine time: 0.11000164784491062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 148.98779283324257,
    "estimated_duration": 3600.028424984188,
    "input_throughput": 7158.501533251974,
    "output_throughput": 6210.272909191876,
    "total_throughput": 13368.77444244385,
    "itl": 81.78965385093574,
    "ttft": 1430365.0777956073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9237610422167934,
    "arrivals": 188561,
    "finished_requests": 104284,
    "scheduler_time": 230.80190419294937
}
#Debug simulation 
Total elapsed time: 148.98795330896974. Arrivals time: 0.7190541415475309 Scheduler time: 147.92200583638623 Scheduler overhead time: 0.1433495795354247 Adapter cache time: 0.02498321747407317 Engine time: 0.13364201691001654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 129.5690521011129,
    "estimated_duration": 3600.0513873467494,
    "input_throughput": 7085.24136340146,
    "output_throughput": 6153.4485529459735,
    "total_throughput": 13238.689916347434,
    "itl": 79.83579133832296,
    "ttft": 1445602.3264068866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4747876197798202,
    "arrivals": 188561,
    "finished_requests": 103299,
    "scheduler_time": 232.8729131834304
}
#Debug simulation 
Total elapsed time: 129.56920074811205. Arrivals time: 0.6794416061602533 Scheduler time: 128.56531534017995 Scheduler overhead time: 0.1322823278605938 Adapter cache time: 0.0240331101231277 Engine time: 0.12562979757785797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 149.64028578298166,
    "estimated_duration": 3600.0332097011014,
    "input_throughput": 7165.324178257151,
    "output_throughput": 6210.322710288624,
    "total_throughput": 13375.646888545776,
    "itl": 81.74000897706551,
    "ttft": 1408708.3324298207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.946227475544437,
    "arrivals": 188561,
    "finished_requests": 104313,
    "scheduler_time": 231.09487157727983
}
#Debug simulation 
Total elapsed time: 149.6404360048473. Arrivals time: 0.7278551389463246 Scheduler time: 148.57090948801488 Scheduler overhead time: 0.14067004201933742 Adapter cache time: 0.024082788731902838 Engine time: 0.1325613772496581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 128.17174040386453,
    "estimated_duration": 3600.0401021650982,
    "input_throughput": 7085.263573775111,
    "output_throughput": 6153.467842393516,
    "total_throughput": 13238.731416168626,
    "itl": 79.83552143342395,
    "ttft": 1445598.5692201923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.463189057889397,
    "arrivals": 188561,
    "finished_requests": 103299,
    "scheduler_time": 232.87282203878198
}
#Debug simulation 
Total elapsed time: 128.1719011850655. Arrivals time: 0.691457930020988 Scheduler time: 127.1506286165677 Scheduler overhead time: 0.1363346274010837 Adapter cache time: 0.023583535104990005 Engine time: 0.12705547083169222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 143.30020756274462,
    "estimated_duration": 3600.022393289436,
    "input_throughput": 7159.037412667538,
    "output_throughput": 6202.498084906989,
    "total_throughput": 13361.535497574527,
    "itl": 81.59333770386894,
    "ttft": 1431833.6945915702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9767399039072896,
    "arrivals": 188561,
    "finished_requests": 104179,
    "scheduler_time": 230.98340742040358
}
#Debug simulation 
Total elapsed time: 143.30035941815004. Arrivals time: 0.7248792797327042 Scheduler time: 142.2337446338497 Scheduler overhead time: 0.14123796578496695 Adapter cache time: 0.025235518347471952 Engine time: 0.13120238902047276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126504783 . Total output tokens: 111365841
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 146.0584420072846,
    "estimated_duration": 3600.038082627383,
    "input_throughput": 7083.769508734814,
    "output_throughput": 6146.131371991421,
    "total_throughput": 13229.900880726234,
    "itl": 79.77473214633157,
    "ttft": 1431632.510538392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1509710166230807,
    "arrivals": 188561,
    "finished_requests": 103179,
    "scheduler_time": 233.15134881935404
}
#Debug simulation 
Total elapsed time: 146.05858997628093. Arrivals time: 0.7255991483107209 Scheduler time: 144.99461809685454 Scheduler overhead time: 0.13846112182363868 Adapter cache time: 0.025215736590325832 Engine time: 0.1314147817902267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 148.84240888478234,
    "estimated_duration": 3600.0045060873376,
    "input_throughput": 7123.832472052417,
    "output_throughput": 6182.595316857091,
    "total_throughput": 13306.427788909508,
    "itl": 82.92002602473906,
    "ttft": 1415580.3573398464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8397759470576411,
    "arrivals": 187655,
    "finished_requests": 103690,
    "scheduler_time": 231.34996527616318
}
#Debug simulation 
Total elapsed time: 148.84258713480085. Arrivals time: 0.7184467762708664 Scheduler time: 147.7786554340273 Scheduler overhead time: 0.14227654319256544 Adapter cache time: 0.02465155255049467 Engine time: 0.13415568135678768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 141.4367960211821,
    "estimated_duration": 3600.0408230596277,
    "input_throughput": 7039.814059236359,
    "output_throughput": 6135.554868854928,
    "total_throughput": 13175.368928091286,
    "itl": 82.05553854756506,
    "ttft": 1430956.6896487596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0123048983374614,
    "arrivals": 187655,
    "finished_requests": 102732,
    "scheduler_time": 233.22512042301597
}
#Debug simulation 
Total elapsed time: 141.43694042274728. Arrivals time: 0.7322532385587692 Scheduler time: 140.3615258494392 Scheduler overhead time: 0.14176427107304335 Adapter cache time: 0.024704910814762115 Engine time: 0.13264474598690867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 141.22063292842358,
    "estimated_duration": 3600.0725738397455,
    "input_throughput": 7012.260303706796,
    "output_throughput": 6102.909191235104,
    "total_throughput": 13115.1694949419,
    "itl": 80.03947616850506,
    "ttft": 1434487.8170477054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0885399736417467,
    "arrivals": 187655,
    "finished_requests": 102262,
    "scheduler_time": 234.39011537103116
}
#Debug simulation 
Total elapsed time: 141.2207729141228. Arrivals time: 0.711690747179091 Scheduler time: 140.16363122081384 Scheduler overhead time: 0.14493115479126573 Adapter cache time: 0.02404164057224989 Engine time: 0.13328191358596087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 138.85362547682598,
    "estimated_duration": 3600.0211765756503,
    "input_throughput": 7058.252091775172,
    "output_throughput": 6141.390540660727,
    "total_throughput": 13199.6426324359,
    "itl": 81.97846841822694,
    "ttft": 1430321.3434961892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 141,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9653792383661486,
    "arrivals": 187655,
    "finished_requests": 102928,
    "scheduler_time": 232.9593299644733
}
#Debug simulation 
Total elapsed time: 138.8537694378756. Arrivals time: 0.724242215976119 Scheduler time: 137.78737096255645 Scheduler overhead time: 0.14228958683088422 Adapter cache time: 0.024195057805627584 Engine time: 0.130955230910331 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 142.0894711241126,
    "estimated_duration": 3600.01345888428,
    "input_throughput": 7012.297395083561,
    "output_throughput": 6102.86301729802,
    "total_throughput": 13115.160412381581,
    "itl": 80.03953386551925,
    "ttft": 1434492.9187305039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0798410522239292,
    "arrivals": 187655,
    "finished_requests": 102261,
    "scheduler_time": 234.38255728975417
}
#Debug simulation 
Total elapsed time: 142.08967411983758. Arrivals time: 0.7098574624396861 Scheduler time: 141.03878565691411 Scheduler overhead time: 0.138563581276685 Adapter cache time: 0.024464554619044065 Engine time: 0.13273669267073274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 138.56838753493503,
    "estimated_duration": 3600.0218356351907,
    "input_throughput": 7088.15117381022,
    "output_throughput": 6154.309337985123,
    "total_throughput": 13242.460511795343,
    "itl": 81.91364559294209,
    "ttft": 1444974.9563237042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7852222756901747,
    "arrivals": 187655,
    "finished_requests": 103176,
    "scheduler_time": 232.4334741479745
}
#Debug simulation 
Total elapsed time: 138.56852452084422. Arrivals time: 0.7162905749864876 Scheduler time: 137.51930452138186 Scheduler overhead time: 0.13737821485847235 Adapter cache time: 0.024108309764415026 Engine time: 0.12826648633927107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 125874114 . Total output tokens: 110790712
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 139.55763612734154,
    "estimated_duration": 3600.0029346477577,
    "input_throughput": 7012.317894810281,
    "output_throughput": 6102.880858387326,
    "total_throughput": 13115.198753197608,
    "itl": 80.03923930295929,
    "ttft": 1434489.0452383154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0692780762165794,
    "arrivals": 187655,
    "finished_requests": 102261,
    "scheduler_time": 234.38249489801586
}
#Debug simulation 
Total elapsed time: 139.55781713221222. Arrivals time: 0.7187619167380035 Scheduler time: 138.50182329770178 Scheduler overhead time: 0.13960877573117614 Adapter cache time: 0.024098755326122046 Engine time: 0.12980984756723046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 142.31395562225953,
    "estimated_duration": 3600.0040780117615,
    "input_throughput": 7133.319697288438,
    "output_throughput": 6237.327101140756,
    "total_throughput": 13370.646798429194,
    "itl": 82.8357753469485,
    "ttft": 1419187.951826787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 187118,
    "finished_requests": 104259,
    "scheduler_time": 229.7117439929992
}
#Debug simulation 
Total elapsed time: 142.31410963321105. Arrivals time: 0.7362432107329369 Scheduler time: 141.2226432459429 Scheduler overhead time: 0.1414055274799466 Adapter cache time: 0.023831399623304605 Engine time: 0.14651989610865712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 156.82206557923928,
    "estimated_duration": 3600.0506735555373,
    "input_throughput": 7064.969164692407,
    "output_throughput": 6173.603933760611,
    "total_throughput": 13238.573098453016,
    "itl": 81.30003775691709,
    "ttft": 1419588.6888553693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8091243184078489,
    "arrivals": 187118,
    "finished_requests": 103349,
    "scheduler_time": 232.14243922109438
}
#Debug simulation 
Total elapsed time: 156.82219961099327. Arrivals time: 0.7198787853121758 Scheduler time: 155.75485296640545 Scheduler overhead time: 0.1432422511279583 Adapter cache time: 0.02431720681488514 Engine time: 0.135638281237334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 153.17279902705923,
    "estimated_duration": 3600.012508578089,
    "input_throughput": 7007.623151277387,
    "output_throughput": 6135.38895972447,
    "total_throughput": 13143.012111001855,
    "itl": 79.56567133153567,
    "ttft": 1422433.840719283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8544968738080935,
    "arrivals": 187118,
    "finished_requests": 102642,
    "scheduler_time": 233.57725319760334
}
#Debug simulation 
Total elapsed time: 153.17298887809739. Arrivals time: 0.7209744425490499 Scheduler time: 152.10277698840946 Scheduler overhead time: 0.14329387759789824 Adapter cache time: 0.02592336619272828 Engine time: 0.13525358587503433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 151.70864591794088,
    "estimated_duration": 3600.040946086188,
    "input_throughput": 7070.342915869331,
    "output_throughput": 6189.143216335677,
    "total_throughput": 13259.486132205007,
    "itl": 81.54835314689211,
    "ttft": 1423393.8004977463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7449923005141315,
    "arrivals": 187118,
    "finished_requests": 103462,
    "scheduler_time": 231.64613722128945
}
#Debug simulation 
Total elapsed time: 151.70879537193105. Arrivals time: 0.7303543714806437 Scheduler time: 150.63943399861455 Scheduler overhead time: 0.14058733824640512 Adapter cache time: 0.02383336052298546 Engine time: 0.13180463016033173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 143.4864324759692,
    "estimated_duration": 3600.055769192662,
    "input_throughput": 7000.282944408829,
    "output_throughput": 6138.372963304328,
    "total_throughput": 13138.655907713157,
    "itl": 79.75910027207829,
    "ttft": 1429747.3178854852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9696815900085495,
    "arrivals": 187118,
    "finished_requests": 102591,
    "scheduler_time": 233.4829785775481
}
#Debug simulation 
Total elapsed time: 143.4865784039721. Arrivals time: 0.710591999348253 Scheduler time: 142.4366653682664 Scheduler overhead time: 0.1400939403101802 Adapter cache time: 0.02377957385033369 Engine time: 0.13132263394072652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 143.52564258268103,
    "estimated_duration": 3600.0718398299114,
    "input_throughput": 7088.248272624924,
    "output_throughput": 6201.672353586931,
    "total_throughput": 13289.920626211855,
    "itl": 81.8475151009758,
    "ttft": 1423582.4911057244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8682132479175911,
    "arrivals": 187118,
    "finished_requests": 103676,
    "scheduler_time": 231.15276562684738
}
#Debug simulation 
Total elapsed time: 143.52580405259505. Arrivals time: 0.7097055884078145 Scheduler time: 142.47938239667565 Scheduler overhead time: 0.13937486475333571 Adapter cache time: 0.02435052627697587 Engine time: 0.13038768246769905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125554273 . Total output tokens: 110513089
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 143.4933382398449,
    "estimated_duration": 3600.0458101139548,
    "input_throughput": 7000.3023098204085,
    "output_throughput": 6138.389944349208,
    "total_throughput": 13138.692254169617,
    "itl": 79.75887189902558,
    "ttft": 1429743.989840921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9595328483544291,
    "arrivals": 187118,
    "finished_requests": 102591,
    "scheduler_time": 233.48286484682924
}
#Debug simulation 
Total elapsed time: 143.49348613293841. Arrivals time: 0.7097194814123213 Scheduler time: 142.44632922951132 Scheduler overhead time: 0.1378940986469388 Adapter cache time: 0.024173196405172348 Engine time: 0.13183195050805807 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 156.077291586902,
    "estimated_duration": 3600.054767181479,
    "input_throughput": 6976.2399808330365,
    "output_throughput": 6129.599805304186,
    "total_throughput": 13105.839786137221,
    "itl": 82.55638174250329,
    "ttft": 1450710.990246792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6215664490032935,
    "arrivals": 186916,
    "finished_requests": 101989,
    "scheduler_time": 233.48809077974263
}
#Debug simulation 
Total elapsed time: 156.0774630936794. Arrivals time: 0.7193612111732364 Scheduler time: 155.0075935581699 Scheduler overhead time: 0.14458840573206544 Adapter cache time: 0.024736971128731966 Engine time: 0.13631535368040204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 144.01740352110937,
    "estimated_duration": 3600.0431082885143,
    "input_throughput": 7004.141684288744,
    "output_throughput": 6152.362717270261,
    "total_throughput": 13156.504401559005,
    "itl": 81.97035594887052,
    "ttft": 1434722.2917996913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8665796068729833,
    "arrivals": 186916,
    "finished_requests": 102341,
    "scheduler_time": 232.59496692738014
}
#Debug simulation 
Total elapsed time: 144.01755103003234. Arrivals time: 0.7294319444335997 Scheduler time: 142.946105488576 Scheduler overhead time: 0.13990172510966659 Adapter cache time: 0.025064246729016304 Engine time: 0.1324886647053063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 151.97118695965037,
    "estimated_duration": 3600.0208654740195,
    "input_throughput": 6910.772723180914,
    "output_throughput": 6087.228607524899,
    "total_throughput": 12998.001330705814,
    "itl": 79.88344842116743,
    "ttft": 1454010.2351228327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9602587168617175,
    "arrivals": 186916,
    "finished_requests": 101086,
    "scheduler_time": 234.9984427068397
}
#Debug simulation 
Total elapsed time: 151.97132883267477. Arrivals time: 0.7071121172048151 Scheduler time: 150.91517159389332 Scheduler overhead time: 0.14321234496310353 Adapter cache time: 0.024748757481575012 Engine time: 0.13541358523070812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 145.16791671374813,
    "estimated_duration": 3600.047603970126,
    "input_throughput": 6944.113175734397,
    "output_throughput": 6104.293725384405,
    "total_throughput": 13048.406901118802,
    "itl": 81.54218069939262,
    "ttft": 1455929.7424942486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6211952138179911,
    "arrivals": 186916,
    "finished_requests": 101574,
    "scheduler_time": 234.50283770886878
}
#Debug simulation 
Total elapsed time: 145.16809783689678. Arrivals time: 0.6472335723228753 Scheduler time: 144.19740515528247 Scheduler overhead time: 0.13410384440794587 Adapter cache time: 0.02271224372088909 Engine time: 0.12369881011545658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 157.38907575793564,
    "estimated_duration": 3600.0119518171778,
    "input_throughput": 6910.789834306485,
    "output_throughput": 6087.2436795490075,
    "total_throughput": 12998.033513855493,
    "itl": 79.88332152997712,
    "ttft": 1454006.615298389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9519740297971294,
    "arrivals": 186916,
    "finished_requests": 101086,
    "scheduler_time": 234.9982313029621
}
#Debug simulation 
Total elapsed time: 157.38921196060255. Arrivals time: 0.69614486861974 Scheduler time: 156.33955167327076 Scheduler overhead time: 0.14711896376684308 Adapter cache time: 0.026105519849807024 Engine time: 0.13516189297661185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 158.24108939664438,
    "estimated_duration": 3600.0669546835716,
    "input_throughput": 6943.304475901632,
    "output_throughput": 6105.750330949618,
    "total_throughput": 13049.054806851249,
    "itl": 81.57297907196649,
    "ttft": 1456916.642719424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5873207265324892,
    "arrivals": 186916,
    "finished_requests": 101569,
    "scheduler_time": 234.42888156390387
}
#Debug simulation 
Total elapsed time: 158.24122742563486. Arrivals time: 0.7455332474783063 Scheduler time: 157.1442182296887 Scheduler overhead time: 0.1446944223716855 Adapter cache time: 0.02573281154036522 Engine time: 0.13518680585548282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125401465 . Total output tokens: 110374744
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 152.2351982682012,
    "estimated_duration": 3600.002477179543,
    "input_throughput": 6910.808022413262,
    "output_throughput": 6087.259700212444,
    "total_throughput": 12998.067722625705,
    "itl": 79.8831082840425,
    "ttft": 1454002.8601859605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9426537568494678,
    "arrivals": 186916,
    "finished_requests": 101086,
    "scheduler_time": 234.9980744021349
}
#Debug simulation 
Total elapsed time: 152.2353420262225. Arrivals time: 0.7134522683918476 Scheduler time: 151.17800598545 Scheduler overhead time: 0.1416362915188074 Adapter cache time: 0.024399515707045794 Engine time: 0.1335782865062356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 150.06599498633295,
    "estimated_duration": 3600.0407303670722,
    "input_throughput": 7139.9767183687145,
    "output_throughput": 6214.657742974695,
    "total_throughput": 13354.63446134341,
    "itl": 80.57753027174175,
    "ttft": 1327261.9183106734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 105,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6943029483547427,
    "arrivals": 164321,
    "finished_requests": 103426,
    "scheduler_time": 224.81616851106875
}
#Debug simulation 
Total elapsed time: 150.06614007707685. Arrivals time: 0.7359457584097981 Scheduler time: 148.98762124264613 Scheduler overhead time: 0.14106342429295182 Adapter cache time: 0.024401255883276463 Engine time: 0.13294249353930354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 151.63082164619118,
    "estimated_duration": 3600.0055520929245,
    "input_throughput": 7113.2109741088,
    "output_throughput": 6191.27267374411,
    "total_throughput": 13304.483647852909,
    "itl": 79.69950424640155,
    "ttft": 1327084.916943543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7724828668590636,
    "arrivals": 164321,
    "finished_requests": 103138,
    "scheduler_time": 225.647790494477
}
#Debug simulation 
Total elapsed time: 151.63097685109824. Arrivals time: 0.7224191934801638 Scheduler time: 150.55754600884393 Scheduler overhead time: 0.14650866482406855 Adapter cache time: 0.024364234879612923 Engine time: 0.13603010773658752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 128.55882423603907,
    "estimated_duration": 3600.0178678265606,
    "input_throughput": 7059.31385150124,
    "output_throughput": 6150.717250014153,
    "total_throughput": 13210.031101515393,
    "itl": 78.18658002188587,
    "ttft": 1341879.7919391494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0970840056147437,
    "arrivals": 164321,
    "finished_requests": 102362,
    "scheduler_time": 226.79834841487005
}
#Debug simulation 
Total elapsed time: 128.55897352378815. Arrivals time: 0.6918728328309953 Scheduler time: 127.53360364539549 Scheduler overhead time: 0.1369922892190516 Adapter cache time: 0.022346262354403734 Engine time: 0.12948653008788824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 153.28921666927636,
    "estimated_duration": 3600.0535841617407,
    "input_throughput": 7112.958849461821,
    "output_throughput": 6190.774520149114,
    "total_throughput": 13303.733369610936,
    "itl": 79.69883488555685,
    "ttft": 1326649.104909616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7225069118198004,
    "arrivals": 164321,
    "finished_requests": 103138,
    "scheduler_time": 225.65680008143093
}
#Debug simulation 
Total elapsed time: 153.28939233301207. Arrivals time: 0.7161020976491272 Scheduler time: 152.22081899968907 Scheduler overhead time: 0.14455179078504443 Adapter cache time: 0.02467708196491003 Engine time: 0.13744552293792367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 128.92011958081275,
    "estimated_duration": 3600.0275471207883,
    "input_throughput": 7058.775986401468,
    "output_throughput": 6151.408207337527,
    "total_throughput": 13210.184193738995,
    "itl": 78.1869259542629,
    "ttft": 1341702.4328438877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.085226457566024,
    "arrivals": 164321,
    "finished_requests": 102367,
    "scheduler_time": 226.81035596275046
}
#Debug simulation 
Total elapsed time: 128.9202781189233. Arrivals time: 0.7016021776944399 Scheduler time: 127.8859258950688 Scheduler overhead time: 0.1355219827964902 Adapter cache time: 0.02312171785160899 Engine time: 0.1302094031125307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 151.60799277480692,
    "estimated_duration": 3600.020227311322,
    "input_throughput": 7113.869473763181,
    "output_throughput": 6191.1388249743195,
    "total_throughput": 13305.0082987375,
    "itl": 79.70072435315383,
    "ttft": 1326843.101453661,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6830795406410467,
    "arrivals": 164321,
    "finished_requests": 103143,
    "scheduler_time": 225.65083936494963
}
#Debug simulation 
Total elapsed time: 151.60814581857994. Arrivals time: 0.7234358750283718 Scheduler time: 150.53857248323038 Scheduler overhead time: 0.14390520798042417 Adapter cache time: 0.023894467391073704 Engine time: 0.13369326433166862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110081560 . Total output tokens: 96948010
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 131.74146864423528,
    "estimated_duration": 3600.0169838703596,
    "input_throughput": 7071.684137620384,
    "output_throughput": 6156.188734469062,
    "total_throughput": 13227.872872089447,
    "itl": 78.20520751954166,
    "ttft": 1335956.1931578477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9759917809627953,
    "arrivals": 164321,
    "finished_requests": 102534,
    "scheduler_time": 226.80112610298315
}
#Debug simulation 
Total elapsed time: 131.74161244696006. Arrivals time: 0.7052771947346628 Scheduler time: 130.69911968149245 Scheduler overhead time: 0.1376455770805478 Adapter cache time: 0.023121375124901533 Engine time: 0.13212889712303877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 178.81534431688488,
    "estimated_duration": 3600.0031844158657,
    "input_throughput": 7200.943074778507,
    "output_throughput": 6295.184709309427,
    "total_throughput": 13496.127784087934,
    "itl": 80.05036729385998,
    "ttft": 1245497.0140147747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.667853312226943,
    "arrivals": 160553,
    "finished_requests": 104956,
    "scheduler_time": 219.15986200599076
}
#Debug simulation 
Total elapsed time: 178.81553419074044. Arrivals time: 0.7619975064881146 Scheduler time: 177.68648533755913 Scheduler overhead time: 0.15149357868358493 Adapter cache time: 0.02514702919870615 Engine time: 0.14409407787024975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 164.21553606493399,
    "estimated_duration": 3600.0510655141798,
    "input_throughput": 7176.874863665245,
    "output_throughput": 6266.971937182133,
    "total_throughput": 13443.846800847377,
    "itl": 79.25850042657451,
    "ttft": 1251228.9790694062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6783861268451439,
    "arrivals": 160553,
    "finished_requests": 104531,
    "scheduler_time": 219.35064067915695
}
#Debug simulation 
Total elapsed time: 164.2156892400235. Arrivals time: 0.7260281019844115 Scheduler time: 163.12204504478723 Scheduler overhead time: 0.15148384775966406 Adapter cache time: 0.026139734778553247 Engine time: 0.14311738079413772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 174.15351337520406,
    "estimated_duration": 3600.087704957845,
    "input_throughput": 7145.545083408245,
    "output_throughput": 6242.44069638941,
    "total_throughput": 13387.985779797655,
    "itl": 77.51611257372424,
    "ttft": 1237501.528520616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8106988941505553,
    "arrivals": 160553,
    "finished_requests": 103943,
    "scheduler_time": 221.63848807537767
}
#Debug simulation 
Total elapsed time: 174.15372299030423. Arrivals time: 0.7314554881304502 Scheduler time: 173.0543424487114 Scheduler overhead time: 0.15064590703696012 Adapter cache time: 0.025586215313524008 Engine time: 0.14448967669159174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 162.29310149606317,
    "estimated_duration": 3600.048950736229,
    "input_throughput": 7185.588405599252,
    "output_throughput": 6272.8835938138345,
    "total_throughput": 13458.471999413086,
    "itl": 79.44280093705665,
    "ttft": 1244220.2748689593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6819841281557458,
    "arrivals": 160553,
    "finished_requests": 104644,
    "scheduler_time": 219.40372815045276
}
#Debug simulation 
Total elapsed time: 162.29325022920966. Arrivals time: 0.7348472732119262 Scheduler time: 161.20546137541533 Scheduler overhead time: 0.14264767244458199 Adapter cache time: 0.02377088228240609 Engine time: 0.1407583011314273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 166.43797584762797,
    "estimated_duration": 3600.082283442234,
    "input_throughput": 7132.493920513474,
    "output_throughput": 6228.401251584835,
    "total_throughput": 13360.89517209831,
    "itl": 77.54155720936852,
    "ttft": 1270615.046343363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8129738313145938,
    "arrivals": 160553,
    "finished_requests": 103905,
    "scheduler_time": 221.41057864611864
}
#Debug simulation 
Total elapsed time: 166.4381516808644. Arrivals time: 0.7395599256269634 Scheduler time: 165.3353894641623 Scheduler overhead time: 0.15072302892804146 Adapter cache time: 0.02473462512716651 Engine time: 0.14187958231195807 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.52047814801335,
    "estimated_duration": 3600.092468984494,
    "input_throughput": 7170.594983989892,
    "output_throughput": 6273.258588374684,
    "total_throughput": 13443.853572364576,
    "itl": 79.1212927560694,
    "ttft": 1249278.9711182942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6511599359381942,
    "arrivals": 160553,
    "finished_requests": 104566,
    "scheduler_time": 220.2995563952777
}
#Debug simulation 
Total elapsed time: 180.52062953822315. Arrivals time: 0.7442589686252177 Scheduler time: 179.4020059313625 Scheduler overhead time: 0.15494097210466862 Adapter cache time: 0.02732425043359399 Engine time: 0.1463322718627751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107574118 . Total output tokens: 94702318
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 165.99071498401463,
    "estimated_duration": 3600.0355578672584,
    "input_throughput": 7132.454273649364,
    "output_throughput": 6228.454591510669,
    "total_throughput": 13360.908865160034,
    "itl": 77.5387049598702,
    "ttft": 1270721.7315546074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8055176129564645,
    "arrivals": 160553,
    "finished_requests": 103906,
    "scheduler_time": 221.41704216838465
}
#Debug simulation 
Total elapsed time: 165.99089763406664. Arrivals time: 0.7325760652311146 Scheduler time: 164.90268750861287 Scheduler overhead time: 0.1467640153132379 Adapter cache time: 0.025375479832291603 Engine time: 0.1380617693066597 
