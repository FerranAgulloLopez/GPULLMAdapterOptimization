INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.167191762942821,
    "estimated_duration": 3600.0034584824753,
    "input_throughput": 5640.12041492844,
    "output_throughput": 4961.625788973378,
    "total_throughput": 10601.74620390182,
    "itl": 116.41071745032559,
    "ttft": 896990.3720124261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.130718101277404,
    "arrivals": 100828,
    "finished_requests": 82729,
    "scheduler_time": 78.90399292160521
}
#Debug simulation 
Total elapsed time: 6.167338474653661. Arrivals time: 0.22559104533866048 Scheduler time: 5.791496625635773 Scheduler overhead time: 0.048027302604168653 Adapter cache time: 0.0311806071549654 Engine time: 0.048751155845820904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.971695506945252,
    "estimated_duration": 3600.0326976929473,
    "input_throughput": 5497.772843197688,
    "output_throughput": 4835.2546939796375,
    "total_throughput": 10333.027537177326,
    "itl": 108.67503934882299,
    "ttft": 996963.3016050493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.347268572402644,
    "arrivals": 100828,
    "finished_requests": 80613,
    "scheduler_time": 79.02813009802173
}
#Debug simulation 
Total elapsed time: 5.9718024311587214. Arrivals time: 0.22959839273244143 Scheduler time: 5.580797937698662 Scheduler overhead time: 0.05050451075658202 Adapter cache time: 0.03584365965798497 Engine time: 0.05168628925457597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.762781966011971,
    "estimated_duration": 3600.032296231974,
    "input_throughput": 5253.366760013465,
    "output_throughput": 4623.7862969792095,
    "total_throughput": 9877.153056992674,
    "itl": 95.48761136084373,
    "ttft": 1162850.5240695605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2954,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.229937058156704,
    "arrivals": 100828,
    "finished_requests": 76978,
    "scheduler_time": 80.0394622657212
}
#Debug simulation 
Total elapsed time: 5.76287728222087. Arrivals time: 0.22815064387395978 Scheduler time: 5.3518561674281955 Scheduler overhead time: 0.055459453258663416 Adapter cache time: 0.0444185552187264 Engine time: 0.05718400329351425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.989533339627087,
    "estimated_duration": 3600.0039509599164,
    "input_throughput": 5499.65118641628,
    "output_throughput": 4836.947191504313,
    "total_throughput": 10336.598377920593,
    "itl": 108.63643465984725,
    "ttft": 995752.2889720881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.142018914446563,
    "arrivals": 100828,
    "finished_requests": 80640,
    "scheduler_time": 79.03534282931176
}
#Debug simulation 
Total elapsed time: 5.989650568924844. Arrivals time: 0.23095208778977394 Scheduler time: 5.596784841734916 Scheduler overhead time: 0.05061305919662118 Adapter cache time: 0.03596547292545438 Engine time: 0.051901023369282484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_96_slots_64_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.764129305724055,
    "estimated_duration": 3600.061578343648,
    "input_throughput": 5254.108183533531,
    "output_throughput": 4624.115904056053,
    "total_throughput": 9878.224087589584,
    "itl": 95.48054920679577,
    "ttft": 1162473.9730242614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2953,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.013035001773712,
    "arrivals": 100828,
    "finished_requests": 76987,
    "scheduler_time": 80.03898131065871
}
#Debug simulation 
Total elapsed time: 5.764246233738959. Arrivals time: 0.23393538361415267 Scheduler time: 5.3454427742399275 Scheduler overhead time: 0.05537157226353884 Adapter cache time: 0.044610756915062666 Engine time: 0.05901895510032773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_96_slots_64_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_96_slots_64_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.994887087959796,
    "estimated_duration": 3600.0814148246714,
    "input_throughput": 5502.015292885997,
    "output_throughput": 4838.394467489649,
    "total_throughput": 10340.409760375647,
    "itl": 108.60197318332494,
    "ttft": 994500.880764334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.927882746724498,
    "arrivals": 100828,
    "finished_requests": 80670,
    "scheduler_time": 79.04606515947324
}
#Debug simulation 
Total elapsed time: 5.995016468223184. Arrivals time: 0.2248752238228917 Scheduler time: 5.608206811826676 Scheduler overhead time: 0.05062135122716427 Adapter cache time: 0.035981266759335995 Engine time: 0.051887504290789366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_96_slots_64_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_96_slots_64_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.744358676951379,
    "estimated_duration": 3600.066747828285,
    "input_throughput": 5254.367022892281,
    "output_throughput": 4624.361481642749,
    "total_throughput": 9878.728504535029,
    "itl": 95.47503531259945,
    "ttft": 1162286.91524144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2960,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.862502207756357,
    "arrivals": 100828,
    "finished_requests": 76992,
    "scheduler_time": 80.04092184279132
}
#Debug simulation 
Total elapsed time: 5.744455590844154. Arrivals time: 0.2320029092952609 Scheduler time: 5.3290172489359975 Scheduler overhead time: 0.05535111716017127 Adapter cache time: 0.044991166796535254 Engine time: 0.05727271595969796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.117624420672655,
    "estimated_duration": 3600.0796027812326,
    "input_throughput": 5848.888170065151,
    "output_throughput": 5027.8502136498255,
    "total_throughput": 10876.738383714977,
    "itl": 114.08663491921095,
    "ttft": 751160.881503885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.064594010957903,
    "arrivals": 99385,
    "finished_requests": 84544,
    "scheduler_time": 77.78827862800345
}
#Debug simulation 
Total elapsed time: 6.117723006755114. Arrivals time: 0.22335712937638164 Scheduler time: 5.741187545005232 Scheduler overhead time: 0.047671846114099026 Adapter cache time: 0.033866249956190586 Engine time: 0.04946580529212952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.060220781713724,
    "estimated_duration": 3600.0158716473875,
    "input_throughput": 5747.466605065742,
    "output_throughput": 4940.133219985412,
    "total_throughput": 10687.599825051155,
    "itl": 105.63101618844593,
    "ttft": 825504.92933301,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.819396000858111,
    "arrivals": 99385,
    "finished_requests": 83059,
    "scheduler_time": 77.99253765729647
}
#Debug simulation 
Total elapsed time: 6.060335851740092. Arrivals time: 0.2274665147997439 Scheduler time: 5.668904157355428 Scheduler overhead time: 0.05088235205039382 Adapter cache time: 0.03666439559310675 Engine time: 0.05270468583330512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.833636875730008,
    "estimated_duration": 3600.0705096301867,
    "input_throughput": 5535.903795964055,
    "output_throughput": 4764.006969897318,
    "total_throughput": 10299.910765861374,
    "itl": 92.0056820148717,
    "ttft": 972115.5177799088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.89917404944044,
    "arrivals": 99385,
    "finished_requests": 80017,
    "scheduler_time": 78.89674322531641
}
#Debug simulation 
Total elapsed time: 5.833742578979582. Arrivals time: 0.23121121665462852 Scheduler time: 5.420039638411254 Scheduler overhead time: 0.05657107196748257 Adapter cache time: 0.040639402344822884 Engine time: 0.05888344766572118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.064862515311688,
    "estimated_duration": 3600.021272758413,
    "input_throughput": 5748.206033278263,
    "output_throughput": 4940.857192871828,
    "total_throughput": 10689.06322615009,
    "itl": 105.60203744661605,
    "ttft": 824781.0917167269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.816826605005433,
    "arrivals": 99385,
    "finished_requests": 83069,
    "scheduler_time": 77.99681869776785
}
#Debug simulation 
Total elapsed time: 6.064960885327309. Arrivals time: 0.23300063097849488 Scheduler time: 5.667816778179258 Scheduler overhead time: 0.050851565320044756 Adapter cache time: 0.03677459200844169 Engine time: 0.052823963575065136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.833584644366056,
    "estimated_duration": 3600.0712680701613,
    "input_throughput": 5535.89151880412,
    "output_throughput": 4763.976244612987,
    "total_throughput": 10299.867763417107,
    "itl": 92.00203316835517,
    "ttft": 972134.8526625147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2115,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.739541568280798,
    "arrivals": 99385,
    "finished_requests": 80015,
    "scheduler_time": 78.89871131710814
}
#Debug simulation 
Total elapsed time: 5.83368109120056. Arrivals time: 0.232583022210747 Scheduler time: 5.418449718039483 Scheduler overhead time: 0.056672687176615 Adapter cache time: 0.04070037975907326 Engine time: 0.058745667804032564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.069866497069597,
    "estimated_duration": 3600.1030577788624,
    "input_throughput": 5748.969034450156,
    "output_throughput": 4941.405763804868,
    "total_throughput": 10690.374798255023,
    "itl": 105.57530759674931,
    "ttft": 824224.5551099782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.802037073514226,
    "arrivals": 99385,
    "finished_requests": 83080,
    "scheduler_time": 78.00339061358206
}
#Debug simulation 
Total elapsed time: 6.069958314765245. Arrivals time: 0.2340823831036687 Scheduler time: 5.672293676994741 Scheduler overhead time: 0.0507765831425786 Adapter cache time: 0.036689328495413065 Engine time: 0.05245212186127901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.889410383999348,
    "estimated_duration": 3600.0548113543487,
    "input_throughput": 5536.417372629324,
    "output_throughput": 4764.551069028623,
    "total_throughput": 10300.968441657948,
    "itl": 91.99461427994349,
    "ttft": 971757.7709935334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.60739781657201,
    "arrivals": 99385,
    "finished_requests": 80024,
    "scheduler_time": 78.897689681068
}
#Debug simulation 
Total elapsed time: 5.889532111119479. Arrivals time: 0.2353901956230402 Scheduler time: 5.471342304255813 Scheduler overhead time: 0.05702139204367995 Adapter cache time: 0.04016800969839096 Engine time: 0.059023293666541576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.2872066190466285,
    "estimated_duration": 3600.093579589546,
    "input_throughput": 5877.804988172715,
    "output_throughput": 5143.54713026965,
    "total_throughput": 11021.352118442364,
    "itl": 111.90161625120918,
    "ttft": 662456.9826401717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.918613547925158,
    "arrivals": 98626,
    "finished_requests": 85766,
    "scheduler_time": 78.3581175622366
}
#Debug simulation 
Total elapsed time: 6.287325291428715. Arrivals time: 0.23709804704412818 Scheduler time: 5.897640998940915 Scheduler overhead time: 0.04855478834360838 Adapter cache time: 0.031027542427182198 Engine time: 0.05036033969372511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.177416366059333,
    "estimated_duration": 3600.038455158959,
    "input_throughput": 5777.273009457797,
    "output_throughput": 5055.8451601863035,
    "total_throughput": 10833.118169644102,
    "itl": 103.5958408297987,
    "ttft": 738291.4991516917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.687072239532073,
    "arrivals": 98626,
    "finished_requests": 84272,
    "scheduler_time": 78.48740079659125
}
#Debug simulation 
Total elapsed time: 6.177509468048811. Arrivals time: 0.2299016173928976 Scheduler time: 5.785143177956343 Scheduler overhead time: 0.051711869426071644 Adapter cache time: 0.03303508600220084 Engine time: 0.05365552520379424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.982951994985342,
    "estimated_duration": 3600.0831511881292,
    "input_throughput": 5558.356893338964,
    "output_throughput": 4871.239152965769,
    "total_throughput": 10429.596046304734,
    "itl": 90.4165277874327,
    "ttft": 892941.7403752061,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.59870969435657,
    "arrivals": 98626,
    "finished_requests": 81124,
    "scheduler_time": 79.18299862938606
}
#Debug simulation 
Total elapsed time: 5.983085065614432. Arrivals time: 0.23862886475399137 Scheduler time: 5.562613639514893 Scheduler overhead time: 0.05744961043819785 Adapter cache time: 0.036829497665166855 Engine time: 0.06022043293341994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.142992779612541,
    "estimated_duration": 3600.0027308543163,
    "input_throughput": 5778.180061286677,
    "output_throughput": 5057.130886031192,
    "total_throughput": 10835.31094731787,
    "itl": 103.57742728290853,
    "ttft": 737331.7036443437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.057924807816821,
    "arrivals": 98626,
    "finished_requests": 84293,
    "scheduler_time": 78.488026302427
}
#Debug simulation 
Total elapsed time: 6.1430836408399045. Arrivals time: 0.2416384289972484 Scheduler time: 5.739064984023571 Scheduler overhead time: 0.051724372897297144 Adapter cache time: 0.03279553772881627 Engine time: 0.05374595941975713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.010824915021658,
    "estimated_duration": 3600.098230150801,
    "input_throughput": 5558.815543530801,
    "output_throughput": 4871.652626896614,
    "total_throughput": 10430.468170427415,
    "itl": 90.41762430264785,
    "ttft": 892847.2384967847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.497845125859522,
    "arrivals": 98626,
    "finished_requests": 81128,
    "scheduler_time": 79.18231878204506
}
#Debug simulation 
Total elapsed time: 6.010946415364742. Arrivals time: 0.24108781944960356 Scheduler time: 5.588125140871853 Scheduler overhead time: 0.05792012112215161 Adapter cache time: 0.03635519463568926 Engine time: 0.06028508860617876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.206533167976886,
    "estimated_duration": 3600.1046049394176,
    "input_throughput": 5778.809030008752,
    "output_throughput": 5057.895533095605,
    "total_throughput": 10836.704563104358,
    "itl": 103.55172208350095,
    "ttft": 736481.3631995802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.403515545460541,
    "arrivals": 98626,
    "finished_requests": 84310,
    "scheduler_time": 78.4915032072565
}
#Debug simulation 
Total elapsed time: 6.206663902383298. Arrivals time: 0.23349650343880057 Scheduler time: 5.8100478714331985 Scheduler overhead time: 0.05200317595154047 Adapter cache time: 0.03280715877190232 Engine time: 0.05406052153557539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.998771085869521,
    "estimated_duration": 3600.0962235535344,
    "input_throughput": 5558.697811762091,
    "output_throughput": 4871.543678543348,
    "total_throughput": 10430.24149030544,
    "itl": 90.41342677056765,
    "ttft": 892738.3333497486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.415564744733196,
    "arrivals": 98626,
    "finished_requests": 81128,
    "scheduler_time": 79.18274106655632
}
#Debug simulation 
Total elapsed time: 5.9988967389799654. Arrivals time: 0.2286032666452229 Scheduler time: 5.588964067865163 Scheduler overhead time: 0.05759083852171898 Adapter cache time: 0.036251075100153685 Engine time: 0.06049756286665797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.318505238741636,
    "estimated_duration": 3600.004325730035,
    "input_throughput": 6009.914722980938,
    "output_throughput": 5194.585702673502,
    "total_throughput": 11204.50042565444,
    "itl": 110.34006440243766,
    "ttft": 581198.1078259351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 913,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.03712944617042,
    "arrivals": 98351,
    "finished_requests": 86997,
    "scheduler_time": 77.74779523333476
}
#Debug simulation 
Total elapsed time: 6.318600967992097. Arrivals time: 0.23088432662189007 Scheduler time: 5.93766026571393 Scheduler overhead time: 0.0487953070551157 Adapter cache time: 0.02761417394503951 Engine time: 0.05087320553138852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.2325986423529685,
    "estimated_duration": 3600.042912248239,
    "input_throughput": 5897.203038266473,
    "output_throughput": 5099.195883900114,
    "total_throughput": 10996.398922166587,
    "itl": 102.24027249655835,
    "ttft": 664163.8289025999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 883,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.397747283899232,
    "arrivals": 98351,
    "finished_requests": 85361,
    "scheduler_time": 77.75246469358211
}
#Debug simulation 
Total elapsed time: 6.23271679924801. Arrivals time: 0.2296536429785192 Scheduler time: 5.842286139726639 Scheduler overhead time: 0.05229205032810569 Adapter cache time: 0.029505970422178507 Engine time: 0.054593734443187714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.043190319091082,
    "estimated_duration": 3600.075873565093,
    "input_throughput": 5669.961888827098,
    "output_throughput": 4908.241276176678,
    "total_throughput": 10578.203165003775,
    "itl": 89.39840206738852,
    "ttft": 827394.9583567021,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 860,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.405440280661005,
    "arrivals": 98351,
    "finished_requests": 82103,
    "scheduler_time": 78.30634698669444
}
#Debug simulation 
Total elapsed time: 6.043310970067978. Arrivals time: 0.23660108214244246 Scheduler time: 5.6276508728042245 Scheduler overhead time: 0.05836905771866441 Adapter cache time: 0.03269325289875269 Engine time: 0.06072093406692147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.222236473113298,
    "estimated_duration": 3600.0159861337597,
    "input_throughput": 5897.944642963342,
    "output_throughput": 5099.961519814715,
    "total_throughput": 10997.906162778057,
    "itl": 102.23112215817844,
    "ttft": 663563.8932811781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 882,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.024873026004051,
    "arrivals": 98351,
    "finished_requests": 85372,
    "scheduler_time": 77.75210875358268
}
#Debug simulation 
Total elapsed time: 6.222347660921514. Arrivals time: 0.23267247946932912 Scheduler time: 5.8291585659608245 Scheduler overhead time: 0.05213950201869011 Adapter cache time: 0.02959102112799883 Engine time: 0.054348791018128395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.0293848579749465,
    "estimated_duration": 3600.067958932034,
    "input_throughput": 5669.88046693853,
    "output_throughput": 4907.886240359045,
    "total_throughput": 10577.766707297575,
    "itl": 89.39781170851782,
    "ttft": 827412.1554314953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 864,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.380476495446657,
    "arrivals": 98351,
    "finished_requests": 82101,
    "scheduler_time": 78.30812025164765
}
#Debug simulation 
Total elapsed time: 6.029479803983122. Arrivals time: 0.23634465504437685 Scheduler time: 5.614080294035375 Scheduler overhead time: 0.05822473857551813 Adapter cache time: 0.03277658810839057 Engine time: 0.060848467983305454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.219251004979014,
    "estimated_duration": 3600.0054205456304,
    "input_throughput": 5898.188619055404,
    "output_throughput": 5100.021765305362,
    "total_throughput": 10998.210384360766,
    "itl": 102.21975072294623,
    "ttft": 663302.1126604459,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 882,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.630618269583183,
    "arrivals": 98351,
    "finished_requests": 85374,
    "scheduler_time": 77.7525843037296
}
#Debug simulation 
Total elapsed time: 6.219346546102315. Arrivals time: 0.22954007796943188 Scheduler time: 5.829247620422393 Scheduler overhead time: 0.05207987688481808 Adapter cache time: 0.029703686013817787 Engine time: 0.054487685207277536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.055582628119737,
    "estimated_duration": 3600.0838267061517,
    "input_throughput": 5669.951029633651,
    "output_throughput": 4908.166267941254,
    "total_throughput": 10578.117297574905,
    "itl": 89.39651381952028,
    "ttft": 827350.2266181664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.341483962982913,
    "arrivals": 98351,
    "finished_requests": 82103,
    "scheduler_time": 78.30641116254374
}
#Debug simulation 
Total elapsed time: 6.055677754804492. Arrivals time: 0.2413433212786913 Scheduler time: 5.635570039507002 Scheduler overhead time: 0.05813251901417971 Adapter cache time: 0.03238378558307886 Engine time: 0.061112865805625916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.360673232004046,
    "estimated_duration": 3600.120704725957,
    "input_throughput": 6132.092452072508,
    "output_throughput": 5231.5970893075055,
    "total_throughput": 11363.689541380014,
    "itl": 109.12132420505274,
    "ttft": 433470.9071131227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1884,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.457778616194016,
    "arrivals": 96609,
    "finished_requests": 88446,
    "scheduler_time": 76.34385460960264
}
#Debug simulation 
Total elapsed time: 6.360767707228661. Arrivals time: 0.22494767094030976 Scheduler time: 5.978017042391002 Scheduler overhead time: 0.04959518415853381 Adapter cache time: 0.0337912579998374 Engine time: 0.05134811485186219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.274351749103516,
    "estimated_duration": 3600.0793908809924,
    "input_throughput": 6029.444254752422,
    "output_throughput": 5153.51996041948,
    "total_throughput": 11182.964215171904,
    "itl": 100.93452888771475,
    "ttft": 510874.0510465784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1861,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.625470633855793,
    "arrivals": 96609,
    "finished_requests": 86990,
    "scheduler_time": 76.27753651306213
}
#Debug simulation 
Total elapsed time: 6.274447313975543. Arrivals time: 0.22546884138137102 Scheduler time: 5.881531520280987 Scheduler overhead time: 0.053030970972031355 Adapter cache time: 0.034702641889452934 Engine time: 0.05498760053887963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.139032028149813,
    "estimated_duration": 3600.058351416463,
    "input_throughput": 5806.076724221958,
    "output_throughput": 4969.509172805733,
    "total_throughput": 10775.585897027691,
    "itl": 88.01297644949649,
    "ttft": 675692.4535777968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1797,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.516469724429532,
    "arrivals": 96609,
    "finished_requests": 83781,
    "scheduler_time": 76.55121201485788
}
#Debug simulation 
Total elapsed time: 6.139150938950479. Arrivals time: 0.23035747790709138 Scheduler time: 5.7217322420328856 Scheduler overhead time: 0.05965413013473153 Adapter cache time: 0.03713095560669899 Engine time: 0.062176449690014124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.272201458923519,
    "estimated_duration": 3600.0673704050732,
    "input_throughput": 6031.08768977203,
    "output_throughput": 5155.057694909699,
    "total_throughput": 11186.14538468173,
    "itl": 100.90915410419365,
    "ttft": 509538.2332570403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1856,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.703701385259796,
    "arrivals": 96609,
    "finished_requests": 87018,
    "scheduler_time": 76.28063398996156
}
#Debug simulation 
Total elapsed time: 6.272323017008603. Arrivals time: 0.2274267333559692 Scheduler time: 5.877129310742021 Scheduler overhead time: 0.05288520082831383 Adapter cache time: 0.03505703527480364 Engine time: 0.055160324554890394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.120717403944582,
    "estimated_duration": 3600.0040147024984,
    "input_throughput": 5806.191024963996,
    "output_throughput": 4969.6377912174285,
    "total_throughput": 10775.828816181425,
    "itl": 88.00896989768282,
    "ttft": 675545.9047752736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1797,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.384432344962608,
    "arrivals": 96609,
    "finished_requests": 83780,
    "scheduler_time": 76.54697320901941
}
#Debug simulation 
Total elapsed time: 6.1208415469154716. Arrivals time: 0.23007477493956685 Scheduler time: 5.704721107147634 Scheduler overhead time: 0.05918356031179428 Adapter cache time: 0.03706330480054021 Engine time: 0.062145309057086706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.279313408769667,
    "estimated_duration": 3600.043517016236,
    "input_throughput": 6032.450690484822,
    "output_throughput": 5156.302392529185,
    "total_throughput": 11188.753083014008,
    "itl": 100.88135039841423,
    "ttft": 508555.34020610724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1860,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.874092949461668,
    "arrivals": 96609,
    "finished_requests": 87037,
    "scheduler_time": 76.28273313863625
}
#Debug simulation 
Total elapsed time: 6.279442344792187. Arrivals time: 0.21882584132254124 Scheduler time: 5.8936485829763114 Scheduler overhead time: 0.05284026311710477 Adapter cache time: 0.03480391902849078 Engine time: 0.054619329515844584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.112464798614383,
    "estimated_duration": 3600.0657624532114,
    "input_throughput": 5806.333100358655,
    "output_throughput": 4969.660884141286,
    "total_throughput": 10775.99398449994,
    "itl": 88.00219367902605,
    "ttft": 675515.1045542551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1798,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.269895049780375,
    "arrivals": 96609,
    "finished_requests": 83785,
    "scheduler_time": 76.54922779228701
}
#Debug simulation 
Total elapsed time: 6.112559472676367. Arrivals time: 0.2223576633259654 Scheduler time: 5.703849242068827 Scheduler overhead time: 0.059453215915709734 Adapter cache time: 0.037334386724978685 Engine time: 0.06199320126324892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.486272584181279,
    "estimated_duration": 3600.0173106284415,
    "input_throughput": 6095.791521671642,
    "output_throughput": 5364.5000380925185,
    "total_throughput": 11460.29155976416,
    "itl": 107.31860035635984,
    "ttft": 347753.067902013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.973039056356287,
    "arrivals": 95912,
    "finished_requests": 89427,
    "scheduler_time": 77.0684733488749
}
#Debug simulation 
Total elapsed time: 6.486393690109253. Arrivals time: 0.22084260825067759 Scheduler time: 6.109649920836091 Scheduler overhead time: 0.050534654408693314 Adapter cache time: 0.029563922435045242 Engine time: 0.05236281035467982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.404647768940777,
    "estimated_duration": 3600.0951443421673,
    "input_throughput": 5988.5278404050205,
    "output_throughput": 5273.508959849751,
    "total_throughput": 11262.036800254771,
    "itl": 99.50046695985866,
    "ttft": 431357.23001756374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.779979326678381,
    "arrivals": 95912,
    "finished_requests": 87884,
    "scheduler_time": 76.84238518113922
}
#Debug simulation 
Total elapsed time: 6.404743057209998. Arrivals time: 0.2306154491379857 Scheduler time: 6.008254304993898 Scheduler overhead time: 0.05392294283956289 Adapter cache time: 0.030952947679907084 Engine time: 0.05600354075431824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.243426612112671,
    "estimated_duration": 3600.0597132965354,
    "input_throughput": 5759.912237959033,
    "output_throughput": 5078.116324703906,
    "total_throughput": 10838.02856266294,
    "itl": 86.93511539859635,
    "ttft": 606805.137773563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.626347430138798,
    "arrivals": 95912,
    "finished_requests": 84560,
    "scheduler_time": 76.94152590235554
}
#Debug simulation 
Total elapsed time: 6.243528419174254. Arrivals time: 0.22623274102807045 Scheduler time: 5.832229403313249 Scheduler overhead time: 0.06076241470873356 Adapter cache time: 0.03332059504464269 Engine time: 0.06293499190360308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.431419885251671,
    "estimated_duration": 3600.0195520246402,
    "input_throughput": 5988.131644417372,
    "output_throughput": 5273.202749502752,
    "total_throughput": 11261.334393920124,
    "itl": 99.40048120257552,
    "ttft": 431488.20993239194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.144457451705858,
    "arrivals": 95912,
    "finished_requests": 87878,
    "scheduler_time": 76.84209452039164
}
#Debug simulation 
Total elapsed time: 6.4315295410342515. Arrivals time: 0.22961177537217736 Scheduler time: 6.035614682361484 Scheduler overhead time: 0.054048304446041584 Adapter cache time: 0.031029153149574995 Engine time: 0.05615347716957331 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.250554632861167,
    "estimated_duration": 3600.066098910102,
    "input_throughput": 5759.865910872488,
    "output_throughput": 5078.135094668026,
    "total_throughput": 10838.001005540513,
    "itl": 86.93249425553962,
    "ttft": 606898.0245829393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.548728861375666,
    "arrivals": 95912,
    "finished_requests": 84560,
    "scheduler_time": 76.94454256217925
}
#Debug simulation 
Total elapsed time: 6.250677488278598. Arrivals time: 0.2310862224549055 Scheduler time: 5.835528636351228 Scheduler overhead time: 0.060033603105694056 Adapter cache time: 0.03308422910049558 Engine time: 0.06283492688089609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.426170688588172,
    "estimated_duration": 3600.0005777616134,
    "input_throughput": 5990.958760737222,
    "output_throughput": 5275.396653355088,
    "total_throughput": 11266.355414092312,
    "itl": 99.46380376937331,
    "ttft": 429489.40677632415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.541686218483404,
    "arrivals": 95912,
    "finished_requests": 87914,
    "scheduler_time": 76.84191295417787
}
#Debug simulation 
Total elapsed time: 6.426267632748932. Arrivals time: 0.2287101186811924 Scheduler time: 6.0321457693353295 Scheduler overhead time: 0.05383565882220864 Adapter cache time: 0.030815341509878635 Engine time: 0.055818636901676655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.2525002476759255,
    "estimated_duration": 3600.0639759650703,
    "input_throughput": 5760.009027184354,
    "output_throughput": 5078.138644772067,
    "total_throughput": 10838.14767195642,
    "itl": 86.93294774775174,
    "ttft": 606726.1586180092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.466242859587027,
    "arrivals": 95912,
    "finished_requests": 84561,
    "scheduler_time": 76.94162160978848
}
#Debug simulation 
Total elapsed time: 6.25259922305122. Arrivals time: 0.23290471779182553 Scheduler time: 5.83534863544628 Scheduler overhead time: 0.060180169995874166 Adapter cache time: 0.03350590821355581 Engine time: 0.0625974703580141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.578052470926195,
    "estimated_duration": 3600.051872104639,
    "input_throughput": 6317.606192352922,
    "output_throughput": 5432.270060199725,
    "total_throughput": 11749.876252552647,
    "itl": 104.76119830144017,
    "ttft": 229192.77312997726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.1495403997135725,
    "arrivals": 95515,
    "finished_requests": 91333,
    "scheduler_time": 76.47678782710692
}
#Debug simulation 
Total elapsed time: 6.57814359292388. Arrivals time: 0.21696136938408017 Scheduler time: 6.205383623950183 Scheduler overhead time: 0.05152443330734968 Adapter cache time: 0.027494080364704132 Engine time: 0.052882051561027765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.483794457744807,
    "estimated_duration": 3600.0729729671907,
    "input_throughput": 6203.694249450854,
    "output_throughput": 5334.838805828571,
    "total_throughput": 11538.533055279424,
    "itl": 97.25632699685389,
    "ttft": 319524.5639969976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 933,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.782189716673464,
    "arrivals": 95515,
    "finished_requests": 89693,
    "scheduler_time": 76.14076553352231
}
#Debug simulation 
Total elapsed time: 6.48388868290931. Arrivals time: 0.227724879514426 Scheduler time: 6.090455411467701 Scheduler overhead time: 0.05453689955174923 Adapter cache time: 0.028795774560421705 Engine time: 0.056881827767938375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.295813815668225,
    "estimated_duration": 3600.0403867909554,
    "input_throughput": 5961.6870073869695,
    "output_throughput": 5130.233001764502,
    "total_throughput": 11091.92000915147,
    "itl": 85.26430389175964,
    "ttft": 508331.8874146903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 889,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.63281118291898,
    "arrivals": 95515,
    "finished_requests": 86162,
    "scheduler_time": 76.09768630387622
}
#Debug simulation 
Total elapsed time: 6.295912097673863. Arrivals time: 0.23344138590618968 Scheduler time: 5.879020553082228 Scheduler overhead time: 0.06087204860523343 Adapter cache time: 0.030467902775853872 Engine time: 0.06360194645822048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.485587179660797,
    "estimated_duration": 3600.034693941694,
    "input_throughput": 6204.334096442946,
    "output_throughput": 5335.63524605052,
    "total_throughput": 11539.969342493467,
    "itl": 97.2467952778999,
    "ttft": 318914.86024263094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.367668829578896,
    "arrivals": 95515,
    "finished_requests": 89704,
    "scheduler_time": 76.14052023977669
}
#Debug simulation 
Total elapsed time: 6.4856794769875705. Arrivals time: 0.22507189540192485 Scheduler time: 6.094784991815686 Scheduler overhead time: 0.05467864125967026 Adapter cache time: 0.02884988859295845 Engine time: 0.05684747966006398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.30701920716092,
    "estimated_duration": 3600.0954039631597,
    "input_throughput": 5961.617010586208,
    "output_throughput": 5130.120435049725,
    "total_throughput": 11091.737445635934,
    "itl": 85.26170593687792,
    "ttft": 508384.96129220247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 890,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.582273095310687,
    "arrivals": 95515,
    "finished_requests": 86162,
    "scheduler_time": 76.10034164895605
}
#Debug simulation 
Total elapsed time: 6.307112412992865. Arrivals time: 0.22402055328711867 Scheduler time: 5.898513794410974 Scheduler overhead time: 0.06107965391129255 Adapter cache time: 0.03081261971965432 Engine time: 0.06415456160902977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.452805287670344,
    "estimated_duration": 3600.108419914899,
    "input_throughput": 6204.870907895613,
    "output_throughput": 5336.041518564628,
    "total_throughput": 11540.91242646024,
    "itl": 97.23561286185625,
    "ttft": 318635.24162449833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 935,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.968966079433419,
    "arrivals": 95515,
    "finished_requests": 89711,
    "scheduler_time": 76.1445657527861
}
#Debug simulation 
Total elapsed time: 6.4528975686989725. Arrivals time: 0.2179837361909449 Scheduler time: 6.0690013617277145 Scheduler overhead time: 0.05479050055146217 Adapter cache time: 0.028837757650762796 Engine time: 0.05664724763482809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.300697235856205,
    "estimated_duration": 3600.0888005702664,
    "input_throughput": 5962.0084917350005,
    "output_throughput": 5130.5081688746795,
    "total_throughput": 11092.516660609681,
    "itl": 85.26032700424147,
    "ttft": 508211.60091904993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 889,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.5164631986432004,
    "arrivals": 95515,
    "finished_requests": 86167,
    "scheduler_time": 76.0987464929416
}
#Debug simulation 
Total elapsed time: 6.3008159240707755. Arrivals time: 0.22875077603384852 Scheduler time: 5.887936633080244 Scheduler overhead time: 0.06105312379077077 Adapter cache time: 0.030583366751670837 Engine time: 0.06402188818901777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.682747499551624,
    "estimated_duration": 3600.000925764956,
    "input_throughput": 6441.1680658312025,
    "output_throughput": 5588.699118382836,
    "total_throughput": 12029.867184214037,
    "itl": 101.67131099780565,
    "ttft": 45779.38475908067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.830618530004438,
    "arrivals": 94463,
    "finished_requests": 93500,
    "scheduler_time": 75.97204387778494
}
#Debug simulation 
Total elapsed time: 6.6828665398061275. Arrivals time: 0.21415790962055326 Scheduler time: 6.313075638841838 Scheduler overhead time: 0.05242752889171243 Adapter cache time: 0.025007328018546104 Engine time: 0.05386914499104023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.600733599625528,
    "estimated_duration": 3600.020632476778,
    "input_throughput": 6319.019895269098,
    "output_throughput": 5484.891620305838,
    "total_throughput": 11803.911515574935,
    "itl": 94.48069405131282,
    "ttft": 144956.69303427322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.474750861739753,
    "arrivals": 94463,
    "finished_requests": 91745,
    "scheduler_time": 75.4696551847557
}
#Debug simulation 
Total elapsed time: 6.60082903271541. Arrivals time: 0.22167497780174017 Scheduler time: 6.213237300049514 Scheduler overhead time: 0.05593245476484299 Adapter cache time: 0.02611657977104187 Engine time: 0.05784260621294379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.438158384989947,
    "estimated_duration": 3600.053031769885,
    "input_throughput": 6065.054544284186,
    "output_throughput": 5262.966915430453,
    "total_throughput": 11328.021459714639,
    "itl": 82.9393791078241,
    "ttft": 351841.00849959586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 979,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.393833914441082,
    "arrivals": 94463,
    "finished_requests": 87991,
    "scheduler_time": 75.06218553520952
}
#Debug simulation 
Total elapsed time: 6.438287317287177. Arrivals time: 0.23422925733029842 Scheduler time: 6.018866542726755 Scheduler overhead time: 0.06289618462324142 Adapter cache time: 0.027455555740743876 Engine time: 0.06540987873449922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.636606973130256,
    "estimated_duration": 3600.083690939532,
    "input_throughput": 6320.929998730469,
    "output_throughput": 5486.279402256192,
    "total_throughput": 11807.209400986661,
    "itl": 94.47369105143405,
    "ttft": 144007.64335911127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1019,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.995257442020801,
    "arrivals": 94463,
    "finished_requests": 91768,
    "scheduler_time": 75.47401607137739
}
#Debug simulation 
Total elapsed time: 6.636730825994164. Arrivals time: 0.22308540530502796 Scheduler time: 6.246660857461393 Scheduler overhead time: 0.05629205470904708 Adapter cache time: 0.02634254703298211 Engine time: 0.05813087476417422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.432984764222056,
    "estimated_duration": 3600.048496000716,
    "input_throughput": 6065.18023972632,
    "output_throughput": 5263.181043546762,
    "total_throughput": 11328.361283273081,
    "itl": 82.93776681040329,
    "ttft": 351690.15385219466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 979,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.322119502350714,
    "arrivals": 94463,
    "finished_requests": 87993,
    "scheduler_time": 75.05825572537186
}
#Debug simulation 
Total elapsed time: 6.433107229415327. Arrivals time: 0.22333478275686502 Scheduler time: 6.024250167887658 Scheduler overhead time: 0.06270287651568651 Adapter cache time: 0.027782222256064415 Engine time: 0.06575788464397192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.639217514079064,
    "estimated_duration": 3600.024425558458,
    "input_throughput": 6322.765711921246,
    "output_throughput": 5487.3349913273305,
    "total_throughput": 11810.100703248578,
    "itl": 94.45531736902636,
    "ttft": 142904.73487547663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.4924475965602,
    "arrivals": 94463,
    "finished_requests": 91788,
    "scheduler_time": 75.470299819932
}
#Debug simulation 
Total elapsed time: 6.639324891380966. Arrivals time: 0.22445772541686893 Scheduler time: 6.248429062310606 Scheduler overhead time: 0.05617378652095795 Adapter cache time: 0.026081552263349295 Engine time: 0.05784473242238164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.422323975712061,
    "estimated_duration": 3600.0413624688476,
    "input_throughput": 6065.442255092688,
    "output_throughput": 5263.340082016617,
    "total_throughput": 11328.782337109305,
    "itl": 82.93275687438323,
    "ttft": 351569.1134100447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.242329016886678,
    "arrivals": 94463,
    "finished_requests": 87996,
    "scheduler_time": 75.0598662948994
}
#Debug simulation 
Total elapsed time: 6.4224355719052255. Arrivals time: 0.22620257083326578 Scheduler time: 6.0115930791944265 Scheduler overhead time: 0.06283894926309586 Adapter cache time: 0.02753250254318118 Engine time: 0.06509824469685555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.602923813275993,
    "estimated_duration": 3600.0959771371404,
    "input_throughput": 6472.508829759001,
    "output_throughput": 5576.098561673233,
    "total_throughput": 12048.607391432233,
    "itl": 95.02570748983835,
    "ttft": 24377.957935478065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 689,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.555949823013586,
    "arrivals": 94079,
    "finished_requests": 93455,
    "scheduler_time": 74.86113405551662
}
#Debug simulation 
Total elapsed time: 6.603023078292608. Arrivals time: 0.21404206100851297 Scheduler time: 6.234218274708837 Scheduler overhead time: 0.05365906422957778 Adapter cache time: 0.02142007416114211 Engine time: 0.054959754925221205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.662062539253384,
    "estimated_duration": 3600.0055517383694,
    "input_throughput": 6434.847576502724,
    "output_throughput": 5543.3483957722265,
    "total_throughput": 11978.19597227495,
    "itl": 93.18161322287659,
    "ttft": 56375.79685099142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 682,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.984086403353147,
    "arrivals": 94079,
    "finished_requests": 92900,
    "scheduler_time": 74.83644077067827
}
#Debug simulation 
Total elapsed time: 6.662158010993153. Arrivals time: 0.2202169420197606 Scheduler time: 6.277732003945857 Scheduler overhead time: 0.05667665833607316 Adapter cache time: 0.023126250132918358 Engine time: 0.05816115206107497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.498778128996491,
    "estimated_duration": 3600.00491831706,
    "input_throughput": 6165.019077355972,
    "output_throughput": 5315.149960668687,
    "total_throughput": 11480.169038024658,
    "itl": 82.10487647032228,
    "ttft": 271144.10879622994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 655,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.927561104097434,
    "arrivals": 94079,
    "finished_requests": 89082,
    "scheduler_time": 74.31140840892317
}
#Debug simulation 
Total elapsed time: 6.498872193042189. Arrivals time: 0.23241850221529603 Scheduler time: 6.08166121551767 Scheduler overhead time: 0.06396606285125017 Adapter cache time: 0.025046683382242918 Engine time: 0.06618778547272086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.663476431276649,
    "estimated_duration": 3600.0042991849286,
    "input_throughput": 6435.384814747387,
    "output_throughput": 5543.828101682716,
    "total_throughput": 11979.212916430104,
    "itl": 93.17280354516441,
    "ttft": 55833.06276447352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 682,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.653689811704673,
    "arrivals": 94079,
    "finished_requests": 92910,
    "scheduler_time": 74.83593180099633
}
#Debug simulation 
Total elapsed time: 6.663568916264921. Arrivals time: 0.22030232986435294 Scheduler time: 6.278796641621739 Scheduler overhead time: 0.05664613237604499 Adapter cache time: 0.023221487645059824 Engine time: 0.058218473102897406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.496284276712686,
    "estimated_duration": 3600.0322952808488,
    "input_throughput": 6164.69414151982,
    "output_throughput": 5314.808154660553,
    "total_throughput": 11479.502296180373,
    "itl": 82.10455741054061,
    "ttft": 271489.5744428978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.869725039354556,
    "arrivals": 94079,
    "finished_requests": 89077,
    "scheduler_time": 74.31626071200859
}
#Debug simulation 
Total elapsed time: 6.496372691821307. Arrivals time: 0.23323440365493298 Scheduler time: 6.079486816190183 Scheduler overhead time: 0.0633799871429801 Adapter cache time: 0.0248713456094265 Engine time: 0.06585152354091406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.679123315028846,
    "estimated_duration": 3600.0371363752406,
    "input_throughput": 6435.766110826319,
    "output_throughput": 5544.160863878268,
    "total_throughput": 11979.926974704587,
    "itl": 93.16030204102734,
    "ttft": 55446.232635671884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3474501605285125,
    "arrivals": 94079,
    "finished_requests": 92918,
    "scheduler_time": 74.83725313526732
}
#Debug simulation 
Total elapsed time: 6.6792179509066045. Arrivals time: 0.2206885484047234 Scheduler time: 6.294321162626147 Scheduler overhead time: 0.05670360988005996 Adapter cache time: 0.023242994211614132 Engine time: 0.05790473008528352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.5019865720532835,
    "estimated_duration": 3600.054884566643,
    "input_throughput": 6165.000454617138,
    "output_throughput": 5315.318408625714,
    "total_throughput": 11480.318863242852,
    "itl": 82.10070571775817,
    "ttft": 271033.5094217266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.827939238660068,
    "arrivals": 94079,
    "finished_requests": 89086,
    "scheduler_time": 74.3112560445742
}
#Debug simulation 
Total elapsed time: 6.502084626350552. Arrivals time: 0.22916816221550107 Scheduler time: 6.087767464574426 Scheduler overhead time: 0.06348088663071394 Adapter cache time: 0.0248641069047153 Engine time: 0.06711299298331141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.5313908802345395,
    "estimated_duration": 3600.079593531777,
    "input_throughput": 6439.777065388753,
    "output_throughput": 5540.916938569882,
    "total_throughput": 11980.694003958635,
    "itl": 82.9890654368605,
    "ttft": 17371.939966241516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.564088468221064,
    "arrivals": 93393,
    "finished_requests": 92946,
    "scheduler_time": 73.2046226417069
}
#Debug simulation 
Total elapsed time: 6.531483347993344. Arrivals time: 0.20317461621016264 Scheduler time: 6.164051486179233 Scheduler overhead time: 0.059088694863021374 Adapter cache time: 0.018243085127323866 Engine time: 0.059862051624804735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.543375717010349,
    "estimated_duration": 3600.082252184992,
    "input_throughput": 6439.772309626856,
    "output_throughput": 5540.91284661431,
    "total_throughput": 11980.685156241167,
    "itl": 83.01174249055309,
    "ttft": 17468.022006270297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.95457514709328,
    "arrivals": 93393,
    "finished_requests": 92946,
    "scheduler_time": 73.2242652461711
}
#Debug simulation 
Total elapsed time: 6.543495982885361. Arrivals time: 0.20940200425684452 Scheduler time: 6.168381849769503 Scheduler overhead time: 0.05938909621909261 Adapter cache time: 0.01844107359647751 Engine time: 0.06069213431328535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.607095945160836,
    "estimated_duration": 3600.078937358778,
    "input_throughput": 6336.316063318216,
    "output_throughput": 5453.325702464581,
    "total_throughput": 11789.641765782797,
    "itl": 79.95371645734237,
    "ttft": 98487.01301138905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.991530607356726,
    "arrivals": 93393,
    "finished_requests": 91514,
    "scheduler_time": 73.21162749891047
}
#Debug simulation 
Total elapsed time: 6.607196889817715. Arrivals time: 0.2321391934528947 Scheduler time: 6.19368614628911 Scheduler overhead time: 0.06444721622392535 Adapter cache time: 0.020496700890362263 Engine time: 0.06648977799341083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.527432953007519,
    "estimated_duration": 3600.0001014008635,
    "input_throughput": 6439.858151942451,
    "output_throughput": 5541.037899481659,
    "total_throughput": 11980.89605142411,
    "itl": 82.99088996659192,
    "ttft": 17467.02917550226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.675542731457384,
    "arrivals": 93393,
    "finished_requests": 92945,
    "scheduler_time": 73.2206153368103
}
#Debug simulation 
Total elapsed time: 6.5275219469331205. Arrivals time: 0.21087039122357965 Scheduler time: 6.1507007335312665 Scheduler overhead time: 0.0592410359531641 Adapter cache time: 0.018398149870336056 Engine time: 0.061268921475857496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.628219374921173,
    "estimated_duration": 3600.033975764508,
    "input_throughput": 6336.579086078107,
    "output_throughput": 5453.558530883229,
    "total_throughput": 11790.137616961336,
    "itl": 79.9494453642369,
    "ttft": 98389.42787153025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.950935640740244,
    "arrivals": 93393,
    "finished_requests": 91516,
    "scheduler_time": 73.20720310033539
}
#Debug simulation 
Total elapsed time: 6.628330435603857. Arrivals time: 0.23878919100388885 Scheduler time: 6.208139076363295 Scheduler overhead time: 0.06447457708418369 Adapter cache time: 0.020619920454919338 Engine time: 0.06633509043604136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.560524175874889,
    "estimated_duration": 3600.0458410031088,
    "input_throughput": 6439.837442053277,
    "output_throughput": 5540.968887896663,
    "total_throughput": 11980.806329949939,
    "itl": 82.97183671957939,
    "ttft": 17466.370610583366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.440933386967502,
    "arrivals": 93393,
    "finished_requests": 92946,
    "scheduler_time": 73.21969729930193
}
#Debug simulation 
Total elapsed time: 6.560639638919383. Arrivals time: 0.2090076277963817 Scheduler time: 6.185649915598333 Scheduler overhead time: 0.059500682167708874 Adapter cache time: 0.01832719799131155 Engine time: 0.06100750109180808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.618353517260402,
    "estimated_duration": 3600.0247053173166,
    "input_throughput": 6336.545126011659,
    "output_throughput": 5453.572018825768,
    "total_throughput": 11790.117144837426,
    "itl": 79.94922623340425,
    "ttft": 98419.1622628736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.916761306598818,
    "arrivals": 93393,
    "finished_requests": 91515,
    "scheduler_time": 73.20925472777411
}
#Debug simulation 
Total elapsed time: 6.6184682520106435. Arrivals time: 0.22615778911858797 Scheduler time: 6.210442586336285 Scheduler overhead time: 0.06450525671243668 Adapter cache time: 0.020636937115341425 Engine time: 0.0666740913875401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.32231471920386,
    "estimated_duration": 3600.0396657257056,
    "input_throughput": 4377.866207988838,
    "output_throughput": 3809.288861609134,
    "total_throughput": 8187.155069597972,
    "itl": 70.26072839509496,
    "ttft": 22840.095101486775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.884532730919721,
    "arrivals": 63713,
    "finished_requests": 63366,
    "scheduler_time": 46.639112116068134
}
#Debug simulation 
Total elapsed time: 5.322403648868203. Arrivals time: 0.15918245911598206 Scheduler time: 4.9579789149574935 Scheduler overhead time: 0.06969255022704601 Adapter cache time: 0.03417096845805645 Engine time: 0.06951025687158108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.355782717000693,
    "estimated_duration": 3600.0700785124754,
    "input_throughput": 4377.8292245111315,
    "output_throughput": 3809.2566813772587,
    "total_throughput": 8187.085905888391,
    "itl": 70.33481331725773,
    "ttft": 22841.696766665205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.594693220397037,
    "arrivals": 63713,
    "finished_requests": 63366,
    "scheduler_time": 46.64885451256703
}
#Debug simulation 
Total elapsed time: 5.355877394787967. Arrivals time: 0.16429694090038538 Scheduler time: 4.985322990454733 Scheduler overhead time: 0.07015844713896513 Adapter cache time: 0.03420964861288667 Engine time: 0.07005637884140015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.3562862179242074,
    "estimated_duration": 3600.0402580993405,
    "input_throughput": 4377.640212368182,
    "output_throughput": 3809.1160145080803,
    "total_throughput": 8186.756226876262,
    "itl": 70.35510149518156,
    "ttft": 22944.061005551477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.04988316529869,
    "arrivals": 63713,
    "finished_requests": 63364,
    "scheduler_time": 46.64955897288948
}
#Debug simulation 
Total elapsed time: 5.356379464734346. Arrivals time: 0.16221769619733095 Scheduler time: 4.989330030512065 Scheduler overhead time: 0.06956494133919477 Adapter cache time: 0.03409080021083355 Engine time: 0.06961333937942982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.350960150826722,
    "estimated_duration": 3600.0543253245287,
    "input_throughput": 4377.558385477796,
    "output_throughput": 3809.026409278938,
    "total_throughput": 8186.5847947567345,
    "itl": 70.2865627464562,
    "ttft": 23004.78821669101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.455237654429196,
    "arrivals": 63713,
    "finished_requests": 63363,
    "scheduler_time": 46.639307535947175
}
#Debug simulation 
Total elapsed time: 5.351046772208065. Arrivals time: 0.15837387368083 Scheduler time: 4.9857296496629715 Scheduler overhead time: 0.07032524514943361 Adapter cache time: 0.03447647066786885 Engine time: 0.07017882401123643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.365051643922925,
    "estimated_duration": 3600.0367421137325,
    "input_throughput": 4377.580044015597,
    "output_throughput": 3809.3533434183914,
    "total_throughput": 8186.933387433988,
    "itl": 70.35662504641365,
    "ttft": 22778.149746452193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.897491943998904,
    "arrivals": 63713,
    "finished_requests": 63367,
    "scheduler_time": 46.65054832919628
}
#Debug simulation 
Total elapsed time: 5.365167028736323. Arrivals time: 0.1584447962231934 Scheduler time: 4.999620140064508 Scheduler overhead time: 0.07014136202633381 Adapter cache time: 0.03463305626064539 Engine time: 0.07010486349463463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.341796719934791,
    "estimated_duration": 3600.0271413661712,
    "input_throughput": 4377.24644320874,
    "output_throughput": 3809.1407263102083,
    "total_throughput": 8186.387169518948,
    "itl": 70.23836165521318,
    "ttft": 22994.804032213997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2263,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.446813088511936,
    "arrivals": 63713,
    "finished_requests": 63362,
    "scheduler_time": 46.6324059917228
}
#Debug simulation 
Total elapsed time: 5.341893699020147. Arrivals time: 0.16034794738516212 Scheduler time: 4.975932529196143 Scheduler overhead time: 0.06998937483876944 Adapter cache time: 0.0343032144010067 Engine time: 0.0694540529511869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.384209138806909,
    "estimated_duration": 3600.059843637948,
    "input_throughput": 4377.688617552034,
    "output_throughput": 3809.3927866881313,
    "total_throughput": 8187.081404240165,
    "itl": 70.34955751795582,
    "ttft": 22719.5560508008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.73656189776926,
    "arrivals": 63713,
    "finished_requests": 63368,
    "scheduler_time": 46.649908785421275
}
#Debug simulation 
Total elapsed time: 5.38430849974975. Arrivals time: 0.16075323382392526 Scheduler time: 5.018883201759309 Scheduler overhead time: 0.06954501289874315 Adapter cache time: 0.034266309812664986 Engine time: 0.0693812589161098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.6126573770307004,
    "estimated_duration": 3600.0359350592785,
    "input_throughput": 4195.54839797767,
    "output_throughput": 3632.9215140972324,
    "total_throughput": 7828.469912074903,
    "itl": 66.47087146962104,
    "ttft": 17612.193537862273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.953197986072833,
    "arrivals": 60841,
    "finished_requests": 60559,
    "scheduler_time": 43.40416267987868
}
#Debug simulation 
Total elapsed time: 4.612746939063072. Arrivals time: 0.15272284392267466 Scheduler time: 4.244002007879317 Scheduler overhead time: 0.07013961160555482 Adapter cache time: 0.044039396569132805 Engine time: 0.06959503144025803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.649822779931128,
    "estimated_duration": 3600.020965603668,
    "input_throughput": 4195.565843730376,
    "output_throughput": 3632.936620358518,
    "total_throughput": 7828.502464088895,
    "itl": 66.57090266592027,
    "ttft": 17617.16539026276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.456379626052623,
    "arrivals": 60841,
    "finished_requests": 60559,
    "scheduler_time": 43.41866875439097
}
#Debug simulation 
Total elapsed time: 4.649911922868341. Arrivals time: 0.15342884650453925 Scheduler time: 4.279571533203125 Scheduler overhead time: 0.07035653619095683 Adapter cache time: 0.04447147250175476 Engine time: 0.0699034072458744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.639343902934343,
    "estimated_duration": 3600.0620814793797,
    "input_throughput": 4195.627924780973,
    "output_throughput": 3632.8956845726307,
    "total_throughput": 7828.523609353603,
    "itl": 66.59502294254169,
    "ttft": 17562.447899360894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.114173072349015,
    "arrivals": 60841,
    "finished_requests": 60560,
    "scheduler_time": 43.422646016374124
}
#Debug simulation 
Total elapsed time: 4.639435912948102. Arrivals time: 0.15142256626859307 Scheduler time: 4.270943361334503 Scheduler overhead time: 0.07043531956151128 Adapter cache time: 0.044461088720709085 Engine time: 0.06961071630939841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.634661592077464,
    "estimated_duration": 3600.016454028074,
    "input_throughput": 4195.571101654252,
    "output_throughput": 3632.941173189985,
    "total_throughput": 7828.512274844237,
    "itl": 66.50350243210019,
    "ttft": 17554.840115725954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.801629703548333,
    "arrivals": 60841,
    "finished_requests": 60559,
    "scheduler_time": 43.40827210056808
}
#Debug simulation 
Total elapsed time: 4.63475507684052. Arrivals time: 0.15372895495966077 Scheduler time: 4.263868073467165 Scheduler overhead time: 0.07045023050159216 Adapter cache time: 0.04441740037873387 Engine time: 0.0700988182798028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.610260176938027,
    "estimated_duration": 3600.066071299527,
    "input_throughput": 4195.623274921639,
    "output_throughput": 3632.8916583686364,
    "total_throughput": 7828.514933290276,
    "itl": 66.58502668175109,
    "ttft": 17560.137123613837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.8386631997914,
    "arrivals": 60841,
    "finished_requests": 60560,
    "scheduler_time": 43.420866727937394
}
#Debug simulation 
Total elapsed time: 4.610352206975222. Arrivals time: 0.14938947511836886 Scheduler time: 4.244401597883552 Scheduler overhead time: 0.07044757576659322 Adapter cache time: 0.044134821742773056 Engine time: 0.06973028928041458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.666859136894345,
    "estimated_duration": 3600.007332485075,
    "input_throughput": 4195.581732210991,
    "output_throughput": 3632.950378179326,
    "total_throughput": 7828.532110390317,
    "itl": 66.4386762015226,
    "ttft": 17551.57069060199,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.188233601755318,
    "arrivals": 60841,
    "finished_requests": 60559,
    "scheduler_time": 43.39919284532589
}
#Debug simulation 
Total elapsed time: 4.666952318977565. Arrivals time: 0.15413336735218763 Scheduler time: 4.296046740375459 Scheduler overhead time: 0.0701461648568511 Adapter cache time: 0.04441373376175761 Engine time: 0.06995135499164462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.641172750853002,
    "estimated_duration": 3600.059637077639,
    "input_throughput": 4195.63077356717,
    "output_throughput": 3632.898151269694,
    "total_throughput": 7828.5289248368645,
    "itl": 66.57438032955054,
    "ttft": 17557.918032149388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.618078917153813,
    "arrivals": 60841,
    "finished_requests": 60560,
    "scheduler_time": 43.419562397242856
}
#Debug simulation 
Total elapsed time: 4.641262642107904. Arrivals time: 0.15585552994161844 Scheduler time: 4.269063520245254 Scheduler overhead time: 0.07016800902783871 Adapter cache time: 0.04412958351895213 Engine time: 0.06976679712533951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.2988454033620656,
    "estimated_duration": 3600.0309113258922,
    "input_throughput": 4093.506517857218,
    "output_throughput": 3542.1076413212095,
    "total_throughput": 7635.614159178427,
    "itl": 62.88563716748108,
    "ttft": 15878.261972471042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.42573149969276,
    "arrivals": 59397,
    "finished_requests": 59137,
    "scheduler_time": 41.4777834806756
}
#Debug simulation 
Total elapsed time: 4.2989436481148005. Arrivals time: 0.1455228696577251 Scheduler time: 3.940771287307143 Scheduler overhead time: 0.0673724957741797 Adapter cache time: 0.045891046058386564 Engine time: 0.06774326739832759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.299294161144644,
    "estimated_duration": 3600.0295784770597,
    "input_throughput": 4093.3852566383584,
    "output_throughput": 3542.0170090364313,
    "total_throughput": 7635.40226567479,
    "itl": 62.98203665652308,
    "ttft": 15940.147815514858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3079,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.499169129360858,
    "arrivals": 59397,
    "finished_requests": 59136,
    "scheduler_time": 41.493031337074456
}
#Debug simulation 
Total elapsed time: 4.299418567214161. Arrivals time: 0.14459387445822358 Scheduler time: 3.942159404978156 Scheduler overhead time: 0.0675392122939229 Adapter cache time: 0.04589582420885563 Engine time: 0.0677647260017693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.331045042723417,
    "estimated_duration": 3600.0473819401786,
    "input_throughput": 4093.3650134510563,
    "output_throughput": 3541.999492553314,
    "total_throughput": 7635.364506004371,
    "itl": 63.00275496542954,
    "ttft": 15941.00599646108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3080,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.10740017888184,
    "arrivals": 59397,
    "finished_requests": 59136,
    "scheduler_time": 41.49654091854674
}
#Debug simulation 
Total elapsed time: 4.3311600419692695. Arrivals time: 0.14368294039741158 Scheduler time: 3.972902580630034 Scheduler overhead time: 0.06841473327949643 Adapter cache time: 0.04620853764936328 Engine time: 0.06819381797686219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.304144072812051,
    "estimated_duration": 3600.024249461555,
    "input_throughput": 4093.391315962404,
    "output_throughput": 3542.0222521854357,
    "total_throughput": 7635.41356814784,
    "itl": 62.91612891205532,
    "ttft": 15939.29243249281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3082,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.07179463799165,
    "arrivals": 59397,
    "finished_requests": 59136,
    "scheduler_time": 41.482836759380916
}
#Debug simulation 
Total elapsed time: 4.304237906821072. Arrivals time: 0.14702397491782904 Scheduler time: 3.9429763038642704 Scheduler overhead time: 0.0684832027181983 Adapter cache time: 0.04615983599796891 Engine time: 0.06797818467020988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.312548299785703,
    "estimated_duration": 3600.0007864575887,
    "input_throughput": 4093.4179946389872,
    "output_throughput": 3542.0453373143237,
    "total_throughput": 7635.463331953311,
    "itl": 62.99331967030417,
    "ttft": 15940.637812349918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3083,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.916899755835054,
    "arrivals": 59397,
    "finished_requests": 59136,
    "scheduler_time": 41.49470585054472
}
#Debug simulation 
Total elapsed time: 4.312639627140015. Arrivals time: 0.14666030369699 Scheduler time: 3.9524338180199265 Scheduler overhead time: 0.06780769862234592 Adapter cache time: 0.04621759569272399 Engine time: 0.06797101208940148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.320177897810936,
    "estimated_duration": 3600.001945869621,
    "input_throughput": 4093.539454029426,
    "output_throughput": 3542.136140962469,
    "total_throughput": 7635.675594991895,
    "itl": 62.856732501726285,
    "ttft": 15877.795011385339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3090,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.72631570636447,
    "arrivals": 59397,
    "finished_requests": 59137,
    "scheduler_time": 41.472624875993695
}
#Debug simulation 
Total elapsed time: 4.320267342031002. Arrivals time: 0.14520932594314218 Scheduler time: 3.961090396158397 Scheduler overhead time: 0.0681690420024097 Adapter cache time: 0.0460767038166523 Engine time: 0.06817649397999048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.307960017118603,
    "estimated_duration": 3600.069831617755,
    "input_throughput": 4093.4622630299873,
    "output_throughput": 3542.0693476575702,
    "total_throughput": 7635.531610687557,
    "itl": 62.98464210805936,
    "ttft": 15940.635275115581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3074,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.636424698345795,
    "arrivals": 59397,
    "finished_requests": 59137,
    "scheduler_time": 41.49441153866587
}
#Debug simulation 
Total elapsed time: 4.308066187892109. Arrivals time: 0.14756301697343588 Scheduler time: 3.947529552038759 Scheduler overhead time: 0.06796488957479596 Adapter cache time: 0.0460181487724185 Engine time: 0.06755785178393126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.1969087570905685,
    "estimated_duration": 3600.025042110839,
    "input_throughput": 4036.8702522910276,
    "output_throughput": 3488.661565708442,
    "total_throughput": 7525.53181799947,
    "itl": 59.94378726209187,
    "ttft": 14954.441247795472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.038144374830102,
    "arrivals": 58689,
    "finished_requests": 58447,
    "scheduler_time": 40.189052810261956
}
#Debug simulation 
Total elapsed time: 4.197030341252685. Arrivals time: 0.14130043890327215 Scheduler time: 3.8440002514980733 Scheduler overhead time: 0.06828077230602503 Adapter cache time: 0.04299138393253088 Engine time: 0.06869153212755919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.19069178821519,
    "estimated_duration": 3600.017094837589,
    "input_throughput": 4036.8791639462015,
    "output_throughput": 3488.669267157077,
    "total_throughput": 7525.548431103279,
    "itl": 60.00197457543838,
    "ttft": 14954.809643617931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.432167648453929,
    "arrivals": 58689,
    "finished_requests": 58447,
    "scheduler_time": 40.19865568557821
}
#Debug simulation 
Total elapsed time: 4.190802240278572. Arrivals time: 0.141338384244591 Scheduler time: 3.839001791086048 Scheduler overhead time: 0.06798913702368736 Adapter cache time: 0.04274505563080311 Engine time: 0.06773464661091566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.205362436827272,
    "estimated_duration": 3600.007898844294,
    "input_throughput": 4036.8894758996107,
    "output_throughput": 3488.678178742854,
    "total_throughput": 7525.567654642465,
    "itl": 60.01766844567422,
    "ttft": 14954.829138630887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.850696483789617,
    "arrivals": 58689,
    "finished_requests": 58447,
    "scheduler_time": 40.201407280993365
}
#Debug simulation 
Total elapsed time: 4.205454433802515. Arrivals time: 0.143076591193676 Scheduler time: 3.8508992847055197 Scheduler overhead time: 0.06877619260922074 Adapter cache time: 0.043062757700681686 Engine time: 0.06775702489539981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.237677329219878,
    "estimated_duration": 3600.0026114376083,
    "input_throughput": 4036.8954049720887,
    "output_throughput": 3488.6833026447835,
    "total_throughput": 7525.578707616872,
    "itl": 59.963074080673934,
    "ttft": 14954.583090563694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.493173086568953,
    "arrivals": 58689,
    "finished_requests": 58447,
    "scheduler_time": 40.19196552188057
}
#Debug simulation 
Total elapsed time: 4.237771259155124. Arrivals time: 0.14363780850544572 Scheduler time: 3.882158037740737 Scheduler overhead time: 0.06832704693078995 Adapter cache time: 0.04297445435076952 Engine time: 0.06851453892886639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.221319668926299,
    "estimated_duration": 3600.0309669748135,
    "input_throughput": 4036.8636084850864,
    "output_throughput": 3488.655824134156,
    "total_throughput": 7525.519432619242,
    "itl": 60.01184295429488,
    "ttft": 14954.758627306475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.700954116875911,
    "arrivals": 58689,
    "finished_requests": 58447,
    "scheduler_time": 40.20061088019291
}
#Debug simulation 
Total elapsed time: 4.22140749078244. Arrivals time: 0.139699581079185 Scheduler time: 3.870368566829711 Scheduler overhead time: 0.06846423353999853 Adapter cache time: 0.04301331611350179 Engine time: 0.06797193130478263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.194551520980895,
    "estimated_duration": 3600.0488975997237,
    "input_throughput": 4036.843502233967,
    "output_throughput": 3488.638448320437,
    "total_throughput": 7525.481950554404,
    "itl": 59.92561480262309,
    "ttft": 14954.268128729225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.527528473069657,
    "arrivals": 58689,
    "finished_requests": 58447,
    "scheduler_time": 40.18568667443535
}
#Debug simulation 
Total elapsed time: 4.194647301919758. Arrivals time: 0.14134722901508212 Scheduler time: 3.8421261915937066 Scheduler overhead time: 0.06814521364867687 Adapter cache time: 0.04271650640293956 Engine time: 0.0681891473941505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.204544715117663,
    "estimated_duration": 3600.041100001742,
    "input_throughput": 4036.8522459349056,
    "output_throughput": 3488.6460046230923,
    "total_throughput": 7525.498250557998,
    "itl": 60.0058983170348,
    "ttft": 14954.70465300112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.561926981135873,
    "arrivals": 58689,
    "finished_requests": 58447,
    "scheduler_time": 40.19981388755432
}
#Debug simulation 
Total elapsed time: 4.20466418704018. Arrivals time: 0.14296690141782165 Scheduler time: 3.850934326183051 Scheduler overhead time: 0.06803114665672183 Adapter cache time: 0.04288571421056986 Engine time: 0.06799390213564038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.236735297366977,
    "estimated_duration": 3599.289908629716,
    "input_throughput": 4016.951778553555,
    "output_throughput": 3513.1235107465895,
    "total_throughput": 7530.0752893001445,
    "itl": 59.164641741152586,
    "ttft": 13980.472886110843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.496945606055876,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.37334349471239
}
#Debug simulation 
Total elapsed time: 4.236852832138538. Arrivals time: 0.1444463487714529 Scheduler time: 3.8834595014341176 Scheduler overhead time: 0.06854825606569648 Adapter cache time: 0.03961087204515934 Engine time: 0.06884104898199439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.232440934982151,
    "estimated_duration": 3599.285547869471,
    "input_throughput": 4016.956645342641,
    "output_throughput": 3513.1277671161215,
    "total_throughput": 7530.084412458762,
    "itl": 59.19322479991749,
    "ttft": 13980.54984936964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.291429798589064,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.378805387102496
}
#Debug simulation 
Total elapsed time: 4.23253349494189. Arrivals time: 0.1439776485785842 Scheduler time: 3.8800681168213487 Scheduler overhead time: 0.06828348292037845 Adapter cache time: 0.039581501856446266 Engine time: 0.06841886602342129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.17933270521462,
    "estimated_duration": 3599.300801043388,
    "input_throughput": 4016.939622220175,
    "output_throughput": 3513.1128791276515,
    "total_throughput": 7530.052501347826,
    "itl": 59.205677938034015,
    "ttft": 13980.474052860207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.5336039262311,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.38075576203808
}
#Debug simulation 
Total elapsed time: 4.179426511283964. Arrivals time: 0.14448867458850145 Scheduler time: 3.828156924340874 Scheduler overhead time: 0.06778207654133439 Adapter cache time: 0.039166671223938465 Engine time: 0.06781658437103033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.1843899236992,
    "estimated_duration": 3599.2912218103247,
    "input_throughput": 4016.950312991905,
    "output_throughput": 3513.1222290037726,
    "total_throughput": 7530.072541995678,
    "itl": 59.17471086074797,
    "ttft": 13980.554278564292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.778619075510667,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.375196057127745
}
#Debug simulation 
Total elapsed time: 4.184487425722182. Arrivals time: 0.1470382777042687 Scheduler time: 3.8303638556972146 Scheduler overhead time: 0.06817033141851425 Adapter cache time: 0.03914032969623804 Engine time: 0.06787891825661063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.243744184263051,
    "estimated_duration": 3599.2536952464075,
    "input_throughput": 4016.9921945472042,
    "output_throughput": 3513.1588575431974,
    "total_throughput": 7530.151052090401,
    "itl": 59.199968780107916,
    "ttft": 13980.500245592633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.445737726143514,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.37951226940179
}
#Debug simulation 
Total elapsed time: 4.243854684289545. Arrivals time: 0.15047095576301217 Scheduler time: 3.8838602877222 Scheduler overhead time: 0.06877061957493424 Adapter cache time: 0.039417387917637825 Engine time: 0.06900247186422348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.193784961011261,
    "estimated_duration": 3599.2701455264178,
    "input_throughput": 4016.973835089945,
    "output_throughput": 3513.142800830422,
    "total_throughput": 7530.116635920367,
    "itl": 59.15269572994105,
    "ttft": 13980.703556180317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.184186645811407,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.370864530459926
}
#Debug simulation 
Total elapsed time: 4.193883060943335. Arrivals time: 0.1459088046103716 Scheduler time: 3.840836226940155 Scheduler overhead time: 0.06772785587236285 Adapter cache time: 0.03925674455240369 Engine time: 0.06824632221832871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.252521886024624,
    "estimated_duration": 3599.291076467529,
    "input_throughput": 4016.950475200178,
    "output_throughput": 3513.1223708669886,
    "total_throughput": 7530.072846067166,
    "itl": 59.19754075359728,
    "ttft": 13980.661731221257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.372777617964848,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.37947861023483
}
#Debug simulation 
Total elapsed time: 4.25261635100469. Arrivals time: 0.13984535913914442 Scheduler time: 3.903552745934576 Scheduler overhead time: 0.06861652294173837 Adapter cache time: 0.03931838320568204 Engine time: 0.06901973811909556 
