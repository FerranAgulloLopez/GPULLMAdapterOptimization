INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 71.36075905384496,
    "estimated_duration": 3600.0122152259737,
    "input_throughput": 6695.217560112378,
    "output_throughput": 5808.4925133198285,
    "total_throughput": 12503.710073432207,
    "itl": 98.4462624722981,
    "ttft": 1972224.0262357134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.902389411716727,
    "arrivals": 743719,
    "finished_requests": 97365,
    "scheduler_time": 250.61622870079637
}
#Debug simulation 
Total elapsed time: 71.36096514109522. Arrivals time: 0.4568621004000306 Scheduler time: 70.70814585825428 Scheduler overhead time: 0.07438128348439932 Adapter cache time: 0.018032357096672058 Engine time: 0.07357608573511243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 69.57682526484132,
    "estimated_duration": 3600.029124381959,
    "input_throughput": 6443.42787198487,
    "output_throughput": 5600.001917612551,
    "total_throughput": 12043.42978959742,
    "itl": 91.89420121838052,
    "ttft": 1988859.5390701091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 509,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8270594856934954,
    "arrivals": 743719,
    "finished_requests": 93759,
    "scheduler_time": 261.43812684123264
}
#Debug simulation 
Total elapsed time: 69.57709276396781. Arrivals time: 0.4478539624251425 Scheduler time: 68.92696460196748 Scheduler overhead time: 0.07747824769467115 Adapter cache time: 0.017977085430175066 Engine time: 0.07586211152374744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 73.24150115996599,
    "estimated_duration": 3600.002267264459,
    "input_throughput": 6643.3094271835125,
    "output_throughput": 5760.195538920382,
    "total_throughput": 12403.504966103896,
    "itl": 97.6527873954681,
    "ttft": 1975008.4119325355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.420743077760558,
    "arrivals": 743719,
    "finished_requests": 96636,
    "scheduler_time": 253.21907151776352
}
#Debug simulation 
Total elapsed time: 73.24166375305504. Arrivals time: 0.4470789101906121 Scheduler time: 72.5985294659622 Scheduler overhead time: 0.07489783642813563 Adapter cache time: 0.01760953664779663 Engine time: 0.07424473809078336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 69.76460723485798,
    "estimated_duration": 3600.0458980083545,
    "input_throughput": 6492.547501389039,
    "output_throughput": 5635.731480874835,
    "total_throughput": 12128.278982263873,
    "itl": 92.26376898833955,
    "ttft": 1983420.3054552733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 554,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.129744509528426,
    "arrivals": 743719,
    "finished_requests": 94454,
    "scheduler_time": 259.42222171756697
}
#Debug simulation 
Total elapsed time: 69.76482976879925. Arrivals time: 0.4469796810299158 Scheduler time: 69.11401635641232 Scheduler overhead time: 0.07836550706997514 Adapter cache time: 0.01844283612444997 Engine time: 0.07625004136934876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 72.03953748801723,
    "estimated_duration": 3600.071118530275,
    "input_throughput": 6688.260650203461,
    "output_throughput": 5793.985816734523,
    "total_throughput": 12482.246466937984,
    "itl": 98.00201892974509,
    "ttft": 1973804.7596960221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5749957267194823,
    "arrivals": 743719,
    "finished_requests": 97302,
    "scheduler_time": 251.54781645807398
}
#Debug simulation 
Total elapsed time: 72.03970150463283. Arrivals time: 0.47063948726281524 Scheduler time: 71.37332313600928 Scheduler overhead time: 0.07474721968173981 Adapter cache time: 0.01811320846900344 Engine time: 0.07337553380057216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_384_slots_32_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 69.24596332991496,
    "estimated_duration": 3600.022140644019,
    "input_throughput": 6501.548625423975,
    "output_throughput": 5642.413353704035,
    "total_throughput": 12143.961979128011,
    "itl": 92.20209850060812,
    "ttft": 1987015.609717424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.172652813661879,
    "arrivals": 743719,
    "finished_requests": 94545,
    "scheduler_time": 259.0998504532016
}
#Debug simulation 
Total elapsed time: 69.24619765207171. Arrivals time: 0.4428925304673612 Scheduler time: 68.60197729850188 Scheduler overhead time: 0.07620251504704356 Adapter cache time: 0.01852517481893301 Engine time: 0.07589725404977798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 70.58385341381654,
    "estimated_duration": 3600.0859879002637,
    "input_throughput": 6717.299831525568,
    "output_throughput": 5840.858821337282,
    "total_throughput": 12558.15865286285,
    "itl": 100.68499867108562,
    "ttft": 1967696.633070753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4053906514542605,
    "arrivals": 740875,
    "finished_requests": 97862,
    "scheduler_time": 249.05015666437953
}
#Debug simulation 
Total elapsed time: 70.58401115471497. Arrivals time: 0.466817413456738 Scheduler time: 69.92543302755803 Scheduler overhead time: 0.07356296805664897 Adapter cache time: 0.017068364191800356 Engine time: 0.07229932770133018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 69.65983624197543,
    "estimated_duration": 3600.0410298658585,
    "input_throughput": 6646.408694095234,
    "output_throughput": 5781.9627130126355,
    "total_throughput": 12428.371407107868,
    "itl": 98.29833614962718,
    "ttft": 1973585.9083999903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7314022896112915,
    "arrivals": 740875,
    "finished_requests": 96811,
    "scheduler_time": 252.0051719558573
}
#Debug simulation 
Total elapsed time: 69.66006238618866. Arrivals time: 0.46687296545132995 Scheduler time: 68.99836354702711 Scheduler overhead time: 0.07464454043656588 Adapter cache time: 0.017291957512497902 Engine time: 0.07339135324582458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 66.63604549504817,
    "estimated_duration": 3600.04721799489,
    "input_throughput": 6497.493111500962,
    "output_throughput": 5650.925326288005,
    "total_throughput": 12148.418437788967,
    "itl": 92.25379855704901,
    "ttft": 1987781.8801887156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 536,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.01606375807434,
    "arrivals": 740875,
    "finished_requests": 94609,
    "scheduler_time": 258.7269851916496
}
#Debug simulation 
Total elapsed time: 66.6362014496699. Arrivals time: 0.4497729716822505 Scheduler time: 65.987559158355 Scheduler overhead time: 0.07637355150654912 Adapter cache time: 0.017724056262522936 Engine time: 0.0744103193283081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 70.21704261703417,
    "estimated_duration": 3600.033163561247,
    "input_throughput": 6655.267024345673,
    "output_throughput": 5790.395547184063,
    "total_throughput": 12445.662571529736,
    "itl": 98.33447328090358,
    "ttft": 1973394.779600021,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 483,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.311102053918871,
    "arrivals": 740875,
    "finished_requests": 96968,
    "scheduler_time": 251.64150365629453
}
#Debug simulation 
Total elapsed time: 70.2174239079468. Arrivals time: 0.7816051919944584 Scheduler time: 69.24266681075096 Scheduler overhead time: 0.07421194249764085 Adapter cache time: 0.017018447164446115 Engine time: 0.07231308333575726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 66.60132964421064,
    "estimated_duration": 3600.090370402849,
    "input_throughput": 6495.97498781207,
    "output_throughput": 5646.409092148803,
    "total_throughput": 12142.384079960873,
    "itl": 92.24758155901215,
    "ttft": 1987757.3412939908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.019732515700196,
    "arrivals": 740875,
    "finished_requests": 94582,
    "scheduler_time": 258.83906825704133
}
#Debug simulation 
Total elapsed time: 66.6014916440472. Arrivals time: 0.5047817248851061 Scheduler time: 65.89676039107144 Scheduler overhead time: 0.07679831981658936 Adapter cache time: 0.017686265986412764 Engine time: 0.07509106025099754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 69.81245136400685,
    "estimated_duration": 3600.087053955909,
    "input_throughput": 6648.825887056772,
    "output_throughput": 5788.989179330895,
    "total_throughput": 12437.815066387668,
    "itl": 98.34551054839899,
    "ttft": 1973637.5950087726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.045130288652131,
    "arrivals": 740875,
    "finished_requests": 96923,
    "scheduler_time": 251.7081042825567
}
#Debug simulation 
Total elapsed time: 69.8126940750517. Arrivals time: 0.5046359202824533 Scheduler time: 69.11201054835692 Scheduler overhead time: 0.07516371831297874 Adapter cache time: 0.0172597817145288 Engine time: 0.07384292408823967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_384_slots_32_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 66.52912760199979,
    "estimated_duration": 3600.089856949568,
    "input_throughput": 6502.286590100499,
    "output_throughput": 5650.963117136715,
    "total_throughput": 12153.249707237213,
    "itl": 92.28853455054147,
    "ttft": 1988305.6924347258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 550,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.045608335398171,
    "arrivals": 740875,
    "finished_requests": 94674,
    "scheduler_time": 258.61441656064585
}
#Debug simulation 
Total elapsed time: 66.52928651031107. Arrivals time: 0.4397138864733279 Scheduler time: 65.89077238319442 Scheduler overhead time: 0.07603981252759695 Adapter cache time: 0.017659739591181278 Engine time: 0.07473091455176473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.08794686803594,
    "estimated_duration": 3600.0564101089763,
    "input_throughput": 6577.373880450729,
    "output_throughput": 5718.485394337702,
    "total_throughput": 12295.85927478843,
    "itl": 97.43429493063067,
    "ttft": 1959628.372455682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.8336710023554925,
    "arrivals": 599139,
    "finished_requests": 95652,
    "scheduler_time": 251.94827125017255
}
#Debug simulation 
Total elapsed time: 83.08819091785699. Arrivals time: 0.5128721939399838 Scheduler time: 82.36661258013919 Scheduler overhead time: 0.08027212927117944 Adapter cache time: 0.020543276332318783 Engine time: 0.07717593759298325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.96540282806382,
    "estimated_duration": 3600.0987555152196,
    "input_throughput": 6409.039186675849,
    "output_throughput": 5578.110869663087,
    "total_throughput": 11987.150056338936,
    "itl": 92.91041583207159,
    "ttft": 1972008.5193983375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 655,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.790897223358049,
    "arrivals": 599139,
    "finished_requests": 93202,
    "scheduler_time": 259.587474318869
}
#Debug simulation 
Total elapsed time: 85.96557255834341. Arrivals time: 0.48934360314160585 Scheduler time: 85.26442108489573 Scheduler overhead time: 0.08151844143867493 Adapter cache time: 0.020166711881756783 Engine time: 0.07885456969961524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.82995649892837,
    "estimated_duration": 3600.03711765033,
    "input_throughput": 6286.749347398892,
    "output_throughput": 5480.247662800981,
    "total_throughput": 11766.997010199873,
    "itl": 88.63169885093711,
    "ttft": 1979157.0425197773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.97725839213471,
    "arrivals": 599139,
    "finished_requests": 91429,
    "scheduler_time": 264.3269507503182
}
#Debug simulation 
Total elapsed time: 81.83012333093211. Arrivals time: 0.4944959622807801 Scheduler time: 81.12038251524791 Scheduler overhead time: 0.08257816964760423 Adapter cache time: 0.02085069427266717 Engine time: 0.08013310795649886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.87702940404415,
    "estimated_duration": 3600.081253318806,
    "input_throughput": 6512.583841930234,
    "output_throughput": 5658.578672695311,
    "total_throughput": 12171.162514625545,
    "itl": 95.04492258581556,
    "ttft": 1964252.1545236085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 725,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.975397925241843,
    "arrivals": 599139,
    "finished_requests": 94697,
    "scheduler_time": 255.0520277717905
}
#Debug simulation 
Total elapsed time: 81.87719784444198. Arrivals time: 0.4956739563494921 Scheduler time: 81.17286034952849 Scheduler overhead time: 0.0797848617658019 Adapter cache time: 0.020774612668901682 Engine time: 0.0771720982156694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 79.12982985377312,
    "estimated_duration": 3600.0415069779187,
    "input_throughput": 6335.306400160317,
    "output_throughput": 5515.764182582741,
    "total_throughput": 11851.070582743057,
    "itl": 89.27209971676909,
    "ttft": 1982150.8555777932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.325239733066439,
    "arrivals": 599139,
    "finished_requests": 92179,
    "scheduler_time": 262.52487220500126
}
#Debug simulation 
Total elapsed time: 79.13000212190673. Arrivals time: 0.4972939873114228 Scheduler time: 78.41851959750056 Scheduler overhead time: 0.08180506015196443 Adapter cache time: 0.020903305616229773 Engine time: 0.07983904425054789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.11668928898871,
    "estimated_duration": 3600.056848958611,
    "input_throughput": 6539.163959816305,
    "output_throughput": 5686.988250177873,
    "total_throughput": 12226.152209994178,
    "itl": 95.74022400019882,
    "ttft": 1964186.8272571787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 742,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.736869337903313,
    "arrivals": 599139,
    "finished_requests": 95119,
    "scheduler_time": 253.6417887731027
}
#Debug simulation 
Total elapsed time: 81.11685511097312. Arrivals time: 0.4944069301709533 Scheduler time: 80.41331547033042 Scheduler overhead time: 0.07951482431963086 Adapter cache time: 0.02063092403113842 Engine time: 0.07828619424253702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.00744399800897,
    "estimated_duration": 3600.086749644157,
    "input_throughput": 6308.078549008489,
    "output_throughput": 5485.996136607584,
    "total_throughput": 11794.074685616073,
    "itl": 88.52835847605895,
    "ttft": 1981958.515232444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.963934203442216,
    "arrivals": 599139,
    "finished_requests": 91642,
    "scheduler_time": 264.14325572858996
}
#Debug simulation 
Total elapsed time: 81.00761169102043. Arrivals time: 0.4831452099606395 Scheduler time: 80.30798032367602 Scheduler overhead time: 0.08313853340223432 Adapter cache time: 0.020667657256126404 Engine time: 0.08066653646528721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.2913537779823,
    "estimated_duration": 3600.0378305737154,
    "input_throughput": 6541.89708785555,
    "output_throughput": 5689.099660582199,
    "total_throughput": 12230.996748437748,
    "itl": 96.70909223753394,
    "ttft": 1948585.696258708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.542725004949686,
    "arrivals": 576120,
    "finished_requests": 95362,
    "scheduler_time": 253.51890972826527
}
#Debug simulation 
Total elapsed time: 84.29151178197935. Arrivals time: 0.5025766468606889 Scheduler time: 83.57952171284705 Scheduler overhead time: 0.08052918501198292 Adapter cache time: 0.02048856532201171 Engine time: 0.07777843531221151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.42302862228826,
    "estimated_duration": 3600.0522986539327,
    "input_throughput": 6484.350521443362,
    "output_throughput": 5636.115899645844,
    "total_throughput": 12120.466421089206,
    "itl": 94.36162310683565,
    "ttft": 1952076.3904313713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 693,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.066803522459238,
    "arrivals": 576120,
    "finished_requests": 94517,
    "scheduler_time": 256.19809196140466
}
#Debug simulation 
Total elapsed time: 82.42319429293275. Arrivals time: 0.48730554338544607 Scheduler time: 81.72311695385724 Scheduler overhead time: 0.08115647407248616 Adapter cache time: 0.0207306076772511 Engine time: 0.080176523886621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.91265902621672,
    "estimated_duration": 3600.061222120515,
    "input_throughput": 6308.058279804382,
    "output_throughput": 5484.632838652047,
    "total_throughput": 11792.69111845643,
    "itl": 88.81725127218854,
    "ttft": 1963603.6682881005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.139552748804007,
    "arrivals": 576120,
    "finished_requests": 91930,
    "scheduler_time": 264.1425177906657
}
#Debug simulation 
Total elapsed time: 79.91282017109916. Arrivals time: 0.48335570376366377 Scheduler time: 79.21182481385767 Scheduler overhead time: 0.08292824216187 Adapter cache time: 0.021214534994214773 Engine time: 0.08116449043154716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 82.30078866612166,
    "estimated_duration": 3600.0985256748672,
    "input_throughput": 6484.670025974833,
    "output_throughput": 5636.410185800726,
    "total_throughput": 12121.08021177556,
    "itl": 94.35418987194068,
    "ttft": 1951946.3419394798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 693,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.755842024437144,
    "arrivals": 576120,
    "finished_requests": 94522,
    "scheduler_time": 256.222637233324
}
#Debug simulation 
Total elapsed time: 82.30094515578821. Arrivals time: 0.4864771827124059 Scheduler time: 81.60160840116441 Scheduler overhead time: 0.08151284465566278 Adapter cache time: 0.02053727675229311 Engine time: 0.07977857207879424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 79.68983329460025,
    "estimated_duration": 3600.013849604037,
    "input_throughput": 6308.141287428044,
    "output_throughput": 5484.705010835372,
    "total_throughput": 11792.846298263416,
    "itl": 88.81620391469067,
    "ttft": 1963583.9734229941,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.093158501242314,
    "arrivals": 576120,
    "finished_requests": 91930,
    "scheduler_time": 264.1419981509511
}
#Debug simulation 
Total elapsed time: 79.6899893488735. Arrivals time: 0.480311734136194 Scheduler time: 78.99384711496532 Scheduler overhead time: 0.08263576682657003 Adapter cache time: 0.020956558641046286 Engine time: 0.08015766413882375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.65773637220263,
    "estimated_duration": 3600.0136362537633,
    "input_throughput": 6485.289601373685,
    "output_throughput": 5636.8922594185315,
    "total_throughput": 12122.181860792218,
    "itl": 94.34667924753793,
    "ttft": 1951859.4448385492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 693,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4240572118153585,
    "arrivals": 576120,
    "finished_requests": 94527,
    "scheduler_time": 256.2380681915413
}
#Debug simulation 
Total elapsed time: 82.65790367824957. Arrivals time: 0.7660395409911871 Scheduler time: 81.6824396327138 Scheduler overhead time: 0.07978767529129982 Adapter cache time: 0.020505443215370178 Engine time: 0.07824860094115138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.64341897005215,
    "estimated_duration": 3600.06741587703,
    "input_throughput": 6308.181313451192,
    "output_throughput": 5484.661735199697,
    "total_throughput": 11792.84304865089,
    "itl": 88.81524399545864,
    "ttft": 1963586.812132923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.044485964737861,
    "arrivals": 576120,
    "finished_requests": 91931,
    "scheduler_time": 264.1487638348199
}
#Debug simulation 
Total elapsed time: 79.64358164090663. Arrivals time: 0.4773288406431675 Scheduler time: 78.9493275503628 Scheduler overhead time: 0.08289534086361527 Adapter cache time: 0.020910984370857477 Engine time: 0.08113718638196588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.65141660813242,
    "estimated_duration": 3600.0602045122523,
    "input_throughput": 6458.106164685633,
    "output_throughput": 5633.851893526291,
    "total_throughput": 12091.958058211925,
    "itl": 96.44870472568103,
    "ttft": 1958736.8122217546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5228877778538354,
    "arrivals": 564654,
    "finished_requests": 93883,
    "scheduler_time": 255.84161813859654
}
#Debug simulation 
Total elapsed time: 82.6515782782808. Arrivals time: 0.47689306596294045 Scheduler time: 81.96569698303938 Scheduler overhead time: 0.0806016786955297 Adapter cache time: 0.020084534771740437 Engine time: 0.07784541556611657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 83.59122632863,
    "estimated_duration": 3600.0608262125784,
    "input_throughput": 6423.520911542092,
    "output_throughput": 5601.081196512353,
    "total_throughput": 12024.602108054445,
    "itl": 94.44522640793888,
    "ttft": 1961449.345349412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 680,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.9810361082851955,
    "arrivals": 564654,
    "finished_requests": 93353,
    "scheduler_time": 257.5841186490621
}
#Debug simulation 
Total elapsed time: 83.59138064458966. Arrivals time: 0.47137688426300883 Scheduler time: 82.91048305248842 Scheduler overhead time: 0.08022910123690963 Adapter cache time: 0.02025232743471861 Engine time: 0.07813336374238133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.48183978302404,
    "estimated_duration": 3600.007902535391,
    "input_throughput": 6168.0631268507905,
    "output_throughput": 5384.387069358507,
    "total_throughput": 11552.450196209296,
    "itl": 87.32983572262295,
    "ttft": 1981669.9364134495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.901624077581836,
    "arrivals": 564654,
    "finished_requests": 89691,
    "scheduler_time": 268.8028265503546
}
#Debug simulation 
Total elapsed time: 79.4819958419539. Arrivals time: 0.4588048066943884 Scheduler time: 78.80503500718623 Scheduler overhead time: 0.08333586063235998 Adapter cache time: 0.020653901156038046 Engine time: 0.08144046133384109 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.62918165512383,
    "estimated_duration": 3600.0759077752514,
    "input_throughput": 6426.431995512338,
    "output_throughput": 5617.334333513306,
    "total_throughput": 12043.766329025644,
    "itl": 94.86323204388671,
    "ttft": 1959889.8996277405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 658,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.512969697890798,
    "arrivals": 564654,
    "finished_requests": 93548,
    "scheduler_time": 256.68740896485207
}
#Debug simulation 
Total elapsed time: 81.62935376306996. Arrivals time: 0.4740278134122491 Scheduler time: 80.9453951464966 Scheduler overhead time: 0.08009500568732619 Adapter cache time: 0.02003775490447879 Engine time: 0.07906370516866446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 78.2633226220496,
    "estimated_duration": 3600.058679246658,
    "input_throughput": 6005.41072417301,
    "output_throughput": 5255.900996580282,
    "total_throughput": 11261.31172075329,
    "itl": 83.08066051413195,
    "ttft": 1988010.8940853733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.810346250762265,
    "arrivals": 564654,
    "finished_requests": 87478,
    "scheduler_time": 275.45792239281315
}
#Debug simulation 
Total elapsed time: 78.26348419813439. Arrivals time: 0.46267283987253904 Scheduler time: 77.57817518059164 Scheduler overhead time: 0.08490988286212087 Adapter cache time: 0.02078039711341262 Engine time: 0.08324700966477394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 80.8385222768411,
    "estimated_duration": 3600.0968249851817,
    "input_throughput": 6412.195316467639,
    "output_throughput": 5587.991650775339,
    "total_throughput": 12000.186967242978,
    "itl": 94.20995147388736,
    "ttft": 1963806.5292799352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 671,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2836109511228075,
    "arrivals": 564654,
    "finished_requests": 93166,
    "scheduler_time": 258.1027809686697
}
#Debug simulation 
Total elapsed time: 80.8386848079972. Arrivals time: 0.4679340645670891 Scheduler time: 80.16103500453755 Scheduler overhead time: 0.0805862913839519 Adapter cache time: 0.020266477018594742 Engine time: 0.0778783317655325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 77.07295022392645,
    "estimated_duration": 3600.0144589251013,
    "input_throughput": 6249.82628728606,
    "output_throughput": 5458.926686052198,
    "total_throughput": 11708.752973338258,
    "itl": 89.01534431127237,
    "ttft": 1974479.7491944798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.291006826385886,
    "arrivals": 564654,
    "finished_requests": 90945,
    "scheduler_time": 264.8419160429712
}
#Debug simulation 
Total elapsed time: 77.07310976693407. Arrivals time: 0.47260747477412224 Scheduler time: 76.38687841035426 Scheduler overhead time: 0.08106426009908319 Adapter cache time: 0.020825377199798822 Engine time: 0.07971643097698689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.71931904414669,
    "estimated_duration": 3600.0882147397524,
    "input_throughput": 6501.079863592146,
    "output_throughput": 5703.311634402346,
    "total_throughput": 12204.391497994491,
    "itl": 97.99394624752907,
    "ttft": 1940128.4364574736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 699,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.622073913333088,
    "arrivals": 558926,
    "finished_requests": 94948,
    "scheduler_time": 252.84286994487567
}
#Debug simulation 
Total elapsed time: 81.71948283584788. Arrivals time: 0.48808279959484935 Scheduler time: 81.02540824748576 Scheduler overhead time: 0.07852352084591985 Adapter cache time: 0.020088703837245703 Engine time: 0.07695722347125411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.31119390204549,
    "estimated_duration": 3600.0475151184473,
    "input_throughput": 6433.930358621369,
    "output_throughput": 5660.246959082027,
    "total_throughput": 12094.177317703396,
    "itl": 95.5444314197655,
    "ttft": 1944009.0111113861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 661,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.841694737761289,
    "arrivals": 558926,
    "finished_requests": 93986,
    "scheduler_time": 255.96675545870406
}
#Debug simulation 
Total elapsed time: 80.31135788094252. Arrivals time: 0.48392114881426096 Scheduler time: 79.61907161353156 Scheduler overhead time: 0.07939898874610662 Adapter cache time: 0.01988027337938547 Engine time: 0.0782670471817255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.58154975809157,
    "estimated_duration": 3600.0874901437205,
    "input_throughput": 6274.36501525085,
    "output_throughput": 5501.6460167220275,
    "total_throughput": 11776.011031972877,
    "itl": 89.90709457580206,
    "ttft": 1959119.0473912207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 692,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.195617303820355,
    "arrivals": 558926,
    "finished_requests": 91602,
    "scheduler_time": 263.23846567586827
}
#Debug simulation 
Total elapsed time: 75.58171363780275. Arrivals time: 0.46845979476347566 Scheduler time: 74.90061735454947 Scheduler overhead time: 0.08115365030243993 Adapter cache time: 0.020465176086872816 Engine time: 0.07922641094774008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 80.64322688896209,
    "estimated_duration": 3600.0485131417963,
    "input_throughput": 6429.99557242029,
    "output_throughput": 5643.012288817965,
    "total_throughput": 12073.007861238255,
    "itl": 95.50008934024261,
    "ttft": 1945266.4863895366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6722938682837345,
    "arrivals": 558926,
    "finished_requests": 93984,
    "scheduler_time": 256.01840773828076
}
#Debug simulation 
Total elapsed time: 80.64338725898415. Arrivals time: 0.4775092527270317 Scheduler time: 79.9574152850546 Scheduler overhead time: 0.0796175068244338 Adapter cache time: 0.020520587451756 Engine time: 0.0778787755407393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 76.31139563024044,
    "estimated_duration": 3600.0244270432645,
    "input_throughput": 6303.716671900036,
    "output_throughput": 5518.74394261193,
    "total_throughput": 11822.460614511965,
    "itl": 90.21070932110051,
    "ttft": 1959517.482424265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 691,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.149742883839676,
    "arrivals": 558926,
    "finished_requests": 92016,
    "scheduler_time": 262.33021296645614
}
#Debug simulation 
Total elapsed time: 76.31156269926578. Arrivals time: 0.46384665556252 Scheduler time: 75.63521772390231 Scheduler overhead time: 0.08144366461783648 Adapter cache time: 0.02022248739376664 Engine time: 0.07918254984542727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 80.75796306086704,
    "estimated_duration": 3600.047392016357,
    "input_throughput": 6434.4614049721795,
    "output_throughput": 5660.776312332337,
    "total_throughput": 12095.237717304515,
    "itl": 95.52382395835733,
    "ttft": 1944285.9890776833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.226155662657673,
    "arrivals": 558926,
    "finished_requests": 94000,
    "scheduler_time": 256.01066671944113
}
#Debug simulation 
Total elapsed time: 80.75812302809209. Arrivals time: 0.48014010954648256 Scheduler time: 80.06740444200113 Scheduler overhead time: 0.0793941980227828 Adapter cache time: 0.019910501316189766 Engine time: 0.08050312614068389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 76.1472825333476,
    "estimated_duration": 3600.0779771301773,
    "input_throughput": 6303.698459912387,
    "output_throughput": 5518.711296314122,
    "total_throughput": 11822.409756226509,
    "itl": 90.20933055414045,
    "ttft": 1959553.6716310545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 691,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.101070347335221,
    "arrivals": 558926,
    "finished_requests": 92018,
    "scheduler_time": 262.3369624642458
}
#Debug simulation 
Total elapsed time: 76.14744818815961. Arrivals time: 0.4654122036881745 Scheduler time: 75.46791072236374 Scheduler overhead time: 0.0816097455099225 Adapter cache time: 0.020607585087418556 Engine time: 0.07996553694829345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 80.90164739778265,
    "estimated_duration": 3600.103275312512,
    "input_throughput": 6573.603363627303,
    "output_throughput": 5701.380885585359,
    "total_throughput": 12274.984249212663,
    "itl": 97.68256033532519,
    "ttft": 1934187.4458558154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.377414779150932,
    "arrivals": 556060,
    "finished_requests": 95599,
    "scheduler_time": 252.6755118257611
}
#Debug simulation 
Total elapsed time: 80.90180771658197. Arrivals time: 0.47823498491197824 Scheduler time: 80.21612916421145 Scheduler overhead time: 0.07927482295781374 Adapter cache time: 0.020051050465554 Engine time: 0.07760715391486883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.69780873088166,
    "estimated_duration": 3600.071179339684,
    "input_throughput": 6543.850614731737,
    "output_throughput": 5675.351120070777,
    "total_throughput": 12219.201734802515,
    "itl": 95.56545542395816,
    "ttft": 1933545.7866214155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 688,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.033495696783073,
    "arrivals": 556060,
    "finished_requests": 95009,
    "scheduler_time": 254.26995462005948
}
#Debug simulation 
Total elapsed time: 79.69797818409279. Arrivals time: 0.5026629269123077 Scheduler time: 78.98568704398349 Scheduler overhead time: 0.0801252000965178 Adapter cache time: 0.020078980829566717 Engine time: 0.0785332159139216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 77.73968025390059,
    "estimated_duration": 3600.0783473812144,
    "input_throughput": 6368.725840835749,
    "output_throughput": 5530.441306779626,
    "total_throughput": 11899.167147615375,
    "itl": 89.97479127867557,
    "ttft": 1952644.3017047653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.8964976583980375,
    "arrivals": 556060,
    "finished_requests": 92553,
    "scheduler_time": 261.61701839683207
}
#Debug simulation 
Total elapsed time: 77.73983978619799. Arrivals time: 0.4523846753872931 Scheduler time: 77.07474181894213 Scheduler overhead time: 0.08136910293251276 Adapter cache time: 0.020148740150034428 Engine time: 0.07964150933548808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 80.33709998195991,
    "estimated_duration": 3600.0276609249313,
    "input_throughput": 6551.746881283708,
    "output_throughput": 5675.84361136554,
    "total_throughput": 12227.590492649248,
    "itl": 95.64416953470158,
    "ttft": 1939529.2935551584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 701,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.809689833908335,
    "arrivals": 556060,
    "finished_requests": 95097,
    "scheduler_time": 254.34937855467422
}
#Debug simulation 
Total elapsed time: 80.33725739410147. Arrivals time: 0.4809225690551102 Scheduler time: 79.64808919792995 Scheduler overhead time: 0.0790196224115789 Adapter cache time: 0.02049063052982092 Engine time: 0.07789696892723441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 75.74229127308354,
    "estimated_duration": 3600.091608444078,
    "input_throughput": 6380.97929122651,
    "output_throughput": 5547.169397900673,
    "total_throughput": 11928.148689127183,
    "itl": 90.21841910680793,
    "ttft": 1950927.1740273177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.105480656111649,
    "arrivals": 556060,
    "finished_requests": 92747,
    "scheduler_time": 260.7406002630636
}
#Debug simulation 
Total elapsed time: 75.74245791230351. Arrivals time: 0.47079461021348834 Scheduler time: 75.05982575379312 Scheduler overhead time: 0.0811298405751586 Adapter cache time: 0.020129634998738766 Engine time: 0.0791088379919529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 79.63349077897146,
    "estimated_duration": 3600.056118551035,
    "input_throughput": 6580.286867732108,
    "output_throughput": 5681.8447619735425,
    "total_throughput": 12262.13162970565,
    "itl": 95.67095402819703,
    "ttft": 1940515.0358834104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 702,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.481512500280493,
    "arrivals": 556060,
    "finished_requests": 95386,
    "scheduler_time": 253.89329591179307
}
#Debug simulation 
Total elapsed time: 79.6336539387703. Arrivals time: 0.4897136650979519 Scheduler time: 78.93640088941902 Scheduler overhead time: 0.0789004759863019 Adapter cache time: 0.020255887415260077 Engine time: 0.07753693126142025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 77.62702791811898,
    "estimated_duration": 3600.0709108524147,
    "input_throughput": 6392.942964156937,
    "output_throughput": 5544.219126304383,
    "total_throughput": 11937.162090461321,
    "itl": 90.10238328185628,
    "ttft": 1951912.9698746442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.881880752928593,
    "arrivals": 556060,
    "finished_requests": 92818,
    "scheduler_time": 260.96308133924487
}
#Debug simulation 
Total elapsed time: 77.62719166325405. Arrivals time: 0.4710600101388991 Scheduler time: 76.94223038200289 Scheduler overhead time: 0.08165867254137993 Adapter cache time: 0.020281735341995955 Engine time: 0.07985312072560191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.76027686381713,
    "estimated_duration": 3600.0507708799128,
    "input_throughput": 6548.542645757702,
    "output_throughput": 5719.110732143281,
    "total_throughput": 12267.653377900982,
    "itl": 97.55293665917615,
    "ttft": 1940507.682355977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.073243963681225,
    "arrivals": 554588,
    "finished_requests": 95134,
    "scheduler_time": 251.62363636698453
}
#Debug simulation 
Total elapsed time: 83.76043487899005. Arrivals time: 0.7714181495830417 Scheduler time: 82.78463638620451 Scheduler overhead time: 0.07788194809108973 Adapter cache time: 0.019211904611438513 Engine time: 0.07692831521853805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.17899237759411,
    "estimated_duration": 3600.020259113973,
    "input_throughput": 6490.280142409688,
    "output_throughput": 5664.701455046555,
    "total_throughput": 12154.981597456243,
    "itl": 95.5229513705382,
    "ttft": 1946332.1181000373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 629,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.612421290143399,
    "arrivals": 554588,
    "finished_requests": 94315,
    "scheduler_time": 254.38772806688976
}
#Debug simulation 
Total elapsed time: 81.17915675695986. Arrivals time: 0.4739375179633498 Scheduler time: 80.49660499114543 Scheduler overhead time: 0.07940874807536602 Adapter cache time: 0.019882839173078537 Engine time: 0.07834640378132463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 76.70556872943416,
    "estimated_duration": 3600.098406405642,
    "input_throughput": 6310.4494475977435,
    "output_throughput": 5516.594758816223,
    "total_throughput": 11827.044206413966,
    "itl": 89.93057183035758,
    "ttft": 1957882.2669875945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 633,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.772101973006538,
    "arrivals": 554588,
    "finished_requests": 91700,
    "scheduler_time": 262.11341746438654
}
#Debug simulation 
Total elapsed time: 76.70572975929826. Arrivals time: 0.45757674146443605 Scheduler time: 76.03516139509156 Scheduler overhead time: 0.0808936688117683 Adapter cache time: 0.02008548704907298 Engine time: 0.07988915825262666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.04834253294393,
    "estimated_duration": 3600.041737258873,
    "input_throughput": 6492.077788463533,
    "output_throughput": 5680.006092254257,
    "total_throughput": 12172.08388071779,
    "itl": 95.48437091483403,
    "ttft": 1948100.298390422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.155186794414179,
    "arrivals": 554588,
    "finished_requests": 94404,
    "scheduler_time": 254.53626230606466
}
#Debug simulation 
Total elapsed time: 81.04850574489683. Arrivals time: 0.4694723663851619 Scheduler time: 80.3707956476137 Scheduler overhead time: 0.07950407126918435 Adapter cache time: 0.01943865930661559 Engine time: 0.07861900981515646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 77.43509859219193,
    "estimated_duration": 3600.0350967242175,
    "input_throughput": 6341.252623001527,
    "output_throughput": 5535.866030343508,
    "total_throughput": 11877.118653345035,
    "itl": 90.12159240451649,
    "ttft": 1959488.3043253825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.793988842102734,
    "arrivals": 554588,
    "finished_requests": 92212,
    "scheduler_time": 261.0935650611797
}
#Debug simulation 
Total elapsed time: 77.43526324210688. Arrivals time: 0.4517412818968296 Scheduler time: 76.77025338169187 Scheduler overhead time: 0.0815742863342166 Adapter cache time: 0.019999125972390175 Engine time: 0.07955157058313489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.37693009804934,
    "estimated_duration": 3600.016273189047,
    "input_throughput": 6492.553707068369,
    "output_throughput": 5680.366822865788,
    "total_throughput": 12172.920529934157,
    "itl": 95.47539862696516,
    "ttft": 1948208.2125860865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8622721690451547,
    "arrivals": 554588,
    "finished_requests": 94410,
    "scheduler_time": 254.55688665895482
}
#Debug simulation 
Total elapsed time: 81.3770995400846. Arrivals time: 0.47729116259142756 Scheduler time: 80.69204286066815 Scheduler overhead time: 0.07965835276991129 Adapter cache time: 0.01955841202288866 Engine time: 0.07774363458156586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 78.57406970625743,
    "estimated_duration": 3600.0216810595803,
    "input_throughput": 6328.062166918648,
    "output_throughput": 5523.356179940439,
    "total_throughput": 11851.418346859087,
    "itl": 89.94923076546206,
    "ttft": 1961042.620978411,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.469030280821063,
    "arrivals": 554588,
    "finished_requests": 92005,
    "scheduler_time": 261.80082752891036
}
#Debug simulation 
Total elapsed time: 78.57423816528171. Arrivals time: 0.47625537449494004 Scheduler time: 77.88468372169882 Scheduler overhead time: 0.08177191298455 Adapter cache time: 0.019693321082741022 Engine time: 0.07988591259345412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.08538923598826,
    "estimated_duration": 3600.0385877704116,
    "input_throughput": 6558.244147772368,
    "output_throughput": 5718.690369024718,
    "total_throughput": 12276.934516797086,
    "itl": 96.81619412725497,
    "ttft": 1863072.3093757913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.238554189479979,
    "arrivals": 438250,
    "finished_requests": 95383,
    "scheduler_time": 250.9519292394071
}
#Debug simulation 
Total elapsed time: 82.08555179461837. Arrivals time: 0.47549430234357715 Scheduler time: 81.40243965992704 Scheduler overhead time: 0.07905125617980957 Adapter cache time: 0.020030064042657614 Engine time: 0.0781416418030858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.40898648370057,
    "estimated_duration": 3600.0028851801458,
    "input_throughput": 6522.0753285104865,
    "output_throughput": 5697.438767184108,
    "total_throughput": 12219.514095694594,
    "itl": 95.08466154964793,
    "ttft": 1864164.1478547743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 664,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.853905395716435,
    "arrivals": 438250,
    "finished_requests": 94870,
    "scheduler_time": 252.72811692340596
}
#Debug simulation 
Total elapsed time: 80.40914568398148. Arrivals time: 0.458208245690912 Scheduler time: 79.74449822353199 Scheduler overhead time: 0.07884803274646401 Adapter cache time: 0.019803053233772516 Engine time: 0.07730423705652356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.63210448995233,
    "estimated_duration": 3600.045091391375,
    "input_throughput": 6381.546179778786,
    "output_throughput": 5573.0424177119985,
    "total_throughput": 11954.588597490783,
    "itl": 90.125863991131,
    "ttft": 1884130.1376978802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.768989290813944,
    "arrivals": 438250,
    "finished_requests": 92907,
    "scheduler_time": 258.5249511790007
}
#Debug simulation 
Total elapsed time: 75.6322712758556. Arrivals time: 0.4618828999809921 Scheduler time: 74.96220456482843 Scheduler overhead time: 0.07979502528905869 Adapter cache time: 0.019346964545547962 Engine time: 0.07812949037179351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 83.1668239152059,
    "estimated_duration": 3600.0832907164568,
    "input_throughput": 6518.420298917538,
    "output_throughput": 5685.441237646095,
    "total_throughput": 12203.861536563634,
    "itl": 94.9842785956116,
    "ttft": 1867525.9916715585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 649,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.437467536772594,
    "arrivals": 438250,
    "finished_requests": 94763,
    "scheduler_time": 252.71229546401025
}
#Debug simulation 
Total elapsed time: 83.16698805987835. Arrivals time: 0.46524460799992085 Scheduler time: 82.49580881232396 Scheduler overhead time: 0.07848909217864275 Adapter cache time: 0.01964589674025774 Engine time: 0.07700510043650866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 76.97279545105994,
    "estimated_duration": 3600.0200869878427,
    "input_throughput": 6346.59264335298,
    "output_throughput": 5540.21825380647,
    "total_throughput": 11886.81089715945,
    "itl": 89.64577824116343,
    "ttft": 1875433.3091604572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 659,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.892499369960308,
    "arrivals": 438250,
    "finished_requests": 92363,
    "scheduler_time": 260.1817882958599
}
#Debug simulation 
Total elapsed time: 76.97295097634196. Arrivals time: 0.45643737632781267 Scheduler time: 76.30730741890147 Scheduler overhead time: 0.08005049033090472 Adapter cache time: 0.01994141284376383 Engine time: 0.07807512721046805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 80.52032961277291,
    "estimated_duration": 3600.001113401126,
    "input_throughput": 6490.223270493917,
    "output_throughput": 5662.217137689184,
    "total_throughput": 12152.440408183102,
    "itl": 94.7633120298804,
    "ttft": 1869199.4805569204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 658,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.200619978895391,
    "arrivals": 438250,
    "finished_requests": 94348,
    "scheduler_time": 253.9344424594017
}
#Debug simulation 
Total elapsed time: 80.5204888228327. Arrivals time: 0.4612070848233998 Scheduler time: 79.8545313575305 Scheduler overhead time: 0.0781180509366095 Adapter cache time: 0.019721122924238443 Engine time: 0.07648903876543045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 78.412667427212,
    "estimated_duration": 3600.075824379006,
    "input_throughput": 6359.528275754922,
    "output_throughput": 5548.821740010375,
    "total_throughput": 11908.350015765296,
    "itl": 89.76727014394382,
    "ttft": 1878819.2574071998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.785951589345962,
    "arrivals": 438250,
    "finished_requests": 92528,
    "scheduler_time": 259.71315310362996
}
#Debug simulation 
Total elapsed time: 78.41282472806051. Arrivals time: 0.4540322911925614 Scheduler time: 77.74907806748524 Scheduler overhead time: 0.07997293118387461 Adapter cache time: 0.01967520173639059 Engine time: 0.07869686419144273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.6333077698946,
    "estimated_duration": 3600.002555704589,
    "input_throughput": 6548.707573177223,
    "output_throughput": 5701.574285683453,
    "total_throughput": 12250.281858860677,
    "itl": 97.0191116825462,
    "ttft": 1865604.5489226354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9872826462658737,
    "arrivals": 426621,
    "finished_requests": 95384,
    "scheduler_time": 251.10813648684896
}
#Debug simulation 
Total elapsed time: 83.63347356114537. Arrivals time: 0.4760825401172042 Scheduler time: 82.94975906424224 Scheduler overhead time: 0.07965010590851307 Adapter cache time: 0.01941059436649084 Engine time: 0.07806797698140144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.89801349118352,
    "estimated_duration": 3600.0938481879316,
    "input_throughput": 6450.91044270679,
    "output_throughput": 5624.089219282779,
    "total_throughput": 12074.999661989568,
    "itl": 94.09713641646825,
    "ttft": 1872007.4920742195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.71928587203846,
    "arrivals": 426621,
    "finished_requests": 93956,
    "scheduler_time": 255.07816576523103
}
#Debug simulation 
Total elapsed time: 81.89817153615877. Arrivals time: 0.467532301787287 Scheduler time: 81.2200720673427 Scheduler overhead time: 0.0810272409580648 Adapter cache time: 0.01971337804570794 Engine time: 0.07883570017293096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.00692490302026,
    "estimated_duration": 3600.031988052996,
    "input_throughput": 6306.426186029152,
    "output_throughput": 5498.8436396383995,
    "total_throughput": 11805.269825667552,
    "itl": 89.02877315183831,
    "ttft": 1882040.021734294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.580452977032433,
    "arrivals": 426621,
    "finished_requests": 91899,
    "scheduler_time": 261.66065051035156
}
#Debug simulation 
Total elapsed time: 80.00707797100767. Arrivals time: 0.4581562769599259 Scheduler time: 79.33409945759922 Scheduler overhead time: 0.08234550710767508 Adapter cache time: 0.020147602073848248 Engine time: 0.08048149524256587 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.5370764308609,
    "estimated_duration": 3600.0481038151042,
    "input_throughput": 6457.17232927116,
    "output_throughput": 5618.621589684974,
    "total_throughput": 12075.793918956135,
    "itl": 94.02220268141541,
    "ttft": 1872903.1827684068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.08440648014191,
    "arrivals": 426621,
    "finished_requests": 93972,
    "scheduler_time": 255.38574246974903
}
#Debug simulation 
Total elapsed time: 81.5372362001799. Arrivals time: 0.4684129813686013 Scheduler time: 80.85848447401077 Scheduler overhead time: 0.08001235453411937 Adapter cache time: 0.02001059101894498 Engine time: 0.07903924398124218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 78.01995763136074,
    "estimated_duration": 3600.0858685928447,
    "input_throughput": 6325.799947905516,
    "output_throughput": 5508.549441280795,
    "total_throughput": 11834.34938918631,
    "itl": 89.15376173397158,
    "ttft": 1886708.575803925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.320356064355034,
    "arrivals": 426621,
    "finished_requests": 92096,
    "scheduler_time": 261.19724579927
}
#Debug simulation 
Total elapsed time: 78.02012600004673. Arrivals time: 0.45901285158470273 Scheduler time: 77.347235056106 Scheduler overhead time: 0.08148056315258145 Adapter cache time: 0.01984679140150547 Engine time: 0.08056870102882385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.54542692285031,
    "estimated_duration": 3600.0055196168714,
    "input_throughput": 6477.1809023452815,
    "output_throughput": 5645.550510756709,
    "total_throughput": 12122.73141310199,
    "itl": 94.74081805004163,
    "ttft": 1869658.2708349437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9005756946885777,
    "arrivals": 426621,
    "finished_requests": 94329,
    "scheduler_time": 254.11770703325269
}
#Debug simulation 
Total elapsed time: 83.54558664793149. Arrivals time: 0.4701944952830672 Scheduler time: 82.86636248510331 Scheduler overhead time: 0.07975635677576065 Adapter cache time: 0.019854965154081583 Engine time: 0.07802142854779959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 78.40978240827098,
    "estimated_duration": 3600.033185780776,
    "input_throughput": 6307.24685808013,
    "output_throughput": 5497.105715070788,
    "total_throughput": 11804.352573150918,
    "itl": 89.12304046270874,
    "ttft": 1882633.6613303781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.460380655620279,
    "arrivals": 426621,
    "finished_requests": 91847,
    "scheduler_time": 261.72691426599374
}
#Debug simulation 
Total elapsed time: 78.40994062414393. Arrivals time: 0.46428667148575187 Scheduler time: 77.73025973839685 Scheduler overhead time: 0.08297139266505837 Adapter cache time: 0.01973877241834998 Engine time: 0.0804799166508019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.994081039913,
    "estimated_duration": 3600.0530563507787,
    "input_throughput": 6523.441636108964,
    "output_throughput": 5731.96357859159,
    "total_throughput": 12255.405214700553,
    "itl": 97.31020997955001,
    "ttft": 1849537.637653431,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.82197242046712,
    "arrivals": 420933,
    "finished_requests": 95523,
    "scheduler_time": 249.46619459679613
}
#Debug simulation 
Total elapsed time: 82.9942408008501. Arrivals time: 0.4656222378835082 Scheduler time: 82.3237323788926 Scheduler overhead time: 0.07854244159534574 Adapter cache time: 0.01904745912179351 Engine time: 0.07675954513251781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 86.49417486134917,
    "estimated_duration": 3600.035823183372,
    "input_throughput": 6405.625147254372,
    "output_throughput": 5623.872370830222,
    "total_throughput": 12029.497518084594,
    "itl": 93.81669870626186,
    "ttft": 1855173.4406731273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.023967240392237,
    "arrivals": 420933,
    "finished_requests": 93770,
    "scheduler_time": 254.97195127761876
}
#Debug simulation 
Total elapsed time: 86.49434411199763. Arrivals time: 0.46603956865146756 Scheduler time: 85.81654565222561 Scheduler overhead time: 0.0812979624606669 Adapter cache time: 0.01940715592354536 Engine time: 0.07927167462185025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 77.1156453024596,
    "estimated_duration": 3600.0288094926386,
    "input_throughput": 6309.138121369927,
    "output_throughput": 5536.857912759089,
    "total_throughput": 11845.996034129015,
    "itl": 89.6014013950933,
    "ttft": 1874789.4186121402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.454653415842947,
    "arrivals": 420933,
    "finished_requests": 92289,
    "scheduler_time": 259.5074041033685
}
#Debug simulation 
Total elapsed time: 77.11581654008478. Arrivals time: 0.46437137201428413 Scheduler time: 76.43845149921253 Scheduler overhead time: 0.08166264928877354 Adapter cache time: 0.019763583317399025 Engine time: 0.07964312424883246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 82.3617046950385,
    "estimated_duration": 3600.1012651041306,
    "input_throughput": 6395.16816461941,
    "output_throughput": 5607.973918871429,
    "total_throughput": 12003.142083490839,
    "itl": 93.71689799130934,
    "ttft": 1864024.3421763154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8237706117471633,
    "arrivals": 420933,
    "finished_requests": 93501,
    "scheduler_time": 255.81480625895384
}
#Debug simulation 
Total elapsed time: 82.36187067534775. Arrivals time: 0.4666928122751415 Scheduler time: 81.68544546468183 Scheduler overhead time: 0.08043196704238653 Adapter cache time: 0.019326732959598303 Engine time: 0.07863534055650234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 76.40761389490217,
    "estimated_duration": 3600.0616937677337,
    "input_throughput": 6281.1843027984805,
    "output_throughput": 5523.532564573581,
    "total_throughput": 11804.716867372063,
    "itl": 89.19866185485463,
    "ttft": 1864025.1266758002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.534576343037222,
    "arrivals": 420933,
    "finished_requests": 91828,
    "scheduler_time": 260.92731646583326
}
#Debug simulation 
Total elapsed time: 76.40778429526836. Arrivals time: 0.46713763987645507 Scheduler time: 75.72836982971057 Scheduler overhead time: 0.08126559574157 Adapter cache time: 0.019495175685733557 Engine time: 0.07983409380540252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.54906895337626,
    "estimated_duration": 3600.000589071886,
    "input_throughput": 6454.131721681237,
    "output_throughput": 5669.848238903279,
    "total_throughput": 12123.979960584516,
    "itl": 94.68709152799268,
    "ttft": 1856889.5343789218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.504772596373207,
    "arrivals": 420933,
    "finished_requests": 94429,
    "scheduler_time": 253.12760334877902
}
#Debug simulation 
Total elapsed time: 81.54924342222512. Arrivals time: 0.462423924356699 Scheduler time: 80.88086805865169 Scheduler overhead time: 0.07970201410353184 Adapter cache time: 0.01871621049940586 Engine time: 0.07710931729525328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 76.62566993804649,
    "estimated_duration": 3600.04609485064,
    "input_throughput": 6290.843340143288,
    "output_throughput": 5520.4190380858245,
    "total_throughput": 11811.262378229112,
    "itl": 89.46018144535518,
    "ttft": 1867198.4908617148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.485489213429425,
    "arrivals": 420933,
    "finished_requests": 91951,
    "scheduler_time": 260.43789472687547
}
#Debug simulation 
Total elapsed time: 76.62583427038044. Arrivals time: 0.45561395585536957 Scheduler time: 75.95763372490183 Scheduler overhead time: 0.08131890557706356 Adapter cache time: 0.019380060490220785 Engine time: 0.07971023814752698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.38306149095297,
    "estimated_duration": 3600.0971721331725,
    "input_throughput": 6557.4268891225165,
    "output_throughput": 5724.337709414938,
    "total_throughput": 12281.764598537455,
    "itl": 96.95460465191573,
    "ttft": 1853853.549309914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.914546146914422,
    "arrivals": 418062,
    "finished_requests": 95739,
    "scheduler_time": 250.07916737752947
}
#Debug simulation 
Total elapsed time: 82.38322971295565. Arrivals time: 0.47425037529319525 Scheduler time: 81.70219671865925 Scheduler overhead time: 0.07917986856773496 Adapter cache time: 0.019404408056288958 Engine time: 0.0773837361484766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 83.55237531661987,
    "estimated_duration": 3600.0575476266013,
    "input_throughput": 6477.050072539152,
    "output_throughput": 5669.681312026919,
    "total_throughput": 12146.73138456607,
    "itl": 94.65305084989417,
    "ttft": 1852086.075127249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.099743254631764,
    "arrivals": 418062,
    "finished_requests": 94618,
    "scheduler_time": 253.17327430536534
}
#Debug simulation 
Total elapsed time: 83.55253909388557. Arrivals time: 0.46119806123897433 Scheduler time: 82.88341880450025 Scheduler overhead time: 0.079234026838094 Adapter cache time: 0.01964516658335924 Engine time: 0.07790187420323491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 78.25841018976644,
    "estimated_duration": 3600.0202001993794,
    "input_throughput": 6330.951975974395,
    "output_throughput": 5542.023069452509,
    "total_throughput": 11872.975045426905,
    "itl": 89.71165563785607,
    "ttft": 1865270.7088861668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.480590442358551,
    "arrivals": 418062,
    "finished_requests": 92499,
    "scheduler_time": 259.5511097375151
}
#Debug simulation 
Total elapsed time: 78.25858351401985. Arrivals time: 0.4673890550620854 Scheduler time: 77.57728654006496 Scheduler overhead time: 0.08216291898861527 Adapter cache time: 0.01985191274434328 Engine time: 0.08037480199709535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 83.80774670094252,
    "estimated_duration": 3600.008409897877,
    "input_throughput": 6475.621261301806,
    "output_throughput": 5665.73315326744,
    "total_throughput": 12141.354414569247,
    "itl": 94.95704150475468,
    "ttft": 1848932.4803206096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8543019954767024,
    "arrivals": 418062,
    "finished_requests": 94692,
    "scheduler_time": 253.01918729384414
}
#Debug simulation 
Total elapsed time: 83.80791126284748. Arrivals time: 0.4851103350520134 Scheduler time: 83.11248427769169 Scheduler overhead time: 0.08101570792496204 Adapter cache time: 0.019171337131410837 Engine time: 0.07881973264738917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 80.87106942711398,
    "estimated_duration": 3600.011227866685,
    "input_throughput": 6337.036069056621,
    "output_throughput": 5549.06380440316,
    "total_throughput": 11886.099873459782,
    "itl": 89.76674082227923,
    "ttft": 1866356.2424281891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.910138825504143,
    "arrivals": 418062,
    "finished_requests": 92659,
    "scheduler_time": 259.24133350086925
}
#Debug simulation 
Total elapsed time: 80.87125078309327. Arrivals time: 0.472105894703418 Scheduler time: 80.18530434882268 Scheduler overhead time: 0.08207029802724719 Adapter cache time: 0.018995018675923347 Engine time: 0.08065114310011268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.6749810827896,
    "estimated_duration": 3600.1096217364434,
    "input_throughput": 6478.699387145357,
    "output_throughput": 5666.591616218699,
    "total_throughput": 12145.291003364056,
    "itl": 94.63820588039233,
    "ttft": 1852947.9856161382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7154419874120332,
    "arrivals": 418062,
    "finished_requests": 94666,
    "scheduler_time": 253.08345847942397
}
#Debug simulation 
Total elapsed time: 83.67514680186287. Arrivals time: 0.47807446494698524 Scheduler time: 82.98928550910205 Scheduler overhead time: 0.0794566161930561 Adapter cache time: 0.019361655693501234 Engine time: 0.077943773008883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.63586886785924,
    "estimated_duration": 3600.081239207033,
    "input_throughput": 6289.996668238454,
    "output_throughput": 5507.6901554497745,
    "total_throughput": 11797.686823688227,
    "itl": 88.96909509304747,
    "ttft": 1866340.2021355764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 537,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9628338646702823,
    "arrivals": 418062,
    "finished_requests": 91945,
    "scheduler_time": 261.5604952402464
}
#Debug simulation 
Total elapsed time: 82.63604253111407. Arrivals time: 0.4689616453833878 Scheduler time: 81.95138359395787 Scheduler overhead time: 0.08308638446033001 Adapter cache time: 0.019255241379141808 Engine time: 0.08117079501971602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.78583828685805,
    "estimated_duration": 3600.0410427834536,
    "input_throughput": 6580.107759461702,
    "output_throughput": 5740.284278543165,
    "total_throughput": 12320.392038004868,
    "itl": 96.61152123873539,
    "ttft": 1842219.8382007915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.960833010138073,
    "arrivals": 416618,
    "finished_requests": 95857,
    "scheduler_time": 250.0690717701963
}
#Debug simulation 
Total elapsed time: 82.78600291069597. Arrivals time: 0.4804783584550023 Scheduler time: 82.0996278594248 Scheduler overhead time: 0.07845943607389927 Adapter cache time: 0.0192595599219203 Engine time: 0.07718445779755712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.23132776189595,
    "estimated_duration": 3600.080764545434,
    "input_throughput": 6531.89207075169,
    "output_throughput": 5690.999269175267,
    "total_throughput": 12222.891339926957,
    "itl": 95.1078402296713,
    "ttft": 1859760.3207034855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.283772071739664,
    "arrivals": 416618,
    "finished_requests": 95236,
    "scheduler_time": 251.59582691717802
}
#Debug simulation 
Total elapsed time: 80.23149446770549. Arrivals time: 0.47351710172370076 Scheduler time: 79.5514299031347 Scheduler overhead time: 0.07930680643767118 Adapter cache time: 0.019229425117373466 Engine time: 0.07745955279096961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.66165647935122,
    "estimated_duration": 3600.0172870007555,
    "input_throughput": 6383.1018486982,
    "output_throughput": 5557.016926623386,
    "total_throughput": 11940.118775321585,
    "itl": 89.4613530976369,
    "ttft": 1870568.1586419686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.21408002102751,
    "arrivals": 416618,
    "finished_requests": 92965,
    "scheduler_time": 258.6323654698143
}
#Debug simulation 
Total elapsed time: 81.66183103015646. Arrivals time: 0.4684727704152465 Scheduler time: 80.9810515101999 Scheduler overhead time: 0.08091250201687217 Adapter cache time: 0.01943565532565117 Engine time: 0.07987526105716825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.26678016083315,
    "estimated_duration": 3600.0078820098215,
    "input_throughput": 6520.597390163147,
    "output_throughput": 5669.805641815624,
    "total_throughput": 12190.40303197877,
    "itl": 94.7607690485309,
    "ttft": 1850552.2972339964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 569,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9073187679005734,
    "arrivals": 416618,
    "finished_requests": 94981,
    "scheduler_time": 252.53176947793116
}
#Debug simulation 
Total elapsed time: 81.26694659376517. Arrivals time: 0.46047317888587713 Scheduler time: 80.59967825934291 Scheduler overhead time: 0.079220084939152 Adapter cache time: 0.018912939354777336 Engine time: 0.07815607590600848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 81.20443534012884,
    "estimated_duration": 3600.083970549337,
    "input_throughput": 6383.401939509027,
    "output_throughput": 5557.168155982551,
    "total_throughput": 11940.57009549158,
    "itl": 89.46034474595463,
    "ttft": 1870549.1040169036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.17617757770702,
    "arrivals": 416618,
    "finished_requests": 92974,
    "scheduler_time": 258.6392998473801
}
#Debug simulation 
Total elapsed time: 81.20460867416114. Arrivals time: 0.4635721673257649 Scheduler time: 80.52719565061852 Scheduler overhead time: 0.08180504525080323 Adapter cache time: 0.019372718408703804 Engine time: 0.08091284381225705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.35468813590705,
    "estimated_duration": 3600.085909248705,
    "input_throughput": 6520.780223519452,
    "output_throughput": 5670.1821885856,
    "total_throughput": 12190.962412105051,
    "itl": 94.75522104785337,
    "ttft": 1850433.0764232732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 569,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6324510151846168,
    "arrivals": 416618,
    "finished_requests": 94987,
    "scheduler_time": 252.5553936254795
}
#Debug simulation 
Total elapsed time: 81.35484633827582. Arrivals time: 0.46115254797041416 Scheduler time: 80.68619630811736 Scheduler overhead time: 0.07935612602159381 Adapter cache time: 0.018912746105343103 Engine time: 0.07836592523381114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.88257578620687,
    "estimated_duration": 3600.062168650433,
    "input_throughput": 6372.595784533422,
    "output_throughput": 5557.199587889288,
    "total_throughput": 11929.79537242271,
    "itl": 89.55944090839967,
    "ttft": 1869432.107328033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.232180925905739,
    "arrivals": 416618,
    "finished_requests": 92974,
    "scheduler_time": 258.7767132640697
}
#Debug simulation 
Total elapsed time: 80.88273943634704. Arrivals time: 0.47242079954594374 Scheduler time: 80.19882851466537 Scheduler overhead time: 0.08175144670531154 Adapter cache time: 0.01928733568638563 Engine time: 0.07913599302992225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.63469248730689,
    "estimated_duration": 3600.051965373104,
    "input_throughput": 6584.824671424757,
    "output_throughput": 5697.440813989545,
    "total_throughput": 12282.265485414302,
    "itl": 96.8335004658958,
    "ttft": 1834342.3945741588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.921158555946372,
    "arrivals": 403575,
    "finished_requests": 95412,
    "scheduler_time": 251.4296678047052
}
#Debug simulation 
Total elapsed time: 84.63485896820202. Arrivals time: 0.46643728064373136 Scheduler time: 83.9608931238763 Scheduler overhead time: 0.08023673854768276 Adapter cache time: 0.019194806460291147 Engine time: 0.07760885544121265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.04286724887788,
    "estimated_duration": 3600.086177949537,
    "input_throughput": 6484.246722475879,
    "output_throughput": 5611.563724151265,
    "total_throughput": 12095.810446627145,
    "itl": 94.31660397571457,
    "ttft": 1848512.1701752748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.131662859334616,
    "arrivals": 403575,
    "finished_requests": 94080,
    "scheduler_time": 255.28142770655785
}
#Debug simulation 
Total elapsed time: 85.04303625971079. Arrivals time: 0.4805807932280004 Scheduler time: 84.35323049500585 Scheduler overhead time: 0.080566780641675 Adapter cache time: 0.018909701146185398 Engine time: 0.07891352707520127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 77.47360011283308,
    "estimated_duration": 3600.064808361432,
    "input_throughput": 6340.173084380891,
    "output_throughput": 5483.436285410924,
    "total_throughput": 11823.609369791815,
    "itl": 88.86649147749523,
    "ttft": 1865821.0515090548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.618188620307512,
    "arrivals": 403575,
    "finished_requests": 92005,
    "scheduler_time": 261.9616280401394
}
#Debug simulation 
Total elapsed time: 77.47377375885844. Arrivals time: 0.47187101701274514 Scheduler time: 76.7887072446756 Scheduler overhead time: 0.08152702683582902 Adapter cache time: 0.02000408712774515 Engine time: 0.079769775737077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 85.80840826081112,
    "estimated_duration": 3600.0756735168516,
    "input_throughput": 6468.199869046725,
    "output_throughput": 5599.386742975818,
    "total_throughput": 12067.586612022544,
    "itl": 93.68550819331303,
    "ttft": 1848592.751634647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.945622293543996,
    "arrivals": 403575,
    "finished_requests": 93829,
    "scheduler_time": 256.79514731296393
}
#Debug simulation 
Total elapsed time: 85.80857191001996. Arrivals time: 0.47062826063483953 Scheduler time: 85.1278495141305 Scheduler overhead time: 0.08048685034736991 Adapter cache time: 0.019264699891209602 Engine time: 0.07889379654079676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_384_slots_32_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 79.45503680966794,
    "estimated_duration": 3600.0746321840957,
    "input_throughput": 6361.138681757457,
    "output_throughput": 5493.9861032827,
    "total_throughput": 11855.124785040158,
    "itl": 89.19223162737657,
    "ttft": 1863665.7752279458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.373831475288639,
    "arrivals": 403575,
    "finished_requests": 92137,
    "scheduler_time": 261.3584714423777
}
#Debug simulation 
Total elapsed time: 79.45520233176649. Arrivals time: 0.4636381482705474 Scheduler time: 78.77590697072446 Scheduler overhead time: 0.08277727058157325 Adapter cache time: 0.019502866081893444 Engine time: 0.08067037304863334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_384_slots_32_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_384_slots_32_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.99845536705106,
    "estimated_duration": 3600.009881848503,
    "input_throughput": 6536.11233642447,
    "output_throughput": 5648.26227353553,
    "total_throughput": 12184.37460996,
    "itl": 94.74195738483249,
    "ttft": 1847747.219848989,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7154419874120332,
    "arrivals": 403575,
    "finished_requests": 94765,
    "scheduler_time": 253.3821049955995
}
#Debug simulation 
Total elapsed time: 83.9986221450381. Arrivals time: 0.46065533673390746 Scheduler time: 83.32884689001366 Scheduler overhead time: 0.08029989525675774 Adapter cache time: 0.0192205305211246 Engine time: 0.07873301394283772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_384_slots_32_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_384_slots_32_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.54763441160321,
    "estimated_duration": 3600.0104678818525,
    "input_throughput": 6362.979275856864,
    "output_throughput": 5497.472903639766,
    "total_throughput": 11860.45217949663,
    "itl": 89.29039260651997,
    "ttft": 1864621.932146093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.500085946712675,
    "arrivals": 403575,
    "finished_requests": 92184,
    "scheduler_time": 261.2877340556909
}
#Debug simulation 
Total elapsed time: 79.54779793787748. Arrivals time: 0.4574855323880911 Scheduler time: 78.87617593444884 Scheduler overhead time: 0.08216217020526528 Adapter cache time: 0.019654833246022463 Engine time: 0.08042425429448485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.83536068489775,
    "estimated_duration": 3600.015097685377,
    "input_throughput": 6566.234406960782,
    "output_throughput": 5695.086949269179,
    "total_throughput": 12261.32135622996,
    "itl": 97.23415688899304,
    "ttft": 1836314.0154704808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.81536001143517,
    "arrivals": 397780,
    "finished_requests": 95477,
    "scheduler_time": 250.75450161757735
}
#Debug simulation 
Total elapsed time: 83.83552624192089. Arrivals time: 0.4655469241552055 Scheduler time: 83.16302529117092 Scheduler overhead time: 0.07956813042983413 Adapter cache time: 0.01893819123506546 Engine time: 0.07800342794507742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.6168970419094,
    "estimated_duration": 3600.0838237615417,
    "input_throughput": 6487.890044625553,
    "output_throughput": 5637.187908251392,
    "total_throughput": 12125.077952876945,
    "itl": 94.69106188662236,
    "ttft": 1846413.692833949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 632,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.613526180312044,
    "arrivals": 397780,
    "finished_requests": 94385,
    "scheduler_time": 253.60241445361038
}
#Debug simulation 
Total elapsed time: 79.61705545708537. Arrivals time: 0.44672922510653734 Scheduler time: 78.96249839989468 Scheduler overhead time: 0.07971184467896819 Adapter cache time: 0.018902692943811417 Engine time: 0.07849895395338535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 78.84758035000414,
    "estimated_duration": 3600.0471168024565,
    "input_throughput": 6313.085429889142,
    "output_throughput": 5483.836005328398,
    "total_throughput": 11796.92143521754,
    "itl": 89.27989543434971,
    "ttft": 1861018.314775817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4987563260971,
    "arrivals": 397780,
    "finished_requests": 91907,
    "scheduler_time": 261.4818643864903
}
#Debug simulation 
Total elapsed time: 78.84774026693776. Arrivals time: 0.461005303543061 Scheduler time: 78.17465028027073 Scheduler overhead time: 0.08166928123682737 Adapter cache time: 0.018893686588853598 Engine time: 0.07985994964838028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 80.75885881995782,
    "estimated_duration": 3600.0961285322105,
    "input_throughput": 6497.7362172652065,
    "output_throughput": 5643.907905393646,
    "total_throughput": 12141.644122658852,
    "itl": 94.9939909841573,
    "ttft": 1844781.9173624704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.151579315420235,
    "arrivals": 397780,
    "finished_requests": 94522,
    "scheduler_time": 253.3104099946029
}
#Debug simulation 
Total elapsed time: 80.7590140006505. Arrivals time: 0.4623407437466085 Scheduler time: 80.08925797138363 Scheduler overhead time: 0.07980447262525558 Adapter cache time: 0.018783019855618477 Engine time: 0.07828380819410086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 75.47521458007395,
    "estimated_duration": 3600.0226221620715,
    "input_throughput": 6290.662136561558,
    "output_throughput": 5466.185373074605,
    "total_throughput": 11756.847509636164,
    "itl": 88.77376201288594,
    "ttft": 1861524.9868882436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.568997752736358,
    "arrivals": 397780,
    "finished_requests": 91450,
    "scheduler_time": 262.48764149113043
}
#Debug simulation 
Total elapsed time: 75.47536861291155. Arrivals time: 0.3963138135150075 Scheduler time: 74.87617293093354 Scheduler overhead time: 0.07847256353124976 Adapter cache time: 0.018067013006657362 Engine time: 0.07567120809108019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 79.34060026006773,
    "estimated_duration": 3600.0523007328848,
    "input_throughput": 6498.334481206695,
    "output_throughput": 5644.389665078841,
    "total_throughput": 12142.724146285536,
    "itl": 94.98767096598179,
    "ttft": 1844769.0565153367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.855888248104584,
    "arrivals": 397780,
    "finished_requests": 94527,
    "scheduler_time": 253.32587977107002
}
#Debug simulation 
Total elapsed time: 79.3407492688857. Arrivals time: 0.38639333564788103 Scheduler time: 78.75932865124196 Scheduler overhead time: 0.07543097855523229 Adapter cache time: 0.017318942118436098 Engine time: 0.07267341017723083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_384_slots_32_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 74.79387436807156,
    "estimated_duration": 3600.0694491347617,
    "input_throughput": 6302.592302893836,
    "output_throughput": 5477.756548485353,
    "total_throughput": 11780.348851379189,
    "itl": 89.07036834575264,
    "ttft": 1862431.9471356054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.309008288551158,
    "arrivals": 397780,
    "finished_requests": 91664,
    "scheduler_time": 261.8725945045582
}
#Debug simulation 
Total elapsed time: 74.79403249081224. Arrivals time: 0.3885558005422354 Scheduler time: 74.20479212002829 Scheduler overhead time: 0.07749897940084338 Adapter cache time: 0.017595713026821613 Engine time: 0.0747595876455307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_384_slots_32_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_384_slots_32_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.95052364422008,
    "estimated_duration": 3600.039514234733,
    "input_throughput": 6517.8620699078365,
    "output_throughput": 5713.200346461232,
    "total_throughput": 12231.062416369068,
    "itl": 97.2943291108636,
    "ttft": 1835353.2867937125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5773132862849644,
    "arrivals": 394936,
    "finished_requests": 95164,
    "scheduler_time": 249.9526869217306
}
#Debug simulation 
Total elapsed time: 81.9506826591678. Arrivals time: 0.4112143316306174 Scheduler time: 81.34708756627515 Scheduler overhead time: 0.07451674295589328 Adapter cache time: 0.016992569901049137 Engine time: 0.07170634716749191 
