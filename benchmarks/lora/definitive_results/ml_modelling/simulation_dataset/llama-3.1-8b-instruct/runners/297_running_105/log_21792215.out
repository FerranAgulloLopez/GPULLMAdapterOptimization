INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.3698826022446156,
    "estimated_duration": 3600.00691820434,
    "input_throughput": 3754.510284869626,
    "output_throughput": 3246.0995952276926,
    "total_throughput": 7000.609880097319,
    "itl": 133.98276921008684,
    "ttft": 2270609.4830030263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.526471824599057,
    "arrivals": 762451,
    "finished_requests": 54779,
    "scheduler_time": 126.79720262394908
}
#Debug simulation 
Total elapsed time: 4.3700354793109. Arrivals time: 0.21409400459378958 Scheduler time: 4.00585356913507 Scheduler overhead time: 0.03987737558782101 Adapter cache time: 0.05111089022830129 Engine time: 0.04009606363251805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.382173624821007,
    "estimated_duration": 3600.0098734927387,
    "input_throughput": 3758.0874707085127,
    "output_throughput": 3249.4669212255412,
    "total_throughput": 7007.554391934054,
    "itl": 134.18497060386653,
    "ttft": 2269483.730498237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.342904951339918,
    "arrivals": 762451,
    "finished_requests": 54829,
    "scheduler_time": 126.73740457131754
}
#Debug simulation 
Total elapsed time: 4.382326439023018. Arrivals time: 0.21347360219806433 Scheduler time: 4.019305336289108 Scheduler overhead time: 0.03958269860595465 Adapter cache time: 0.05109948804602027 Engine time: 0.04000193113461137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.431223657913506,
    "estimated_duration": 3600.0033736431837,
    "input_throughput": 3758.6831443178417,
    "output_throughput": 3250.105565362087,
    "total_throughput": 7008.788709679929,
    "itl": 134.14380872044742,
    "ttft": 2269156.877851971,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2379,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.187347917618217,
    "arrivals": 762451,
    "finished_requests": 54838,
    "scheduler_time": 126.77689087374611
}
#Debug simulation 
Total elapsed time: 4.431320417672396. Arrivals time: 0.25987083464860916 Scheduler time: 4.021951470524073 Scheduler overhead time: 0.03962744725868106 Adapter cache time: 0.05108420830219984 Engine time: 0.03998050047084689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.8593649729155,
    "estimated_duration": 3600.117877442002,
    "input_throughput": 3939.5373937248214,
    "output_throughput": 3439.511821984636,
    "total_throughput": 7379.049215709458,
    "itl": 150.07242294890244,
    "ttft": 2241525.5205086013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.029498265292784,
    "arrivals": 761080,
    "finished_requests": 57646,
    "scheduler_time": 120.91755991871452
}
#Debug simulation 
Total elapsed time: 4.859507228713483. Arrivals time: 0.21913100965321064 Scheduler time: 4.5132495649158955 Scheduler overhead time: 0.03625704953446984 Adapter cache time: 0.03712711716070771 Engine time: 0.03666659025475383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.348039359785616,
    "estimated_duration": 3600.0929789714537,
    "input_throughput": 3715.6012575602053,
    "output_throughput": 3245.0861875622227,
    "total_throughput": 6960.6874451224285,
    "itl": 133.48726096673096,
    "ttft": 2272328.3926866576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.72400528025315,
    "arrivals": 761080,
    "finished_requests": 54411,
    "scheduler_time": 126.87898742814073
}
#Debug simulation 
Total elapsed time: 4.348136269953102. Arrivals time: 0.2110820198431611 Scheduler time: 3.9883537217974663 Scheduler overhead time: 0.03977624559774995 Adapter cache time: 0.04978160699829459 Engine time: 0.04026949033141136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.3347220327705145,
    "estimated_duration": 3600.1172179681507,
    "input_throughput": 3717.0936916194564,
    "output_throughput": 3246.6815084973173,
    "total_throughput": 6963.775200116774,
    "itl": 133.4521653897286,
    "ttft": 2272203.5294918865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.731427284335044,
    "arrivals": 761080,
    "finished_requests": 54430,
    "scheduler_time": 126.91384194438224
}
#Debug simulation 
Total elapsed time: 4.334858441725373. Arrivals time: 0.2098266794346273 Scheduler time: 3.9769011572934687 Scheduler overhead time: 0.039697746746242046 Adapter cache time: 0.049587334506213665 Engine time: 0.039981253910809755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.410689448006451,
    "estimated_duration": 3600.093893730664,
    "input_throughput": 3718.396351637349,
    "output_throughput": 3247.655295980097,
    "total_throughput": 6966.051647617446,
    "itl": 133.40582601369502,
    "ttft": 2272241.8881790475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.574491507323364,
    "arrivals": 761080,
    "finished_requests": 54444,
    "scheduler_time": 126.95300555433023
}
#Debug simulation 
Total elapsed time: 4.410782124381512. Arrivals time: 0.2622855016961694 Scheduler time: 3.9997220430523157 Scheduler overhead time: 0.03988423477858305 Adapter cache time: 0.04991622036322951 Engine time: 0.04007040709257126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.7963462779298425,
    "estimated_duration": 3600.165651140809,
    "input_throughput": 3964.3059189450028,
    "output_throughput": 3449.2365638967417,
    "total_throughput": 7413.5424828417445,
    "itl": 151.31540106168748,
    "ttft": 2241366.805865455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.355032544033868,
    "arrivals": 753814,
    "finished_requests": 58046,
    "scheduler_time": 120.61785537243651
}
#Debug simulation 
Total elapsed time: 4.79649368301034. Arrivals time: 0.27130325976759195 Scheduler time: 4.400388282723725 Scheduler overhead time: 0.036629597656428814 Adapter cache time: 0.034750310238450766 Engine time: 0.03638761257752776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.279654300305992,
    "estimated_duration": 3600.041018668726,
    "input_throughput": 3724.204232805641,
    "output_throughput": 3241.7633408842867,
    "total_throughput": 6965.967573689928,
    "itl": 133.32155318782395,
    "ttft": 2273739.7268644897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2066,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.189607085762223,
    "arrivals": 753814,
    "finished_requests": 54528,
    "scheduler_time": 127.09751191001557
}
#Debug simulation 
Total elapsed time: 4.27974589029327. Arrivals time: 0.2593083130195737 Scheduler time: 3.876268347725272 Scheduler overhead time: 0.03949905373156071 Adapter cache time: 0.045775840524584055 Engine time: 0.04003895167261362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.301385059952736,
    "estimated_duration": 3600.0464823086186,
    "input_throughput": 3744.3110988266503,
    "output_throughput": 3258.7062577250085,
    "total_throughput": 7003.017356551659,
    "itl": 134.49527616647723,
    "ttft": 2272179.536872472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2071,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.23588979941349,
    "arrivals": 753814,
    "finished_requests": 54811,
    "scheduler_time": 126.6238213405108
}
#Debug simulation 
Total elapsed time: 4.301478911191225. Arrivals time: 0.20983247552067041 Scheduler time: 3.9478096487000585 Scheduler overhead time: 0.03941375343129039 Adapter cache time: 0.04579243715852499 Engine time: 0.03993816953152418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.333473629783839,
    "estimated_duration": 3600.1244143945396,
    "input_throughput": 3741.4654188458953,
    "output_throughput": 3256.3655170155002,
    "total_throughput": 6997.830935861396,
    "itl": 134.2144469379239,
    "ttft": 2272481.5988896815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2059,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.144493216635373,
    "arrivals": 753814,
    "finished_requests": 54771,
    "scheduler_time": 126.76006838365886
}
#Debug simulation 
Total elapsed time: 4.333567761816084. Arrivals time: 0.2600105828605592 Scheduler time: 3.9295925558544695 Scheduler overhead time: 0.0397522309795022 Adapter cache time: 0.04540766915306449 Engine time: 0.03997710766270757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.518186813686043,
    "estimated_duration": 3600.1451729401742,
    "input_throughput": 3955.781868754285,
    "output_throughput": 3438.4510638748366,
    "total_throughput": 7394.232932629121,
    "itl": 150.3176755647565,
    "ttft": 2239956.218711958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.912001138893208,
    "arrivals": 750842,
    "finished_requests": 57717,
    "scheduler_time": 120.9342314661469
}
#Debug simulation 
Total elapsed time: 4.518281058873981. Arrivals time: 0.22001731442287564 Scheduler time: 4.174600967206061 Scheduler overhead time: 0.03617778839543462 Adapter cache time: 0.03393827471882105 Engine time: 0.036444720812141895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.189916886854917,
    "estimated_duration": 3600.0698518927647,
    "input_throughput": 3739.2105025191536,
    "output_throughput": 3246.9058881883557,
    "total_throughput": 6986.11639070751,
    "itl": 133.51657990760438,
    "ttft": 2272150.2342018737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1992,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.604751037321742,
    "arrivals": 750842,
    "finished_requests": 54482,
    "scheduler_time": 126.96094487090193
}
#Debug simulation 
Total elapsed time: 4.190017751883715. Arrivals time: 0.21517111686989665 Scheduler time: 3.8304262151941657 Scheduler overhead time: 0.039414051454514265 Adapter cache time: 0.04440042097121477 Engine time: 0.041888329200446606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.249270737171173,
    "estimated_duration": 3600.049616235494,
    "input_throughput": 3740.171785209977,
    "output_throughput": 3247.6127404670765,
    "total_throughput": 6987.784525677053,
    "itl": 133.48393497593054,
    "ttft": 2271977.7779309144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1992,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.67047832228268,
    "arrivals": 750842,
    "finished_requests": 54496,
    "scheduler_time": 126.99145160559533
}
#Debug simulation 
Total elapsed time: 4.249366395175457. Arrivals time: 0.2627454246394336 Scheduler time: 3.8430540980771184 Scheduler overhead time: 0.0397052438929677 Adapter cache time: 0.044874528888612986 Engine time: 0.04017433384433389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.218841176014394,
    "estimated_duration": 3600.0123658414514,
    "input_throughput": 3737.878271663869,
    "output_throughput": 3245.531907296391,
    "total_throughput": 6983.41017896026,
    "itl": 133.23703847837413,
    "ttft": 2272058.7885357323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1986,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.678466987973662,
    "arrivals": 750842,
    "finished_requests": 54460,
    "scheduler_time": 127.11413166756664
}
#Debug simulation 
Total elapsed time: 4.218942567706108. Arrivals time: 0.21797949820756912 Scheduler time: 3.8569998848252 Scheduler overhead time: 0.03983383486047387 Adapter cache time: 0.044922164641320705 Engine time: 0.04022701736539602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.507781425025314,
    "estimated_duration": 3600.07208068103,
    "input_throughput": 3910.8519730906587,
    "output_throughput": 3444.3007590154493,
    "total_throughput": 7355.1527321061085,
    "itl": 151.0271287659623,
    "ttft": 2242137.7199762138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.290434689889894,
    "arrivals": 749452,
    "finished_requests": 57503,
    "scheduler_time": 120.74513959685635
}
#Debug simulation 
Total elapsed time: 4.507873855065554. Arrivals time: 0.21990908728912473 Scheduler time: 4.165926047600806 Scheduler overhead time: 0.035995145328342915 Adapter cache time: 0.03234732476994395 Engine time: 0.036617414094507694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.158436056692153,
    "estimated_duration": 3600.0287713073376,
    "input_throughput": 3699.279879689348,
    "output_throughput": 3254.145103886415,
    "total_throughput": 6953.424983575764,
    "itl": 134.25471377633679,
    "ttft": 2274448.4625918753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1897,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.889997312049172,
    "arrivals": 749452,
    "finished_requests": 54374,
    "scheduler_time": 126.71352687484112
}
#Debug simulation 
Total elapsed time: 4.15852488996461. Arrivals time: 0.21015115547925234 Scheduler time: 3.806916710920632 Scheduler overhead time: 0.03938248800113797 Adapter cache time: 0.043364882469177246 Engine time: 0.040019477251917124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.157471282407641,
    "estimated_duration": 3600.122636152725,
    "input_throughput": 3698.965936957897,
    "output_throughput": 3253.98831760881,
    "total_throughput": 6952.954254566707,
    "itl": 134.1352757687879,
    "ttft": 2274254.875876042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1897,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.984877237449592,
    "arrivals": 749452,
    "finished_requests": 54371,
    "scheduler_time": 126.78475099418527
}
#Debug simulation 
Total elapsed time: 4.157560656312853. Arrivals time: 0.20905230846256018 Scheduler time: 3.807642268948257 Scheduler overhead time: 0.039463568944483995 Adapter cache time: 0.042285280767828226 Engine time: 0.04033504147082567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.180098952259868,
    "estimated_duration": 3600.1110006573635,
    "input_throughput": 3699.6503712157723,
    "output_throughput": 3254.2554931947298,
    "total_throughput": 6953.905864410502,
    "itl": 134.10207292212556,
    "ttft": 2274162.1564083737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1898,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.11668194520338,
    "arrivals": 749452,
    "finished_requests": 54379,
    "scheduler_time": 126.81378320685631
}
#Debug simulation 
Total elapsed time: 4.180191804189235. Arrivals time: 0.21306920936331153 Scheduler time: 3.82606320688501 Scheduler overhead time: 0.03942024800926447 Adapter cache time: 0.04283659206703305 Engine time: 0.040023261215537786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.440163630992174,
    "estimated_duration": 3600.152018121773,
    "input_throughput": 3956.505427632252,
    "output_throughput": 3447.299430004295,
    "total_throughput": 7403.804857636547,
    "itl": 151.03406811165166,
    "ttft": 2243211.263724982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.729906158349658,
    "arrivals": 745115,
    "finished_requests": 57578,
    "scheduler_time": 120.7909863660371
}
#Debug simulation 
Total elapsed time: 4.440261153038591. Arrivals time: 0.2194715836085379 Scheduler time: 4.1026237909682095 Scheduler overhead time: 0.03610859205946326 Adapter cache time: 0.028448792174458504 Engine time: 0.03661501267924905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.14867154089734,
    "estimated_duration": 3600.1016543193205,
    "input_throughput": 3731.404912937074,
    "output_throughput": 3255.5047399671153,
    "total_throughput": 6986.909652904189,
    "itl": 134.06577418811855,
    "ttft": 2273275.309422513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1682,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.292562512149887,
    "arrivals": 745115,
    "finished_requests": 54294,
    "scheduler_time": 126.88070190528703
}
#Debug simulation 
Total elapsed time: 4.148777346126735. Arrivals time: 0.21100028371438384 Scheduler time: 3.8000200213864446 Scheduler overhead time: 0.03920597489923239 Adapter cache time: 0.03960732230916619 Engine time: 0.04018385522067547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.103498077951372,
    "estimated_duration": 3600.0584182845896,
    "input_throughput": 3724.944554202487,
    "output_throughput": 3249.6556001928693,
    "total_throughput": 6974.600154395356,
    "itl": 133.6043767192338,
    "ttft": 2273219.94523502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1699,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.697261134670256,
    "arrivals": 745115,
    "finished_requests": 54211,
    "scheduler_time": 127.08611152909339
}
#Debug simulation 
Total elapsed time: 4.103588727768511. Arrivals time: 0.20775695517659187 Scheduler time: 3.757653212174773 Scheduler overhead time: 0.039401326794177294 Adapter cache time: 0.0399302514269948 Engine time: 0.04009438073262572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.068048911169171,
    "estimated_duration": 3600.0306490131843,
    "input_throughput": 3735.0484790138676,
    "output_throughput": 3257.681154246487,
    "total_throughput": 6992.729633260355,
    "itl": 134.00857695840244,
    "ttft": 2273174.8577137454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.769674626742818,
    "arrivals": 745115,
    "finished_requests": 54335,
    "scheduler_time": 126.92953186492795
}
#Debug simulation 
Total elapsed time: 4.068145798984915. Arrivals time: 0.20548286847770214 Scheduler time: 3.726190131623298 Scheduler overhead time: 0.03905271692201495 Adapter cache time: 0.039270782843232155 Engine time: 0.03961632587015629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.366320221219212,
    "estimated_duration": 3600.142070640287,
    "input_throughput": 3986.073526661621,
    "output_throughput": 3445.396808408163,
    "total_throughput": 7431.470335069785,
    "itl": 150.835353300579,
    "ttft": 2237021.0968390927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.300099571272899,
    "arrivals": 743719,
    "finished_requests": 58046,
    "scheduler_time": 120.87651223192658
}
#Debug simulation 
Total elapsed time: 4.3664158610627055. Arrivals time: 0.2663794159889221 Scheduler time: 3.9838948831893504 Scheduler overhead time: 0.0356866386719048 Adapter cache time: 0.027168472297489643 Engine time: 0.03631378384307027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.115561195183545,
    "estimated_duration": 3600.134432575813,
    "input_throughput": 3764.337486226527,
    "output_throughput": 3258.767476526154,
    "total_throughput": 7023.104962752681,
    "itl": 134.1348460063676,
    "ttft": 2266791.171114862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1591,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.657485088598774,
    "arrivals": 743719,
    "finished_requests": 54835,
    "scheduler_time": 126.91370603082147
}
#Debug simulation 
Total elapsed time: 4.115660226903856. Arrivals time: 0.21060353284701705 Scheduler time: 3.768400528933853 Scheduler overhead time: 0.03939653327688575 Adapter cache time: 0.038592968601733446 Engine time: 0.03992926189675927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.084801279939711,
    "estimated_duration": 3600.1409482534236,
    "input_throughput": 3759.013381563687,
    "output_throughput": 3255.6475339350914,
    "total_throughput": 7014.660915498778,
    "itl": 133.8058248286061,
    "ttft": 2267240.443851274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.055252083935853,
    "arrivals": 743719,
    "finished_requests": 54771,
    "scheduler_time": 127.0668987945346
}
#Debug simulation 
Total elapsed time: 4.084897969849408. Arrivals time: 0.20949900429695845 Scheduler time: 3.7390481303445995 Scheduler overhead time: 0.03918590350076556 Adapter cache time: 0.03827047161757946 Engine time: 0.04016613168641925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.152097766287625,
    "estimated_duration": 3600.0672161049533,
    "input_throughput": 3760.2006261000197,
    "output_throughput": 3256.2767016009634,
    "total_throughput": 7016.477327700983,
    "itl": 133.7750275902704,
    "ttft": 2267079.150222645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.239809188675393,
    "arrivals": 743719,
    "finished_requests": 54785,
    "scheduler_time": 127.09126293674234
}
#Debug simulation 
Total elapsed time: 4.1521943300031126. Arrivals time: 0.25854317704215646 Scheduler time: 3.7573430105112493 Scheduler overhead time: 0.039366637356579304 Adapter cache time: 0.03823363408446312 Engine time: 0.040037938859313726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.266259664203972,
    "estimated_duration": 3600.1492704656835,
    "input_throughput": 3997.0459330811696,
    "output_throughput": 3453.399585954337,
    "total_throughput": 7450.445519035507,
    "itl": 150.7024247884673,
    "ttft": 2240662.450110492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 903,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.971005355850918,
    "arrivals": 740875,
    "finished_requests": 58228,
    "scheduler_time": 120.87133814613591
}
#Debug simulation 
Total elapsed time: 4.266357130836695. Arrivals time: 0.21732307691127062 Scheduler time: 3.9362129285000265 Scheduler overhead time: 0.03562659164890647 Adapter cache time: 0.024077093228697777 Engine time: 0.03628399083390832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.095604653004557,
    "estimated_duration": 3600.0766652133543,
    "input_throughput": 3783.1349903191162,
    "output_throughput": 3271.634494289855,
    "total_throughput": 7054.769484608972,
    "itl": 133.2901488462648,
    "ttft": 2269402.649185231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.073007221003037,
    "arrivals": 740875,
    "finished_requests": 55126,
    "scheduler_time": 127.44535200081354
}
#Debug simulation 
Total elapsed time: 4.095699955243617. Arrivals time: 0.20832113130018115 Scheduler time: 3.751690802164376 Scheduler overhead time: 0.039045663084834814 Adapter cache time: 0.038008172530680895 Engine time: 0.03999599115923047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.106973761692643,
    "estimated_duration": 3600.1170362867065,
    "input_throughput": 3784.0042039441914,
    "output_throughput": 3272.169732612506,
    "total_throughput": 7056.173936556697,
    "itl": 133.2696446753716,
    "ttft": 2269397.0593714826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.519107052651238,
    "arrivals": 740875,
    "finished_requests": 55136,
    "scheduler_time": 127.46496270376574
}
#Debug simulation 
Total elapsed time: 4.107065812684596. Arrivals time: 0.20746223581954837 Scheduler time: 3.764657615683973 Scheduler overhead time: 0.03903136635199189 Adapter cache time: 0.03733657067641616 Engine time: 0.03999101463705301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.094398838002235,
    "estimated_duration": 3600.1001991566654,
    "input_throughput": 3784.600496172769,
    "output_throughput": 3272.722798870987,
    "total_throughput": 7057.323295043756,
    "itl": 133.2481906051057,
    "ttft": 2269250.3751107873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.90967804536685,
    "arrivals": 740875,
    "finished_requests": 55144,
    "scheduler_time": 127.48406892489
}
#Debug simulation 
Total elapsed time: 4.094497323967516. Arrivals time: 0.21254937211051583 Scheduler time: 3.746390228625387 Scheduler overhead time: 0.03919620253145695 Adapter cache time: 0.037761265411973 Engine time: 0.03994710138067603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 11.323192752432078,
    "estimated_duration": 3600.1116166216902,
    "input_throughput": 3691.902200652431,
    "output_throughput": 3219.3412966667775,
    "total_throughput": 6911.2434973192085,
    "itl": 131.51151257313055,
    "ttft": 2260123.4478892307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.713628867934704,
    "arrivals": 599139,
    "finished_requests": 53717,
    "scheduler_time": 127.56441384110614
}
#Debug simulation 
Total elapsed time: 11.323287789244205. Arrivals time: 0.301624140702188 Scheduler time: 10.87782286433503 Scheduler overhead time: 0.04397079534828663 Adapter cache time: 0.03709740564227104 Engine time: 0.043178973719477654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.66557403607294,
    "estimated_duration": 3600.047030662737,
    "input_throughput": 3709.250708744697,
    "output_throughput": 3235.9813360147673,
    "total_throughput": 6945.232044759465,
    "itl": 134.419067904964,
    "ttft": 2256399.2638567155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3940,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.780069909113617,
    "arrivals": 599139,
    "finished_requests": 53980,
    "scheduler_time": 125.91516161632126
}
#Debug simulation 
Total elapsed time: 6.665673607960343. Arrivals time: 0.23508036183193326 Scheduler time: 6.251913185697049 Scheduler overhead time: 0.04043635260313749 Adapter cache time: 0.07871107151731849 Engine time: 0.040545505471527576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.647333614062518,
    "estimated_duration": 3600.0677485489127,
    "input_throughput": 3707.1027358802703,
    "output_throughput": 3235.0535638376095,
    "total_throughput": 6942.15629971788,
    "itl": 134.22673117916318,
    "ttft": 2256249.1090628114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.212144902535588,
    "arrivals": 599139,
    "finished_requests": 53945,
    "scheduler_time": 126.03480949413677
}
#Debug simulation 
Total elapsed time: 6.647431053221226. Arrivals time: 0.23533384036272764 Scheduler time: 6.231580628082156 Scheduler overhead time: 0.04078798461705446 Adapter cache time: 0.08041069190949202 Engine time: 0.04039079276844859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.608690820168704,
    "estimated_duration": 3600.0772533947425,
    "input_throughput": 3709.8789997925505,
    "output_throughput": 3235.513623774674,
    "total_throughput": 6945.392623567225,
    "itl": 134.15833702657108,
    "ttft": 2256486.1759633804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.333673879855855,
    "arrivals": 599139,
    "finished_requests": 53978,
    "scheduler_time": 126.06642063483258
}
#Debug simulation 
Total elapsed time: 6.60878777038306. Arrivals time: 0.27539352886378765 Scheduler time: 6.151096393354237 Scheduler overhead time: 0.04100063629448414 Adapter cache time: 0.08207189058884978 Engine time: 0.04026072099804878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.766225075349212,
    "estimated_duration": 3600.0946827540783,
    "input_throughput": 3965.5912574717204,
    "output_throughput": 3431.1553135459008,
    "total_throughput": 7396.746571017621,
    "itl": 150.85188625105576,
    "ttft": 2213962.777502309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.05543659390365,
    "arrivals": 576120,
    "finished_requests": 57763,
    "scheduler_time": 120.06217091583449
}
#Debug simulation 
Total elapsed time: 7.766318137291819. Arrivals time: 0.2463646549731493 Scheduler time: 7.364462580066174 Scheduler overhead time: 0.037906288634985685 Adapter cache time: 0.06295313313603401 Engine time: 0.037291585467755795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.383863124065101,
    "estimated_duration": 3600.1481940009253,
    "input_throughput": 3735.540115379079,
    "output_throughput": 3233.806047039909,
    "total_throughput": 6969.346162418989,
    "itl": 133.9516997504497,
    "ttft": 2248211.0881855413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4003,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.268326628714902,
    "arrivals": 576120,
    "finished_requests": 54383,
    "scheduler_time": 125.97154678870207
}
#Debug simulation 
Total elapsed time: 6.38395609613508. Arrivals time: 0.22681662812829018 Scheduler time: 5.976490543689579 Scheduler overhead time: 0.04088973067700863 Adapter cache time: 0.07995662372559309 Engine time: 0.040756728034466505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.390002956148237,
    "estimated_duration": 3600.112251408436,
    "input_throughput": 3737.8828937168364,
    "output_throughput": 3236.492694204634,
    "total_throughput": 6974.3755879214705,
    "itl": 134.159833034644,
    "ttft": 2246789.255851954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3995,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.387580018365593,
    "arrivals": 576120,
    "finished_requests": 54410,
    "scheduler_time": 125.95380171352573
}
#Debug simulation 
Total elapsed time: 6.390095326118171. Arrivals time: 0.22803811123594642 Scheduler time: 5.981810002587736 Scheduler overhead time: 0.04067767085507512 Adapter cache time: 0.07971682585775852 Engine time: 0.040878340136259794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.4540083119645715,
    "estimated_duration": 3600.1508158311535,
    "input_throughput": 3740.0891487073472,
    "output_throughput": 3237.6413090097235,
    "total_throughput": 6977.730457717071,
    "itl": 133.90794382284776,
    "ttft": 2247632.776449549,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3991,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.47822847381929,
    "arrivals": 576120,
    "finished_requests": 54446,
    "scheduler_time": 126.06998251098273
}
#Debug simulation 
Total elapsed time: 6.454102738760412. Arrivals time: 0.22628120565786958 Scheduler time: 6.041809146758169 Scheduler overhead time: 0.04062546184286475 Adapter cache time: 0.0853398465551436 Engine time: 0.04108941275626421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.329992865677923,
    "estimated_duration": 3600.050175181205,
    "input_throughput": 3914.886269408899,
    "output_throughput": 3431.759114129055,
    "total_throughput": 7346.645383537953,
    "itl": 150.76506511279334,
    "ttft": 2213551.524744898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2996,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.81077745972156,
    "arrivals": 564654,
    "finished_requests": 57144,
    "scheduler_time": 120.05261269670565
}
#Debug simulation 
Total elapsed time: 7.330085491761565. Arrivals time: 0.23768539307639003 Scheduler time: 6.939717350061983 Scheduler overhead time: 0.0374773102812469 Adapter cache time: 0.060692137107253075 Engine time: 0.0371983852237463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.054070896003395,
    "estimated_duration": 3600.0123180437963,
    "input_throughput": 3686.242386862454,
    "output_throughput": 3234.8161537202627,
    "total_throughput": 6921.058540582717,
    "itl": 134.09563556327836,
    "ttft": 2247227.2515390026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3925,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.70791085155134,
    "arrivals": 564654,
    "finished_requests": 53818,
    "scheduler_time": 125.88659174353388
}
#Debug simulation 
Total elapsed time: 6.054164227563888. Arrivals time: 0.22138732578605413 Scheduler time: 5.653447821736336 Scheduler overhead time: 0.04058908438310027 Adapter cache time: 0.07933098031207919 Engine time: 0.04040077095851302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.10457701375708,
    "estimated_duration": 3600.0006366710913,
    "input_throughput": 3692.2290692401357,
    "output_throughput": 3236.606372040636,
    "total_throughput": 6928.835441280771,
    "itl": 133.9849403638061,
    "ttft": 2246815.4665401513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.17405890036446,
    "arrivals": 564654,
    "finished_requests": 53871,
    "scheduler_time": 125.97071809955524
}
#Debug simulation 
Total elapsed time: 6.104666257742792. Arrivals time: 0.22392318677157164 Scheduler time: 5.704915264621377 Scheduler overhead time: 0.040311402175575495 Adapter cache time: 0.07569766277447343 Engine time: 0.04085798189043999 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.039540829136968,
    "estimated_duration": 3600.137038665966,
    "input_throughput": 3690.583679815523,
    "output_throughput": 3235.80293607842,
    "total_throughput": 6926.386615893943,
    "itl": 133.99907501035426,
    "ttft": 2246469.4176761257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3939,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.146264584909577,
    "arrivals": 564654,
    "finished_requests": 53877,
    "scheduler_time": 125.98392235493532
}
#Debug simulation 
Total elapsed time: 6.039632133208215. Arrivals time: 0.22216136427596211 Scheduler time: 5.639091143384576 Scheduler overhead time: 0.040386993903666735 Adapter cache time: 0.07863422203809023 Engine time: 0.0403568334877491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.138626974076033,
    "estimated_duration": 3600.0824204202913,
    "input_throughput": 3934.8607464231923,
    "output_throughput": 3436.188829964565,
    "total_throughput": 7371.049576387757,
    "itl": 151.05976962363215,
    "ttft": 2207774.101529897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.818916104929304,
    "arrivals": 558926,
    "finished_requests": 57327,
    "scheduler_time": 119.98515717762386
}
#Debug simulation 
Total elapsed time: 7.138719947077334. Arrivals time: 0.23403979651629925 Scheduler time: 6.7552532949484885 Scheduler overhead time: 0.037336830515414476 Adapter cache time: 0.057741239201277494 Engine time: 0.03716723108664155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.220271746627986,
    "estimated_duration": 3600.0223188760424,
    "input_throughput": 3708.0259002859916,
    "output_throughput": 3239.2249178168295,
    "total_throughput": 6947.250818102821,
    "itl": 134.11210284313321,
    "ttft": 2242798.3088387065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3721,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.230675137037583,
    "arrivals": 558926,
    "finished_requests": 54040,
    "scheduler_time": 125.84132871027641
}
#Debug simulation 
Total elapsed time: 6.220338254701346. Arrivals time: 0.5218836711719632 Scheduler time: 5.524854050949216 Scheduler overhead time: 0.040204179007560015 Adapter cache time: 0.07418438792228699 Engine time: 0.04030007077381015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.997629021294415,
    "estimated_duration": 3600.0857844259385,
    "input_throughput": 3707.2266604691968,
    "output_throughput": 3237.2867475596563,
    "total_throughput": 6944.513408028853,
    "itl": 133.8972969815236,
    "ttft": 2241250.2012439677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.100899280356888,
    "arrivals": 558926,
    "finished_requests": 53993,
    "scheduler_time": 126.0291805827623
}
#Debug simulation 
Total elapsed time: 5.997721066232771. Arrivals time: 0.22151961969211698 Scheduler time: 5.604033376090229 Scheduler overhead time: 0.04007607698440552 Adapter cache time: 0.07247791765257716 Engine time: 0.04060150450095534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.959406659938395,
    "estimated_duration": 3600.1332252222114,
    "input_throughput": 3714.3108778077617,
    "output_throughput": 3242.230570308836,
    "total_throughput": 6956.541448116598,
    "itl": 134.0669648959337,
    "ttft": 2242089.5800911384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.03957067452102,
    "arrivals": 558926,
    "finished_requests": 54122,
    "scheduler_time": 125.94766649363733
}
#Debug simulation 
Total elapsed time: 5.959501393139362. Arrivals time: 0.2189427763223648 Scheduler time: 5.568647358100861 Scheduler overhead time: 0.03992975223809481 Adapter cache time: 0.07294153608381748 Engine time: 0.04017415689304471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.909130384214222,
    "estimated_duration": 3600.0392447039667,
    "input_throughput": 3952.544967650786,
    "output_throughput": 3430.9492648366045,
    "total_throughput": 7383.494232487391,
    "itl": 150.50194417648953,
    "ttft": 2207013.0596201606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2728,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.038651839159396,
    "arrivals": 556060,
    "finished_requests": 57542,
    "scheduler_time": 120.15115264961136
}
#Debug simulation 
Total elapsed time: 6.909220666158944. Arrivals time: 0.23544428311288357 Scheduler time: 6.526100013405085 Scheduler overhead time: 0.037470586597919464 Adapter cache time: 0.05582690052688122 Engine time: 0.03711580950766802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.91142363473773,
    "estimated_duration": 3600.026940709527,
    "input_throughput": 3731.7173513574444,
    "output_throughput": 3241.0004680966144,
    "total_throughput": 6972.717819454059,
    "itl": 134.0723870548998,
    "ttft": 2240654.4209188865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3305,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.17793282739289,
    "arrivals": 556060,
    "finished_requests": 54332,
    "scheduler_time": 125.9190788557431
}
#Debug simulation 
Total elapsed time: 5.911528546828777. Arrivals time: 0.22509461035951972 Scheduler time: 5.518406695686281 Scheduler overhead time: 0.04059159057214856 Adapter cache time: 0.0680277612991631 Engine time: 0.040509707760065794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.940436574164778,
    "estimated_duration": 3600.0157876271514,
    "input_throughput": 3733.7886256492543,
    "output_throughput": 3242.1452261722216,
    "total_throughput": 6975.933851821476,
    "itl": 134.01846334571889,
    "ttft": 2240257.1084197564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.701139564910527,
    "arrivals": 556060,
    "finished_requests": 54355,
    "scheduler_time": 125.96948442609992
}
#Debug simulation 
Total elapsed time: 5.940527739003301. Arrivals time: 0.23079491592943668 Scheduler time: 5.541603268124163 Scheduler overhead time: 0.0403951657935977 Adapter cache time: 0.06821348192170262 Engine time: 0.04052376141771674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.8999267793260515,
    "estimated_duration": 3600.1300194435003,
    "input_throughput": 3735.4856428431985,
    "output_throughput": 3242.67955239143,
    "total_throughput": 6978.165195234628,
    "itl": 134.02287232187354,
    "ttft": 2240335.4335466526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.318187608236453,
    "arrivals": 556060,
    "finished_requests": 54382,
    "scheduler_time": 125.9789697173303
}
#Debug simulation 
Total elapsed time: 5.900025289971381. Arrivals time: 0.2514136047102511 Scheduler time: 5.478990248404443 Scheduler overhead time: 0.040279945358633995 Adapter cache time: 0.07008424354717135 Engine time: 0.04021798074245453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.968663648236543,
    "estimated_duration": 3600.1564040744474,
    "input_throughput": 3936.876460133622,
    "output_throughput": 3433.29500518678,
    "total_throughput": 7370.171465320402,
    "itl": 150.4338966487952,
    "ttft": 2213971.7901536296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.476597071443784,
    "arrivals": 554588,
    "finished_requests": 57084,
    "scheduler_time": 120.32907841016198
}
#Debug simulation 
Total elapsed time: 6.968758221250027. Arrivals time: 0.23368395864963531 Scheduler time: 6.587906001601368 Scheduler overhead time: 0.03720283741131425 Adapter cache time: 0.0553699298761785 Engine time: 0.03728118119761348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.825071899686009,
    "estimated_duration": 3600.042461424548,
    "input_throughput": 3704.9424119075893,
    "output_throughput": 3231.7610485616888,
    "total_throughput": 6936.703460469278,
    "itl": 133.54620991675276,
    "ttft": 2249777.725518507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.29561984626981,
    "arrivals": 554588,
    "finished_requests": 53672,
    "scheduler_time": 126.23111656308055
}
#Debug simulation 
Total elapsed time: 5.825165355578065. Arrivals time: 0.22280595870688558 Scheduler time: 5.433154966682196 Scheduler overhead time: 0.04039645893499255 Adapter cache time: 0.06886208150535822 Engine time: 0.0409001475200057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.814026058651507,
    "estimated_duration": 3600.063299952265,
    "input_throughput": 3706.0759459915357,
    "output_throughput": 3233.211204968073,
    "total_throughput": 6939.287150959609,
    "itl": 133.49782561293475,
    "ttft": 2249312.9352949257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.831320572547245,
    "arrivals": 554588,
    "finished_requests": 53698,
    "scheduler_time": 126.28214859667833
}
#Debug simulation 
Total elapsed time: 5.814117880072445. Arrivals time: 0.21931016817688942 Scheduler time: 5.4270266317762434 Scheduler overhead time: 0.04025871027261019 Adapter cache time: 0.06808331049978733 Engine time: 0.040471441112458706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.845795246306807,
    "estimated_duration": 3600.0962318574334,
    "input_throughput": 3707.573392590517,
    "output_throughput": 3234.3888190989655,
    "total_throughput": 6941.9622116894825,
    "itl": 133.43918241591476,
    "ttft": 2249145.46986774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.232921048339318,
    "arrivals": 554588,
    "finished_requests": 53723,
    "scheduler_time": 126.3378630750232
}
#Debug simulation 
Total elapsed time: 5.84588733734563. Arrivals time: 0.25365387462079525 Scheduler time: 5.424299251753837 Scheduler overhead time: 0.040571740828454494 Adapter cache time: 0.06800809921696782 Engine time: 0.04031231300905347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.1433581481687725,
    "estimated_duration": 3600.0946548430297,
    "input_throughput": 3924.3012627444373,
    "output_throughput": 3428.452911202182,
    "total_throughput": 7352.754173946619,
    "itl": 150.3582417954138,
    "ttft": 2176433.1306224987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.557979680331727,
    "arrivals": 438250,
    "finished_requests": 57215,
    "scheduler_time": 119.4847841284375
}
#Debug simulation 
Total elapsed time: 7.143449921160936. Arrivals time: 0.2533142720349133 Scheduler time: 6.734906517434865 Scheduler overhead time: 0.037681208457797766 Adapter cache time: 0.06291507929563522 Engine time: 0.03735942393541336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.110670465044677,
    "estimated_duration": 3600.040647378785,
    "input_throughput": 3673.9607397572695,
    "output_throughput": 3220.066420205696,
    "total_throughput": 6894.027159962966,
    "itl": 132.788976044139,
    "ttft": 2215423.54788572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3907,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.54441254055516,
    "arrivals": 438250,
    "finished_requests": 53694,
    "scheduler_time": 125.69234868388399
}
#Debug simulation 
Total elapsed time: 6.110735631082207. Arrivals time: 0.49211298348382115 Scheduler time: 5.440128104295582 Scheduler overhead time: 0.04030313063412905 Adapter cache time: 0.07855054177343845 Engine time: 0.04050935013219714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.862337628845125,
    "estimated_duration": 3600.036317013995,
    "input_throughput": 3687.669465237895,
    "output_throughput": 3231.8585079303716,
    "total_throughput": 6919.527973168267,
    "itl": 133.55319851938697,
    "ttft": 2213752.950853791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4021,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.521632880434275,
    "arrivals": 438250,
    "finished_requests": 53898,
    "scheduler_time": 125.3961496540679
}
#Debug simulation 
Total elapsed time: 5.862431575078517. Arrivals time: 0.21890708524733782 Scheduler time: 5.462837654165924 Scheduler overhead time: 0.04132583737373352 Adapter cache time: 0.07965160952880979 Engine time: 0.04065021639689803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.897880951408297,
    "estimated_duration": 3600.0179446795764,
    "input_throughput": 3693.127702224097,
    "output_throughput": 3235.4591502006297,
    "total_throughput": 6928.586852424726,
    "itl": 133.5223562498822,
    "ttft": 2213172.6478713523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3844,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.539792095555296,
    "arrivals": 438250,
    "finished_requests": 53967,
    "scheduler_time": 125.45059445202261
}
#Debug simulation 
Total elapsed time: 5.897971878293902. Arrivals time: 0.21199989272281528 Scheduler time: 5.509859325364232 Scheduler overhead time: 0.0403898092918098 Adapter cache time: 0.07649591471999884 Engine time: 0.040321031119674444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.561251537874341,
    "estimated_duration": 3600.0225652029376,
    "input_throughput": 3936.181438685288,
    "output_throughput": 3432.2723750214777,
    "total_throughput": 7368.453813706766,
    "itl": 150.94814477658088,
    "ttft": 2174035.2086665793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3004,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.863676731977147,
    "arrivals": 426621,
    "finished_requests": 57389,
    "scheduler_time": 119.32504095572911
}
#Debug simulation 
Total elapsed time: 6.56135322060436. Arrivals time: 0.2247184319421649 Scheduler time: 6.184794589877129 Scheduler overhead time: 0.03678090497851372 Adapter cache time: 0.060683056246489286 Engine time: 0.03708271076902747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.639240525197238,
    "estimated_duration": 3600.1190973789144,
    "input_throughput": 3703.3335951876575,
    "output_throughput": 3231.51253759079,
    "total_throughput": 6934.846132778448,
    "itl": 133.50365283802708,
    "ttft": 2212065.2782593425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3537,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.88250606230857,
    "arrivals": 426621,
    "finished_requests": 53973,
    "scheduler_time": 125.45487502226358
}
#Debug simulation 
Total elapsed time: 5.639332201331854. Arrivals time: 0.23145643435418606 Scheduler time: 5.237898992374539 Scheduler overhead time: 0.04038580181077123 Adapter cache time: 0.07035559043288231 Engine time: 0.04027896421030164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.664966979064047,
    "estimated_duration": 3600.0983041590994,
    "input_throughput": 3705.4902041392866,
    "output_throughput": 3232.9106087336568,
    "total_throughput": 6938.400812872944,
    "itl": 133.44237821680693,
    "ttft": 2211825.6891852748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.237180878129518,
    "arrivals": 426621,
    "finished_requests": 54001,
    "scheduler_time": 125.51025189007659
}
#Debug simulation 
Total elapsed time: 5.665088092908263. Arrivals time: 0.21320152888074517 Scheduler time: 5.279187992680818 Scheduler overhead time: 0.04093964537605643 Adapter cache time: 0.07159499172121286 Engine time: 0.040964019019156694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.61984627507627,
    "estimated_duration": 3600.100564078094,
    "input_throughput": 3715.5625966313523,
    "output_throughput": 3239.925883289015,
    "total_throughput": 6955.488479920367,
    "itl": 133.82110003885094,
    "ttft": 2210784.3937568055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.53524092021588,
    "arrivals": 426621,
    "finished_requests": 54113,
    "scheduler_time": 125.37752021645663
}
#Debug simulation 
Total elapsed time: 5.619939874857664. Arrivals time: 0.21179084852337837 Scheduler time: 5.235959996469319 Scheduler overhead time: 0.04049274604767561 Adapter cache time: 0.07194081414490938 Engine time: 0.04062739899381995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.433534648269415,
    "estimated_duration": 3600.1466727861066,
    "input_throughput": 3919.5300310028124,
    "output_throughput": 3439.5915293131466,
    "total_throughput": 7359.121560315959,
    "itl": 150.92018603009362,
    "ttft": 2170361.5928515955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.623596306322444,
    "arrivals": 420933,
    "finished_requests": 57467,
    "scheduler_time": 119.20241780803042
}
#Debug simulation 
Total elapsed time: 6.433656106237322. Arrivals time: 0.2262818911112845 Scheduler time: 6.06280646007508 Scheduler overhead time: 0.03701942041516304 Adapter cache time: 0.053258690517395735 Engine time: 0.03704192955046892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.3718945030122995,
    "estimated_duration": 3600.1236993032144,
    "input_throughput": 3699.393441002509,
    "output_throughput": 3242.128597486544,
    "total_throughput": 6941.522038489054,
    "itl": 134.04614452109374,
    "ttft": 2208451.4880185206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.100494739058718,
    "arrivals": 420933,
    "finished_requests": 54223,
    "scheduler_time": 125.00929923732187
}
#Debug simulation 
Total elapsed time: 5.37199094472453. Arrivals time: 0.21011962043121457 Scheduler time: 4.994309300556779 Scheduler overhead time: 0.040155884344130754 Adapter cache time: 0.06820966582745314 Engine time: 0.040309317875653505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.400167210958898,
    "estimated_duration": 3600.006746469083,
    "input_throughput": 3699.8236220150893,
    "output_throughput": 3242.056424324047,
    "total_throughput": 6941.880046339136,
    "itl": 133.75042749086654,
    "ttft": 2209785.7977572354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.42883126711996,
    "arrivals": 420933,
    "finished_requests": 54219,
    "scheduler_time": 125.15250178587502
}
#Debug simulation 
Total elapsed time: 5.400263585150242. Arrivals time: 0.2123040771111846 Scheduler time: 5.021696323528886 Scheduler overhead time: 0.040129104163497686 Adapter cache time: 0.06701799994334579 Engine time: 0.040209812112152576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.404513937886804,
    "estimated_duration": 3600.0610216641617,
    "input_throughput": 3703.267783454727,
    "output_throughput": 3246.3505284134176,
    "total_throughput": 6949.618311868145,
    "itl": 133.92181521971,
    "ttft": 2208382.8276014547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.83711795002389,
    "arrivals": 420933,
    "finished_requests": 54289,
    "scheduler_time": 125.10901597427309
}
#Debug simulation 
Total elapsed time: 5.404636851046234. Arrivals time: 0.21080310875549912 Scheduler time: 5.023654862772673 Scheduler overhead time: 0.04015166312456131 Adapter cache time: 0.07077049231156707 Engine time: 0.04030399117618799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.242340357974172,
    "estimated_duration": 3600.026296683989,
    "input_throughput": 3940.8276025838554,
    "output_throughput": 3435.877957723089,
    "total_throughput": 7376.705560306945,
    "itl": 150.84064279299014,
    "ttft": 2164738.3428511755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.446587498635484,
    "arrivals": 418062,
    "finished_requests": 57655,
    "scheduler_time": 119.35405445366257
}
#Debug simulation 
Total elapsed time: 6.242463571019471. Arrivals time: 0.24228342482820153 Scheduler time: 5.859579322859645 Scheduler overhead time: 0.036896660923957825 Adapter cache time: 0.04952079802751541 Engine time: 0.036974654998630285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.218270658981055,
    "estimated_duration": 3600.096823073967,
    "input_throughput": 3706.239486250023,
    "output_throughput": 3235.4235378742997,
    "total_throughput": 6941.663024124323,
    "itl": 133.53101570278986,
    "ttft": 2203183.9745746283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3141,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.969936160235076,
    "arrivals": 418062,
    "finished_requests": 54237,
    "scheduler_time": 125.37763741226551
}
#Debug simulation 
Total elapsed time: 5.2184007917530835. Arrivals time: 0.2094545061700046 Scheduler time: 4.845616051461548 Scheduler overhead time: 0.03999815275892615 Adapter cache time: 0.06417965330183506 Engine time: 0.04024642426520586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.208064583595842,
    "estimated_duration": 3600.0572639309257,
    "input_throughput": 3710.728196976901,
    "output_throughput": 3238.609595689949,
    "total_throughput": 6949.33779266685,
    "itl": 133.74234308689094,
    "ttft": 2202221.4908577306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.895915533984567,
    "arrivals": 418062,
    "finished_requests": 54293,
    "scheduler_time": 125.31584178894698
}
#Debug simulation 
Total elapsed time: 5.208184223622084. Arrivals time: 0.21095023909583688 Scheduler time: 4.830994289834052 Scheduler overhead time: 0.04009017162024975 Adapter cache time: 0.06668795924633741 Engine time: 0.04047699039801955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.214389125350863,
    "estimated_duration": 3600.0124340689576,
    "input_throughput": 3712.7910652499636,
    "output_throughput": 3240.209086393551,
    "total_throughput": 6953.000151643514,
    "itl": 133.6885979836903,
    "ttft": 2202034.8403470716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.396627405124466,
    "arrivals": 418062,
    "finished_requests": 54320,
    "scheduler_time": 125.36537054005896
}
#Debug simulation 
Total elapsed time: 5.214508518110961. Arrivals time: 0.23351968079805374 Scheduler time: 4.815468674059957 Scheduler overhead time: 0.040117721538990736 Adapter cache time: 0.06612699665129185 Engine time: 0.040232195518910885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.1025480958633125,
    "estimated_duration": 3600.1094768594603,
    "input_throughput": 3936.4291811377116,
    "output_throughput": 3441.7561687065618,
    "total_throughput": 7378.185349844273,
    "itl": 150.87198546272415,
    "ttft": 2165358.120114067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.177004964501055,
    "arrivals": 416618,
    "finished_requests": 57620,
    "scheduler_time": 119.27273884650701
}
#Debug simulation 
Total elapsed time: 6.102636863011867. Arrivals time: 0.21515896543860435 Scheduler time: 5.749118683394045 Scheduler overhead time: 0.03656161157414317 Adapter cache time: 0.0480480445548892 Engine time: 0.03666007751598954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.230342461261898,
    "estimated_duration": 3600.0120639482348,
    "input_throughput": 3699.3098254771558,
    "output_throughput": 3241.8946916528494,
    "total_throughput": 6941.204517130005,
    "itl": 133.49807319812916,
    "ttft": 2202651.3276138497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.97697211514177,
    "arrivals": 416618,
    "finished_requests": 54193,
    "scheduler_time": 125.30841990666431
}
#Debug simulation 
Total elapsed time: 5.230437083169818. Arrivals time: 0.20492648659273982 Scheduler time: 4.863395003136247 Scheduler overhead time: 0.040308522060513496 Adapter cache time: 0.0619376883842051 Engine time: 0.04074813285842538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.164992136880755,
    "estimated_duration": 3600.0364618844383,
    "input_throughput": 3706.7980119886806,
    "output_throughput": 3248.2998780181188,
    "total_throughput": 6955.097890006799,
    "itl": 133.9531529578725,
    "ttft": 2201417.377087617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2939,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.176940816143045,
    "arrivals": 416618,
    "finished_requests": 54297,
    "scheduler_time": 125.133443158845
}
#Debug simulation 
Total elapsed time: 5.165113898925483. Arrivals time: 0.21320400293916464 Scheduler time: 4.791673345956951 Scheduler overhead time: 0.03987752553075552 Adapter cache time: 0.061349096707999706 Engine time: 0.04008796252310276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.216197345871478,
    "estimated_duration": 3600.1068792422593,
    "input_throughput": 3707.084386008287,
    "output_throughput": 3249.4501947848394,
    "total_throughput": 6956.534580793126,
    "itl": 133.8984532289877,
    "ttft": 2200817.6848076866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2942,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.781495407159905,
    "arrivals": 416618,
    "finished_requests": 54310,
    "scheduler_time": 125.18405718828531
}
#Debug simulation 
Total elapsed time: 5.216315014287829. Arrivals time: 0.216687825974077 Scheduler time: 4.8382243840023875 Scheduler overhead time: 0.04022616986185312 Adapter cache time: 0.061770942993462086 Engine time: 0.04044318804517388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_384_slots_128_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_384_slots_128_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.30667231278494,
    "estimated_duration": 3600.002495185129,
    "input_throughput": 3999.891672091606,
    "output_throughput": 3431.8240102677123,
    "total_throughput": 7431.715682359318,
    "itl": 150.27246222252518,
    "ttft": 2160815.6204537004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.099689756622727,
    "arrivals": 403575,
    "finished_requests": 57817,
    "scheduler_time": 119.34479259590329
}
#Debug simulation 
Total elapsed time: 6.306789038237184. Arrivals time: 0.22993796411901712 Scheduler time: 5.931816088967025 Scheduler overhead time: 0.03701744042336941 Adapter cache time: 0.05345442332327366 Engine time: 0.037124231457710266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_384_slots_128_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_384_slots_128_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.1620789701119065,
    "estimated_duration": 3600.029721240694,
    "input_throughput": 3767.625561526067,
    "output_throughput": 3233.1146966166416,
    "total_throughput": 7000.740258142709,
    "itl": 133.496241794372,
    "ttft": 2198720.307431701,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3459,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.24018524771959,
    "arrivals": 403575,
    "finished_requests": 54406,
    "scheduler_time": 125.18683215958256
}
#Debug simulation 
Total elapsed time: 5.162172225303948. Arrivals time: 0.2070929817855358 Scheduler time: 4.784983427263796 Scheduler overhead time: 0.040118541568517685 Adapter cache time: 0.07062970427796245 Engine time: 0.04036980075761676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_384_slots_128_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_384_slots_128_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.434913609176874,
    "estimated_duration": 3600.1180358489664,
    "input_throughput": 3770.3174909372456,
    "output_throughput": 3235.187814405489,
    "total_throughput": 7005.505305342735,
    "itl": 133.3480120746754,
    "ttft": 2198734.878377374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.306223833291483,
    "arrivals": 403575,
    "finished_requests": 54441,
    "scheduler_time": 125.28302246158518
}
#Debug simulation 
Total elapsed time: 5.434977799188346. Arrivals time: 0.47962277568876743 Scheduler time: 4.787694253027439 Scheduler overhead time: 0.0401083086617291 Adapter cache time: 0.06795391859486699 Engine time: 0.04062555497512221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_384_slots_128_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_384_slots_128_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.250987253151834,
    "estimated_duration": 3600.0764491451528,
    "input_throughput": 3772.2182269836717,
    "output_throughput": 3237.4159728656587,
    "total_throughput": 7009.63419984933,
    "itl": 133.3860587478032,
    "ttft": 2198128.5864411145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.56488493724903,
    "arrivals": 403575,
    "finished_requests": 54478,
    "scheduler_time": 125.29919018478911
}
#Debug simulation 
Total elapsed time: 5.251115750987083. Arrivals time: 0.22908629570156336 Scheduler time: 4.854044388979673 Scheduler overhead time: 0.04073999775573611 Adapter cache time: 0.06754074105992913 Engine time: 0.040670521557331085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.866812182124704,
    "estimated_duration": 3600.1527218213314,
    "input_throughput": 3964.6681968477787,
    "output_throughput": 3435.206491391107,
    "total_throughput": 7399.874688238886,
    "itl": 150.4045547202745,
    "ttft": 2156680.493906094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2397,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.849944449584443,
    "arrivals": 397780,
    "finished_requests": 57560,
    "scheduler_time": 119.23310217886417
}
#Debug simulation 
Total elapsed time: 5.866939378902316. Arrivals time: 0.2152084745466709 Scheduler time: 5.510575463529676 Scheduler overhead time: 0.03677290212363005 Adapter cache time: 0.05035011703148484 Engine time: 0.036839236970990896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.979977107141167,
    "estimated_duration": 3600.076202472517,
    "input_throughput": 3731.5110137873685,
    "output_throughput": 3238.1531235348884,
    "total_throughput": 6969.6641373222565,
    "itl": 133.47600772875143,
    "ttft": 2195092.3636216475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.28053850598573,
    "arrivals": 397780,
    "finished_requests": 54221,
    "scheduler_time": 125.10914098676713
}
#Debug simulation 
Total elapsed time: 4.980108024086803. Arrivals time: 0.22350542899221182 Scheduler time: 4.59088469017297 Scheduler overhead time: 0.04034134140238166 Adapter cache time: 0.06565106566995382 Engine time: 0.040650369599461555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.936125391628593,
    "estimated_duration": 3600.031292403365,
    "input_throughput": 3734.889757311997,
    "output_throughput": 3241.6029340148443,
    "total_throughput": 6976.492691326842,
    "itl": 133.63955056047837,
    "ttft": 2193982.707637276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.81652256306781,
    "arrivals": 397780,
    "finished_requests": 54267,
    "scheduler_time": 125.0659153152252
}
#Debug simulation 
Total elapsed time: 4.93621861981228. Arrivals time: 0.20303862681612372 Scheduler time: 4.568613800685853 Scheduler overhead time: 0.04006402846425772 Adapter cache time: 0.06508970214053988 Engine time: 0.040418400429189205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.0909161400049925,
    "estimated_duration": 3600.1281777598006,
    "input_throughput": 3737.341098886204,
    "output_throughput": 3243.1928596679572,
    "total_throughput": 6980.5339585541615,
    "itl": 133.58043252665263,
    "ttft": 2194178.037287707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.377475642302752,
    "arrivals": 397780,
    "finished_requests": 54294,
    "scheduler_time": 125.12220031976328
}
#Debug simulation 
Total elapsed time: 5.09103929810226. Arrivals time: 0.2237070226110518 Scheduler time: 4.701619881205261 Scheduler overhead time: 0.04084204277023673 Adapter cache time: 0.06490608863532543 Engine time: 0.040800897404551506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.71607776498422,
    "estimated_duration": 3600.084558492892,
    "input_throughput": 3913.9586226548963,
    "output_throughput": 3442.238035983195,
    "total_throughput": 7356.196658638091,
    "itl": 151.08341112018283,
    "ttft": 2160259.667127464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.9323458303189,
    "arrivals": 394936,
    "finished_requests": 57278,
    "scheduler_time": 119.16477582037435
}
#Debug simulation 
Total elapsed time: 5.716196972876787. Arrivals time: 0.21673978073522449 Scheduler time: 5.363137332722545 Scheduler overhead time: 0.036691928282380104 Adapter cache time: 0.045336305629462004 Engine time: 0.036996906157583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.868951107840985,
    "estimated_duration": 3600.099216155598,
    "input_throughput": 3690.738005323276,
    "output_throughput": 3247.0657885043033,
    "total_throughput": 6937.803793827579,
    "itl": 134.24249991262707,
    "ttft": 2198188.0889195777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2845,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.808204251056985,
    "arrivals": 394936,
    "finished_requests": 53998,
    "scheduler_time": 124.97251318715001
}
#Debug simulation 
Total elapsed time: 4.869050635024905. Arrivals time: 0.20412424160167575 Scheduler time: 4.506487806327641 Scheduler overhead time: 0.04004025785252452 Adapter cache time: 0.0589897558093071 Engine time: 0.04042937979102135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_384_slots_128_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_384_slots_128_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.8074049688875675,
    "estimated_duration": 3600.070735932609,
    "input_throughput": 3688.384471856823,
    "output_throughput": 3245.244012399511,
    "total_throughput": 6933.628484256335,
    "itl": 134.02295199068857,
    "ttft": 2198042.1551752416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2875,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.7197821418806,
    "arrivals": 394936,
    "finished_requests": 53975,
    "scheduler_time": 125.07756360749775
}
#Debug simulation 
Total elapsed time: 4.807502815965563. Arrivals time: 0.2047836664132774 Scheduler time: 4.44410235574469 Scheduler overhead time: 0.039915002416819334 Adapter cache time: 0.059818115551024675 Engine time: 0.040082776453346014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_384_slots_128_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_384_slots_128_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.808549481909722,
    "estimated_duration": 3600.1194228958398,
    "input_throughput": 3691.8303085926664,
    "output_throughput": 3247.063118422062,
    "total_throughput": 6938.893427014728,
    "itl": 133.93607988324456,
    "ttft": 2198320.1590588596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2868,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.309085257557623,
    "arrivals": 394936,
    "finished_requests": 54003,
    "scheduler_time": 125.13699423329251
}
#Debug simulation 
Total elapsed time: 4.808681454043835. Arrivals time: 0.20471743401139975 Scheduler time: 4.445676789619029 Scheduler overhead time: 0.039790306240320206 Adapter cache time: 0.059234782587736845 Engine time: 0.04027564264833927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.51896709529683,
    "estimated_duration": 3600.092505890602,
    "input_throughput": 3988.770837555907,
    "output_throughput": 3442.647648559234,
    "total_throughput": 7431.418486115141,
    "itl": 150.54386440396107,
    "ttft": 2153343.875984945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1982,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.10579470132513,
    "arrivals": 393551,
    "finished_requests": 58108,
    "scheduler_time": 119.14724867442975
}
#Debug simulation 
Total elapsed time: 5.519087831955403. Arrivals time: 0.21816018410027027 Scheduler time: 5.165958740282804 Scheduler overhead time: 0.03675886010751128 Adapter cache time: 0.043914567679166794 Engine time: 0.037047225050628185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.732704693917185,
    "estimated_duration": 3600.143131218425,
    "input_throughput": 3750.9197573013676,
    "output_throughput": 3249.1674840830933,
    "total_throughput": 7000.087241384461,
    "itl": 133.9192434733188,
    "ttft": 2190835.2944123447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.41571582324755,
    "arrivals": 393551,
    "finished_requests": 54723,
    "scheduler_time": 124.89109344780981
}
#Debug simulation 
Total elapsed time: 4.7327949199825525. Arrivals time: 0.20498018339276314 Scheduler time: 4.369219919666648 Scheduler overhead time: 0.04035347420722246 Adapter cache time: 0.05868919985368848 Engine time: 0.040707026142627 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.711523897945881,
    "estimated_duration": 3600.0232028041128,
    "input_throughput": 3752.374426219786,
    "output_throughput": 3249.9471089205153,
    "total_throughput": 7002.321535140301,
    "itl": 133.88959569144862,
    "ttft": 2190734.098398139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2818,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.35034576437479,
    "arrivals": 393551,
    "finished_requests": 54743,
    "scheduler_time": 124.91327401235598
}
#Debug simulation 
Total elapsed time: 4.711648647207767. Arrivals time: 0.20447724778205156 Scheduler time: 4.3483636933378875 Scheduler overhead time: 0.04020931897684932 Adapter cache time: 0.059136563912034035 Engine time: 0.04044060595333576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.7249845350161195,
    "estimated_duration": 3600.150579064237,
    "input_throughput": 3752.5185970169805,
    "output_throughput": 3249.828789950522,
    "total_throughput": 7002.347386967503,
    "itl": 133.65741052794036,
    "ttft": 2190872.4827662986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.951585684885625,
    "arrivals": 393551,
    "finished_requests": 54743,
    "scheduler_time": 125.04019831149884
}
#Debug simulation 
Total elapsed time: 4.725107172969729. Arrivals time: 0.20475158654153347 Scheduler time: 4.360966869164258 Scheduler overhead time: 0.0402106698602438 Adapter cache time: 0.059803908225148916 Engine time: 0.040475417859852314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.341298209037632,
    "estimated_duration": 3600.152343301984,
    "input_throughput": 3956.483126748952,
    "output_throughput": 3441.1227133342554,
    "total_throughput": 7397.605840083207,
    "itl": 150.81705965442853,
    "ttft": 2147870.8120133374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.196842191596906,
    "arrivals": 386177,
    "finished_requests": 57686,
    "scheduler_time": 119.06777909950289
}
#Debug simulation 
Total elapsed time: 5.34139137621969. Arrivals time: 0.23055795300751925 Scheduler time: 4.9749095579609275 Scheduler overhead time: 0.03646178497001529 Adapter cache time: 0.04549402929842472 Engine time: 0.03686563204973936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.572460372932255,
    "estimated_duration": 3600.106131921186,
    "input_throughput": 3733.71270386044,
    "output_throughput": 3244.545180607391,
    "total_throughput": 6978.257884467831,
    "itl": 133.83084353250567,
    "ttft": 2186036.2746741334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2818,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.646944153448064,
    "arrivals": 386177,
    "finished_requests": 54409,
    "scheduler_time": 124.9549236033141
}
#Debug simulation 
Total elapsed time: 4.572555881924927. Arrivals time: 0.20274569420143962 Scheduler time: 4.212120361626148 Scheduler overhead time: 0.040141148027032614 Adapter cache time: 0.05845840834081173 Engine time: 0.04022923531010747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.565035831183195,
    "estimated_duration": 3600.0179096890893,
    "input_throughput": 3737.114741510247,
    "output_throughput": 3248.129118615934,
    "total_throughput": 6985.243860126181,
    "itl": 133.95727150604247,
    "ttft": 2185425.690672274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.249024588689657,
    "arrivals": 386177,
    "finished_requests": 54462,
    "scheduler_time": 124.92365821034242
}
#Debug simulation 
Total elapsed time: 4.565159337129444. Arrivals time: 0.20448978012427688 Scheduler time: 4.202456011902541 Scheduler overhead time: 0.039941752795130014 Adapter cache time: 0.05911334231495857 Engine time: 0.0402654898352921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.586585457902402,
    "estimated_duration": 3600.0941791947703,
    "input_throughput": 3738.0969302892036,
    "output_throughput": 3249.0938897093706,
    "total_throughput": 6987.190819998574,
    "itl": 133.90989741899082,
    "ttft": 2184972.605865033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2826,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.040960578053625,
    "arrivals": 386177,
    "finished_requests": 54483,
    "scheduler_time": 124.97262950264574
}
#Debug simulation 
Total elapsed time: 4.586712216958404. Arrivals time: 0.2026306837797165 Scheduler time: 4.22565625840798 Scheduler overhead time: 0.04027825500816107 Adapter cache time: 0.05886220932006836 Engine time: 0.04038836667314172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.100472921971232,
    "estimated_duration": 3600.117188558675,
    "input_throughput": 3940.008410025906,
    "output_throughput": 3440.246345135924,
    "total_throughput": 7380.25475516183,
    "itl": 150.57912125820727,
    "ttft": 2149808.788634764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1990,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.158693973580732,
    "arrivals": 383371,
    "finished_requests": 57515,
    "scheduler_time": 119.17256006663459
}
#Debug simulation 
Total elapsed time: 5.100593160837889. Arrivals time: 0.21103519201278687 Scheduler time: 4.755109124816954 Scheduler overhead time: 0.03752426663413644 Adapter cache time: 0.04287262726575136 Engine time: 0.03677482716739178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.442253345157951,
    "estimated_duration": 3600.105934222644,
    "input_throughput": 3711.816886545424,
    "output_throughput": 3244.7478528218153,
    "total_throughput": 6956.56473936724,
    "itl": 133.8393489567288,
    "ttft": 2187589.4560616785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.8096873220704,
    "arrivals": 383371,
    "finished_requests": 54272,
    "scheduler_time": 124.98890086888709
}
#Debug simulation 
Total elapsed time: 4.442346375901252. Arrivals time: 0.1980618080124259 Scheduler time: 4.091226349584758 Scheduler overhead time: 0.039911650121212006 Adapter cache time: 0.05408230796456337 Engine time: 0.04029879346489906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_384_slots_128_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_384_slots_128_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.553589484654367,
    "estimated_duration": 3600.0324527548296,
    "input_throughput": 3717.749541329338,
    "output_throughput": 3248.808768668206,
    "total_throughput": 6966.558309997545,
    "itl": 133.98076436004615,
    "ttft": 2186947.2484437786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.610547731933575,
    "arrivals": 383371,
    "finished_requests": 54335,
    "scheduler_time": 124.94980455583561
}
#Debug simulation 
Total elapsed time: 4.553695595823228. Arrivals time: 0.21046349965035915 Scheduler time: 4.187212185934186 Scheduler overhead time: 0.0410280036740005 Adapter cache time: 0.05486115161329508 Engine time: 0.040972234681248665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_384_slots_128_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_384_slots_128_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.463318991009146,
    "estimated_duration": 3600.1001395525996,
    "input_throughput": 3718.9026640975158,
    "output_throughput": 3249.8493226507817,
    "total_throughput": 6968.751986748297,
    "itl": 133.93210375569973,
    "ttft": 2186798.753396257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.425828580089064,
    "arrivals": 383371,
    "finished_requests": 54352,
    "scheduler_time": 124.99068385923827
}
#Debug simulation 
Total elapsed time: 4.4634246290661395. Arrivals time: 0.2023357325233519 Scheduler time: 4.107775694690645 Scheduler overhead time: 0.040046296548098326 Adapter cache time: 0.05411146627739072 Engine time: 0.04020550940185785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_384_slots_128_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.054431160911918,
    "estimated_duration": 3600.0029738914263,
    "input_throughput": 3971.3978304150114,
    "output_throughput": 3442.447434037525,
    "total_throughput": 7413.845264452537,
    "itl": 150.53162534160452,
    "ttft": 2145702.6600433174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1823,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.054421665245057,
    "arrivals": 381963,
    "finished_requests": 57886,
    "scheduler_time": 119.09861865447584
}
#Debug simulation 
Total elapsed time: 5.054524997714907. Arrivals time: 0.21189778205007315 Scheduler time: 4.712758008390665 Scheduler overhead time: 0.036459123715758324 Adapter cache time: 0.03943304065614939 Engine time: 0.0367928221821785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_384_slots_128_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.714024176355451,
    "estimated_duration": 3600.0171454871793,
    "input_throughput": 3748.9357562972373,
    "output_throughput": 3247.7406433067886,
    "total_throughput": 6996.676399604025,
    "itl": 133.71843723193095,
    "ttft": 2184473.7156221434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.74658490932974,
    "arrivals": 381963,
    "finished_requests": 54646,
    "scheduler_time": 124.95048882085676
}
#Debug simulation 
Total elapsed time: 4.714089555200189. Arrivals time: 0.4618582404218614 Scheduler time: 4.100193836260587 Scheduler overhead time: 0.03987287450581789 Adapter cache time: 0.05259580630809069 Engine time: 0.04074283828958869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_384_slots_128_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.402337090112269,
    "estimated_duration": 3600.141233136442,
    "input_throughput": 3750.460086321919,
    "output_throughput": 3249.53696047308,
    "total_throughput": 6999.997046794999,
    "itl": 133.72556128256974,
    "ttft": 2184425.32800338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.86251461403476,
    "arrivals": 381963,
    "finished_requests": 54674,
    "scheduler_time": 124.96845319810501
}
#Debug simulation 
Total elapsed time: 4.402417093049735. Arrivals time: 0.18819561321288347 Scheduler time: 4.063178609590977 Scheduler overhead time: 0.03989315405488014 Adapter cache time: 0.05202660197392106 Engine time: 0.04025995219126344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_384_slots_128_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.43399456795305,
    "estimated_duration": 3600.085435885911,
    "input_throughput": 3751.3620830714053,
    "output_throughput": 3250.010370134669,
    "total_throughput": 7001.372453206074,
    "itl": 133.68338518413842,
    "ttft": 2184230.186153745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2471,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.774668644150784,
    "arrivals": 381963,
    "finished_requests": 54682,
    "scheduler_time": 125.00253334533475
}
#Debug simulation 
Total elapsed time: 4.434079298749566. Arrivals time: 0.19073968566954136 Scheduler time: 4.090531790163368 Scheduler overhead time: 0.04014947218820453 Adapter cache time: 0.053070973604917526 Engine time: 0.040557350032031536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_384_slots_128_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.69922700477764,
    "estimated_duration": 3600.0912464244057,
    "input_throughput": 3964.445905135113,
    "output_throughput": 3441.86887271447,
    "total_throughput": 7406.314777849583,
    "itl": 150.24590311234516,
    "ttft": 2145507.851620676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.712102631759176,
    "arrivals": 377711,
    "finished_requests": 57792,
    "scheduler_time": 119.13330456301055
}
#Debug simulation 
Total elapsed time: 4.699310340918601. Arrivals time: 0.19600156787782907 Scheduler time: 4.376340003218502 Scheduler overhead time: 0.036653371062129736 Adapter cache time: 0.03609406063333154 Engine time: 0.037000511307269335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_384_slots_128_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.21654126374051,
    "estimated_duration": 3600.128519113512,
    "input_throughput": 3740.121200816345,
    "output_throughput": 3248.566804742797,
    "total_throughput": 6988.688005559143,
    "itl": 133.50545398555735,
    "ttft": 2183330.8906079116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2076,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.175705920662486,
    "arrivals": 377711,
    "finished_requests": 54499,
    "scheduler_time": 125.00560313176183
}
#Debug simulation 
Total elapsed time: 4.21662743575871. Arrivals time: 0.1854445617645979 Scheduler time: 3.885788789950311 Scheduler overhead time: 0.03981743985787034 Adapter cache time: 0.0460260477848351 Engine time: 0.040473305620253086 
