INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 59.681037123315036,
    "estimated_duration": 3600.048773820812,
    "input_throughput": 6425.900162304702,
    "output_throughput": 5585.427938149607,
    "total_throughput": 12011.32810045431,
    "itl": 84.61661608021565,
    "ttft": 1812954.1067293726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 435333,
    "finished_requests": 93014,
    "scheduler_time": 166.46720115208186
}
#Debug simulation 
Total elapsed time: 59.68124331533909. Arrivals time: 0.36819294886663556 Scheduler time: 59.100064306519926 Scheduler overhead time: 0.08110915357246995 Adapter cache time: 0.0151881854981184 Engine time: 0.08275950467213988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 67.98802407784387,
    "estimated_duration": 3600.096643316413,
    "input_throughput": 6921.47752373823,
    "output_throughput": 6018.078442483579,
    "total_throughput": 12939.55596622181,
    "itl": 95.95549563225276,
    "ttft": 1820945.696113402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7377868202095854,
    "arrivals": 435333,
    "finished_requests": 100186,
    "scheduler_time": 159.21450605453668
}
#Debug simulation 
Total elapsed time: 67.98821238381788. Arrivals time: 0.38886896008625627 Scheduler time: 67.40665149269626 Scheduler overhead time: 0.07436038786545396 Adapter cache time: 0.013940485659986734 Engine time: 0.07391363568603992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 51.22288273181766,
    "estimated_duration": 3600.0347623174525,
    "input_throughput": 6749.834822249766,
    "output_throughput": 5865.06503242985,
    "total_throughput": 12614.899854679617,
    "itl": 89.81607079055422,
    "ttft": 1840699.3819507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7512167263450101,
    "arrivals": 435333,
    "finished_requests": 97611,
    "scheduler_time": 161.65012836936543
}
#Debug simulation 
Total elapsed time: 51.22304241079837. Arrivals time: 0.36810618126764894 Scheduler time: 50.661500678863376 Scheduler overhead time: 0.0740631502121687 Adapter cache time: 0.013492254074662924 Engine time: 0.07454930152744055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 68.00245980126783,
    "estimated_duration": 3600.0225045109814,
    "input_throughput": 6930.91556198184,
    "output_throughput": 6023.77012166643,
    "total_throughput": 12954.68568364827,
    "itl": 96.09325377501047,
    "ttft": 1820556.9947561289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6961401910101997,
    "arrivals": 435333,
    "finished_requests": 100316,
    "scheduler_time": 159.15483951342904
}
#Debug simulation 
Total elapsed time: 68.00262202508748. Arrivals time: 0.3845595666207373 Scheduler time: 67.42470914591104 Scheduler overhead time: 0.0750727434642613 Adapter cache time: 0.013953396119177341 Engine time: 0.0738984108902514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 51.57264590682462,
    "estimated_duration": 3600.0714882863545,
    "input_throughput": 6744.472735889374,
    "output_throughput": 5857.657568360663,
    "total_throughput": 12602.130304250037,
    "itl": 89.69635154428978,
    "ttft": 1840477.2772511237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7447960938699544,
    "arrivals": 435333,
    "finished_requests": 97525,
    "scheduler_time": 161.63347522684873
}
#Debug simulation 
Total elapsed time: 51.572808586061. Arrivals time: 0.3757291631773114 Scheduler time: 50.999569532461464 Scheduler overhead time: 0.07624380337074399 Adapter cache time: 0.014155713375657797 Engine time: 0.07556151878088713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 68.50345525564626,
    "estimated_duration": 3600.0206424029548,
    "input_throughput": 6546.7863496105865,
    "output_throughput": 5697.273165164328,
    "total_throughput": 12244.059514774915,
    "itl": 86.25554087532944,
    "ttft": 1816518.0069173563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6192403312353417,
    "arrivals": 435333,
    "finished_requests": 94754,
    "scheduler_time": 164.45707540333217
}
#Debug simulation 
Total elapsed time: 68.503619230818. Arrivals time: 0.37279646238312125 Scheduler time: 67.92217379435897 Scheduler overhead time: 0.08050276571884751 Adapter cache time: 0.014790347777307034 Engine time: 0.08045428292825818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 49.83878688607365,
    "estimated_duration": 3600.0570869686044,
    "input_throughput": 6747.46605767139,
    "output_throughput": 5864.745610958732,
    "total_throughput": 12612.211668630123,
    "itl": 89.72453550338031,
    "ttft": 1841505.417246976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7530777879059316,
    "arrivals": 435333,
    "finished_requests": 97604,
    "scheduler_time": 161.73926412352193
}
#Debug simulation 
Total elapsed time: 49.83893861994147. Arrivals time: 0.37017392506822944 Scheduler time: 49.27564547723159 Scheduler overhead time: 0.07387707382440567 Adapter cache time: 0.013568480964750051 Engine time: 0.07460367772728205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.69487853907049,
    "estimated_duration": 3600.078343894942,
    "input_throughput": 7016.40258547027,
    "output_throughput": 6097.358696991894,
    "total_throughput": 13113.761282462165,
    "itl": 98.3581939819707,
    "ttft": 1793644.9721553477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7075277664186426,
    "arrivals": 404972,
    "finished_requests": 101909,
    "scheduler_time": 158.05087248437732
}
#Debug simulation 
Total elapsed time: 77.69505007006228. Arrivals time: 0.4440438821911812 Scheduler time: 77.05742873298004 Scheduler overhead time: 0.07524337992072105 Adapter cache time: 0.013848502654582262 Engine time: 0.0742327868938446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 44.97878482285887,
    "estimated_duration": 3600.045184657057,
    "input_throughput": 6945.900875514164,
    "output_throughput": 6037.79560674337,
    "total_throughput": 12983.696482257534,
    "itl": 95.80419730468043,
    "ttft": 1797512.2247323417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7752593088056896,
    "arrivals": 404972,
    "finished_requests": 100839,
    "scheduler_time": 158.9400701120734
}
#Debug simulation 
Total elapsed time: 44.97894868813455. Arrivals time: 0.6357201659120619 Scheduler time: 44.15998068964109 Scheduler overhead time: 0.06993792485445738 Adapter cache time: 0.013372599612921476 Engine time: 0.07046004617586732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 39.115902305115014,
    "estimated_duration": 3600.013278446,
    "input_throughput": 6753.555923131213,
    "output_throughput": 5881.549695044438,
    "total_throughput": 12635.10561817565,
    "itl": 89.61383429019642,
    "ttft": 1816932.8756407537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8043316271714867,
    "arrivals": 404972,
    "finished_requests": 98127,
    "scheduler_time": 161.44340545941247
}
#Debug simulation 
Total elapsed time: 39.11614769510925. Arrivals time: 0.3861178206279874 Scheduler time: 38.54115605214611 Scheduler overhead time: 0.07227507280185819 Adapter cache time: 0.013377035036683083 Engine time: 0.07253750134259462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 44.50136915408075,
    "estimated_duration": 3600.008353015159,
    "input_throughput": 6946.409993481117,
    "output_throughput": 6038.300433884706,
    "total_throughput": 12984.710427365822,
    "itl": 95.7778041897438,
    "ttft": 1797472.3037803164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7535954794753342,
    "arrivals": 404972,
    "finished_requests": 100846,
    "scheduler_time": 158.9619680807611
}
#Debug simulation 
Total elapsed time: 44.501540596131235. Arrivals time: 0.4107132130302489 Scheduler time: 43.90724613936618 Scheduler overhead time: 0.06995263788849115 Adapter cache time: 0.013124249875545502 Engine time: 0.07111457455903292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 39.65154344495386,
    "estimated_duration": 3600.015861711376,
    "input_throughput": 6752.768858202606,
    "output_throughput": 5880.9376995176635,
    "total_throughput": 12633.70655772027,
    "itl": 89.62502829654571,
    "ttft": 1816901.1578130296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7968754088133575,
    "arrivals": 404972,
    "finished_requests": 98112,
    "scheduler_time": 161.4182421852725
}
#Debug simulation 
Total elapsed time: 39.651680442970246. Arrivals time: 0.39405265636742115 Scheduler time: 39.066982621792704 Scheduler overhead time: 0.07288031652569771 Adapter cache time: 0.01339171128347516 Engine time: 0.07356544118374586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 44.85430449899286,
    "estimated_duration": 3600.031980048241,
    "input_throughput": 6946.150517158714,
    "output_throughput": 6037.948585031381,
    "total_throughput": 12984.099102190095,
    "itl": 95.80396935870063,
    "ttft": 1797489.8299815992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6766956197004762,
    "arrivals": 404972,
    "finished_requests": 100842,
    "scheduler_time": 158.93965837654727
}
#Debug simulation 
Total elapsed time: 44.854454333893955. Arrivals time: 0.35838004294782877 Scheduler time: 44.31242325017229 Scheduler overhead time: 0.07006678963080049 Adapter cache time: 0.013309875968843699 Engine time: 0.07075797393918037 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 39.23833038797602,
    "estimated_duration": 3600.0789330922557,
    "input_throughput": 6752.562499822567,
    "output_throughput": 5880.677449984531,
    "total_throughput": 12633.239949807097,
    "itl": 89.62321781388779,
    "ttft": 1816945.1119188005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7894191904552282,
    "arrivals": 404972,
    "finished_requests": 98111,
    "scheduler_time": 161.42267026147547
}
#Debug simulation 
Total elapsed time: 39.238496703095734. Arrivals time: 0.36216096859425306 Scheduler time: 38.68756748503074 Scheduler overhead time: 0.07211670465767384 Adapter cache time: 0.012974285054951906 Engine time: 0.07306510023772717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 103.63947358494624,
    "estimated_duration": 3600.036422634605,
    "input_throughput": 7021.377295261208,
    "output_throughput": 6096.705817197701,
    "total_throughput": 13118.08311245891,
    "itl": 98.92625763811598,
    "ttft": 1732513.2516220578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.44303140514064543,
    "arrivals": 382341,
    "finished_requests": 102132,
    "scheduler_time": 157.31448644510604
}
#Debug simulation 
Total elapsed time: 103.63962880661711. Arrivals time: 0.43935093190521 Scheduler time: 102.98972665937617 Scheduler overhead time: 0.0825457233004272 Adapter cache time: 0.01598745957016945 Engine time: 0.08014921052381396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.0395256569609,
    "estimated_duration": 3600.082416420619,
    "input_throughput": 6945.650434542309,
    "output_throughput": 6032.001073350656,
    "total_throughput": 12977.651507892966,
    "itl": 96.27636108696213,
    "ttft": 1734025.8922705078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6620108059700581,
    "arrivals": 382341,
    "finished_requests": 101028,
    "scheduler_time": 158.22838311116206
}
#Debug simulation 
Total elapsed time: 102.03968049399555. Arrivals time: 0.44996923953294754 Scheduler time: 101.37617047829553 Scheduler overhead time: 0.08340244367718697 Adapter cache time: 0.016387557610869408 Engine time: 0.08157316315919161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 99.57916555600241,
    "estimated_duration": 3600.0525749407243,
    "input_throughput": 6754.367469314076,
    "output_throughput": 5869.347338725438,
    "total_throughput": 12623.714808039513,
    "itl": 90.01021148441845,
    "ttft": 1738815.662881496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.580846296302043,
    "arrivals": 382341,
    "finished_requests": 98280,
    "scheduler_time": 160.7757733199851
}
#Debug simulation 
Total elapsed time: 99.57933121500537. Arrivals time: 0.4451067838817835 Scheduler time: 98.91325701586902 Scheduler overhead time: 0.08666268503293395 Adapter cache time: 0.017129092011600733 Engine time: 0.08372003445401788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 101.86870000092313,
    "estimated_duration": 3600.063548068727,
    "input_throughput": 6948.6126191910325,
    "output_throughput": 6034.121262013148,
    "total_throughput": 12982.733881204182,
    "itl": 96.23083476608937,
    "ttft": 1734305.655357278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4660357063449921,
    "arrivals": 382341,
    "finished_requests": 101065,
    "scheduler_time": 158.30184220780575
}
#Debug simulation 
Total elapsed time: 101.86885169381276. Arrivals time: 0.46462454879656434 Scheduler time: 101.18994211219251 Scheduler overhead time: 0.08345213532447815 Adapter cache time: 0.01643399754539132 Engine time: 0.0822346406057477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 101.37305319029838,
    "estimated_duration": 3600.002194058605,
    "input_throughput": 6754.788660999393,
    "output_throughput": 5869.801422586578,
    "total_throughput": 12624.59008358597,
    "itl": 90.00835907774474,
    "ttft": 1738758.8672564253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5754612497100607,
    "arrivals": 382341,
    "finished_requests": 98285,
    "scheduler_time": 160.774424613589
}
#Debug simulation 
Total elapsed time: 101.37321628537029. Arrivals time: 0.4228508067317307 Scheduler time: 100.72845632722601 Scheduler overhead time: 0.08687427407130599 Adapter cache time: 0.016341283917427063 Engine time: 0.08499868400394917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 102.07999147940427,
    "estimated_duration": 3600.0556521108615,
    "input_throughput": 6948.971465297122,
    "output_throughput": 6034.374215093667,
    "total_throughput": 12983.34568039079,
    "itl": 96.24281133600064,
    "ttft": 1734432.1078637377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43410662395879673,
    "arrivals": 382341,
    "finished_requests": 101071,
    "scheduler_time": 158.29578034648137
}
#Debug simulation 
Total elapsed time: 102.08014763519168. Arrivals time: 0.4560912041924894 Scheduler time: 101.41026630811393 Scheduler overhead time: 0.08300340734422207 Adapter cache time: 0.01638419646769762 Engine time: 0.08191634528338909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 99.62508028792217,
    "estimated_duration": 3600.0644098803255,
    "input_throughput": 6754.532483714186,
    "output_throughput": 5869.286933314175,
    "total_throughput": 12623.81941702836,
    "itl": 90.01030863832935,
    "ttft": 1738793.7434976485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5698690859414637,
    "arrivals": 382341,
    "finished_requests": 98280,
    "scheduler_time": 160.77586866663148
}
#Debug simulation 
Total elapsed time: 99.62523761997. Arrivals time: 0.4560537035576999 Scheduler time: 98.94511281047016 Scheduler overhead time: 0.08717260230332613 Adapter cache time: 0.016925726551562548 Engine time: 0.08613970596343279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 113.59668666310608,
    "estimated_duration": 3600.0227734841355,
    "input_throughput": 7018.240325059805,
    "output_throughput": 6086.426497461866,
    "total_throughput": 13104.66682252167,
    "itl": 98.47381865120984,
    "ttft": 1699091.0830125918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.482705859332345,
    "arrivals": 378554,
    "finished_requests": 101548,
    "scheduler_time": 157.5425271524223
}
#Debug simulation 
Total elapsed time: 113.59684466803446. Arrivals time: 0.4412637073546648 Scheduler time: 112.94021251378581 Scheduler overhead time: 0.08404788095504045 Adapter cache time: 0.016667602118104696 Engine time: 0.08286687871441245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 113.29294083593413,
    "estimated_duration": 3600.0612800664917,
    "input_throughput": 6942.82098429679,
    "output_throughput": 6021.814717442699,
    "total_throughput": 12964.635701739488,
    "itl": 95.85408365702159,
    "ttft": 1699302.2001605292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5340490563539788,
    "arrivals": 378554,
    "finished_requests": 100463,
    "scheduler_time": 158.4656070637566
}
#Debug simulation 
Total elapsed time: 113.29323141975328. Arrivals time: 0.43049946799874306 Scheduler time: 112.6437757620588 Scheduler overhead time: 0.08563652727752924 Adapter cache time: 0.01638972945511341 Engine time: 0.0837864694185555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 110.73888539616019,
    "estimated_duration": 3600.0874672843524,
    "input_throughput": 6757.5799813417325,
    "output_throughput": 5872.427320758249,
    "total_throughput": 12630.00730209998,
    "itl": 89.64213049018566,
    "ttft": 1699759.7947957015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41000790765974676,
    "arrivals": 378554,
    "finished_requests": 97749,
    "scheduler_time": 160.956400442138
}
#Debug simulation 
Total elapsed time: 110.73905132291839. Arrivals time: 0.4207183588296175 Scheduler time: 110.08850908279419 Scheduler overhead time: 0.08978106174618006 Adapter cache time: 0.01705391937866807 Engine time: 0.0883992025628686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 113.01106276595965,
    "estimated_duration": 3600.0163669884478,
    "input_throughput": 6942.593714069135,
    "output_throughput": 6021.44429085857,
    "total_throughput": 12964.038004927705,
    "itl": 95.85721084144194,
    "ttft": 1699173.0433547094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49934353202115767,
    "arrivals": 378554,
    "finished_requests": 100455,
    "scheduler_time": 158.46147530804993
}
#Debug simulation 
Total elapsed time: 113.01122409012169. Arrivals time: 0.4398627788759768 Scheduler time: 112.35270386189222 Scheduler overhead time: 0.08502117218449712 Adapter cache time: 0.016990493517369032 Engine time: 0.08413546858355403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 111.00971388397738,
    "estimated_duration": 3600.0050322817488,
    "input_throughput": 6756.733610614659,
    "output_throughput": 5872.219291482787,
    "total_throughput": 12628.952902097446,
    "itl": 89.64883339569168,
    "ttft": 1699630.057500311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4058655641274527,
    "arrivals": 378554,
    "finished_requests": 97738,
    "scheduler_time": 160.95167068036275
}
#Debug simulation 
Total elapsed time: 111.00987926404923. Arrivals time: 0.42460048804059625 Scheduler time: 110.35879551526159 Scheduler overhead time: 0.088577417191118 Adapter cache time: 0.016857516020536423 Engine time: 0.08667539758607745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 113.2472770079039,
    "estimated_duration": 3600.050857937126,
    "input_throughput": 6942.31775778888,
    "output_throughput": 6021.3754903510835,
    "total_throughput": 12963.693248139964,
    "itl": 95.85659632167486,
    "ttft": 1699207.1213055248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4660262286616495,
    "arrivals": 378554,
    "finished_requests": 100453,
    "scheduler_time": 158.46239509477425
}
#Debug simulation 
Total elapsed time: 113.24743395112455. Arrivals time: 0.44231480034068227 Scheduler time: 112.58686003554612 Scheduler overhead time: 0.08623293973505497 Adapter cache time: 0.016783259343355894 Engine time: 0.08238801406696439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 111.07824114197865,
    "estimated_duration": 3600.073958441528,
    "input_throughput": 6756.835631935279,
    "output_throughput": 5872.128807362488,
    "total_throughput": 12628.964439297766,
    "itl": 89.64833069330939,
    "ttft": 1699668.3459225409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4027588064782322,
    "arrivals": 378554,
    "finished_requests": 97739,
    "scheduler_time": 160.95419506700483
}
#Debug simulation 
Total elapsed time: 111.07840189011768. Arrivals time: 0.42470216611400247 Scheduler time: 110.42604303685948 Scheduler overhead time: 0.0893952832557261 Adapter cache time: 0.016445400658994913 Engine time: 0.08726965356618166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.47779711103067,
    "estimated_duration": 3600.0059731194815,
    "input_throughput": 7073.319375060622,
    "output_throughput": 6197.927771954689,
    "total_throughput": 13271.247147015312,
    "itl": 97.41714065729332,
    "ttft": 1765691.4983919798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7009153573866926,
    "arrivals": 376669,
    "finished_requests": 103598,
    "scheduler_time": 159.84594619743854
}
#Debug simulation 
Total elapsed time: 83.47795379627496. Arrivals time: 0.418582777492702 Scheduler time: 82.85751184029505 Scheduler overhead time: 0.07795992586761713 Adapter cache time: 0.016018325462937355 Engine time: 0.07706221472471952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.34946008119732,
    "estimated_duration": 3600.0574526589044,
    "input_throughput": 7017.500229431377,
    "output_throughput": 6150.417400601935,
    "total_throughput": 13167.917630033311,
    "itl": 94.5185922226286,
    "ttft": 1775191.183397422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6733904268778859,
    "arrivals": 376669,
    "finished_requests": 102795,
    "scheduler_time": 161.25268961464957
}
#Debug simulation 
Total elapsed time: 82.34962253412232. Arrivals time: 0.41847587190568447 Scheduler time: 81.7264743335545 Scheduler overhead time: 0.07913353061303496 Adapter cache time: 0.01652081310749054 Engine time: 0.077345653437078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.12893875781447,
    "estimated_duration": 3600.0477509582365,
    "input_throughput": 6827.891378234432,
    "output_throughput": 5989.812216868608,
    "total_throughput": 12817.70359510304,
    "itl": 88.41758806697186,
    "ttft": 1795614.1793810166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5294402018701658,
    "arrivals": 376669,
    "finished_requests": 100049,
    "scheduler_time": 163.88317065133145
}
#Debug simulation 
Total elapsed time: 80.12909659417346. Arrivals time: 0.40265042381361127 Scheduler time: 79.51576993754134 Scheduler overhead time: 0.08167793741449714 Adapter cache time: 0.016372821759432554 Engine time: 0.08013169094920158 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 82.6786590591073,
    "estimated_duration": 3600.0026277052198,
    "input_throughput": 7020.267098002084,
    "output_throughput": 6153.236341974652,
    "total_throughput": 13173.503439976736,
    "itl": 94.63992676090157,
    "ttft": 1775056.9764287171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6339630556991321,
    "arrivals": 376669,
    "finished_requests": 102842,
    "scheduler_time": 161.1975605812134
}
#Debug simulation 
Total elapsed time: 82.67882486479357. Arrivals time: 0.4364439002238214 Scheduler time: 82.03902240656316 Scheduler overhead time: 0.07807260565459728 Adapter cache time: 0.01620350917801261 Engine time: 0.07736480282619596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 79.77531974157318,
    "estimated_duration": 3600.057195897243,
    "input_throughput": 6829.271498246983,
    "output_throughput": 5990.628711282168,
    "total_throughput": 12819.900209529153,
    "itl": 88.45361187353232,
    "ttft": 1795645.0560486703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5240551552781836,
    "arrivals": 376669,
    "finished_requests": 100067,
    "scheduler_time": 163.85965729536883
}
#Debug simulation 
Total elapsed time: 79.7754712337628. Arrivals time: 0.41000167839229107 Scheduler time: 79.15285226842389 Scheduler overhead time: 0.08214036095887423 Adapter cache time: 0.01682852441444993 Engine time: 0.08069251198321581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.43157579796389,
    "estimated_duration": 3600.079673455137,
    "input_throughput": 7019.36048424927,
    "output_throughput": 6152.589389429252,
    "total_throughput": 13171.94987367852,
    "itl": 94.58257576158245,
    "ttft": 1775124.4103251158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5937046474730597,
    "arrivals": 376669,
    "finished_requests": 102831,
    "scheduler_time": 161.23180525835247
}
#Debug simulation 
Total elapsed time: 82.4317313330248. Arrivals time: 0.4297599089331925 Scheduler time: 81.79779796302319 Scheduler overhead time: 0.07870724750682712 Adapter cache time: 0.016271653585135937 Engine time: 0.07774658920243382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.73189508682117,
    "estimated_duration": 3600.026909367293,
    "input_throughput": 6829.277285685882,
    "output_throughput": 5990.661054194824,
    "total_throughput": 12819.938339880706,
    "itl": 88.45363277357472,
    "ttft": 1795605.5244005276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5192914602160454,
    "arrivals": 376669,
    "finished_requests": 100066,
    "scheduler_time": 163.85958158756995
}
#Debug simulation 
Total elapsed time: 79.73205846594647. Arrivals time: 0.3993652407079935 Scheduler time: 79.12288430286571 Scheduler overhead time: 0.08050320111215115 Adapter cache time: 0.016600524075329304 Engine time: 0.07988106925040483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 111.95824332488701,
    "estimated_duration": 3600.05618815626,
    "input_throughput": 6965.142122640567,
    "output_throughput": 6099.017307628469,
    "total_throughput": 13064.159430269036,
    "itl": 98.88659087906504,
    "ttft": 1676382.396279639,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 59,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.390132132885046,
    "arrivals": 375735,
    "finished_requests": 101845,
    "scheduler_time": 157.2728580643232
}
#Debug simulation 
Total elapsed time: 111.95853026863188. Arrivals time: 0.7000329988077283 Scheduler time: 111.04330526152626 Scheduler overhead time: 0.08519380493089557 Adapter cache time: 0.016167251858860254 Engine time: 0.08182822167873383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 110.57602646993473,
    "estimated_duration": 3600.0066757790323,
    "input_throughput": 6887.745005260087,
    "output_throughput": 6034.2263102329225,
    "total_throughput": 12921.97131549301,
    "itl": 96.25921759216362,
    "ttft": 1675789.7549906587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3791634020023048,
    "arrivals": 375735,
    "finished_requests": 100716,
    "scheduler_time": 158.13308093590274
}
#Debug simulation 
Total elapsed time: 110.57619049865752. Arrivals time: 0.44233453553169966 Scheduler time: 109.916214492172 Scheduler overhead time: 0.08534118440002203 Adapter cache time: 0.016348653007298708 Engine time: 0.08329760422930121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.81213637627661,
    "estimated_duration": 3600.016128962405,
    "input_throughput": 6713.766309420999,
    "output_throughput": 5877.521722686972,
    "total_throughput": 12591.288032107972,
    "itl": 90.06125406860573,
    "ttft": 1695232.907036619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2413462840113788,
    "arrivals": 375735,
    "finished_requests": 98140,
    "scheduler_time": 160.57291457839614
}
#Debug simulation 
Total elapsed time: 105.81232151342556. Arrivals time: 0.4325148114003241 Scheduler time: 105.15539707290009 Scheduler overhead time: 0.08773435140028596 Adapter cache time: 0.016803817357867956 Engine time: 0.0856244689784944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 110.4395858142525,
    "estimated_duration": 3600.0557238405927,
    "input_throughput": 6887.466167759217,
    "output_throughput": 6034.515203787981,
    "total_throughput": 12921.981371547197,
    "itl": 96.25745808008217,
    "ttft": 1675740.4702892487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3541754244826735,
    "arrivals": 375735,
    "finished_requests": 100718,
    "scheduler_time": 158.13618138526843
}
#Debug simulation 
Total elapsed time: 110.4397496022284. Arrivals time: 0.43809467647224665 Scheduler time: 109.78358530532569 Scheduler overhead time: 0.08576589915901423 Adapter cache time: 0.01662153471261263 Engine time: 0.08315086644142866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 106.23378848005086,
    "estimated_duration": 3600.0826683509754,
    "input_throughput": 6713.809716783875,
    "output_throughput": 5877.565864256181,
    "total_throughput": 12591.375581040056,
    "itl": 90.06583452863046,
    "ttft": 1695187.0743462236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23865376071538769,
    "arrivals": 375735,
    "finished_requests": 98142,
    "scheduler_time": 160.57446227743932
}
#Debug simulation 
Total elapsed time: 106.23396244365722. Arrivals time: 0.42825884791091084 Scheduler time: 105.58153165969998 Scheduler overhead time: 0.08720728615298867 Adapter cache time: 0.016615196131169796 Engine time: 0.0860760216601193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 110.84192097419873,
    "estimated_duration": 3600.0700428097753,
    "input_throughput": 6887.415162802752,
    "output_throughput": 6034.536201146883,
    "total_throughput": 12921.951363949636,
    "itl": 96.25751839480586,
    "ttft": 1675728.8104653584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3319638889096679,
    "arrivals": 375735,
    "finished_requests": 100721,
    "scheduler_time": 158.13909187641488
}
#Debug simulation 
Total elapsed time: 110.84207523800433. Arrivals time: 0.4714344646781683 Scheduler time: 110.15261965058744 Scheduler overhead time: 0.08503049472346902 Adapter cache time: 0.01642045984044671 Engine time: 0.0838530333712697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.19711076840758,
    "estimated_duration": 3600.076333060394,
    "input_throughput": 6713.682367799543,
    "output_throughput": 5877.480376093191,
    "total_throughput": 12591.162743892733,
    "itl": 90.06283541518158,
    "ttft": 1695228.290999984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585537,
    "arrivals": 375735,
    "finished_requests": 98141,
    "scheduler_time": 160.57530168287695
}
#Debug simulation 
Total elapsed time: 105.1972898771055. Arrivals time: 0.434992300812155 Scheduler time: 104.53685277979821 Scheduler overhead time: 0.08770206663757563 Adapter cache time: 0.016633883584290743 Engine time: 0.08705835416913033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 95.83680628286675,
    "estimated_duration": 3600.0877073256806,
    "input_throughput": 7119.724596665567,
    "output_throughput": 6243.817325411208,
    "total_throughput": 13363.541922076776,
    "itl": 96.89682219778197,
    "ttft": 1765076.734297345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.44303140514064543,
    "arrivals": 375251,
    "finished_requests": 104198,
    "scheduler_time": 160.82214520677144
}
#Debug simulation 
Total elapsed time: 95.83696869108826. Arrivals time: 0.43184224609285593 Scheduler time: 95.19583494355902 Scheduler overhead time: 0.08180571161210537 Adapter cache time: 0.01606304058805108 Engine time: 0.08006108598783612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 95.7271528490819,
    "estimated_duration": 3600.0928842153426,
    "input_throughput": 7030.818041106399,
    "output_throughput": 6167.952804039868,
    "total_throughput": 13198.770845146268,
    "itl": 94.4314341493254,
    "ttft": 1770499.5630889891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4965765677578745,
    "arrivals": 375251,
    "finished_requests": 102916,
    "scheduler_time": 161.55231717434125
}
#Debug simulation 
Total elapsed time: 95.7273115911521. Arrivals time: 0.43284971360117197 Scheduler time: 95.08327631605789 Scheduler overhead time: 0.0816876501776278 Adapter cache time: 0.016104968264698982 Engine time: 0.0809791972860694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 92.41445876425132,
    "estimated_duration": 3600.023774493151,
    "input_throughput": 6821.433839963303,
    "output_throughput": 5986.016579302732,
    "total_throughput": 12807.450419266035,
    "itl": 88.44516467944517,
    "ttft": 1783274.6060187435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5017943689599634,
    "arrivals": 375251,
    "finished_requests": 99825,
    "scheduler_time": 163.74958598101875
}
#Debug simulation 
Total elapsed time: 92.41461262712255. Arrivals time: 0.4194809449836612 Scheduler time: 91.77610238827765 Scheduler overhead time: 0.08494978863745928 Adapter cache time: 0.016356983222067356 Engine time: 0.083953398745507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 95.01521949702874,
    "estimated_duration": 3600.038889981607,
    "input_throughput": 7033.966513658794,
    "output_throughput": 6170.705283718059,
    "total_throughput": 13204.671797376854,
    "itl": 94.37770204302537,
    "ttft": 1770396.3012756116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4724196272855626,
    "arrivals": 375251,
    "finished_requests": 102964,
    "scheduler_time": 161.65882955474427
}
#Debug simulation 
Total elapsed time: 95.01537125371397. Arrivals time: 0.4309981851838529 Scheduler time: 94.37339011626318 Scheduler overhead time: 0.08251278335228562 Adapter cache time: 0.016047777142375708 Engine time: 0.08039666758850217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 92.51049703219905,
    "estimated_duration": 3600.0568938079114,
    "input_throughput": 6825.906846713067,
    "output_throughput": 5989.929225032574,
    "total_throughput": 12815.836071745642,
    "itl": 88.38987423835874,
    "ttft": 1783548.2387336309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49723779107444,
    "arrivals": 375251,
    "finished_requests": 99885,
    "scheduler_time": 163.8674061349813
}
#Debug simulation 
Total elapsed time: 92.5106568671763. Arrivals time: 0.43501048209145665 Scheduler time: 91.85576000576839 Scheduler overhead time: 0.08510089013725519 Adapter cache time: 0.016248460393399 Engine time: 0.08483352232724428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 95.04793107928708,
    "estimated_duration": 3600.0777076765985,
    "input_throughput": 7029.3246576424435,
    "output_throughput": 6166.424394857599,
    "total_throughput": 13195.749052500043,
    "itl": 94.36072059257727,
    "ttft": 1770074.6486045893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43410662395879673,
    "arrivals": 375251,
    "finished_requests": 102892,
    "scheduler_time": 161.5914176285319
}
#Debug simulation 
Total elapsed time: 95.04808401502669. Arrivals time: 0.43655247520655394 Scheduler time: 94.4009729186073 Scheduler overhead time: 0.08208359126001596 Adapter cache time: 0.01602362422272563 Engine time: 0.08038114150986075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 92.50886984495446,
    "estimated_duration": 3600.009634424453,
    "input_throughput": 6825.745899407443,
    "output_throughput": 5989.6389703544,
    "total_throughput": 12815.384869761843,
    "itl": 88.38931432117766,
    "ttft": 1783510.8836718183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49268121318891656,
    "arrivals": 375251,
    "finished_requests": 99881,
    "scheduler_time": 163.86520262188813
}
#Debug simulation 
Total elapsed time: 92.50914016319439. Arrivals time: 0.4216633504256606 Scheduler time: 91.86925349896774 Scheduler overhead time: 0.08370721293613315 Adapter cache time: 0.016831522807478905 Engine time: 0.0842135981656611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.43819727003574,
    "estimated_duration": 3600.048412676086,
    "input_throughput": 7047.1252305024655,
    "output_throughput": 6179.1157923524015,
    "total_throughput": 13226.241022854867,
    "itl": 97.92947180408918,
    "ttft": 1734782.9230390852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 65,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42980658707674557,
    "arrivals": 375015,
    "finished_requests": 103322,
    "scheduler_time": 159.16537581480983
}
#Debug simulation 
Total elapsed time: 105.43835655180737. Arrivals time: 0.7038786564953625 Scheduler time: 104.52329590218142 Scheduler overhead time: 0.08243818720802665 Adapter cache time: 0.01593959704041481 Engine time: 0.08094197185710073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.10581533890218,
    "estimated_duration": 3600.0096549682744,
    "input_throughput": 6963.610212934533,
    "output_throughput": 6107.573342103652,
    "total_throughput": 13071.183555038186,
    "itl": 95.43205583348657,
    "ttft": 1736139.7329754608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489666,
    "arrivals": 375015,
    "finished_requests": 102094,
    "scheduler_time": 159.92770272829196
}
#Debug simulation 
Total elapsed time: 104.1059783026576. Arrivals time: 0.6973185986280441 Scheduler time: 103.19362138025463 Scheduler overhead time: 0.08441172819584608 Adapter cache time: 0.0162306884303689 Engine time: 0.08180209062993526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.41132919397205,
    "estimated_duration": 3600.012711980474,
    "input_throughput": 6748.059782998834,
    "output_throughput": 5923.902693184621,
    "total_throughput": 12671.962476183455,
    "itl": 89.56587489286332,
    "ttft": 1739155.3921330073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 61,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45675554150715475,
    "arrivals": 375015,
    "finished_requests": 99011,
    "scheduler_time": 161.83118022934528
}
#Debug simulation 
Total elapsed time: 102.41149726090953. Arrivals time: 0.6917575062252581 Scheduler time: 101.50060039479285 Scheduler overhead time: 0.08527438342571259 Adapter cache time: 0.016395492013543844 Engine time: 0.08438410609960556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 104.24936828576028,
    "estimated_duration": 3600.020670547423,
    "input_throughput": 6963.588349671329,
    "output_throughput": 6107.341321642107,
    "total_throughput": 13070.929671313435,
    "itl": 95.43217888919463,
    "ttft": 1736130.9968278327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4377235806360842,
    "arrivals": 375015,
    "finished_requests": 102092,
    "scheduler_time": 159.9295363270186
}
#Debug simulation 
Total elapsed time: 104.24953053472564. Arrivals time: 0.4529470312409103 Scheduler time: 103.58199532376602 Scheduler overhead time: 0.0835376032628119 Adapter cache time: 0.01627821708098054 Engine time: 0.0820210981182754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 101.48155706888065,
    "estimated_duration": 3600.071254580752,
    "input_throughput": 6748.464761381308,
    "output_throughput": 5924.071634099824,
    "total_throughput": 12672.536395481133,
    "itl": 89.56528292667987,
    "ttft": 1739143.7392105053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 61,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4526131979748607,
    "arrivals": 375015,
    "finished_requests": 99016,
    "scheduler_time": 161.83560736898153
}
#Debug simulation 
Total elapsed time: 101.48171820864081. Arrivals time: 0.44196767872199416 Scheduler time: 100.8190792305395 Scheduler overhead time: 0.08578881807625294 Adapter cache time: 0.01619982486590743 Engine time: 0.0849655563943088 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.71431881282479,
    "estimated_duration": 3600.0894721465606,
    "input_throughput": 6963.578876013949,
    "output_throughput": 6107.5226518994905,
    "total_throughput": 13071.10152791344,
    "itl": 95.43406978925177,
    "ttft": 1736197.0206692936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 375015,
    "finished_requests": 102097,
    "scheduler_time": 159.9317196638816
}
#Debug simulation 
Total elapsed time: 104.71448337472975. Arrivals time: 0.6920247315429151 Scheduler time: 103.80867288634181 Scheduler overhead time: 0.0835013622418046 Adapter cache time: 0.015912785194814205 Engine time: 0.0820196601562202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.24801676301286,
    "estimated_duration": 3600.0183947808346,
    "input_throughput": 6747.787187759323,
    "output_throughput": 5923.737787261578,
    "total_throughput": 12671.5249750209,
    "itl": 89.56095269695993,
    "ttft": 1739093.0936371295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 61,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4484708544425667,
    "arrivals": 375015,
    "finished_requests": 99006,
    "scheduler_time": 161.8396521062951
}
#Debug simulation 
Total elapsed time: 102.24817793397233. Arrivals time: 0.4281793241389096 Scheduler time: 101.59864263981581 Scheduler overhead time: 0.0861580646596849 Adapter cache time: 0.016486759763211012 Engine time: 0.08437668578699231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 33.34627242293209,
    "estimated_duration": 3600.034670428737,
    "input_throughput": 7006.97446255312,
    "output_throughput": 6103.158166913806,
    "total_throughput": 13110.132629466927,
    "itl": 98.6761598322698,
    "ttft": 1727730.0467697673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.94557449156884,
    "arrivals": 344340,
    "finished_requests": 102104,
    "scheduler_time": 157.04935447520896
}
#Debug simulation 
Total elapsed time: 33.34639721503481. Arrivals time: 0.35766285518184304 Scheduler time: 32.81103288382292 Scheduler overhead time: 0.06805191840976477 Adapter cache time: 0.012963585089892149 Engine time: 0.0680943476036191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 39.70859559485689,
    "estimated_duration": 3600.1023071822933,
    "input_throughput": 6913.766020022071,
    "output_throughput": 6028.976720105459,
    "total_throughput": 12942.74274012753,
    "itl": 96.05559640111177,
    "ttft": 1717996.9426548088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.811343576428481,
    "arrivals": 344340,
    "finished_requests": 100917,
    "scheduler_time": 157.76183373610579
}
#Debug simulation 
Total elapsed time: 39.70875326730311. Arrivals time: 0.37348873913288116 Scheduler time: 39.148000861518085 Scheduler overhead time: 0.07223389437422156 Adapter cache time: 0.013048947788774967 Engine time: 0.07220979919657111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 34.58568080607802,
    "estimated_duration": 3600.0167651240936,
    "input_throughput": 6735.150856766693,
    "output_throughput": 5868.815446827986,
    "total_throughput": 12603.96630359468,
    "itl": 89.7054896103643,
    "ttft": 1754296.1104325198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 142,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0681973249092704,
    "arrivals": 344340,
    "finished_requests": 98214,
    "scheduler_time": 160.39336782736189
}
#Debug simulation 
Total elapsed time: 34.58583743683994. Arrivals time: 0.3525774157606065 Scheduler time: 34.04180875932798 Scheduler overhead time: 0.07284351252019405 Adapter cache time: 0.013809983618557453 Engine time: 0.07394316419959068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 29.572100874967873,
    "estimated_duration": 3600.053317343152,
    "input_throughput": 6905.464394162576,
    "output_throughput": 6018.275311541675,
    "total_throughput": 12923.739705704253,
    "itl": 95.60168425687655,
    "ttft": 1740096.2894483998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1857661762181657,
    "arrivals": 344340,
    "finished_requests": 100698,
    "scheduler_time": 158.12882436158904
}
#Debug simulation 
Total elapsed time: 29.5722087030299. Arrivals time: 0.347319349180907 Scheduler time: 29.05369525635615 Scheduler overhead time: 0.06464573740959167 Adapter cache time: 0.012430269736796618 Engine time: 0.06574324145913124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 27.62488870183006,
    "estimated_duration": 3600.0348161758398,
    "input_throughput": 6718.147527720655,
    "output_throughput": 5851.573964047758,
    "total_throughput": 12569.721491768412,
    "itl": 89.52579739781561,
    "ttft": 1762894.915951885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2804964729771033,
    "arrivals": 344340,
    "finished_requests": 97881,
    "scheduler_time": 160.45581773282515
}
#Debug simulation 
Total elapsed time: 27.62505567399785. Arrivals time: 0.3735110559500754 Scheduler time: 27.07100719353184 Scheduler overhead time: 0.06826890306547284 Adapter cache time: 0.013023835141211748 Engine time: 0.06934796599671245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 38.7583332513459,
    "estimated_duration": 3600.0503532013804,
    "input_throughput": 6923.813712170509,
    "output_throughput": 6035.398360658593,
    "total_throughput": 12959.212072829101,
    "itl": 96.18375947867055,
    "ttft": 1717220.3829814275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7086152244033287,
    "arrivals": 344340,
    "finished_requests": 101027,
    "scheduler_time": 157.78214989176684
}
#Debug simulation 
Total elapsed time: 38.758578130975366. Arrivals time: 0.3660435504280031 Scheduler time: 38.20773183275014 Scheduler overhead time: 0.07059306977316737 Adapter cache time: 0.01351557020097971 Engine time: 0.07130559999495745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 35.59301056433469,
    "estimated_duration": 3600.0202764383025,
    "input_throughput": 6740.242869967029,
    "output_throughput": 5868.189448338529,
    "total_throughput": 12608.432318305557,
    "itl": 89.83345853217207,
    "ttft": 1743596.1559243822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8257605930045249,
    "arrivals": 344340,
    "finished_requests": 98278,
    "scheduler_time": 160.29348116962825
}
#Debug simulation 
Total elapsed time: 35.59319341601804. Arrivals time: 0.39705316396430135 Scheduler time: 35.0032181320712 Scheduler overhead time: 0.07356727961450815 Adapter cache time: 0.014193602371960878 Engine time: 0.07402423350140452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 44.84876862214878,
    "estimated_duration": 3600.015096200554,
    "input_throughput": 7005.636178197568,
    "output_throughput": 6093.553891802323,
    "total_throughput": 13099.190069999891,
    "itl": 98.55740679194392,
    "ttft": 1669581.4137467504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7273649935144924,
    "arrivals": 321710,
    "finished_requests": 101850,
    "scheduler_time": 156.43212443584517
}
#Debug simulation 
Total elapsed time: 44.84889134392142. Arrivals time: 0.6366773177869618 Scheduler time: 44.023127330001444 Scheduler overhead time: 0.07245299592614174 Adapter cache time: 0.01367608504369855 Engine time: 0.07312855776399374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 34.60765524068847,
    "estimated_duration": 3600.096939798792,
    "input_throughput": 6942.961097430054,
    "output_throughput": 6035.34748184152,
    "total_throughput": 12978.308579271574,
    "itl": 96.07663662416735,
    "ttft": 1687921.5171697417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8346600021701311,
    "arrivals": 321710,
    "finished_requests": 100883,
    "scheduler_time": 157.3443015866535
}
#Debug simulation 
Total elapsed time: 34.607823906000704. Arrivals time: 0.3765562307089567 Scheduler time: 34.04476605542004 Scheduler overhead time: 0.071430838201195 Adapter cache time: 0.013170668389648199 Engine time: 0.0721277049742639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.10470896633342,
    "estimated_duration": 3600.0024650687164,
    "input_throughput": 6774.3809168570915,
    "output_throughput": 5884.408748480022,
    "total_throughput": 12658.789665337114,
    "itl": 89.68519107756093,
    "ttft": 1724051.2438304028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.449318551863546,
    "arrivals": 321710,
    "finished_requests": 98393,
    "scheduler_time": 160.10745002874876
}
#Debug simulation 
Total elapsed time: 21.104828851297498. Arrivals time: 0.33337704138830304 Scheduler time: 20.58910584729165 Scheduler overhead time: 0.068870744202286 Adapter cache time: 0.013172876089811325 Engine time: 0.07026093685999513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 26.465447298251092,
    "estimated_duration": 3600.053037297667,
    "input_throughput": 6962.581312084006,
    "output_throughput": 6044.331506941852,
    "total_throughput": 13006.912819025858,
    "itl": 96.0239097294274,
    "ttft": 1702480.9613091252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.250719753475859,
    "arrivals": 321710,
    "finished_requests": 101099,
    "scheduler_time": 157.47487670509764
}
#Debug simulation 
Total elapsed time: 26.465537197887897. Arrivals time: 0.3658431158401072 Scheduler time: 25.91957211261615 Scheduler overhead time: 0.0681553864851594 Adapter cache time: 0.013280089478939772 Engine time: 0.06941214762628078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 21.496302573941648,
    "estimated_duration": 3600.016192775435,
    "input_throughput": 6772.03148389304,
    "output_throughput": 5879.90438556353,
    "total_throughput": 12651.935869456569,
    "itl": 89.66209956135056,
    "ttft": 1724097.176407068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.41820209941827,
    "arrivals": 321710,
    "finished_requests": 98319,
    "scheduler_time": 160.07665083957096
}
#Debug simulation 
Total elapsed time: 21.49643814796582. Arrivals time: 0.3405523463152349 Scheduler time: 20.9739333614707 Scheduler overhead time: 0.06953787012025714 Adapter cache time: 0.013057316653430462 Engine time: 0.06925534596666694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.48711267206818,
    "estimated_duration": 3600.0064809292835,
    "input_throughput": 6948.480268719653,
    "output_throughput": 6036.043855785172,
    "total_throughput": 12984.524124504826,
    "itl": 95.99469935738324,
    "ttft": 1702600.9120765624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2576324252923916,
    "arrivals": 321710,
    "finished_requests": 100913,
    "scheduler_time": 157.36064086097397
}
#Debug simulation 
Total elapsed time: 30.487262472976. Arrivals time: 0.36208063177764416 Scheduler time: 29.940779976081103 Scheduler overhead time: 0.07012489438056946 Adapter cache time: 0.013420991599559784 Engine time: 0.07125056348741055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.87593678338453,
    "estimated_duration": 3600.090376535218,
    "input_throughput": 6774.041329337564,
    "output_throughput": 5881.657898927155,
    "total_throughput": 12655.699228264719,
    "itl": 89.53977054740301,
    "ttft": 1724342.4358198568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4195970576442802,
    "arrivals": 321710,
    "finished_requests": 98355,
    "scheduler_time": 160.20079445218198
}
#Debug simulation 
Total elapsed time: 21.876029423438013. Arrivals time: 0.5733733023516834 Scheduler time: 21.121646714396775 Scheduler overhead time: 0.06806415831670165 Adapter cache time: 0.012979301158338785 Engine time: 0.07014974765479565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 38.57341658184305,
    "estimated_duration": 3600.057186231101,
    "input_throughput": 6984.289054120021,
    "output_throughput": 6104.0891472631565,
    "total_throughput": 13088.378201383179,
    "itl": 99.11188238622287,
    "ttft": 1693184.9091710192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6546284941630431,
    "arrivals": 317887,
    "finished_requests": 101804,
    "scheduler_time": 156.1255473029575
}
#Debug simulation 
Total elapsed time: 38.57355835288763. Arrivals time: 0.3849232802167535 Scheduler time: 38.00057188887149 Scheduler overhead time: 0.07173404330387712 Adapter cache time: 0.01375131355598569 Engine time: 0.07278111856430769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 32.59126027999446,
    "estimated_duration": 3600.013134631158,
    "input_throughput": 6903.406757305141,
    "output_throughput": 6042.824897145506,
    "total_throughput": 12946.231654450647,
    "itl": 96.31024360710678,
    "ttft": 1697421.9576735673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7377868202095852,
    "arrivals": 317887,
    "finished_requests": 100756,
    "scheduler_time": 157.20425873777944
}
#Debug simulation 
Total elapsed time: 32.591416930779815. Arrivals time: 0.3653179877437651 Scheduler time: 32.0405711773783 Scheduler overhead time: 0.07113794842734933 Adapter cache time: 0.013074897695332766 Engine time: 0.07155054528266191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.846799206919968,
    "estimated_duration": 3600.04509478784,
    "input_throughput": 6712.997577444021,
    "output_throughput": 5870.081747196947,
    "total_throughput": 12583.079324640968,
    "itl": 89.93819162196209,
    "ttft": 1723009.289475965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7327540091378657,
    "arrivals": 317887,
    "finished_requests": 97863,
    "scheduler_time": 159.6160853539609
}
#Debug simulation 
Total elapsed time: 17.8469191188924. Arrivals time: 0.31610419042408466 Scheduler time: 17.354014318902045 Scheduler overhead time: 0.06613755924627185 Adapter cache time: 0.01282856659963727 Engine time: 0.06798651721328497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 34.140267624054104,
    "estimated_duration": 3600.0087078897736,
    "input_throughput": 6912.546890639895,
    "output_throughput": 6045.145933204319,
    "total_throughput": 12957.692823844214,
    "itl": 96.44020932700887,
    "ttft": 1696020.8031358693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 105,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7175112118525427,
    "arrivals": 317887,
    "finished_requests": 100877,
    "scheduler_time": 157.14463353840983
}
#Debug simulation 
Total elapsed time: 34.14038430713117. Arrivals time: 0.3782793916761875 Scheduler time: 33.57427652506158 Scheduler overhead time: 0.0720595414750278 Adapter cache time: 0.013516833074390888 Engine time: 0.07251395098865032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 19.075156040955335,
    "estimated_duration": 3600.077637953328,
    "input_throughput": 6723.350003573948,
    "output_throughput": 5877.431024523453,
    "total_throughput": 12600.7810280974,
    "itl": 90.19512791242661,
    "ttft": 1721075.5549140377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.721464447123002,
    "arrivals": 317887,
    "finished_requests": 98091,
    "scheduler_time": 159.51364774096083
}
#Debug simulation 
Total elapsed time: 19.075386380776763. Arrivals time: 0.34193140966817737 Scheduler time: 18.554485202301294 Scheduler overhead time: 0.06692021898925304 Adapter cache time: 0.01312000211328268 Engine time: 0.06900414358824492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 42.74001664901152,
    "estimated_duration": 3600.0397872957715,
    "input_throughput": 6887.509434618054,
    "output_throughput": 6043.235432223456,
    "total_throughput": 12930.74486684151,
    "itl": 96.46486628800204,
    "ttft": 1687968.5571808768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7086152244033287,
    "arrivals": 317887,
    "finished_requests": 100634,
    "scheduler_time": 157.11581934409242
}
#Debug simulation 
Total elapsed time: 42.74019149504602. Arrivals time: 0.39096238603815436 Scheduler time: 42.155645829159766 Scheduler overhead time: 0.07414078153669834 Adapter cache time: 0.014170389622449875 Engine time: 0.07477617263793945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.02642088616267,
    "estimated_duration": 3600.0598823065834,
    "input_throughput": 6713.14916698429,
    "output_throughput": 5863.530243967853,
    "total_throughput": 12576.679410952142,
    "itl": 89.7072191169651,
    "ttft": 1729677.5264780738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2180263834074236,
    "arrivals": 317887,
    "finished_requests": 97830,
    "scheduler_time": 159.69927335620926
}
#Debug simulation 
Total elapsed time: 21.02649352606386. Arrivals time: 0.33429859625175595 Scheduler time: 20.511075352318585 Scheduler overhead time: 0.0681314468383789 Adapter cache time: 0.013586649671196938 Engine time: 0.06959552317857742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 29.240335774142295,
    "estimated_duration": 3600.0395904607085,
    "input_throughput": 7022.460271545151,
    "output_throughput": 6115.796909106325,
    "total_throughput": 13138.257180651475,
    "itl": 98.57225818114784,
    "ttft": 1681712.3560269005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0976598993036883,
    "arrivals": 315978,
    "finished_requests": 102241,
    "scheduler_time": 156.63951085640204
}
#Debug simulation 
Total elapsed time: 29.240508070215583. Arrivals time: 0.6136910021305084 Scheduler time: 28.4458672744222 Scheduler overhead time: 0.07012931443750858 Adapter cache time: 0.013362369500100613 Engine time: 0.06862334627658129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 24.950641013216227,
    "estimated_duration": 3600.0659195598028,
    "input_throughput": 6980.690232214659,
    "output_throughput": 6063.221754191938,
    "total_throughput": 13043.911986406598,
    "itl": 95.8966948908911,
    "ttft": 1694986.0797181653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.563286459492521,
    "arrivals": 315978,
    "finished_requests": 101480,
    "scheduler_time": 157.7683682907183
}
#Debug simulation 
Total elapsed time: 24.950789041351527. Arrivals time: 0.5926459059119225 Scheduler time: 24.18033421691507 Scheduler overhead time: 0.06685021705925465 Adapter cache time: 0.013780086301267147 Engine time: 0.06840114388614893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.541637438349426,
    "estimated_duration": 3600.0299229057523,
    "input_throughput": 6770.4912242303335,
    "output_throughput": 5889.771322479948,
    "total_throughput": 12660.262546710283,
    "itl": 89.76695349556623,
    "ttft": 1722063.0312347577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6549429295910552,
    "arrivals": 315978,
    "finished_requests": 98457,
    "scheduler_time": 160.0170118252555
}
#Debug simulation 
Total elapsed time: 21.541721824090928. Arrivals time: 0.3417948819696903 Scheduler time: 21.018794012721628 Scheduler overhead time: 0.06816283892840147 Adapter cache time: 0.01380632258951664 Engine time: 0.06911865575239062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 34.41405653068796,
    "estimated_duration": 3600.0831596542534,
    "input_throughput": 6962.034733221857,
    "output_throughput": 6058.913095244958,
    "total_throughput": 13020.947828466815,
    "itl": 95.91078976077755,
    "ttft": 1684822.4939749173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0913955830829212,
    "arrivals": 315978,
    "finished_requests": 101218,
    "scheduler_time": 157.68970897202303
}
#Debug simulation 
Total elapsed time: 34.414219573605806. Arrivals time: 0.3737010029144585 Scheduler time: 33.853009378071874 Scheduler overhead time: 0.07197597529739141 Adapter cache time: 0.014107523020356894 Engine time: 0.07183411298319697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 17.09369342867285,
    "estimated_duration": 3600.0399975866535,
    "input_throughput": 6762.557087232473,
    "output_throughput": 5880.247167862322,
    "total_throughput": 12642.804255094796,
    "itl": 89.91282962268271,
    "ttft": 1720494.3233569432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.026796111329464,
    "arrivals": 315978,
    "finished_requests": 98350,
    "scheduler_time": 159.74749091564917
}
#Debug simulation 
Total elapsed time: 17.09377919510007. Arrivals time: 0.5688669341616333 Scheduler time: 16.349315219093114 Scheduler overhead time: 0.06564033310860395 Adapter cache time: 0.013111164327710867 Engine time: 0.06710101431235671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.51011251611635,
    "estimated_duration": 3600.0775226769283,
    "input_throughput": 6946.016535056726,
    "output_throughput": 6045.952861541553,
    "total_throughput": 12991.969396598279,
    "itl": 95.60405683558739,
    "ttft": 1688433.1419657264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0661147970752767,
    "arrivals": 315978,
    "finished_requests": 100995,
    "scheduler_time": 157.74065505889558
}
#Debug simulation 
Total elapsed time: 30.510261623188853. Arrivals time: 0.6223095636814833 Scheduler time: 29.70348867820576 Scheduler overhead time: 0.07036552717909217 Adapter cache time: 0.013948550447821617 Engine time: 0.07069584541022778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 20.30925388680771,
    "estimated_duration": 3600.024332597805,
    "input_throughput": 6778.402795514171,
    "output_throughput": 5895.922371358958,
    "total_throughput": 12674.32516687313,
    "itl": 89.69934476978261,
    "ttft": 1721832.6228542924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.011571322958922,
    "arrivals": 315978,
    "finished_requests": 98582,
    "scheduler_time": 160.13445334071778
}
#Debug simulation 
Total elapsed time: 20.30934621486813. Arrivals time: 0.5682109813205898 Scheduler time: 19.56096913618967 Scheduler overhead time: 0.06763504818081856 Adapter cache time: 0.013802612666040659 Engine time: 0.0688575110398233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 32.5616041989997,
    "estimated_duration": 3600.036873320964,
    "input_throughput": 7058.7863108629845,
    "output_throughput": 6132.456076660775,
    "total_throughput": 13191.242387523758,
    "itl": 97.9920038040152,
    "ttft": 1680334.403860533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9918613547924895,
    "arrivals": 314998,
    "finished_requests": 102624,
    "scheduler_time": 157.1909737311102
}
#Debug simulation 
Total elapsed time: 32.5617195321247. Arrivals time: 0.3771054074168205 Scheduler time: 32.00048193708062 Scheduler overhead time: 0.0706159183755517 Adapter cache time: 0.013598897960036993 Engine time: 0.07070036698132753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 42.30447427928448,
    "estimated_duration": 3600.090123723922,
    "input_throughput": 6978.7594578359185,
    "output_throughput": 6066.032585153887,
    "total_throughput": 13044.792042989806,
    "itl": 95.65322982642853,
    "ttft": 1675584.2064588766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6967068526195365,
    "arrivals": 314998,
    "finished_requests": 101458,
    "scheduler_time": 157.8778902996608
}
#Debug simulation 
Total elapsed time: 42.30464952532202. Arrivals time: 0.3929037721827626 Scheduler time: 41.714472581166774 Scheduler overhead time: 0.07564081670716405 Adapter cache time: 0.015261448454111814 Engine time: 0.07603586930781603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 28.46029674494639,
    "estimated_duration": 3600.0655686405394,
    "input_throughput": 6794.132366104753,
    "output_throughput": 5909.267371492186,
    "total_throughput": 12703.399737596937,
    "itl": 89.2904656918816,
    "ttft": 1713645.2043666593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1340467596938841,
    "arrivals": 314998,
    "finished_requests": 98785,
    "scheduler_time": 160.58428899806734
}
#Debug simulation 
Total elapsed time: 28.460393182002008. Arrivals time: 0.34579015150666237 Scheduler time: 27.924263399560004 Scheduler overhead time: 0.07255687797442079 Adapter cache time: 0.0137158059515059 Engine time: 0.07325656712055206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 42.62791610416025,
    "estimated_duration": 3600.0817312504955,
    "input_throughput": 6986.520550816324,
    "output_throughput": 6071.490769296403,
    "total_throughput": 13058.011320112728,
    "itl": 95.7775658959145,
    "ttft": 1675401.0044745277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6600559233874081,
    "arrivals": 314998,
    "finished_requests": 101560,
    "scheduler_time": 157.8637904255985
}
#Debug simulation 
Total elapsed time: 42.628185452893376. Arrivals time: 0.6347711156122386 Scheduler time: 41.797936491668224 Scheduler overhead time: 0.07489747414365411 Adapter cache time: 0.014769701287150383 Engine time: 0.07503600977361202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 27.954536608885974,
    "estimated_duration": 3600.0208832749813,
    "input_throughput": 6783.570926895271,
    "output_throughput": 5898.703004378646,
    "total_throughput": 12682.273931273918,
    "itl": 89.08457577744947,
    "ttft": 1714811.5768394063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0979609915241602,
    "arrivals": 314998,
    "finished_requests": 98599,
    "scheduler_time": 160.61734071086573
}
#Debug simulation 
Total elapsed time: 27.95465803006664. Arrivals time: 0.34663453279063106 Scheduler time: 27.41838274570182 Scheduler overhead time: 0.07185530895367265 Adapter cache time: 0.013794036582112312 Engine time: 0.07282279757782817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 38.40255993884057,
    "estimated_duration": 3600.018205989627,
    "input_throughput": 6971.258355928524,
    "output_throughput": 6060.628238962541,
    "total_throughput": 13031.886594891064,
    "itl": 95.37308024248587,
    "ttft": 1679955.8447782665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8745971688581616,
    "arrivals": 314998,
    "finished_requests": 101336,
    "scheduler_time": 158.0135736098014
}
#Debug simulation 
Total elapsed time: 38.40272937202826. Arrivals time: 0.3844416872598231 Scheduler time: 37.825705100782216 Scheduler overhead time: 0.07357461517676711 Adapter cache time: 0.014693233650177717 Engine time: 0.07394590834155679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 32.12772994861007,
    "estimated_duration": 3600.0913201206217,
    "input_throughput": 6782.853774715315,
    "output_throughput": 5907.644031454016,
    "total_throughput": 12690.497806169331,
    "itl": 89.19378417444523,
    "ttft": 1707603.2281853957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.095888323243708,
    "arrivals": 314998,
    "finished_requests": 98639,
    "scheduler_time": 160.6721577601776
}
#Debug simulation 
Total elapsed time: 32.127874192781746. Arrivals time: 0.36150836665183306 Scheduler time: 31.575697123538703 Scheduler overhead time: 0.07285737758502364 Adapter cache time: 0.013596199918538332 Engine time: 0.07380211725831032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 25.05014587799087,
    "estimated_duration": 3600.0311205292105,
    "input_throughput": 7082.6984951873865,
    "output_throughput": 6117.242118941097,
    "total_throughput": 13199.940614128484,
    "itl": 98.58944599700469,
    "ttft": 1682768.0968913976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.031535808984189,
    "arrivals": 314533,
    "finished_requests": 102680,
    "scheduler_time": 156.44267788118754
}
#Debug simulation 
Total elapsed time: 25.050314818974584. Arrivals time: 0.38507654517889023 Scheduler time: 24.490314908791333 Scheduler overhead time: 0.06626546662300825 Adapter cache time: 0.01248214952647686 Engine time: 0.06812018156051636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 34.85791282309219,
    "estimated_duration": 3600.0388452074317,
    "input_throughput": 7087.541856379557,
    "output_throughput": 6115.040683326723,
    "total_throughput": 13202.582539706282,
    "itl": 94.91984154802505,
    "ttft": 1690065.9957039566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1585873737279335,
    "arrivals": 314533,
    "finished_requests": 102708,
    "scheduler_time": 159.079332100461
}
#Debug simulation 
Total elapsed time: 34.85806513624266. Arrivals time: 0.38033012254163623 Scheduler time: 34.29440474975854 Scheduler overhead time: 0.07002142257988453 Adapter cache time: 0.013122707139700651 Engine time: 0.07102020550519228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 20.82213489804417,
    "estimated_duration": 3600.0453880328228,
    "input_throughput": 6864.489287315255,
    "output_throughput": 5929.022748144695,
    "total_throughput": 12793.51203545995,
    "itl": 89.03151981955506,
    "ttft": 1710940.1094728746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4769643847737484,
    "arrivals": 314533,
    "finished_requests": 99443,
    "scheduler_time": 161.03625844042864
}
#Debug simulation 
Total elapsed time: 20.822278298903257. Arrivals time: 0.34139445424079895 Scheduler time: 20.300521065015346 Scheduler overhead time: 0.06808582367375493 Adapter cache time: 0.012634183745831251 Engine time: 0.06948455795645714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 34.86070851702243,
    "estimated_duration": 3600.06684789432,
    "input_throughput": 7087.598391381042,
    "output_throughput": 6115.050061605478,
    "total_throughput": 13202.648452986521,
    "itl": 94.91649425327736,
    "ttft": 1690130.438304319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0919527670089153,
    "arrivals": 314533,
    "finished_requests": 102709,
    "scheduler_time": 159.08569904374326
}
#Debug simulation 
Total elapsed time: 34.86087950784713. Arrivals time: 0.3644459587521851 Scheduler time: 34.31369191640988 Scheduler overhead time: 0.06979001266881824 Adapter cache time: 0.01362962881103158 Engine time: 0.0703789284452796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 24.611161144915968,
    "estimated_duration": 3600.0986271001625,
    "input_throughput": 6833.868609821145,
    "output_throughput": 5905.157108743989,
    "total_throughput": 12739.025718565134,
    "itl": 89.41493169669756,
    "ttft": 1702439.812517134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1101297534629713,
    "arrivals": 314533,
    "finished_requests": 99101,
    "scheduler_time": 160.3149244841445
}
#Debug simulation 
Total elapsed time: 24.611284733749926. Arrivals time: 0.3205017331056297 Scheduler time: 24.111049212515354 Scheduler overhead time: 0.06889841426163912 Adapter cache time: 0.012219371274113655 Engine time: 0.06921709422022104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 34.54007377894595,
    "estimated_duration": 3600.0457591079994,
    "input_throughput": 7087.63990997775,
    "output_throughput": 6115.085883090181,
    "total_throughput": 13202.72579306793,
    "itl": 94.91826466195292,
    "ttft": 1690061.5974527409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0022755876695717,
    "arrivals": 314533,
    "finished_requests": 102709,
    "scheduler_time": 159.08427293096773
}
#Debug simulation 
Total elapsed time: 34.540200368035585. Arrivals time: 0.33369891718029976 Scheduler time: 34.02888247882947 Scheduler overhead time: 0.06797676021233201 Adapter cache time: 0.012903409544378519 Engine time: 0.06834124261513352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 20.171782149001956,
    "estimated_duration": 3600.0670676067707,
    "input_throughput": 6840.424508083041,
    "output_throughput": 5906.691625646871,
    "total_throughput": 12747.116133729913,
    "itl": 89.39525433907185,
    "ttft": 1707556.4953178898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6069503171555737,
    "arrivals": 314533,
    "finished_requests": 99107,
    "scheduler_time": 160.38440564951262
}
#Debug simulation 
Total elapsed time: 20.171885369811207. Arrivals time: 0.3319555423222482 Scheduler time: 19.666825907304883 Scheduler overhead time: 0.0654188352636993 Adapter cache time: 0.012048238422721624 Engine time: 0.06675177533179522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 25.69711770117283,
    "estimated_duration": 3600.11546452424,
    "input_throughput": 7098.797316873647,
    "output_throughput": 6133.1846763192425,
    "total_throughput": 13231.98199319289,
    "itl": 97.93461306379375,
    "ttft": 1679513.1659555545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9125124464090903,
    "arrivals": 314275,
    "finished_requests": 102857,
    "scheduler_time": 157.15413770537802
}
#Debug simulation 
Total elapsed time: 25.69724951405078. Arrivals time: 0.3328989278525114 Scheduler time: 25.195393613073975 Scheduler overhead time: 0.06475194590166211 Adapter cache time: 0.011509895790368319 Engine time: 0.06515452638268471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 20.196017841808498,
    "estimated_duration": 3600.0902934732253,
    "input_throughput": 7031.174480787577,
    "output_throughput": 6078.094774364508,
    "total_throughput": 13109.269255152085,
    "itl": 95.48276333831696,
    "ttft": 1692222.766946788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3043126651924115,
    "arrivals": 314275,
    "finished_requests": 101939,
    "scheduler_time": 158.097931161144
}
#Debug simulation 
Total elapsed time: 20.196164777036756. Arrivals time: 0.31943374406546354 Scheduler time: 19.711159855592996 Scheduler overhead time: 0.06242844182997942 Adapter cache time: 0.011520400177687407 Engine time: 0.06395922973752022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.39303099317476,
    "estimated_duration": 3600.002328942672,
    "input_throughput": 6844.419183261139,
    "output_throughput": 5919.803392532574,
    "total_throughput": 12764.222575793712,
    "itl": 89.24440720972316,
    "ttft": 1718340.6669675978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.542813819558363,
    "arrivals": 314275,
    "finished_requests": 99217,
    "scheduler_time": 160.72549909067612
}
#Debug simulation 
Total elapsed time: 18.393169756047428. Arrivals time: 0.30993545008823276 Scheduler time: 17.911312616895884 Scheduler overhead time: 0.06479944195598364 Adapter cache time: 0.012235988862812519 Engine time: 0.06595567846670747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 20.049493901897222,
    "estimated_duration": 3600.1051553036855,
    "input_throughput": 7040.423517257491,
    "output_throughput": 6086.977478342983,
    "total_throughput": 13127.400995600474,
    "itl": 95.36286359942184,
    "ttft": 1692700.3107319009,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5502163314400226,
    "arrivals": 314275,
    "finished_requests": 102087,
    "scheduler_time": 158.3194999817844
}
#Debug simulation 
Total elapsed time: 20.049617485143244. Arrivals time: 0.319186307489872 Scheduler time: 19.56224779691547 Scheduler overhead time: 0.06355967512354255 Adapter cache time: 0.012035741470754147 Engine time: 0.06480667600408196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 18.2680617258884,
    "estimated_duration": 3600.0065775569233,
    "input_throughput": 6851.762203373356,
    "output_throughput": 5923.221122132194,
    "total_throughput": 12774.983325505551,
    "itl": 89.13334142066198,
    "ttft": 1719470.295751806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.495806061788465,
    "arrivals": 314275,
    "finished_requests": 99286,
    "scheduler_time": 160.87078499208266
}
#Debug simulation 
Total elapsed time: 18.268182198051363. Arrivals time: 0.3052212055772543 Scheduler time: 17.789435668382794 Scheduler overhead time: 0.06525924382731318 Adapter cache time: 0.012311403173953295 Engine time: 0.06677130237221718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.665923865046352,
    "estimated_duration": 3600.022966017539,
    "input_throughput": 7013.032482936943,
    "output_throughput": 6065.072697065018,
    "total_throughput": 13078.10518000196,
    "itl": 95.53473931675491,
    "ttft": 1686712.171197964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8618293269770206,
    "arrivals": 314275,
    "finished_requests": 101658,
    "scheduler_time": 157.8595443375452
}
#Debug simulation 
Total elapsed time: 21.66607220424339. Arrivals time: 0.32246439019218087 Scheduler time: 21.176563007757068 Scheduler overhead time: 0.0635804058983922 Adapter cache time: 0.011308491695672274 Engine time: 0.06439626682549715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.631206225138158,
    "estimated_duration": 3600.0816839648205,
    "input_throughput": 6853.022004997675,
    "output_throughput": 5926.811909584578,
    "total_throughput": 12779.833914582252,
    "itl": 89.12973619149663,
    "ttft": 1718808.4298459445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5076572650298503,
    "arrivals": 314275,
    "finished_requests": 99341,
    "scheduler_time": 160.9429257957057
}
#Debug simulation 
Total elapsed time: 18.631291944999248. Arrivals time: 0.3075164766050875 Scheduler time: 18.150623950641602 Scheduler overhead time: 0.06583078764379025 Adapter cache time: 0.012355598155409098 Engine time: 0.06592354783788323 
