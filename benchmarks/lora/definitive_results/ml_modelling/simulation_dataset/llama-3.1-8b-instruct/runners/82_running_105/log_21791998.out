INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 90.67909795296146,
    "estimated_duration": 3600.024743091967,
    "input_throughput": 7449.696019857292,
    "output_throughput": 6509.1055401674985,
    "total_throughput": 13958.80156002479,
    "itl": 90.22558916870771,
    "ttft": 1800249.0952310676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2828073521982863,
    "arrivals": 501405,
    "finished_requests": 108929,
    "scheduler_time": 272.3272719450038
}
#Debug simulation 
Total elapsed time: 90.67930150800385. Arrivals time: 0.5353076219907962 Scheduler time: 89.92790767975384 Scheduler overhead time: 0.0829427334247157 Adapter cache time: 0.017101347155403346 Engine time: 0.0830500001902692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 83.8766360640293,
    "estimated_duration": 3600.01857495444,
    "input_throughput": 7555.264905916276,
    "output_throughput": 6608.145348333673,
    "total_throughput": 14163.410254249948,
    "itl": 89.76576884182931,
    "ttft": 1795883.4377163139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5915985852014287,
    "arrivals": 501405,
    "finished_requests": 110513,
    "scheduler_time": 267.17171635329976
}
#Debug simulation 
Total elapsed time: 83.87684262701077. Arrivals time: 0.4865659460192546 Scheduler time: 83.17727247055154 Scheduler overhead time: 0.08204039296833798 Adapter cache time: 0.01693658228032291 Engine time: 0.08154304878553376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.49546080001164,
    "estimated_duration": 3600.028689891434,
    "input_throughput": 7452.012556269109,
    "output_throughput": 6519.674430902331,
    "total_throughput": 13971.68698717144,
    "itl": 87.44295593136839,
    "ttft": 1810574.2509372123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.835098239402291,
    "arrivals": 501405,
    "finished_requests": 109078,
    "scheduler_time": 271.24413651335885
}
#Debug simulation 
Total elapsed time: 78.49570623104228. Arrivals time: 0.4877993487752974 Scheduler time: 77.79230601404561 Scheduler overhead time: 0.0828462980571203 Adapter cache time: 0.016943119408097118 Engine time: 0.0824941429309547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 79.25041872699512,
    "estimated_duration": 3600.0922618966288,
    "input_throughput": 7535.060500285286,
    "output_throughput": 6589.502788881905,
    "total_throughput": 14124.56328916719,
    "itl": 89.70723363685615,
    "ttft": 1804230.7798574844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.521904205731115,
    "arrivals": 501405,
    "finished_requests": 110312,
    "scheduler_time": 268.0872027185283
}
#Debug simulation 
Total elapsed time: 79.2505923419958. Arrivals time: 0.5294113634736277 Scheduler time: 78.51065293117426 Scheduler overhead time: 0.08103294391185045 Adapter cache time: 0.01631931692827493 Engine time: 0.08077650266932324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 78.23917776497547,
    "estimated_duration": 3600.0390776221325,
    "input_throughput": 7445.268071285454,
    "output_throughput": 6513.455963785853,
    "total_throughput": 13958.724035071307,
    "itl": 87.37752166247762,
    "ttft": 1810843.129537001,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6834142381791066,
    "arrivals": 501405,
    "finished_requests": 109069,
    "scheduler_time": 271.6020776300542
}
#Debug simulation 
Total elapsed time: 78.23935411096318. Arrivals time: 0.4837399305542931 Scheduler time: 77.54162634623935 Scheduler overhead time: 0.08291765843750909 Adapter cache time: 0.01623563445173204 Engine time: 0.08222833398031071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 89.26337471598526,
    "estimated_duration": 3600.0932349222903,
    "input_throughput": 7494.290908435981,
    "output_throughput": 6560.26787609288,
    "total_throughput": 14054.558784528861,
    "itl": 89.31411642388228,
    "ttft": 1797519.7557244264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.391694765044372,
    "arrivals": 501405,
    "finished_requests": 109804,
    "scheduler_time": 269.5296407933853
}
#Debug simulation 
Total elapsed time: 89.26353992696386. Arrivals time: 0.5515263482811861 Scheduler time: 88.49837376159849 Scheduler overhead time: 0.08257315878290683 Adapter cache time: 0.01556827564490959 Engine time: 0.08257872733520344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 84.15003280597739,
    "estimated_duration": 3600.0769281931543,
    "input_throughput": 7414.678500605591,
    "output_throughput": 6486.883604379198,
    "total_throughput": 13901.562104984789,
    "itl": 87.30222327367098,
    "ttft": 1808226.6511490073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5391330898180644,
    "arrivals": 501405,
    "finished_requests": 108572,
    "scheduler_time": 272.80978607117504
}
#Debug simulation 
Total elapsed time: 84.15021103800973. Arrivals time: 0.4960149864782579 Scheduler time: 83.43770766281523 Scheduler overhead time: 0.08361925079952925 Adapter cache time: 0.016363441187422723 Engine time: 0.08335890987655148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 91.36277073004749,
    "estimated_duration": 3600.01860939719,
    "input_throughput": 7545.133497114972,
    "output_throughput": 6625.0999196900475,
    "total_throughput": 14170.233416805018,
    "itl": 90.7448005237014,
    "ttft": 1799447.5597122535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2497453070385367,
    "arrivals": 500396,
    "finished_requests": 110351,
    "scheduler_time": 266.2864788315799
}
#Debug simulation 
Total elapsed time: 91.36294428503606. Arrivals time: 0.5549279403639957 Scheduler time: 90.59100167936413 Scheduler overhead time: 0.08335810265270993 Adapter cache time: 0.017511183104943484 Engine time: 0.08308318734634668 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 81.3800454409793,
    "estimated_duration": 3600.0925497296785,
    "input_throughput": 7486.589199498159,
    "output_throughput": 6560.544395386018,
    "total_throughput": 14047.133594884177,
    "itl": 89.50072458127141,
    "ttft": 1810133.6845919192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.339839748889209,
    "arrivals": 500396,
    "finished_requests": 109391,
    "scheduler_time": 269.4449318948398
}
#Debug simulation 
Total elapsed time: 81.38021577801555. Arrivals time: 0.49505715124541894 Scheduler time: 80.67237545980606 Scheduler overhead time: 0.08222966850735247 Adapter cache time: 0.015681321383453906 Engine time: 0.08259655884467065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 76.48228526196908,
    "estimated_duration": 3600.0011158383363,
    "input_throughput": 7395.061318974189,
    "output_throughput": 6485.976045194245,
    "total_throughput": 13881.037364168433,
    "itl": 87.2785569228949,
    "ttft": 1812922.3995202347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4862813059426874,
    "arrivals": 500396,
    "finished_requests": 108061,
    "scheduler_time": 272.74325752069063
}
#Debug simulation 
Total elapsed time: 76.48245215497445. Arrivals time: 0.4762853598804213 Scheduler time: 75.79060692951316 Scheduler overhead time: 0.08286044810665771 Adapter cache time: 0.016553245368413627 Engine time: 0.08268766809487715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 82.89638646895764,
    "estimated_duration": 3600.0034335077676,
    "input_throughput": 7479.982310395177,
    "output_throughput": 6555.916802835759,
    "total_throughput": 14035.899113230937,
    "itl": 89.49725066610557,
    "ttft": 1809060.491840349,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.357027151444926,
    "arrivals": 500396,
    "finished_requests": 109303,
    "scheduler_time": 269.5683977426063
}
#Debug simulation 
Total elapsed time: 82.89656672399724. Arrivals time: 0.542168820858933 Scheduler time: 82.1399637045688 Scheduler overhead time: 0.08340092422440648 Adapter cache time: 0.016544964164495468 Engine time: 0.08198678668122739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 77.00820758595364,
    "estimated_duration": 3600.028540087,
    "input_throughput": 7411.902628792816,
    "output_throughput": 6499.666527486621,
    "total_throughput": 13911.569156279438,
    "itl": 87.28630525580459,
    "ttft": 1816445.638218502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5002036809502197,
    "arrivals": 500396,
    "finished_requests": 108306,
    "scheduler_time": 272.13121540589674
}
#Debug simulation 
Total elapsed time: 77.00837905600201. Arrivals time: 0.47874470928218216 Scheduler time: 76.31342889246298 Scheduler overhead time: 0.08289732987759635 Adapter cache time: 0.01656210666988045 Engine time: 0.08336083177709952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 78.34178277302999,
    "estimated_duration": 3600.0363034012803,
    "input_throughput": 7467.121088362978,
    "output_throughput": 6545.94815550482,
    "total_throughput": 14013.069243867798,
    "itl": 89.5452923757007,
    "ttft": 1807283.9973186867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3597751603415196,
    "arrivals": 500396,
    "finished_requests": 109143,
    "scheduler_time": 270.02819747225664
}
#Debug simulation 
Total elapsed time: 78.34194901003502. Arrivals time: 0.47396791219944134 Scheduler time: 77.65405127586564 Scheduler overhead time: 0.08254942775238305 Adapter cache time: 0.016627847333438694 Engine time: 0.0819404162466526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 85.26713195297634,
    "estimated_duration": 3600.0555406275566,
    "input_throughput": 7436.159164181503,
    "output_throughput": 6517.392783309661,
    "total_throughput": 13953.551947491165,
    "itl": 87.16615937089665,
    "ttft": 1811801.044478922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4435644363425693,
    "arrivals": 500396,
    "finished_requests": 108687,
    "scheduler_time": 271.23417392282846
}
#Debug simulation 
Total elapsed time: 85.26730938500259. Arrivals time: 0.4931190001661889 Scheduler time: 84.55509810644435 Scheduler overhead time: 0.08462524105561897 Adapter cache time: 0.01680020004278049 Engine time: 0.08423329039942473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 80.4809112730436,
    "estimated_duration": 3600.074277721367,
    "input_throughput": 7602.370087020264,
    "output_throughput": 6650.419728326791,
    "total_throughput": 14252.789815347054,
    "itl": 91.34452779498702,
    "ttft": 1794408.314923537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6332650308916326,
    "arrivals": 499947,
    "finished_requests": 111168,
    "scheduler_time": 265.0821772305534
}
#Debug simulation 
Total elapsed time: 80.4810811750358. Arrivals time: 0.5047616679221392 Scheduler time: 79.76557637087535 Scheduler overhead time: 0.08096991019556299 Adapter cache time: 0.016485016501974314 Engine time: 0.08114582032430917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 89.04638903599698,
    "estimated_duration": 3600.0548824044013,
    "input_throughput": 7518.083719302498,
    "output_throughput": 6572.715631545193,
    "total_throughput": 14090.799350847692,
    "itl": 89.49138867812094,
    "ttft": 1804298.3149796047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.223814804106952,
    "arrivals": 499947,
    "finished_requests": 109790,
    "scheduler_time": 268.8154762402234
}
#Debug simulation 
Total elapsed time: 89.04656357999193. Arrivals time: 0.5051638268632814 Scheduler time: 88.3243847397971 Scheduler overhead time: 0.08408657071413472 Adapter cache time: 0.01635890529723838 Engine time: 0.08353017922490835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.7409971379675,
    "estimated_duration": 3600.060785599222,
    "input_throughput": 7478.762610814795,
    "output_throughput": 6535.2124314434195,
    "total_throughput": 14013.975042258215,
    "itl": 87.36562897323095,
    "ttft": 1812608.7256130525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7318180919392072,
    "arrivals": 499947,
    "finished_requests": 109228,
    "scheduler_time": 270.5323356992596
}
#Debug simulation 
Total elapsed time: 77.74116894998588. Arrivals time: 0.48452074982924387 Scheduler time: 77.04114401113475 Scheduler overhead time: 0.08265938988188282 Adapter cache time: 0.017155167879536748 Engine time: 0.0823748717084527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 81.77216460002819,
    "estimated_duration": 3600.0184164436077,
    "input_throughput": 7555.494126296804,
    "output_throughput": 6610.822292267792,
    "total_throughput": 14166.316418564596,
    "itl": 89.82076519587912,
    "ttft": 1801849.2274080198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3747906932933245,
    "arrivals": 499947,
    "finished_requests": 110431,
    "scheduler_time": 267.0320367143117
}
#Debug simulation 
Total elapsed time: 81.77233364002313. Arrivals time: 0.5021137042785995 Scheduler time: 81.0569193342817 Scheduler overhead time: 0.0815614748862572 Adapter cache time: 0.016450171649921685 Engine time: 0.08266510628163815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 77.29758884996409,
    "estimated_duration": 3600.0947675532457,
    "input_throughput": 7468.342567624462,
    "output_throughput": 6532.311930217038,
    "total_throughput": 14000.6544978415,
    "itl": 87.28644081888815,
    "ttft": 1812234.2981623914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6306598474597596,
    "arrivals": 499947,
    "finished_requests": 109137,
    "scheduler_time": 270.6125652635304
}
#Debug simulation 
Total elapsed time: 77.29775873798644. Arrivals time: 0.4982755313394591 Scheduler time: 76.58453404548345 Scheduler overhead time: 0.08306736015947536 Adapter cache time: 0.01668271527159959 Engine time: 0.08264144486747682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.45001261500875,
    "estimated_duration": 3600.032501624339,
    "input_throughput": 7574.477726991764,
    "output_throughput": 6621.840494285506,
    "total_throughput": 14196.31822127727,
    "itl": 89.96511435653747,
    "ttft": 1802518.9872033792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4746857372717885,
    "arrivals": 499947,
    "finished_requests": 110661,
    "scheduler_time": 266.430055129317
}
#Debug simulation 
Total elapsed time: 81.45017750700936. Arrivals time: 0.48793175170430914 Scheduler time: 80.74786324927118 Scheduler overhead time: 0.08226045442279428 Adapter cache time: 0.01632671430706978 Engine time: 0.08314303221413866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_128_slots_16_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.56694212200819,
    "estimated_duration": 3600.0793338458852,
    "input_throughput": 7477.068004288687,
    "output_throughput": 6545.11104199147,
    "total_throughput": 14022.179046280158,
    "itl": 87.83845368708268,
    "ttft": 1807471.0146623491,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.90627151396126,
    "arrivals": 499947,
    "finished_requests": 109363,
    "scheduler_time": 269.9652101998304
}
#Debug simulation 
Total elapsed time: 75.5671105600195. Arrivals time: 0.49722309864591807 Scheduler time: 74.85638050356647 Scheduler overhead time: 0.08111182024003938 Adapter cache time: 0.016760500380769372 Engine time: 0.08262467372696847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.48993509297725,
    "estimated_duration": 3600.045627879914,
    "input_throughput": 7691.036409531748,
    "output_throughput": 6674.832622649625,
    "total_throughput": 14365.869032181372,
    "itl": 90.7988100510794,
    "ttft": 1792128.17069615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7324511663708815,
    "arrivals": 498492,
    "finished_requests": 111585,
    "scheduler_time": 263.9522147313882
}
#Debug simulation 
Total elapsed time: 81.49010914599057. Arrivals time: 0.49138095299713314 Scheduler time: 80.78557225689292 Scheduler overhead time: 0.08196426648646593 Adapter cache time: 0.016794044291600585 Engine time: 0.08180145523510873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 82.9920932520181,
    "estimated_duration": 3600.0108415501104,
    "input_throughput": 7584.421603642535,
    "output_throughput": 6581.050458669909,
    "total_throughput": 14165.472062312445,
    "itl": 89.36270461435875,
    "ttft": 1796174.0865324906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.636843215711416,
    "arrivals": 498492,
    "finished_requests": 110018,
    "scheduler_time": 268.37779256044007
}
#Debug simulation 
Total elapsed time: 82.99226277699927. Arrivals time: 0.48764873715117574 Scheduler time: 82.29167942237109 Scheduler overhead time: 0.0816916047479026 Adapter cache time: 0.016404483292717487 Engine time: 0.08243439643410966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.61842702300055,
    "estimated_duration": 3600.0447484321817,
    "input_throughput": 7472.578226066675,
    "output_throughput": 6483.767183773304,
    "total_throughput": 13956.345409839978,
    "itl": 86.7941417349516,
    "ttft": 1809161.6610014895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5699916938692358,
    "arrivals": 498492,
    "finished_requests": 108342,
    "scheduler_time": 272.9409198024636
}
#Debug simulation 
Total elapsed time: 77.61859915201785. Arrivals time: 0.48705821082694456 Scheduler time: 76.91523218364455 Scheduler overhead time: 0.08311648853123188 Adapter cache time: 0.016782030405011028 Engine time: 0.08324288862058893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 80.71560479898471,
    "estimated_duration": 3600.0063699715142,
    "input_throughput": 7625.695117927589,
    "output_throughput": 6616.5185702681065,
    "total_throughput": 14242.213688195696,
    "itl": 89.57854905070404,
    "ttft": 1794671.353817757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7847593321464936,
    "arrivals": 498492,
    "finished_requests": 110601,
    "scheduler_time": 266.6956744667942
}
#Debug simulation 
Total elapsed time: 80.71578379900893. Arrivals time: 0.510524618148338 Scheduler time: 79.99088388390373 Scheduler overhead time: 0.08205350785283372 Adapter cache time: 0.0170032064197585 Engine time: 0.082309203047771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 77.47120046598138,
    "estimated_duration": 3600.0485300240025,
    "input_throughput": 7460.19359906266,
    "output_throughput": 6473.982449298985,
    "total_throughput": 13934.176048361644,
    "itl": 86.86781590022429,
    "ttft": 1809082.7820674053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6281244276231182,
    "arrivals": 498492,
    "finished_requests": 108168,
    "scheduler_time": 273.3660384814671
}
#Debug simulation 
Total elapsed time: 77.47137979697436. Arrivals time: 0.4826961829676293 Scheduler time: 76.77458706917241 Scheduler overhead time: 0.08206438372144476 Adapter cache time: 0.016132897755596787 Engine time: 0.08279037190368399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 80.89676897099707,
    "estimated_duration": 3600.0404940746353,
    "input_throughput": 7676.489207686968,
    "output_throughput": 6659.733422293819,
    "total_throughput": 14336.222629980788,
    "itl": 89.77732660114866,
    "ttft": 1796128.759172379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.608748077023769,
    "arrivals": 498492,
    "finished_requests": 111333,
    "scheduler_time": 264.57677033825894
}
#Debug simulation 
Total elapsed time: 80.89694760099519. Arrivals time: 0.5054084130097181 Scheduler time: 80.17990541638574 Scheduler overhead time: 0.08118825237033889 Adapter cache time: 0.01694816362578422 Engine time: 0.08134381636045873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_128_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.87376437103376,
    "estimated_duration": 3600.075345458749,
    "input_throughput": 7519.860114636093,
    "output_throughput": 6521.2304596942795,
    "total_throughput": 14041.090574330374,
    "itl": 86.89259957800321,
    "ttft": 1809521.5372038607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7005971225164862,
    "arrivals": 498492,
    "finished_requests": 108999,
    "scheduler_time": 271.1332880734819
}
#Debug simulation 
Total elapsed time: 78.87393835699186. Arrivals time: 0.5178676898940466 Scheduler time: 78.14021650119685 Scheduler overhead time: 0.08216955850366503 Adapter cache time: 0.017004586232360452 Engine time: 0.08328733476810157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 80.82088062103139,
    "estimated_duration": 3600.013052734478,
    "input_throughput": 7614.194614983174,
    "output_throughput": 6665.35027748684,
    "total_throughput": 14279.544892470014,
    "itl": 90.94551445261185,
    "ttft": 1795008.461205888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4216679418692348,
    "arrivals": 497997,
    "finished_requests": 111023,
    "scheduler_time": 264.30268461635217
}
#Debug simulation 
Total elapsed time: 80.8210530130309. Arrivals time: 0.5013375906855799 Scheduler time: 80.11014307592995 Scheduler overhead time: 0.0808425341383554 Adapter cache time: 0.01653345685917884 Engine time: 0.08037971338490024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.00857424305286,
    "estimated_duration": 3600.017952702216,
    "input_throughput": 7552.729557804259,
    "output_throughput": 6605.369559935351,
    "total_throughput": 14158.09911773961,
    "itl": 89.76213925537266,
    "ttft": 1798413.2615825061,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5202610870031654,
    "arrivals": 497997,
    "finished_requests": 110090,
    "scheduler_time": 267.00516068903687
}
#Debug simulation 
Total elapsed time: 79.00873735605273. Arrivals time: 0.48947170813335106 Scheduler time: 78.3062886573025 Scheduler overhead time: 0.0823436402133666 Adapter cache time: 0.01611487433547154 Engine time: 0.08194155152887106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.48917862999951,
    "estimated_duration": 3600.0019255934885,
    "input_throughput": 7489.431549555382,
    "output_throughput": 6557.781492327405,
    "total_throughput": 14047.213041882787,
    "itl": 87.47801924727507,
    "ttft": 1803755.3952011785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6625510443653944,
    "arrivals": 497997,
    "finished_requests": 109175,
    "scheduler_time": 269.24603783490755
}
#Debug simulation 
Total elapsed time: 77.48934900597669. Arrivals time: 0.5049132165731862 Scheduler time: 76.76996782876085 Scheduler overhead time: 0.08259036793606356 Adapter cache time: 0.016505003266502172 Engine time: 0.0823622882599011 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 80.18052372103557,
    "estimated_duration": 3600.0741383569793,
    "input_throughput": 7497.832811942893,
    "output_throughput": 6565.8706158723335,
    "total_throughput": 14063.703427815228,
    "itl": 89.67386750677058,
    "ttft": 1797827.8814384427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 188,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2904115000925949,
    "arrivals": 497997,
    "finished_requests": 109350,
    "scheduler_time": 268.97341424186595
}
#Debug simulation 
Total elapsed time: 80.18069331505103. Arrivals time: 0.47917581780347973 Scheduler time: 79.48807345860405 Scheduler overhead time: 0.08173582941526547 Adapter cache time: 0.01628797932062298 Engine time: 0.08279423811472952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 79.29707220802084,
    "estimated_duration": 3600.0021315268873,
    "input_throughput": 7464.123358338992,
    "output_throughput": 6535.645296971201,
    "total_throughput": 13999.768655310194,
    "itl": 87.37798209501682,
    "ttft": 1808173.3182454424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5194588748179398,
    "arrivals": 497997,
    "finished_requests": 108853,
    "scheduler_time": 270.2912880842977
}
#Debug simulation 
Total elapsed time: 79.29723504802678. Arrivals time: 0.4913631036761217 Scheduler time: 78.58968488516985 Scheduler overhead time: 0.0828859779285267 Adapter cache time: 0.01670285197906196 Engine time: 0.08366961649153382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.49806934397202,
    "estimated_duration": 3600.077344642436,
    "input_throughput": 7564.179986445452,
    "output_throughput": 6616.699787144683,
    "total_throughput": 14180.879773590135,
    "itl": 89.747490834272,
    "ttft": 1798871.9900647898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3853108441038016,
    "arrivals": 497997,
    "finished_requests": 110270,
    "scheduler_time": 266.5288021851496
}
#Debug simulation 
Total elapsed time: 79.49824110098416. Arrivals time: 0.48969521949766204 Scheduler time: 78.79890627146233 Scheduler overhead time: 0.08085956930881366 Adapter cache time: 0.01649863546481356 Engine time: 0.08018095092847943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_128_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.56181629799539,
    "estimated_duration": 3600.0055103638374,
    "input_throughput": 7467.35329226846,
    "output_throughput": 6535.37527436234,
    "total_throughput": 14002.7285666308,
    "itl": 87.53482629877459,
    "ttft": 1802551.0145448628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6316387160681216,
    "arrivals": 497997,
    "finished_requests": 108855,
    "scheduler_time": 270.3900555563857
}
#Debug simulation 
Total elapsed time: 77.56199042900698. Arrivals time: 0.4894705719780177 Scheduler time: 76.85807351151016 Scheduler overhead time: 0.08214402024168521 Adapter cache time: 0.01654501911252737 Engine time: 0.08245811919914559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 75.5039571269881,
    "estimated_duration": 3600.096320032269,
    "input_throughput": 7628.644224650587,
    "output_throughput": 6688.701873338812,
    "total_throughput": 14317.346097989399,
    "itl": 91.35359620129928,
    "ttft": 1797063.2641826079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8977613921696297,
    "arrivals": 497033,
    "finished_requests": 111536,
    "scheduler_time": 263.07049505977744
}
#Debug simulation 
Total elapsed time: 75.50412127701566. Arrivals time: 0.47829929349245504 Scheduler time: 74.81998401705641 Scheduler overhead time: 0.07948123686946929 Adapter cache time: 0.015503558446653187 Engine time: 0.07913409412140027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 85.00939332699636,
    "estimated_duration": 3600.0586240698894,
    "input_throughput": 7555.407519794866,
    "output_throughput": 6635.27000374099,
    "total_throughput": 14190.677523535856,
    "itl": 89.83318783540663,
    "ttft": 1782327.470926886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5441346966708094,
    "arrivals": 497033,
    "finished_requests": 110180,
    "scheduler_time": 267.0795342672442
}
#Debug simulation 
Total elapsed time: 85.00955813698238. Arrivals time: 0.4936441601603292 Scheduler time: 84.3025994200143 Scheduler overhead time: 0.08215715747792274 Adapter cache time: 0.016646157018840313 Engine time: 0.08184717950643972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 80.430883153982,
    "estimated_duration": 3600.046265541824,
    "input_throughput": 7501.3110966049235,
    "output_throughput": 6567.807538007127,
    "total_throughput": 14069.11863461205,
    "itl": 87.50722058903624,
    "ttft": 1808233.7486189543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.237965990887032,
    "arrivals": 497033,
    "finished_requests": 109607,
    "scheduler_time": 268.75923732630883
}
#Debug simulation 
Total elapsed time: 80.43105422001099. Arrivals time: 0.47382319514872506 Scheduler time: 79.74410506273853 Scheduler overhead time: 0.08148757566232234 Adapter cache time: 0.016597015783190727 Engine time: 0.08204542548628524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 83.68143272801535,
    "estimated_duration": 3600.0771502724333,
    "input_throughput": 7564.483166128588,
    "output_throughput": 6624.041098173519,
    "total_throughput": 14188.524264302107,
    "itl": 89.88020029472038,
    "ttft": 1789437.0728516218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5985112570179605,
    "arrivals": 497033,
    "finished_requests": 110385,
    "scheduler_time": 266.21409726191047
}
#Debug simulation 
Total elapsed time: 83.68159293598728. Arrivals time: 0.48599783645477146 Scheduler time: 82.98277703113854 Scheduler overhead time: 0.08233035897137597 Adapter cache time: 0.016824945516418666 Engine time: 0.08111725933849812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 82.21750264998991,
    "estimated_duration": 3600.079396635665,
    "input_throughput": 7472.639915980871,
    "output_throughput": 6544.106783316103,
    "total_throughput": 14016.746699296975,
    "itl": 87.45186749660647,
    "ttft": 1804198.6149523207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3880227019358446,
    "arrivals": 497033,
    "finished_requests": 109079,
    "scheduler_time": 269.99010032501116
}
#Debug simulation 
Total elapsed time: 82.21767800097587. Arrivals time: 0.4976578642963432 Scheduler time: 81.50486068660393 Scheduler overhead time: 0.0825868378742598 Adapter cache time: 0.016276479815132916 Engine time: 0.08336145128123462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 85.0088303599623,
    "estimated_duration": 3600.048041281695,
    "input_throughput": 7600.772458097012,
    "output_throughput": 6659.195578808149,
    "total_throughput": 14259.96803690516,
    "itl": 90.02591103025273,
    "ttft": 1785518.5215629793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.289552029995244,
    "arrivals": 497033,
    "finished_requests": 110816,
    "scheduler_time": 264.9277017906759
}
#Debug simulation 
Total elapsed time: 85.00900448596803. Arrivals time: 0.5074956351309083 Scheduler time: 84.29159842687659 Scheduler overhead time: 0.0804191076895222 Adapter cache time: 0.016135128389578313 Engine time: 0.08098911470733583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_128_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 83.15945639798883,
    "estimated_duration": 3600.0529537816633,
    "input_throughput": 7483.130761090978,
    "output_throughput": 6557.517154074547,
    "total_throughput": 14040.647915165524,
    "itl": 87.54478878691246,
    "ttft": 1790089.5005517504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4810470180027209,
    "arrivals": 497033,
    "finished_requests": 109151,
    "scheduler_time": 269.58616154254634
}
#Debug simulation 
Total elapsed time: 83.15962084702915. Arrivals time: 0.4867923126439564 Scheduler time: 82.45812736317748 Scheduler overhead time: 0.08277530642226338 Adapter cache time: 0.016575276211369783 Engine time: 0.08272009156644344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 96.71867122902768,
    "estimated_duration": 3600.0268296767335,
    "input_throughput": 7010.697751457125,
    "output_throughput": 6124.744354191362,
    "total_throughput": 13135.442105648486,
    "itl": 81.85279837972544,
    "ttft": 1844416.2920971944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6398774399235825,
    "arrivals": 432423,
    "finished_requests": 102075,
    "scheduler_time": 284.06384436888777
}
#Debug simulation 
Total elapsed time: 96.7188420430175. Arrivals time: 0.5486813444294967 Scheduler time: 95.93308252369752 Scheduler overhead time: 0.09277722972910851 Adapter cache time: 0.01799150707665831 Engine time: 0.09056041680742055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 94.58702635101508,
    "estimated_duration": 3600.076054102042,
    "input_throughput": 7174.531207630954,
    "output_throughput": 6292.372344242124,
    "total_throughput": 13466.903551873078,
    "itl": 86.13119283690216,
    "ttft": 1826499.7279229162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.95660592434928,
    "arrivals": 432423,
    "finished_requests": 104651,
    "scheduler_time": 276.6008104031855
}
#Debug simulation 
Total elapsed time: 94.58719660103088. Arrivals time: 0.5773814486456104 Scheduler time: 93.77971647342201 Scheduler overhead time: 0.0887455121264793 Adapter cache time: 0.01835530629614368 Engine time: 0.08840329508529976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 91.52743868494872,
    "estimated_duration": 3600.0605225178174,
    "input_throughput": 7103.555576369739,
    "output_throughput": 6225.794499789295,
    "total_throughput": 13329.350076159033,
    "itl": 84.0063770512417,
    "ttft": 1830123.003894986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.055633916081867,
    "arrivals": 432423,
    "finished_requests": 103523,
    "scheduler_time": 279.82386467856816
}
#Debug simulation 
Total elapsed time: 91.52760565496283. Arrivals time: 0.5382900823606178 Scheduler time: 90.75706890388392 Scheduler overhead time: 0.08931035606656224 Adapter cache time: 0.018521148071158677 Engine time: 0.08917008066782728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 96.93105566001032,
    "estimated_duration": 3600.0418233901964,
    "input_throughput": 7151.693858865882,
    "output_throughput": 6249.921557525557,
    "total_throughput": 13401.615416391438,
    "itl": 85.66489999526479,
    "ttft": 1836076.347540288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.778375411205923,
    "arrivals": 432423,
    "finished_requests": 104210,
    "scheduler_time": 278.04989552184014
}
#Debug simulation 
Total elapsed time: 96.93123019800987. Arrivals time: 0.5383714107447304 Scheduler time: 96.15803567803232 Scheduler overhead time: 0.09215050871716812 Adapter cache time: 0.017480001028161496 Engine time: 0.09081881173187867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 90.51057726499857,
    "estimated_duration": 3600.02077740979,
    "input_throughput": 7140.8071201456205,
    "output_throughput": 6249.185599474985,
    "total_throughput": 13389.992719620606,
    "itl": 84.61744069959494,
    "ttft": 1835003.53079138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.068469194974757,
    "arrivals": 432423,
    "finished_requests": 104075,
    "scheduler_time": 278.1471394968218
}
#Debug simulation 
Total elapsed time: 90.51074671105016. Arrivals time: 0.5470148801105097 Scheduler time: 89.72978316847002 Scheduler overhead time: 0.09040809335419908 Adapter cache time: 0.01833804027410224 Engine time: 0.08977990021230653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 97.4315282330499,
    "estimated_duration": 3600.0662007545525,
    "input_throughput": 7005.775336774022,
    "output_throughput": 6126.18040062082,
    "total_throughput": 13131.955737394843,
    "itl": 81.59524572342544,
    "ttft": 1844853.2687219025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5385249466774935,
    "arrivals": 432423,
    "finished_requests": 102036,
    "scheduler_time": 284.0712053785197
}
#Debug simulation 
Total elapsed time: 97.4316995520494. Arrivals time: 0.5301316173863597 Scheduler time: 96.66325614560628 Scheduler overhead time: 0.09233344678068534 Adapter cache time: 0.018787165521644056 Engine time: 0.0909311044961214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 90.28696222603321,
    "estimated_duration": 3600.001121077128,
    "input_throughput": 7140.846109600208,
    "output_throughput": 6249.219720595195,
    "total_throughput": 13390.065830195404,
    "itl": 84.61701475078351,
    "ttft": 1834994.5257658223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.049414414726204,
    "arrivals": 432423,
    "finished_requests": 104075,
    "scheduler_time": 278.1467402068528
}
#Debug simulation 
Total elapsed time: 90.28714087803382. Arrivals time: 0.5748320889542811 Scheduler time: 89.48010322451591 Scheduler overhead time: 0.08985370123991743 Adapter cache time: 0.01815701712621376 Engine time: 0.08916520833736286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 98.29729918402154,
    "estimated_duration": 3600.0004015967875,
    "input_throughput": 7214.068084125954,
    "output_throughput": 6331.263182606957,
    "total_throughput": 13545.33126673291,
    "itl": 86.16531255878313,
    "ttft": 1772945.0441231478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6332650308916326,
    "arrivals": 386952,
    "finished_requests": 105558,
    "scheduler_time": 274.17996965095557
}
#Debug simulation 
Total elapsed time: 98.29746771103237. Arrivals time: 0.5311454279581085 Scheduler time: 97.53601400944171 Scheduler overhead time: 0.08923563524149358 Adapter cache time: 0.018003991688601673 Engine time: 0.08882108627585694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 99.9666262390092,
    "estimated_duration": 3600.0580231196623,
    "input_throughput": 7128.104556982313,
    "output_throughput": 6247.580971072699,
    "total_throughput": 13375.685528055012,
    "itl": 84.80770831217137,
    "ttft": 1782635.042327771,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7392692815652135,
    "arrivals": 386952,
    "finished_requests": 104428,
    "scheduler_time": 277.40170341850364
}
#Debug simulation 
Total elapsed time: 99.96679094003048. Arrivals time: 0.5273071443079971 Scheduler time: 99.20541852671886 Scheduler overhead time: 0.09068986348574981 Adapter cache time: 0.01776084367884323 Engine time: 0.09066430549137294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 102.78313498798525,
    "estimated_duration": 3600.061017211583,
    "input_throughput": 7012.991135232242,
    "output_throughput": 6136.90765083539,
    "total_throughput": 13149.898786067632,
    "itl": 82.25443528393207,
    "ttft": 1802172.030366238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.615498479921375,
    "arrivals": 386952,
    "finished_requests": 102637,
    "scheduler_time": 282.65679701735354
}
#Debug simulation 
Total elapsed time: 102.78330462798476. Arrivals time: 0.5238915618974715 Scheduler time: 102.0210405709804 Scheduler overhead time: 0.09222338808467612 Adapter cache time: 0.018932155333459377 Engine time: 0.09097089181886986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 99.01873649901245,
    "estimated_duration": 3600.0680116789395,
    "input_throughput": 7165.391297141006,
    "output_throughput": 6278.797769006113,
    "total_throughput": 13444.18906614712,
    "itl": 84.99127482308397,
    "ttft": 1774924.9155353636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6573547664564086,
    "arrivals": 386952,
    "finished_requests": 104915,
    "scheduler_time": 276.01567392073747
}
#Debug simulation 
Total elapsed time: 99.01890583202476. Arrivals time: 0.5273984673549421 Scheduler time: 98.26065626274794 Scheduler overhead time: 0.08881020115222782 Adapter cache time: 0.01793183258268982 Engine time: 0.08900554082356393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 105.99085052200826,
    "estimated_duration": 3600.0865859290993,
    "input_throughput": 6896.786065380758,
    "output_throughput": 6040.956649487737,
    "total_throughput": 12937.742714868496,
    "itl": 81.02760726374174,
    "ttft": 1801970.2677275236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5347366084205027,
    "arrivals": 386952,
    "finished_requests": 100940,
    "scheduler_time": 287.2922692058947
}
#Debug simulation 
Total elapsed time: 105.9910091549973. Arrivals time: 0.5167021232191473 Scheduler time: 105.23737734789029 Scheduler overhead time: 0.09141833777539432 Adapter cache time: 0.018175908538978547 Engine time: 0.09148724452825263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 99.72235219203867,
    "estimated_duration": 3600.063977592382,
    "input_throughput": 7197.0962630858185,
    "output_throughput": 6303.317980247669,
    "total_throughput": 13500.414243333487,
    "itl": 85.41790700918888,
    "ttft": 1776845.4639335657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5385249466774935,
    "arrivals": 386952,
    "finished_requests": 105407,
    "scheduler_time": 274.7688874482298
}
#Debug simulation 
Total elapsed time: 99.72251006803708. Arrivals time: 0.5419381228857674 Scheduler time: 98.94885814475128 Scheduler overhead time: 0.0898236294160597 Adapter cache time: 0.017519029672257602 Engine time: 0.08923738374141976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 103.89812633098336,
    "estimated_duration": 3600.000920716253,
    "input_throughput": 7031.85292379409,
    "output_throughput": 6158.63148045775,
    "total_throughput": 13190.48440425184,
    "itl": 82.29604854497423,
    "ttft": 1797009.704203784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6211865403316958,
    "arrivals": 386952,
    "finished_requests": 102946,
    "scheduler_time": 281.7041391073305
}
#Debug simulation 
Total elapsed time: 103.89828774996568. Arrivals time: 0.5299662420875393 Scheduler time: 103.13161447609309 Scheduler overhead time: 0.09193730418337509 Adapter cache time: 0.018003540753852576 Engine time: 0.09141793200979009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 95.18505212001037,
    "estimated_duration": 3600.0295619996173,
    "input_throughput": 7205.774995245097,
    "output_throughput": 6322.388360432697,
    "total_throughput": 13528.163355677794,
    "itl": 86.85111256526503,
    "ttft": 1779707.4761676975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5803657586360331,
    "arrivals": 379429,
    "finished_requests": 105094,
    "scheduler_time": 274.0985295036231
}
#Debug simulation 
Total elapsed time: 95.18521460605552. Arrivals time: 0.5256101426202804 Scheduler time: 94.43152679438936 Scheduler overhead time: 0.08822841494111344 Adapter cache time: 0.01710393250687048 Engine time: 0.0880376409040764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 96.0741272099549,
    "estimated_duration": 3600.049823592945,
    "input_throughput": 7166.026100786811,
    "output_throughput": 6279.103097923108,
    "total_throughput": 13445.12919870992,
    "itl": 85.87022505613129,
    "ttft": 1784097.8888108183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6218561158096434,
    "arrivals": 379429,
    "finished_requests": 104567,
    "scheduler_time": 275.31603904608295
}
#Debug simulation 
Total elapsed time: 96.07427767699119. Arrivals time: 0.44497607985977083 Scheduler time: 95.41154520190321 Scheduler overhead time: 0.0855057262815535 Adapter cache time: 0.015297242673113942 Engine time: 0.08348406717414036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 91.9013858870021,
    "estimated_duration": 3600.040368715568,
    "input_throughput": 7112.78912939965,
    "output_throughput": 6232.552888845879,
    "total_throughput": 13345.34201824553,
    "itl": 83.72275193993649,
    "ttft": 1789593.884235025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6681454221485217,
    "arrivals": 379429,
    "finished_requests": 103824,
    "scheduler_time": 277.69486080253654
}
#Debug simulation 
Total elapsed time: 91.90154696500394. Arrivals time: 0.4434450063854456 Scheduler time: 91.24025463528233 Scheduler overhead time: 0.08479239803273231 Adapter cache time: 0.015696860966272652 Engine time: 0.08353964181151241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 93.14976641599787,
    "estimated_duration": 3600.008502716653,
    "input_throughput": 7160.286977252411,
    "output_throughput": 6285.236266226925,
    "total_throughput": 13445.523243479336,
    "itl": 85.66698081707169,
    "ttft": 1777064.8388211136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.621827682759611,
    "arrivals": 379429,
    "finished_requests": 104409,
    "scheduler_time": 276.1122239846674
}
#Debug simulation 
Total elapsed time: 93.14991462702164. Arrivals time: 0.46774961246410385 Scheduler time: 92.46646776684793 Scheduler overhead time: 0.08364374528173357 Adapter cache time: 0.015709007973782718 Engine time: 0.08292429707944393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 91.7499603619799,
    "estimated_duration": 3600.025100052677,
    "input_throughput": 7112.8192966280485,
    "output_throughput": 6232.579322758524,
    "total_throughput": 13345.398619386571,
    "itl": 83.72246851407914,
    "ttft": 1789587.0835382896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.653232985432263,
    "arrivals": 379429,
    "finished_requests": 103824,
    "scheduler_time": 277.6946057075829
}
#Debug simulation 
Total elapsed time: 91.7501203379943. Arrivals time: 0.44735389592824504 Scheduler time: 91.0833597604651 Scheduler overhead time: 0.084980545041617 Adapter cache time: 0.015784020768478513 Engine time: 0.0842929148930125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 92.99244361097226,
    "estimated_duration": 3600.0591446715684,
    "input_throughput": 7166.269487040227,
    "output_throughput": 6279.634053640647,
    "total_throughput": 13445.903540680874,
    "itl": 85.86661973077497,
    "ttft": 1783969.6989790956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4108465278660836,
    "arrivals": 379429,
    "finished_requests": 104573,
    "scheduler_time": 275.3311717160911
}
#Debug simulation 
Total elapsed time: 92.99260830698768. Arrivals time: 0.4508312252582982 Scheduler time: 92.32702021225123 Scheduler overhead time: 0.08384576631942764 Adapter cache time: 0.015445564466062933 Engine time: 0.08218648988986388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 91.54748944297899,
    "estimated_duration": 3600.0086369373107,
    "input_throughput": 7112.851824095748,
    "output_throughput": 6232.607824821371,
    "total_throughput": 13345.45964891712,
    "itl": 83.72211456059519,
    "ttft": 1789579.4078004046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.637284962832931,
    "arrivals": 379429,
    "finished_requests": 103824,
    "scheduler_time": 277.6941917460389
}
#Debug simulation 
Total elapsed time: 91.54764056997374. Arrivals time: 0.45712990761967376 Scheduler time: 90.87505471229088 Scheduler overhead time: 0.08434418746037409 Adapter cache time: 0.015395194059237838 Engine time: 0.08161994436522946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 93.70758596801898,
    "estimated_duration": 3600.030517561006,
    "input_throughput": 7265.4556322262515,
    "output_throughput": 6369.574060037511,
    "total_throughput": 13635.029692263763,
    "itl": 87.24448248046052,
    "ttft": 1763022.6364313727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6464898489555324,
    "arrivals": 375670,
    "finished_requests": 106158,
    "scheduler_time": 272.30324664359904
}
#Debug simulation 
Total elapsed time: 93.70773425197694. Arrivals time: 0.4634791253483854 Scheduler time: 93.03205908456584 Scheduler overhead time: 0.08347268536454067 Adapter cache time: 0.015415501547977328 Engine time: 0.08020822337130085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 93.44760690897238,
    "estimated_duration": 3600.027778002978,
    "input_throughput": 7245.604369883399,
    "output_throughput": 6337.5022102346475,
    "total_throughput": 13583.106580118047,
    "itl": 86.33083903856202,
    "ttft": 1768750.4974191869,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8205981796979926,
    "arrivals": 375670,
    "finished_requests": 105837,
    "scheduler_time": 272.87434775758646
}
#Debug simulation 
Total elapsed time: 93.44775776198367. Arrivals time: 0.4636861885082908 Scheduler time: 92.770754978701 Scheduler overhead time: 0.08340800425503403 Adapter cache time: 0.015906209824606776 Engine time: 0.08091424126178026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 91.47280811803648,
    "estimated_duration": 3600.003374348508,
    "input_throughput": 7143.349693291284,
    "output_throughput": 6268.528013278293,
    "total_throughput": 13411.877706569576,
    "itl": 84.05317623266318,
    "ttft": 1766536.9739970034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.792704135542741,
    "arrivals": 375670,
    "finished_requests": 104365,
    "scheduler_time": 276.8325575358809
}
#Debug simulation 
Total elapsed time: 91.47295182599919. Arrivals time: 0.45717895129928365 Scheduler time: 90.80064739187947 Scheduler overhead time: 0.08426884794607759 Adapter cache time: 0.01566077396273613 Engine time: 0.08125738915987313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 93.60926699603442,
    "estimated_duration": 3600.0359705957144,
    "input_throughput": 7204.841065993008,
    "output_throughput": 6306.003658136622,
    "total_throughput": 13510.84472412963,
    "itl": 86.01519859011208,
    "ttft": 1770484.4246114672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.650139808468519,
    "arrivals": 375670,
    "finished_requests": 105255,
    "scheduler_time": 274.37492293831735
}
#Debug simulation 
Total elapsed time: 93.60940327105345. Arrivals time: 0.4579429376171902 Scheduler time: 92.93724648770876 Scheduler overhead time: 0.08399985980940983 Adapter cache time: 0.01565614645369351 Engine time: 0.08128108439268544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 91.45589045499219,
    "estimated_duration": 3600.0510130604225,
    "input_throughput": 7143.255166859044,
    "output_throughput": 6268.445063175899,
    "total_throughput": 13411.700230034943,
    "itl": 84.0528017594934,
    "ttft": 1766529.5266027907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7757205270603356,
    "arrivals": 375670,
    "finished_requests": 104365,
    "scheduler_time": 276.83845303652487
}
#Debug simulation 
Total elapsed time: 91.45602026203414. Arrivals time: 0.4561873031198047 Scheduler time: 90.78574245306663 Scheduler overhead time: 0.08345872513018548 Adapter cache time: 0.01591211202321574 Engine time: 0.08105888543650508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 92.86458265699912,
    "estimated_duration": 3600.0009297644556,
    "input_throughput": 7193.253975546704,
    "output_throughput": 6295.898373971521,
    "total_throughput": 13489.152349518225,
    "itl": 86.10002311597543,
    "ttft": 1768764.0008534486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5768284723209165,
    "arrivals": 375670,
    "finished_requests": 105102,
    "scheduler_time": 274.7095183357947
}
#Debug simulation 
Total elapsed time: 92.86471018503653. Arrivals time: 0.45648822345538065 Scheduler time: 92.19351484434446 Scheduler overhead time: 0.08483093051472679 Adapter cache time: 0.015821080247405916 Engine time: 0.08064789400668815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 90.62178039399441,
    "estimated_duration": 3600.033402888771,
    "input_throughput": 7105.548237267381,
    "output_throughput": 6217.015092704548,
    "total_throughput": 13322.563329971928,
    "itl": 83.67376797721644,
    "ttft": 1773996.436285643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8946781591139779,
    "arrivals": 375670,
    "finished_requests": 103760,
    "scheduler_time": 278.5453632147228
}
#Debug simulation 
Total elapsed time: 90.62190949899377. Arrivals time: 0.4397014295682311 Scheduler time: 89.9672183888033 Scheduler overhead time: 0.08367191121215001 Adapter cache time: 0.015803271904587746 Engine time: 0.08151113940402865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 92.00107655901229,
    "estimated_duration": 3600.0941557676388,
    "input_throughput": 7294.993926179711,
    "output_throughput": 6391.673107531129,
    "total_throughput": 13686.66703371084,
    "itl": 87.84974223665911,
    "ttft": 1763839.293110018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9175986192654795,
    "arrivals": 373765,
    "finished_requests": 106865,
    "scheduler_time": 269.7353835215908
}
#Debug simulation 
Total elapsed time: 92.00120778597193. Arrivals time: 0.45410560409072787 Scheduler time: 91.33657686470542 Scheduler overhead time: 0.08209429966518655 Adapter cache time: 0.01570849440759048 Engine time: 0.07962689350824803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 93.8049122520024,
    "estimated_duration": 3600.0357821757143,
    "input_throughput": 7229.631196685047,
    "output_throughput": 6344.2188861236245,
    "total_throughput": 13573.85008280867,
    "itl": 86.40664485590978,
    "ttft": 1767039.4037828331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8752770262165013,
    "arrivals": 373765,
    "finished_requests": 105864,
    "scheduler_time": 272.979572414571
}
#Debug simulation 
Total elapsed time: 93.80504337098682. Arrivals time: 0.4684995095594786 Scheduler time: 93.12276780750835 Scheduler overhead time: 0.08347315335413441 Adapter cache time: 0.015666685183532536 Engine time: 0.0810616918024607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 89.55875317903701,
    "estimated_duration": 3600.0334065580723,
    "input_throughput": 7180.296147505493,
    "output_throughput": 6296.076574931187,
    "total_throughput": 13476.37272243668,
    "itl": 84.67845133675034,
    "ttft": 1776936.3627920868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1843831314612254,
    "arrivals": 373765,
    "finished_requests": 105169,
    "scheduler_time": 274.421495197829
}
#Debug simulation 
Total elapsed time: 89.55888548400253. Arrivals time: 0.4550280303810723 Scheduler time: 88.88673605548684 Scheduler overhead time: 0.08445034164469689 Adapter cache time: 0.016773297684267163 Engine time: 0.08165664237458259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 99.36286796600325,
    "estimated_duration": 3600.054029131576,
    "input_throughput": 7143.239738044532,
    "output_throughput": 6259.877995618924,
    "total_throughput": 13403.117733663456,
    "itl": 85.88697037764608,
    "ttft": 1772874.8912804998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5141320638172313,
    "arrivals": 373765,
    "finished_requests": 104466,
    "scheduler_time": 276.0515139624712
}
#Debug simulation 
Total elapsed time: 99.3630054720561. Arrivals time: 0.4969583566999063 Scheduler time: 98.64864217035938 Scheduler overhead time: 0.08542476745788008 Adapter cache time: 0.01616313954582438 Engine time: 0.08197754679713398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 89.05651667603524,
    "estimated_duration": 3600.003685741094,
    "input_throughput": 7169.366548769748,
    "output_throughput": 6281.803013027914,
    "total_throughput": 13451.169561797662,
    "itl": 84.50157074537846,
    "ttft": 1779450.4363031853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.227192545924344,
    "arrivals": 373765,
    "finished_requests": 105023,
    "scheduler_time": 275.0165356134448
}
#Debug simulation 
Total elapsed time: 89.0566428170423. Arrivals time: 0.4547682353295386 Scheduler time: 88.38497984065907 Scheduler overhead time: 0.0849219950614497 Adapter cache time: 0.016235002200119197 Engine time: 0.08194600441493094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 92.33319176797522,
    "estimated_duration": 3600.0062808645716,
    "input_throughput": 7241.708754391124,
    "output_throughput": 6344.495875300176,
    "total_throughput": 13586.2046296913,
    "itl": 86.73503145107537,
    "ttft": 1761159.0578049703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.787497863359743,
    "arrivals": 373765,
    "finished_requests": 106009,
    "scheduler_time": 271.96362295640546
}
#Debug simulation 
Total elapsed time: 92.33332001697272. Arrivals time: 0.46411871997406706 Scheduler time: 91.65559043298708 Scheduler overhead time: 0.08296602102927864 Adapter cache time: 0.015800364373717457 Engine time: 0.08177212986629456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 88.3184144339757,
    "estimated_duration": 3600.0786773032573,
    "input_throughput": 7169.281372298704,
    "output_throughput": 6281.747991391193,
    "total_throughput": 13451.029363689897,
    "itl": 84.50120619492819,
    "ttft": 1779517.9357502225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.205652359556415,
    "arrivals": 373765,
    "finished_requests": 105024,
    "scheduler_time": 275.0236071556798
}
#Debug simulation 
Total elapsed time: 88.31854413298424. Arrivals time: 0.45298127800924703 Scheduler time: 87.65031650150195 Scheduler overhead time: 0.08375630993396044 Adapter cache time: 0.016331215505488217 Engine time: 0.08126369048841298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 94.40445473702857,
    "estimated_duration": 3600.009598717872,
    "input_throughput": 7248.014007877335,
    "output_throughput": 6324.4123038196085,
    "total_throughput": 13572.426311696943,
    "itl": 86.29621903389345,
    "ttft": 1762744.0824474576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6266526218596826,
    "arrivals": 372804,
    "finished_requests": 105360,
    "scheduler_time": 273.5143126630686
}
#Debug simulation 
Total elapsed time: 94.40458410599967. Arrivals time: 0.46234866563463584 Scheduler time: 93.72785075311549 Scheduler overhead time: 0.08382136269938201 Adapter cache time: 0.015777364606037736 Engine time: 0.08140354626812041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 92.0866285560187,
    "estimated_duration": 3600.0639334008306,
    "input_throughput": 7246.640471563905,
    "output_throughput": 6321.513845589293,
    "total_throughput": 13568.154317153198,
    "itl": 85.66651914615855,
    "ttft": 1763910.404926654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7648049653274958,
    "arrivals": 372804,
    "finished_requests": 105354,
    "scheduler_time": 273.69906915947934
}
#Debug simulation 
Total elapsed time: 92.08676033501979. Arrivals time: 0.45745291467756033 Scheduler time: 91.41538800421404 Scheduler overhead time: 0.0831615095376037 Adapter cache time: 0.016393042460549623 Engine time: 0.08087510196492076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 90.71236531599425,
    "estimated_duration": 3600.0868770967472,
    "input_throughput": 7181.0080930181,
    "output_throughput": 6260.789189114417,
    "total_throughput": 13441.797282132518,
    "itl": 83.67779752534756,
    "ttft": 1771893.2508560754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.773134376006206,
    "arrivals": 372804,
    "finished_requests": 104343,
    "scheduler_time": 276.6464811020568
}
#Debug simulation 
Total elapsed time: 90.71250353404321. Arrivals time: 0.46339690434979275 Scheduler time: 90.03333873784868 Scheduler overhead time: 0.08404959691688418 Adapter cache time: 0.015901589184068143 Engine time: 0.08160618238616735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 90.71421534300316,
    "estimated_duration": 3600.0989645146333,
    "input_throughput": 7302.489531296645,
    "output_throughput": 6372.987583438067,
    "total_throughput": 13675.477114734711,
    "itl": 85.94149236235053,
    "ttft": 1764021.4098686208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9732266652956578,
    "arrivals": 372804,
    "finished_requests": 106170,
    "scheduler_time": 271.2221174220206
}
#Debug simulation 
Total elapsed time: 90.71433727897238. Arrivals time: 0.463089780183509 Scheduler time: 90.03863856184762 Scheduler overhead time: 0.08266090234974399 Adapter cache time: 0.016367185453418642 Engine time: 0.08041969774058089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 90.56915536400629,
    "estimated_duration": 3600.0698606378187,
    "input_throughput": 7181.042035506443,
    "output_throughput": 6260.818782001839,
    "total_throughput": 13441.860817508283,
    "itl": 83.67748291779499,
    "ttft": 1771886.0691636351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7569792362302592,
    "arrivals": 372804,
    "finished_requests": 104343,
    "scheduler_time": 276.6461308716068
}
#Debug simulation 
Total elapsed time: 90.56928718701238. Arrivals time: 0.465251253452152 Scheduler time: 89.8858675713418 Scheduler overhead time: 0.08571168535854667 Adapter cache time: 0.015960475197061896 Engine time: 0.08227460307534784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 90.77052557701245,
    "estimated_duration": 3600.0927749331586,
    "input_throughput": 7302.651526942419,
    "output_throughput": 6373.18797997532,
    "total_throughput": 13675.839506917739,
    "itl": 85.93812136703518,
    "ttft": 1764052.3148164186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.838569230884307,
    "arrivals": 372804,
    "finished_requests": 106173,
    "scheduler_time": 271.2319339982511
}
#Debug simulation 
Total elapsed time: 90.77065133897122. Arrivals time: 0.46416844899067655 Scheduler time: 90.0928587243543 Scheduler overhead time: 0.08352349553024396 Adapter cache time: 0.016165203880518675 Engine time: 0.08036000560969114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 91.11167113803094,
    "estimated_duration": 3600.052076552045,
    "input_throughput": 7181.077509512038,
    "output_throughput": 6260.849710148395,
    "total_throughput": 13441.927219660432,
    "itl": 83.67713701681224,
    "ttft": 1771878.5113152568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7395813933946243,
    "arrivals": 372804,
    "finished_requests": 104343,
    "scheduler_time": 276.6457475251232
}
#Debug simulation 
Total elapsed time: 91.11179917003028. Arrivals time: 0.46321334544336423 Scheduler time: 90.43059901555534 Scheduler overhead time: 0.08598454046295956 Adapter cache time: 0.016012080828659236 Engine time: 0.08207332802703604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 101.00344635697547,
    "estimated_duration": 3600.0944289919153,
    "input_throughput": 7337.685586040867,
    "output_throughput": 6409.73214873743,
    "total_throughput": 13747.417734778297,
    "itl": 87.17169987692269,
    "ttft": 1744948.2850007475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.501016850252634,
    "arrivals": 372307,
    "finished_requests": 106674,
    "scheduler_time": 269.81799787140216
}
#Debug simulation 
Total elapsed time: 101.00357368198456. Arrivals time: 0.510762290447019 Scheduler time: 100.27461369114462 Scheduler overhead time: 0.08677057473687455 Adapter cache time: 0.01553401816636324 Engine time: 0.08208197430940345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 99.43158480199054,
    "estimated_duration": 3600.038318236924,
    "input_throughput": 7277.893367766007,
    "output_throughput": 6354.061256548705,
    "total_throughput": 13631.954624314712,
    "itl": 86.07854098610942,
    "ttft": 1752740.9788780336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5419154386501779,
    "arrivals": 372307,
    "finished_requests": 105839,
    "scheduler_time": 272.43467518905317
}
#Debug simulation 
Total elapsed time: 99.43172394199064. Arrivals time: 0.47826018487103283 Scheduler time: 98.73596212960547 Scheduler overhead time: 0.08553166594356298 Adapter cache time: 0.016083277645520866 Engine time: 0.08221140771638602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 95.52258830901701,
    "estimated_duration": 3600.088136393303,
    "input_throughput": 7159.812766646117,
    "output_throughput": 6253.280793995696,
    "total_throughput": 13413.093560641813,
    "itl": 83.06983240935604,
    "ttft": 1766961.2064401598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.651993275401186,
    "arrivals": 372307,
    "finished_requests": 104109,
    "scheduler_time": 277.26610767763265
}
#Debug simulation 
Total elapsed time: 95.52271280798595. Arrivals time: 0.49465422390494496 Scheduler time: 94.81043040571967 Scheduler overhead time: 0.08587254449957982 Adapter cache time: 0.016524908889550716 Engine time: 0.08138306823093444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 89.86423929297598,
    "estimated_duration": 3600.021523137062,
    "input_throughput": 7297.492759739756,
    "output_throughput": 6377.842702449265,
    "total_throughput": 13675.335462189021,
    "itl": 86.5088150229959,
    "ttft": 1754861.7350043235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7062068759603404,
    "arrivals": 372307,
    "finished_requests": 106171,
    "scheduler_time": 271.24978064931895
}
#Debug simulation 
Total elapsed time: 89.86436457396485. Arrivals time: 0.4640205652685836 Scheduler time: 89.18728991644457 Scheduler overhead time: 0.08411346364300698 Adapter cache time: 0.015645529725588858 Engine time: 0.08020576369017363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 96.41052325902274,
    "estimated_duration": 3600.0500849126033,
    "input_throughput": 7159.744001346508,
    "output_throughput": 6253.254946186815,
    "total_throughput": 13412.998947533324,
    "itl": 83.07021035670932,
    "ttft": 1766922.733362833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.637287955861542,
    "arrivals": 372307,
    "finished_requests": 104105,
    "scheduler_time": 277.26850196739787
}
#Debug simulation 
Total elapsed time: 96.41065354598686. Arrivals time: 0.4691572656738572 Scheduler time: 95.72082104568835 Scheduler overhead time: 0.0868092950549908 Adapter cache time: 0.016690077201928943 Engine time: 0.08330352534539998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 98.83561829203973,
    "estimated_duration": 3600.043331015998,
    "input_throughput": 7240.22702044643,
    "output_throughput": 6323.068337506865,
    "total_throughput": 13563.295357953295,
    "itl": 85.37920927635746,
    "ttft": 1755011.4081275847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3725430022226606,
    "arrivals": 372307,
    "finished_requests": 105299,
    "scheduler_time": 273.86909029118635
}
#Debug simulation 
Total elapsed time: 98.83574373001466. Arrivals time: 0.5073875568341464 Scheduler time: 98.11150805617217 Scheduler overhead time: 0.08602417929796502 Adapter cache time: 0.015671042434405535 Engine time: 0.08157446736004204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_128_slots_16_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 98.93705343001056,
    "estimated_duration": 3600.0458272197925,
    "input_throughput": 7194.818133746671,
    "output_throughput": 6298.310379429422,
    "total_throughput": 13493.128513176094,
    "itl": 83.81283893119456,
    "ttft": 1754036.619804373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6717641660571139,
    "arrivals": 372307,
    "finished_requests": 104607,
    "scheduler_time": 276.0818524809517
}
#Debug simulation 
Total elapsed time: 98.93718231405364. Arrivals time: 0.46805456560105085 Scheduler time: 98.24920459435089 Scheduler overhead time: 0.08723054465372115 Adapter cache time: 0.015738084621261805 Engine time: 0.08245362446177751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 98.01041520398576,
    "estimated_duration": 3600.0101906068294,
    "input_throughput": 7230.521199055914,
    "output_throughput": 6368.393917278182,
    "total_throughput": 13598.915116334096,
    "itl": 86.1324504431998,
    "ttft": 1687148.0593380602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5539161225082334,
    "arrivals": 324936,
    "finished_requests": 106083,
    "scheduler_time": 269.8515758293642
}
#Debug simulation 
Total elapsed time: 98.01055750797968. Arrivals time: 0.4737184252589941 Scheduler time: 97.31950627855258 Scheduler overhead time: 0.08594265626743436 Adapter cache time: 0.015760797541588545 Engine time: 0.08229119761381298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 95.01453188998858,
    "estimated_duration": 3600.0009228554172,
    "input_throughput": 7204.011764371866,
    "output_throughput": 6372.224199821771,
    "total_throughput": 13576.235964193638,
    "itl": 85.0126854360618,
    "ttft": 1679272.0510644396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.880829910109753,
    "arrivals": 324936,
    "finished_requests": 105765,
    "scheduler_time": 270.50810640690395
}
#Debug simulation 
Total elapsed time: 95.01466262200847. Arrivals time: 0.4736926893237978 Scheduler time: 94.32388503971742 Scheduler overhead time: 0.08527433854760602 Adapter cache time: 0.015831500000786036 Engine time: 0.08159218664513901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 98.52052938600536,
    "estimated_duration": 3600.0452978050994,
    "input_throughput": 7154.310812617553,
    "output_throughput": 6312.861678117246,
    "total_throughput": 13467.172490734798,
    "itl": 83.28825677067603,
    "ttft": 1689681.7694736104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.846591925565158,
    "arrivals": 324936,
    "finished_requests": 104897,
    "scheduler_time": 272.86579671425585
}
#Debug simulation 
Total elapsed time: 98.52065770101035. Arrivals time: 0.48021552519639954 Scheduler time: 97.81834353745217 Scheduler overhead time: 0.08850295754382387 Adapter cache time: 0.015993025503121316 Engine time: 0.08329816185869277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 98.50625740300165,
    "estimated_duration": 3600.0080062957695,
    "input_throughput": 7209.679799214201,
    "output_throughput": 6375.893597975017,
    "total_throughput": 13585.573397189217,
    "itl": 85.15482634328967,
    "ttft": 1676755.5996412667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6912197760585674,
    "arrivals": 324936,
    "finished_requests": 105724,
    "scheduler_time": 270.87831563047996
}
#Debug simulation 
Total elapsed time: 98.50639123999281. Arrivals time: 0.47689146967604756 Scheduler time: 97.81045410363004 Scheduler overhead time: 0.08632675290573388 Adapter cache time: 0.016278724011499435 Engine time: 0.08218303631292656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_128_slots_16_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 94.10838701901957,
    "estimated_duration": 3600.0229836737067,
    "input_throughput": 7141.815515234035,
    "output_throughput": 6300.820328889299,
    "total_throughput": 13442.635844123333,
    "itl": 82.92286017756336,
    "ttft": 1683278.940155512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8693154634395663,
    "arrivals": 324936,
    "finished_requests": 104700,
    "scheduler_time": 273.832414376904
}
#Debug simulation 
Total elapsed time: 94.10851638601162. Arrivals time: 0.472754453367088 Scheduler time: 93.41349793394329 Scheduler overhead time: 0.0878151886863634 Adapter cache time: 0.017227736359927803 Engine time: 0.08324887533672154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_16_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_16_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 98.98415845801355,
    "estimated_duration": 3600.0394575997234,
    "input_throughput": 7202.036062484403,
    "output_throughput": 6352.24870986474,
    "total_throughput": 13554.284772349143,
    "itl": 84.87258018415889,
    "ttft": 1686353.3257928167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4938375000935,
    "arrivals": 324936,
    "finished_requests": 105612,
    "scheduler_time": 271.2621685723312
}
#Debug simulation 
Total elapsed time: 98.9842823370127. Arrivals time: 0.47952229605289176 Scheduler time: 98.28643777716206 Scheduler overhead time: 0.08556215348653495 Adapter cache time: 0.016288242768496275 Engine time: 0.0825177383958362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_128_slots_16_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_128_slots_16_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 93.96035789401503,
    "estimated_duration": 3600.0360732633,
    "input_throughput": 7187.0407611073,
    "output_throughput": 6340.3186900040255,
    "total_throughput": 13527.359451111324,
    "itl": 83.49512026277493,
    "ttft": 1687552.1346020843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.86470402354375,
    "arrivals": 324936,
    "finished_requests": 105359,
    "scheduler_time": 271.9235861813385
}
#Debug simulation 
Total elapsed time: 93.96049495699117. Arrivals time: 0.47346942697186023 Scheduler time: 93.2663201567484 Scheduler overhead time: 0.08660501439590007 Adapter cache time: 0.016640313668176532 Engine time: 0.08347406768007204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 100.68397315399488,
    "estimated_duration": 3600.0942193470087,
    "input_throughput": 7231.779896227908,
    "output_throughput": 6306.6046655073815,
    "total_throughput": 13538.384561735289,
    "itl": 85.05423343080832,
    "ttft": 1706178.284844217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4282803509011848,
    "arrivals": 317347,
    "finished_requests": 104960,
    "scheduler_time": 271.9803650574472
}
#Debug simulation 
Total elapsed time: 100.68410208600108. Arrivals time: 0.472332680830732 Scheduler time: 99.98979536490515 Scheduler overhead time: 0.08804383483948186 Adapter cache time: 0.016493033210281283 Engine time: 0.08335218374850228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 105.77048517297953,
    "estimated_duration": 3600.094579590742,
    "input_throughput": 7172.1201843911695,
    "output_throughput": 6256.2264690724905,
    "total_throughput": 13428.34665346366,
    "itl": 83.50955462044743,
    "ttft": 1702733.572100366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.51998723388184,
    "arrivals": 317347,
    "finished_requests": 104197,
    "scheduler_time": 274.72128274716937
}
#Debug simulation 
Total elapsed time: 105.77061513502849. Arrivals time: 0.4779577230801806 Scheduler time: 105.06959725631168 Scheduler overhead time: 0.08823329507140443 Adapter cache time: 0.016456469777040184 Engine time: 0.08338565530721098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 99.43995746201836,
    "estimated_duration": 3600.0204419116894,
    "input_throughput": 7168.86401519858,
    "output_throughput": 6245.730368147605,
    "total_throughput": 13414.594383346184,
    "itl": 82.36432672229296,
    "ttft": 1707240.7160796965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6669045743532522,
    "arrivals": 317347,
    "finished_requests": 104125,
    "scheduler_time": 274.8609558961137
}
#Debug simulation 
Total elapsed time: 99.44008916500024. Arrivals time: 0.4678079634322785 Scheduler time: 98.74986159062246 Scheduler overhead time: 0.08818986802361906 Adapter cache time: 0.015816920262295753 Engine time: 0.08379084849730134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 102.36023157299496,
    "estimated_duration": 3600.0994460354786,
    "input_throughput": 7172.3418719543715,
    "output_throughput": 6256.2688441296,
    "total_throughput": 13428.610716083973,
    "itl": 83.50844020170096,
    "ttft": 1702670.7827402807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4228117657499375,
    "arrivals": 317347,
    "finished_requests": 104199,
    "scheduler_time": 274.7274843726544
}
#Debug simulation 
Total elapsed time: 102.3603698189836. Arrivals time: 0.4750342862098478 Scheduler time: 101.66317762475228 Scheduler overhead time: 0.08790961874183267 Adapter cache time: 0.016169807873666286 Engine time: 0.08376130316173658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_128_slots_16_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 99.03885556897148,
    "estimated_duration": 3600.00475697691,
    "input_throughput": 7168.895249369675,
    "output_throughput": 6245.757580298724,
    "total_throughput": 13414.6528296684,
    "itl": 82.36398976305917,
    "ttft": 1707234.0733225937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6519921376369937,
    "arrivals": 317347,
    "finished_requests": 104125,
    "scheduler_time": 274.86068905415897
}
#Debug simulation 
Total elapsed time: 99.03898662497522. Arrivals time: 0.47305877931648865 Scheduler time: 98.34490408207057 Scheduler overhead time: 0.08695693005574867 Adapter cache time: 0.01625832507852465 Engine time: 0.08377726963954046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_16_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_16_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 102.8888646630221,
    "estimated_duration": 3600.000726217741,
    "input_throughput": 7205.6121575435045,
    "output_throughput": 6286.159287466557,
    "total_throughput": 13491.771445010061,
    "itl": 84.28244495330966,
    "ttft": 1695674.4269577346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.353391239400949,
    "arrivals": 317347,
    "finished_requests": 104641,
    "scheduler_time": 273.33843996521676
}
#Debug simulation 
Total elapsed time: 102.88900526199723. Arrivals time: 0.47149734024424106 Scheduler time: 102.19499354582513 Scheduler overhead time: 0.08817464130697772 Adapter cache time: 0.015937052201479673 Engine time: 0.08341175020905212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_128_slots_16_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_128_slots_16_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 99.39286039501894,
    "estimated_duration": 3600.087041162059,
    "input_throughput": 7168.863059396852,
    "output_throughput": 6245.717323751735,
    "total_throughput": 13414.580383148588,
    "itl": 82.36378008867646,
    "ttft": 1707291.1904729684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6354227635078173,
    "arrivals": 317347,
    "finished_requests": 104126,
    "scheduler_time": 274.86784870614974
}
#Debug simulation 
Total elapsed time: 99.3929972180049. Arrivals time: 0.4560354783316143 Scheduler time: 98.7136064528604 Scheduler overhead time: 0.08882164856186137 Adapter cache time: 0.016371605219319463 Engine time: 0.08347387332469225 
