INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 14.322127315215766,
    "estimated_duration": 3600.031877448385,
    "input_throughput": 5476.340674509114,
    "output_throughput": 4810.837678547282,
    "total_throughput": 10287.178353056395,
    "itl": 46.242034999705616,
    "ttft": 53672.47353181977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.044760627048089,
    "arrivals": 80362,
    "finished_requests": 79724,
    "scheduler_time": 59.79085691565638
}
#Debug simulation 
Total elapsed time: 14.322349292226136. Arrivals time: 0.21785458642989397 Scheduler time: 13.829925447236747 Scheduler overhead time: 0.10871526086702943 Adapter cache time: 0.017183436546474695 Engine time: 0.10194951761513948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 14.3745680982247,
    "estimated_duration": 3600.0388325654244,
    "input_throughput": 5476.330094459256,
    "output_throughput": 4810.828384220007,
    "total_throughput": 10287.158478679263,
    "itl": 46.24430056672407,
    "ttft": 53672.89061776577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1447051639948045,
    "arrivals": 80362,
    "finished_requests": 79724,
    "scheduler_time": 59.791518987894406
}
#Debug simulation 
Total elapsed time: 14.37470533605665. Arrivals time: 0.22003339556977153 Scheduler time: 13.8787591913715 Scheduler overhead time: 0.10917495656758547 Adapter cache time: 0.017254877369850874 Engine time: 0.10320603754371405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 14.304685927927494,
    "estimated_duration": 3600.0509259494274,
    "input_throughput": 5476.311698229835,
    "output_throughput": 4810.812223561111,
    "total_throughput": 10287.123921790946,
    "itl": 46.24510582016093,
    "ttft": 53673.07911299612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.173491209363566,
    "arrivals": 80362,
    "finished_requests": 79724,
    "scheduler_time": 59.79191286452851
}
#Debug simulation 
Total elapsed time: 14.304779461119324. Arrivals time: 0.21724011516198516 Scheduler time: 13.81419987929985 Scheduler overhead time: 0.10929701756685972 Adapter cache time: 0.017312093172222376 Engine time: 0.10006019519641995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 14.357967640738934,
    "estimated_duration": 3600.018070055489,
    "input_throughput": 5476.361678289055,
    "output_throughput": 4810.856129878552,
    "total_throughput": 10287.217808167607,
    "itl": 46.24266691096169,
    "ttft": 53672.54672695239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0628001265693445,
    "arrivals": 80362,
    "finished_requests": 79724,
    "scheduler_time": 59.79066230760932
}
#Debug simulation 
Total elapsed time: 14.358059965074062. Arrivals time: 0.21470527350902557 Scheduler time: 13.869664243422449 Scheduler overhead time: 0.10888136085122824 Adapter cache time: 0.01725151063874364 Engine time: 0.10127558279782534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_32_slots_16_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 14.417891050223261,
    "estimated_duration": 3600.016516569123,
    "input_throughput": 5467.865191562947,
    "output_throughput": 4802.397689129614,
    "total_throughput": 10270.262880692562,
    "itl": 46.1333813408487,
    "ttft": 58707.96674477593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1534023397462452,
    "arrivals": 80362,
    "finished_requests": 79609,
    "scheduler_time": 59.68740345505067
}
#Debug simulation 
Total elapsed time: 14.4179883049801. Arrivals time: 0.21954090194776654 Scheduler time: 13.923835600726306 Scheduler overhead time: 0.1094303042627871 Adapter cache time: 0.017257753759622574 Engine time: 0.10147028369829059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_32_slots_16_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_32_slots_16_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 14.395482532214373,
    "estimated_duration": 3600.006184801208,
    "input_throughput": 5467.8808839565845,
    "output_throughput": 4802.411471677703,
    "total_throughput": 10270.292355634287,
    "itl": 46.130037141561765,
    "ttft": 58707.6980142187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0022755876695717,
    "arrivals": 80362,
    "finished_requests": 79609,
    "scheduler_time": 59.68646751013841
}
#Debug simulation 
Total elapsed time: 14.395591270178556. Arrivals time: 0.2179931397549808 Scheduler time: 13.900593500118703 Scheduler overhead time: 0.10984581429511309 Adapter cache time: 0.01741199241951108 Engine time: 0.10319403000175953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_32_slots_16_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_32_slots_16_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 14.295134724583477,
    "estimated_duration": 3600.0073940569123,
    "input_throughput": 5476.377918708332,
    "output_throughput": 4810.870396708469,
    "total_throughput": 10287.248315416802,
    "itl": 46.245216074338515,
    "ttft": 53629.128303343576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1531937260553249,
    "arrivals": 80362,
    "finished_requests": 79724,
    "scheduler_time": 59.7911073111986
}
#Debug simulation 
Total elapsed time: 14.29531455365941. Arrivals time: 0.21897495817393064 Scheduler time: 13.803574577905238 Scheduler overhead time: 0.10893276892602444 Adapter cache time: 0.01719127595424652 Engine time: 0.10012758569791913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 13.647210981231183,
    "estimated_duration": 3600.0228621337196,
    "input_throughput": 5439.699899125975,
    "output_throughput": 4770.979701451137,
    "total_throughput": 10210.679600577112,
    "itl": 45.584747068546235,
    "ttft": 58241.244388597755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9918613547924895,
    "arrivals": 79870,
    "finished_requests": 79067,
    "scheduler_time": 58.61943655907801
}
#Debug simulation 
Total elapsed time: 13.647324906196445. Arrivals time: 0.22084875870496035 Scheduler time: 13.150265530217439 Scheduler overhead time: 0.11006234167143703 Adapter cache time: 0.017324105836451054 Engine time: 0.10216562077403069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 13.635981757659465,
    "estimated_duration": 3600.0192173746746,
    "input_throughput": 5439.705406428635,
    "output_throughput": 4770.984531722968,
    "total_throughput": 10210.689938151603,
    "itl": 45.58715438064168,
    "ttft": 58241.44596231985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.085304470630363,
    "arrivals": 79870,
    "finished_requests": 79067,
    "scheduler_time": 58.619944339952646
}
#Debug simulation 
Total elapsed time: 13.636110319755971. Arrivals time: 0.20284016197547317 Scheduler time: 13.159363536629826 Scheduler overhead time: 0.10989477019757032 Adapter cache time: 0.01721198484301567 Engine time: 0.10021102009341121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 13.688086536712945,
    "estimated_duration": 3600.022029829977,
    "input_throughput": 5439.701156752331,
    "output_throughput": 4770.980804473348,
    "total_throughput": 10210.68196122568,
    "itl": 45.5882760389,
    "ttft": 58241.45933846381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1123002351634212,
    "arrivals": 79870,
    "finished_requests": 79067,
    "scheduler_time": 58.62010661660824
}
#Debug simulation 
Total elapsed time: 13.688192626927048. Arrivals time: 0.20189409283921123 Scheduler time: 13.209271584637463 Scheduler overhead time: 0.11054126359522343 Adapter cache time: 0.017363056540489197 Engine time: 0.10225754138082266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 13.585816853214055,
    "estimated_duration": 3600.016642162239,
    "input_throughput": 5439.709297632038,
    "output_throughput": 4770.987944567941,
    "total_throughput": 10210.697242199978,
    "itl": 45.58565550140053,
    "ttft": 58240.89675500388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0103405380714674,
    "arrivals": 79870,
    "finished_requests": 79067,
    "scheduler_time": 58.61945648553643
}
#Debug simulation 
Total elapsed time: 13.585936817806214. Arrivals time: 0.20396010857075453 Scheduler time: 13.108765403740108 Scheduler overhead time: 0.10912621160969138 Adapter cache time: 0.0171850873157382 Engine time: 0.10023306449875236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 13.656064931768924,
    "estimated_duration": 3600.005709136747,
    "input_throughput": 5439.725817739289,
    "output_throughput": 4771.002433804079,
    "total_throughput": 10210.728251543367,
    "itl": 45.587960506539176,
    "ttft": 58240.96040822234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1011159076262271,
    "arrivals": 79870,
    "finished_requests": 79067,
    "scheduler_time": 58.61982454956778
}
#Debug simulation 
Total elapsed time: 13.656160153914243. Arrivals time: 0.1996638816781342 Scheduler time: 13.18226139806211 Scheduler overhead time: 0.10957643808797002 Adapter cache time: 0.017313335556536913 Engine time: 0.10071896063163877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 13.656148108653724,
    "estimated_duration": 3600.0048876759406,
    "input_throughput": 5439.7270589935915,
    "output_throughput": 4771.003522466908,
    "total_throughput": 10210.7305814605,
    "itl": 45.58421466992295,
    "ttft": 58240.983697265525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9575881410855781,
    "arrivals": 79870,
    "finished_requests": 79067,
    "scheduler_time": 58.61896816801836
}
#Debug simulation 
Total elapsed time: 13.656257732771337. Arrivals time: 0.20223874878138304 Scheduler time: 13.176968818064779 Scheduler overhead time: 0.1105012884363532 Adapter cache time: 0.017342858482152224 Engine time: 0.10237130522727966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 13.650595426093787,
    "estimated_duration": 3600.0319906734153,
    "input_throughput": 5439.686105771752,
    "output_throughput": 4770.967603759309,
    "total_throughput": 10210.653709531061,
    "itl": 45.58769433054559,
    "ttft": 58241.37189849208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0932454549148685,
    "arrivals": 79870,
    "finished_requests": 79067,
    "scheduler_time": 58.6201271543502
}
#Debug simulation 
Total elapsed time: 13.650855696294457. Arrivals time: 0.20433710981160402 Scheduler time: 13.171840590890497 Scheduler overhead time: 0.10939745372161269 Adapter cache time: 0.01714611705392599 Engine time: 0.10157635994255543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 12.278797107748687,
    "estimated_duration": 3600.0010209818643,
    "input_throughput": 5456.850119070831,
    "output_throughput": 4716.032551393415,
    "total_throughput": 10172.882670464247,
    "itl": 44.694585468038746,
    "ttft": 52486.517442754994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0050861728563893,
    "arrivals": 79675,
    "finished_requests": 78926,
    "scheduler_time": 57.065651420048816
}
#Debug simulation 
Total elapsed time: 12.278888477943838. Arrivals time: 0.2024849527515471 Scheduler time: 11.800273893866688 Scheduler overhead time: 0.11035872809588909 Adapter cache time: 0.017307069152593613 Engine time: 0.10039024613797665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 12.307874274905771,
    "estimated_duration": 3599.9685720159882,
    "input_throughput": 5456.799860060766,
    "output_throughput": 4715.950614672366,
    "total_throughput": 10172.750474733131,
    "itl": 44.696833997763434,
    "ttft": 52532.20106745568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1077898593246942,
    "arrivals": 79675,
    "finished_requests": 78925,
    "scheduler_time": 57.065863409651
}
#Debug simulation 
Total elapsed time: 12.308000168763101. Arrivals time: 0.2061035525985062 Scheduler time: 11.825503448024392 Scheduler overhead time: 0.11053481604903936 Adapter cache time: 0.017343516927212477 Engine time: 0.10148541489616036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 12.280187021940947,
    "estimated_duration": 3599.977350204428,
    "input_throughput": 5456.786554194426,
    "output_throughput": 4715.9391152935805,
    "total_throughput": 10172.725669488007,
    "itl": 44.69757716178711,
    "ttft": 52532.35804657374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1369964138837538,
    "arrivals": 79675,
    "finished_requests": 78925,
    "scheduler_time": 57.06615138042178
}
#Debug simulation 
Total elapsed time: 12.28027890296653. Arrivals time: 0.20581479649990797 Scheduler time: 11.798501573503017 Scheduler overhead time: 0.11054586386308074 Adapter cache time: 0.017318057361990213 Engine time: 0.10118851996958256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 12.310578268021345,
    "estimated_duration": 3599.995000262508,
    "input_throughput": 5456.8592452399325,
    "output_throughput": 4716.040438601165,
    "total_throughput": 10172.899683841099,
    "itl": 44.69519916467592,
    "ttft": 52487.12166173868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.025884821899234,
    "arrivals": 79675,
    "finished_requests": 78926,
    "scheduler_time": 57.06578301831841
}
#Debug simulation 
Total elapsed time: 12.31065976480022. Arrivals time: 0.20279572252184153 Scheduler time: 11.83134744502604 Scheduler overhead time: 0.11021390883252025 Adapter cache time: 0.01726188324391842 Engine time: 0.10215872200205922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 12.269596040714532,
    "estimated_duration": 3600.001178859758,
    "input_throughput": 5456.849879760909,
    "output_throughput": 4716.032344572014,
    "total_throughput": 10172.882224332923,
    "itl": 44.69777449277252,
    "ttft": 52486.927861680175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1247765004634864,
    "arrivals": 79675,
    "finished_requests": 78926,
    "scheduler_time": 57.06643704017561
}
#Debug simulation 
Total elapsed time: 12.26969362795353. Arrivals time: 0.20592585764825344 Scheduler time: 11.788574610371143 Scheduler overhead time: 0.1099070836789906 Adapter cache time: 0.017277168575674295 Engine time: 0.10112408362329006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 12.291201018728316,
    "estimated_duration": 3599.9784043376403,
    "input_throughput": 5456.78495635708,
    "output_throughput": 4715.937734388617,
    "total_throughput": 10172.722690745697,
    "itl": 44.69379413147662,
    "ttft": 52531.735172219924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9703559829667191,
    "arrivals": 79675,
    "finished_requests": 78925,
    "scheduler_time": 57.065036789648545
}
#Debug simulation 
Total elapsed time: 12.29131715092808. Arrivals time: 0.2059880718588829 Scheduler time: 11.810915261041373 Scheduler overhead time: 0.10987109830603004 Adapter cache time: 0.017218926455825567 Engine time: 0.10056883608922362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 12.278645809739828,
    "estimated_duration": 3599.983525160571,
    "input_throughput": 5456.777194313355,
    "output_throughput": 4715.931026168449,
    "total_throughput": 10172.708220481803,
    "itl": 44.697396200027626,
    "ttft": 52532.54631906865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1164918133988981,
    "arrivals": 79675,
    "finished_requests": 78925,
    "scheduler_time": 57.066151239659014
}
#Debug simulation 
Total elapsed time: 12.278800746891648. Arrivals time: 0.20124828023836017 Scheduler time: 11.803504052571952 Scheduler overhead time: 0.10955414921045303 Adapter cache time: 0.017129015643149614 Engine time: 0.1006724457256496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 11.341884201858193,
    "estimated_duration": 3599.9908495516693,
    "input_throughput": 5400.644005087201,
    "output_throughput": 4739.280657372994,
    "total_throughput": 10139.924662460195,
    "itl": 44.76718908378701,
    "ttft": 44247.6433614332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0050861728563893,
    "arrivals": 79540,
    "finished_requests": 78923,
    "scheduler_time": 57.04821699312603
}
#Debug simulation 
Total elapsed time: 11.341997700743377. Arrivals time: 0.20256258454173803 Scheduler time: 10.865363121964037 Scheduler overhead time: 0.10997334588319063 Adapter cache time: 0.017264985479414463 Engine time: 0.10011835815384984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 11.364935635123402,
    "estimated_duration": 3600.0109927525828,
    "input_throughput": 5400.613786774679,
    "output_throughput": 4739.254139597727,
    "total_throughput": 10139.867926372406,
    "itl": 44.77018972026393,
    "ttft": 44246.66226380693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1119545222446328,
    "arrivals": 79540,
    "finished_requests": 78923,
    "scheduler_time": 57.04901157493617
}
#Debug simulation 
Total elapsed time: 11.365042094141245. Arrivals time: 0.2016154807060957 Scheduler time: 10.889107066206634 Scheduler overhead time: 0.10985628515481949 Adapter cache time: 0.017195957247167826 Engine time: 0.10030585201457143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 11.32877974724397,
    "estimated_duration": 3599.978848651686,
    "input_throughput": 5400.639230778192,
    "output_throughput": 4739.272289444152,
    "total_throughput": 10139.911520222344,
    "itl": 44.770502817217654,
    "ttft": 44293.151634176225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1421228330675524,
    "arrivals": 79540,
    "finished_requests": 78922,
    "scheduler_time": 57.0489317741549
}
#Debug simulation 
Total elapsed time: 11.32889323728159. Arrivals time: 0.20699535682797432 Scheduler time: 10.84893789049238 Scheduler overhead time: 0.10931706381961703 Adapter cache time: 0.017147859558463097 Engine time: 0.0997365158982575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 11.329998346976936,
    "estimated_duration": 3600.038771050877,
    "input_throughput": 5394.769677530656,
    "output_throughput": 4733.853739865502,
    "total_throughput": 10128.623417396158,
    "itl": 44.74725789119329,
    "ttft": 47540.92113457744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0464248056942587,
    "arrivals": 79540,
    "finished_requests": 78845,
    "scheduler_time": 56.9644666537804
}
#Debug simulation 
Total elapsed time: 11.330114756710827. Arrivals time: 0.20268570678308606 Scheduler time: 10.853569550439715 Scheduler overhead time: 0.10954182129353285 Adapter cache time: 0.017244229558855295 Engine time: 0.10046767769381404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 11.322634411044419,
    "estimated_duration": 3599.9987420253583,
    "input_throughput": 5394.819385151662,
    "output_throughput": 4733.89832086779,
    "total_throughput": 10128.717706019452,
    "itl": 44.749904054173435,
    "ttft": 47581.79227561556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1530955538852152,
    "arrivals": 79540,
    "finished_requests": 78844,
    "scheduler_time": 56.96421339197511
}
#Debug simulation 
Total elapsed time: 11.322732434142381. Arrivals time: 0.20100589748471975 Scheduler time: 10.84699262632057 Scheduler overhead time: 0.10968414740636945 Adapter cache time: 0.017201769631356 Engine time: 0.10126437013968825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 11.339495809748769,
    "estimated_duration": 3600.012648934379,
    "input_throughput": 5400.6113022283425,
    "output_throughput": 4739.2519593091565,
    "total_throughput": 10139.863261537499,
    "itl": 44.766300002580884,
    "ttft": 44247.90015022775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9703559829667191,
    "arrivals": 79540,
    "finished_requests": 78923,
    "scheduler_time": 57.0482855578322
}
#Debug simulation 
Total elapsed time: 11.339642070699483. Arrivals time: 0.20350303407758474 Scheduler time: 10.862772148102522 Scheduler overhead time: 0.1094320104457438 Adapter cache time: 0.01724102720618248 Engine time: 0.09984008595347404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 11.342444228939712,
    "estimated_duration": 3599.9839726416394,
    "input_throughput": 5400.654321728388,
    "output_throughput": 4739.289710637379,
    "total_throughput": 10139.944032365767,
    "itl": 44.76962911736892,
    "ttft": 44248.339392765076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1209968810528526,
    "arrivals": 79540,
    "finished_requests": 78923,
    "scheduler_time": 57.048919709258726
}
#Debug simulation 
Total elapsed time: 11.34259781986475. Arrivals time: 0.1996973450295627 Scheduler time: 10.868717074394226 Scheduler overhead time: 0.10933099407702684 Adapter cache time: 0.017153050284832716 Engine time: 0.10108953667804599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.740772107150406,
    "estimated_duration": 3600.025819849986,
    "input_throughput": 4775.516026914605,
    "output_throughput": 4151.935221570952,
    "total_throughput": 8927.451248485557,
    "itl": 39.10161775250994,
    "ttft": 13562.27533503014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.71464763978039,
    "arrivals": 69453,
    "finished_requests": 69242,
    "scheduler_time": 44.467544070184495
}
#Debug simulation 
Total elapsed time: 5.740893877111375. Arrivals time: 0.17362508736550808 Scheduler time: 5.290643196552992 Scheduler overhead time: 0.10742854699492455 Adapter cache time: 0.020177978090941906 Engine time: 0.10084797069430351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.756155251059681,
    "estimated_duration": 3600.010606039075,
    "input_throughput": 4775.523152948565,
    "output_throughput": 4151.830268201651,
    "total_throughput": 8927.353421150217,
    "itl": 39.10986643425968,
    "ttft": 13562.746950063012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.165329300831076,
    "arrivals": 69453,
    "finished_requests": 69241,
    "scheduler_time": 44.47087440411995
}
#Debug simulation 
Total elapsed time: 5.756226536817849. Arrivals time: 0.17042724788188934 Scheduler time: 5.308317557908595 Scheduler overhead time: 0.10697597917169333 Adapter cache time: 0.020196195226162672 Engine time: 0.10198387503623962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.733425671234727,
    "estimated_duration": 3600.0186131563783,
    "input_throughput": 4775.869751663742,
    "output_throughput": 4151.775200655262,
    "total_throughput": 8927.644952319004,
    "itl": 39.1071629720636,
    "ttft": 13337.574233045807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 705,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.233962808288657,
    "arrivals": 69453,
    "finished_requests": 69246,
    "scheduler_time": 44.472981260353166
}
#Debug simulation 
Total elapsed time: 5.73352311225608. Arrivals time: 0.1759370630607009 Scheduler time: 5.280611165333539 Scheduler overhead time: 0.10792810097336769 Adapter cache time: 0.020041150506585836 Engine time: 0.10054898587986827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.728553986176848,
    "estimated_duration": 3600.03173601303,
    "input_throughput": 4775.508179002835,
    "output_throughput": 4151.928398429513,
    "total_throughput": 8927.436577432349,
    "itl": 39.10342443661105,
    "ttft": 13562.220766155799,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.79050963803659,
    "arrivals": 69453,
    "finished_requests": 69242,
    "scheduler_time": 44.468137926151144
}
#Debug simulation 
Total elapsed time: 5.728629179298878. Arrivals time: 0.16977330297231674 Scheduler time: 5.283175542484969 Scheduler overhead time: 0.10712491115555167 Adapter cache time: 0.020115297753363848 Engine time: 0.10032002627849579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.746402495075017,
    "estimated_duration": 3600.01324626201,
    "input_throughput": 4775.51965061541,
    "output_throughput": 4151.8272232802165,
    "total_throughput": 8927.346873895627,
    "itl": 39.1116308756627,
    "ttft": 13562.88923169955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.239232144802833,
    "arrivals": 69453,
    "finished_requests": 69241,
    "scheduler_time": 44.471448256617826
}
#Debug simulation 
Total elapsed time: 5.746479255147278. Arrivals time: 0.17104510078206658 Scheduler time: 5.301079901866615 Scheduler overhead time: 0.10646754736080766 Adapter cache time: 0.020073588471859694 Engine time: 0.09978336421772838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.76322067482397,
    "estimated_duration": 3600.0349154528185,
    "input_throughput": 4775.503961421319,
    "output_throughput": 4151.924731574425,
    "total_throughput": 8927.428692995743,
    "itl": 39.09830053791786,
    "ttft": 13561.93038176105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5517356306267684,
    "arrivals": 69453,
    "finished_requests": 69242,
    "scheduler_time": 44.466288226047865
}
#Debug simulation 
Total elapsed time: 5.763312420807779. Arrivals time: 0.17539719957858324 Scheduler time: 5.310279014986008 Scheduler overhead time: 0.10751898773014545 Adapter cache time: 0.02018696628510952 Engine time: 0.10132541786879301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.7162522207945585,
    "estimated_duration": 3600.0370009965914,
    "input_throughput": 4775.501194915712,
    "output_throughput": 4151.922326315599,
    "total_throughput": 8927.42352123131,
    "itl": 39.1111983993971,
    "ttft": 13562.82480171466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.203607990425105,
    "arrivals": 69453,
    "finished_requests": 69242,
    "scheduler_time": 44.471507411319706
}
#Debug simulation 
Total elapsed time: 5.716372488997877. Arrivals time: 0.16886731563135982 Scheduler time: 5.270834852475673 Scheduler overhead time: 0.10786032304167747 Adapter cache time: 0.020088572055101395 Engine time: 0.10017933370545506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.421189034823328,
    "estimated_duration": 3599.963586344575,
    "input_throughput": 4682.764587937817,
    "output_throughput": 4131.704847354619,
    "total_throughput": 8814.469435292436,
    "itl": 39.03861370493786,
    "ttft": 14337.063329378414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.455237451358807,
    "arrivals": 68538,
    "finished_requests": 68296,
    "scheduler_time": 43.99736791954988
}
#Debug simulation 
Total elapsed time: 5.42127729812637. Arrivals time: 0.17044360609725118 Scheduler time: 4.972944817971438 Scheduler overhead time: 0.10738316178321838 Adapter cache time: 0.0206175004132092 Engine time: 0.10157633479684591 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.428311393596232,
    "estimated_duration": 3599.9904945754192,
    "input_throughput": 4682.729586481366,
    "output_throughput": 4131.6739648097955,
    "total_throughput": 8814.403551291161,
    "itl": 39.04909338385688,
    "ttft": 14337.744412585085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.0011036708531975,
    "arrivals": 68538,
    "finished_requests": 68296,
    "scheduler_time": 44.002105648499935
}
#Debug simulation 
Total elapsed time: 5.428385633975267. Arrivals time: 0.16977641778066754 Scheduler time: 4.980921418871731 Scheduler overhead time: 0.1068326523527503 Adapter cache time: 0.02071251580491662 Engine time: 0.1019293051213026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.423171482048929,
    "estimated_duration": 3599.968952890699,
    "input_throughput": 4682.757607246462,
    "output_throughput": 4131.698688139103,
    "total_throughput": 8814.456295385566,
    "itl": 39.052152806970575,
    "ttft": 14337.781912412098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.156953840474618,
    "arrivals": 68538,
    "finished_requests": 68296,
    "scheduler_time": 44.003127989876745
}
#Debug simulation 
Total elapsed time: 5.423285077326. Arrivals time: 0.17202838324010372 Scheduler time: 4.97652602288872 Scheduler overhead time: 0.10617127316072583 Adapter cache time: 0.02060456946492195 Engine time: 0.09998015174642205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.437568285968155,
    "estimated_duration": 3599.962496448396,
    "input_throughput": 4682.766005654596,
    "output_throughput": 4131.706098236908,
    "total_throughput": 8814.472103891503,
    "itl": 39.04046516448507,
    "ttft": 14337.22606593775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.549931854526499,
    "arrivals": 68538,
    "finished_requests": 68296,
    "scheduler_time": 43.9981275750547
}
#Debug simulation 
Total elapsed time: 5.43769765086472. Arrivals time: 0.17927746567875147 Scheduler time: 4.979531850200146 Scheduler overhead time: 0.10797156300395727 Adapter cache time: 0.020505086053162813 Engine time: 0.10151769686490297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.4831337430514395,
    "estimated_duration": 3599.9744927786724,
    "input_throughput": 4682.750401097473,
    "output_throughput": 4131.692329997422,
    "total_throughput": 8814.442731094896,
    "itl": 39.05097739197742,
    "ttft": 14337.654957178042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.089640758074841,
    "arrivals": 68538,
    "finished_requests": 68296,
    "scheduler_time": 44.002540523127884
}
#Debug simulation 
Total elapsed time: 5.483208861202002. Arrivals time: 0.16927796555683017 Scheduler time: 5.037024349439889 Scheduler overhead time: 0.10690265242010355 Adapter cache time: 0.02081954898312688 Engine time: 0.10075660282745957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.398830954916775,
    "estimated_duration": 3599.9738795749868,
    "input_throughput": 4682.75119873654,
    "output_throughput": 4131.693033771685,
    "total_throughput": 8814.444232508225,
    "itl": 39.03522928191785,
    "ttft": 14336.968673970068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.266734775970664,
    "arrivals": 68538,
    "finished_requests": 68296,
    "scheduler_time": 43.99602886823576
}
#Debug simulation 
Total elapsed time: 5.398921892978251. Arrivals time: 0.17036523343995214 Scheduler time: 4.9526735548861325 Scheduler overhead time: 0.10670238267630339 Adapter cache time: 0.020706331357359886 Engine time: 0.10059488099068403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.416624820791185,
    "estimated_duration": 3599.9891788342106,
    "input_throughput": 4682.731297947701,
    "output_throughput": 4131.675474873695,
    "total_throughput": 8814.406772821396,
    "itl": 39.050247428172796,
    "ttft": 14337.666694598089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.047388854045442,
    "arrivals": 68538,
    "finished_requests": 68296,
    "scheduler_time": 44.00250505561019
}
#Debug simulation 
Total elapsed time: 5.416738606989384. Arrivals time: 0.1679810700006783 Scheduler time: 4.972416344098747 Scheduler overhead time: 0.10629781894385815 Adapter cache time: 0.020730972290039062 Engine time: 0.10120776016265154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.1615705620497465,
    "estimated_duration": 3600.0028787811366,
    "input_throughput": 4707.142624768501,
    "output_throughput": 4062.4106403358,
    "total_throughput": 8769.553265104301,
    "itl": 38.44794173455694,
    "ttft": 11357.883532423553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 889,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.878431629403616,
    "arrivals": 68105,
    "finished_requests": 67909,
    "scheduler_time": 42.686133330287866
}
#Debug simulation 
Total elapsed time: 5.161658640019596. Arrivals time: 0.16918212128803134 Scheduler time: 4.711716608610004 Scheduler overhead time: 0.10714686987921596 Adapter cache time: 0.021167913917452097 Engine time: 0.10368968918919563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.151236480567604,
    "estimated_duration": 3600.0303884205914,
    "input_throughput": 4707.106655128666,
    "output_throughput": 4062.37959741672,
    "total_throughput": 8769.486252545385,
    "itl": 38.45954097713146,
    "ttft": 11411.090115741752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 893,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.546267972677025,
    "arrivals": 68105,
    "finished_requests": 67909,
    "scheduler_time": 42.69178766841916
}
#Debug simulation 
Total elapsed time: 5.151310878805816. Arrivals time: 0.16692410595715046 Scheduler time: 4.707152089104056 Scheduler overhead time: 0.10658876458182931 Adapter cache time: 0.021043404936790466 Engine time: 0.10122525412589312 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.155587535817176,
    "estimated_duration": 3600.0076795432446,
    "input_throughput": 4707.136347595239,
    "output_throughput": 4062.405222939837,
    "total_throughput": 8769.541570535075,
    "itl": 38.463372180952895,
    "ttft": 11358.43185307416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 893,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.726632506619231,
    "arrivals": 68105,
    "finished_requests": 67909,
    "scheduler_time": 42.692982742864686
}
#Debug simulation 
Total elapsed time: 5.15568281384185. Arrivals time: 0.1679855538532138 Scheduler time: 4.7095836154185236 Scheduler overhead time: 0.10651237983256578 Adapter cache time: 0.020995075348764658 Engine time: 0.10221360623836517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.220908549148589,
    "estimated_duration": 3600.028144842848,
    "input_throughput": 4707.1095886501,
    "output_throughput": 4062.382129137052,
    "total_throughput": 8769.491717787152,
    "itl": 38.450203263721626,
    "ttft": 11410.839934084526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 889,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.987655435162583,
    "arrivals": 68105,
    "finished_requests": 67909,
    "scheduler_time": 42.68734127605283
}
#Debug simulation 
Total elapsed time: 5.221030599437654. Arrivals time: 0.1720657185651362 Scheduler time: 4.767693218309432 Scheduler overhead time: 0.10820526350289583 Adapter cache time: 0.02125069545581937 Engine time: 0.10270308796316385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.170051094144583,
    "estimated_duration": 3600.03976034383,
    "input_throughput": 4707.094401196713,
    "output_throughput": 4062.3690218919237,
    "total_throughput": 8769.463423088637,
    "itl": 38.46111659659819,
    "ttft": 11411.047689922589,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 893,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.647099510799187,
    "arrivals": 68105,
    "finished_requests": 67909,
    "scheduler_time": 42.69276595713477
}
#Debug simulation 
Total elapsed time: 5.170125611126423. Arrivals time: 0.16801122203469276 Scheduler time: 4.722815586254001 Scheduler overhead time: 0.10746680619195104 Adapter cache time: 0.02123846486210823 Engine time: 0.10179408313706517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.173845046199858,
    "estimated_duration": 3600.0323848628523,
    "input_throughput": 4707.104044744744,
    "output_throughput": 4062.3773445741226,
    "total_throughput": 8769.481389318868,
    "itl": 38.44419859498359,
    "ttft": 11410.472317201918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 893,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.700841399929458,
    "arrivals": 68105,
    "finished_requests": 67909,
    "scheduler_time": 42.68512782870145
}
#Debug simulation 
Total elapsed time: 5.173934900201857. Arrivals time: 0.16875879326835275 Scheduler time: 4.726269442588091 Scheduler overhead time: 0.10702361445873976 Adapter cache time: 0.021120506804436445 Engine time: 0.10234043467789888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.171740269288421,
    "estimated_duration": 3600.043590711094,
    "input_throughput": 4707.089392951716,
    "output_throughput": 4062.364699620561,
    "total_throughput": 8769.454092572278,
    "itl": 38.460837271378665,
    "ttft": 11411.11178914543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 893,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.600498146060879,
    "arrivals": 68105,
    "finished_requests": 67909,
    "scheduler_time": 42.692466139971266
}
#Debug simulation 
Total elapsed time: 5.171854231040925. Arrivals time: 0.16832011798396707 Scheduler time: 4.724120525177568 Scheduler overhead time: 0.10762305837124586 Adapter cache time: 0.020896507427096367 Engine time: 0.10242624208331108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.132777926977724,
    "estimated_duration": 3599.9461946873694,
    "input_throughput": 4692.165406507392,
    "output_throughput": 4091.640597778204,
    "total_throughput": 8783.806004285596,
    "itl": 38.666655146442004,
    "ttft": 10501.605795672936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 849,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.61393526812561,
    "arrivals": 67887,
    "finished_requests": 67704,
    "scheduler_time": 43.14050803622321
}
#Debug simulation 
Total elapsed time: 5.132905984763056. Arrivals time: 0.16778700659051538 Scheduler time: 4.687395246233791 Scheduler overhead time: 0.10655553452670574 Adapter cache time: 0.020903228782117367 Engine time: 0.10198339121416211 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.113926827907562,
    "estimated_duration": 3599.947901521356,
    "input_throughput": 4692.163181823145,
    "output_throughput": 4091.6386578192314,
    "total_throughput": 8783.801839642376,
    "itl": 38.67917646960736,
    "ttft": 10502.094386837736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 849,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.266763672265236,
    "arrivals": 67887,
    "finished_requests": 67704,
    "scheduler_time": 43.14573818371321
}
#Debug simulation 
Total elapsed time: 5.113995892927051. Arrivals time: 0.16701681073755026 Scheduler time: 4.671644467394799 Scheduler overhead time: 0.10552642121911049 Adapter cache time: 0.02082775440067053 Engine time: 0.10072935139760375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.110749378334731,
    "estimated_duration": 3599.9679897283067,
    "input_throughput": 4692.136999050045,
    "output_throughput": 4091.615826037293,
    "total_throughput": 8783.752825087338,
    "itl": 38.68272670536301,
    "ttft": 10502.183172240444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 852,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.470700979661226,
    "arrivals": 67887,
    "finished_requests": 67704,
    "scheduler_time": 43.14763701041121
}
#Debug simulation 
Total elapsed time: 5.110837389249355. Arrivals time: 0.16785644087940454 Scheduler time: 4.668352570850402 Scheduler overhead time: 0.10532606672495604 Adapter cache time: 0.020874348003417253 Engine time: 0.10042157722637057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.136797537095845,
    "estimated_duration": 3599.9706335762617,
    "input_throughput": 4692.35772160144,
    "output_throughput": 4091.665321549341,
    "total_throughput": 8784.02304315078,
    "itl": 38.6683477940517,
    "ttft": 10395.71366978492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 850,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.722023866800578,
    "arrivals": 67887,
    "finished_requests": 67706,
    "scheduler_time": 43.141766319054284
}
#Debug simulation 
Total elapsed time: 5.136866707820445. Arrivals time: 0.16908743837848306 Scheduler time: 4.690851143095642 Scheduler overhead time: 0.1061223759315908 Adapter cache time: 0.020842317957431078 Engine time: 0.1017793184146285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.107145860325545,
    "estimated_duration": 3599.9504798362395,
    "input_throughput": 4692.159821256317,
    "output_throughput": 4091.63572735313,
    "total_throughput": 8783.795548609445,
    "itl": 38.681152857061285,
    "ttft": 10502.096199875732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 852,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.388268343368575,
    "arrivals": 67887,
    "finished_requests": 67704,
    "scheduler_time": 43.14676048518127
}
#Debug simulation 
Total elapsed time: 5.107234480325133. Arrivals time: 0.1676473282277584 Scheduler time: 4.662774592638016 Scheduler overhead time: 0.10589335486292839 Adapter cache time: 0.020736208651214838 Engine time: 0.10195794561877847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.114363058004528,
    "estimated_duration": 3599.9459812616565,
    "input_throughput": 4692.165684686218,
    "output_throughput": 4091.6408403544306,
    "total_throughput": 8783.806525040649,
    "itl": 38.66354843585107,
    "ttft": 10501.514457317158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 849,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.419948878544356,
    "arrivals": 67887,
    "finished_requests": 67704,
    "scheduler_time": 43.13898241793312
}
#Debug simulation 
Total elapsed time: 5.114435649011284. Arrivals time: 0.16721309861168265 Scheduler time: 4.672195308841765 Scheduler overhead time: 0.10508241085335612 Adapter cache time: 0.020529229659587145 Engine time: 0.10112515231594443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.113017541822046,
    "estimated_duration": 3599.9428508349333,
    "input_throughput": 4692.169764884559,
    "output_throughput": 4091.64439835031,
    "total_throughput": 8783.814163234869,
    "itl": 38.68016469180351,
    "ttft": 10501.92539636504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 851,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.336076311375976,
    "arrivals": 67887,
    "finished_requests": 67704,
    "scheduler_time": 43.146216571368605
}
#Debug simulation 
Total elapsed time: 5.113187989685684. Arrivals time: 0.16737485583871603 Scheduler time: 4.670685375109315 Scheduler overhead time: 0.10594662791118026 Adapter cache time: 0.020744207315146923 Engine time: 0.10043212585151196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.041874480899423,
    "estimated_duration": 3600.0167577170005,
    "input_throughput": 4680.055436932067,
    "output_throughput": 4074.4140894782627,
    "total_throughput": 8754.46952641033,
    "itl": 38.575900241165925,
    "ttft": 10175.244094840684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.481687087486607,
    "arrivals": 67772,
    "finished_requests": 67592,
    "scheduler_time": 42.80381595049372
}
#Debug simulation 
Total elapsed time: 5.041949474718422. Arrivals time: 0.16674489760771394 Scheduler time: 4.600309266708791 Scheduler overhead time: 0.10543928667902946 Adapter cache time: 0.02061427617445588 Engine time: 0.10083741741254926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.0568873062729836,
    "estimated_duration": 3600.0384383059177,
    "input_throughput": 4680.128917714702,
    "output_throughput": 4074.390107596282,
    "total_throughput": 8754.519025310985,
    "itl": 38.58838493431386,
    "ttft": 10173.87342128457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.193499724534354,
    "arrivals": 67772,
    "finished_requests": 67593,
    "scheduler_time": 42.809512914745866
}
#Debug simulation 
Total elapsed time: 5.056986134964973. Arrivals time: 0.1673116567544639 Scheduler time: 4.6124991569668055 Scheduler overhead time: 0.10637153265997767 Adapter cache time: 0.020751687698066235 Engine time: 0.10168002685531974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.054687303956598,
    "estimated_duration": 3600.008463825401,
    "input_throughput": 4680.278440816791,
    "output_throughput": 4074.6173647639025,
    "total_throughput": 8754.895805580694,
    "itl": 38.593295524644255,
    "ttft": 10169.557844365298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.357657077838706,
    "arrivals": 67772,
    "finished_requests": 67592,
    "scheduler_time": 42.81148063293118
}
#Debug simulation 
Total elapsed time: 5.054809964261949. Arrivals time: 0.17376027023419738 Scheduler time: 4.602491730824113 Scheduler overhead time: 0.10640704305842519 Adapter cache time: 0.020789131987839937 Engine time: 0.10303241293877363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.0379923661239445,
    "estimated_duration": 3600.0206265211204,
    "input_throughput": 4680.152073540114,
    "output_throughput": 4074.410266414052,
    "total_throughput": 8754.562339954166,
    "itl": 38.578173475044935,
    "ttft": 10175.183260811476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.604620178728353,
    "arrivals": 67772,
    "finished_requests": 67593,
    "scheduler_time": 42.80482103964544
}
#Debug simulation 
Total elapsed time: 5.038076134864241. Arrivals time: 0.16748879896476865 Scheduler time: 4.596097289584577 Scheduler overhead time: 0.1053744088858366 Adapter cache time: 0.020650161895900965 Engine time: 0.10054826037958264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.053471556864679,
    "estimated_duration": 3600.041095017778,
    "input_throughput": 4680.337683733244,
    "output_throughput": 4074.580987506078,
    "total_throughput": 8754.918671239322,
    "itl": 38.59262840621964,
    "ttft": 10170.44502259052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 828,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.280193757270502,
    "arrivals": 67772,
    "finished_requests": 67593,
    "scheduler_time": 42.81136367937553
}
#Debug simulation 
Total elapsed time: 5.0535596227273345. Arrivals time: 0.16796348430216312 Scheduler time: 4.609483242500573 Scheduler overhead time: 0.10511514823883772 Adapter cache time: 0.02077594818547368 Engine time: 0.10198063217103481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.038641054183245,
    "estimated_duration": 3600.0042626787217,
    "input_throughput": 4680.07140287756,
    "output_throughput": 4074.4098978054512,
    "total_throughput": 8754.481300683012,
    "itl": 38.57277885616882,
    "ttft": 10226.67648116851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.305038301614087,
    "arrivals": 67772,
    "finished_requests": 67591,
    "scheduler_time": 42.802058891479014
}
#Debug simulation 
Total elapsed time: 5.038716262206435. Arrivals time: 0.1689267219044268 Scheduler time: 4.594715970568359 Scheduler overhead time: 0.10550873726606369 Adapter cache time: 0.020653407089412212 Engine time: 0.10082331206649542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.03734204499051,
    "estimated_duration": 3600.027846062019,
    "input_throughput": 4680.142687904571,
    "output_throughput": 4074.4020955407104,
    "total_throughput": 8754.54478344528,
    "itl": 38.58942983803312,
    "ttft": 10175.468336734912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.236542046722064,
    "arrivals": 67772,
    "finished_requests": 67593,
    "scheduler_time": 42.80991527445189
}
#Debug simulation 
Total elapsed time: 5.037481708917767. Arrivals time: 0.16769640427082777 Scheduler time: 4.593967444263399 Scheduler overhead time: 0.1057986463420093 Adapter cache time: 0.02067658631131053 Engine time: 0.10113386111333966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.930499364156276,
    "estimated_duration": 3599.9724116893453,
    "input_throughput": 4589.154057502161,
    "output_throughput": 3990.6103039435466,
    "total_throughput": 8579.764361445708,
    "itl": 37.96730195323885,
    "ttft": 8658.156593029877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.483720787991976,
    "arrivals": 66605,
    "finished_requests": 66456,
    "scheduler_time": 41.306380765910205
}
#Debug simulation 
Total elapsed time: 4.930569877848029. Arrivals time: 0.16290351608768106 Scheduler time: 4.489040911663324 Scheduler overhead time: 0.10572944302111864 Adapter cache time: 0.022775927558541298 Engine time: 0.10164977563545108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.944024690892547,
    "estimated_duration": 3599.943655881359,
    "input_throughput": 4589.028490212694,
    "output_throughput": 3990.6294023601386,
    "total_throughput": 8579.657892572834,
    "itl": 37.983163873936526,
    "ttft": 8714.118075258875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.376942315008453,
    "arrivals": 66605,
    "finished_requests": 66455,
    "scheduler_time": 41.31360315294586
}
#Debug simulation 
Total elapsed time: 4.944124135188758. Arrivals time: 0.16330833733081818 Scheduler time: 4.4992862194776535 Scheduler overhead time: 0.10669151600450277 Adapter cache time: 0.02284226892516017 Engine time: 0.10338677372783422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.949515788815916,
    "estimated_duration": 3599.939261441158,
    "input_throughput": 4589.0340920325625,
    "output_throughput": 3990.634273715181,
    "total_throughput": 8579.668365747744,
    "itl": 37.98465833219275,
    "ttft": 8713.741470680356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.544345848606849,
    "arrivals": 66605,
    "finished_requests": 66455,
    "scheduler_time": 41.314813553048374
}
#Debug simulation 
Total elapsed time: 4.949607521761209. Arrivals time: 0.16544310003519058 Scheduler time: 4.503125654533505 Scheduler overhead time: 0.106511608697474 Adapter cache time: 0.02291700290516019 Engine time: 0.10315144807100296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.919434547889978,
    "estimated_duration": 3599.953832626638,
    "input_throughput": 4589.015517442433,
    "output_throughput": 3990.618121210208,
    "total_throughput": 8579.63363865264,
    "itl": 37.96861873841419,
    "ttft": 8712.949720646759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.593164126696093,
    "arrivals": 66605,
    "finished_requests": 66455,
    "scheduler_time": 41.30720992419266
}
#Debug simulation 
Total elapsed time: 4.9195336741395295. Arrivals time: 0.16391664138063788 Scheduler time: 4.475705210585147 Scheduler overhead time: 0.10662853252142668 Adapter cache time: 0.022862679790705442 Engine time: 0.101749655790627 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_32_slots_16_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.96792815765366,
    "estimated_duration": 3599.9427562333085,
    "input_throughput": 4589.029637039412,
    "output_throughput": 3990.630399643208,
    "total_throughput": 8579.66003668262,
    "itl": 37.98431468432196,
    "ttft": 8713.029332368658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.47241683676834,
    "arrivals": 66605,
    "finished_requests": 66455,
    "scheduler_time": 41.31417536016072
}
#Debug simulation 
Total elapsed time: 4.968021588865668. Arrivals time: 0.16592415096238256 Scheduler time: 4.52233633864671 Scheduler overhead time: 0.10617653606459498 Adapter cache time: 0.02270588232204318 Engine time: 0.10232168855145574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_32_slots_16_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_32_slots_16_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.9505755878053606,
    "estimated_duration": 3599.960634756313,
    "input_throughput": 4589.0068464924425,
    "output_throughput": 3990.610580932772,
    "total_throughput": 8579.617427425213,
    "itl": 37.96224531372038,
    "ttft": 8712.025647402079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.190570566751978,
    "arrivals": 66605,
    "finished_requests": 66455,
    "scheduler_time": 41.3038335382217
}
#Debug simulation 
Total elapsed time: 4.950647271703929. Arrivals time: 0.1640098374336958 Scheduler time: 4.504675124306232 Scheduler overhead time: 0.10692068561911583 Adapter cache time: 0.022820104379206896 Engine time: 0.10389010002836585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_32_slots_16_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_32_slots_16_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.922448489814997,
    "estimated_duration": 3599.9572358348573,
    "input_throughput": 4589.011179231086,
    "output_throughput": 3990.6143486919523,
    "total_throughput": 8579.625527923039,
    "itl": 37.98598959581569,
    "ttft": 8713.915809261825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.449003257974933,
    "arrivals": 66605,
    "finished_requests": 66455,
    "scheduler_time": 41.314879730212965
}
#Debug simulation 
Total elapsed time: 4.922598049975932. Arrivals time: 0.165347658097744 Scheduler time: 4.4783410439267755 Scheduler overhead time: 0.10612783627584577 Adapter cache time: 0.02268135966733098 Engine time: 0.10159370908513665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.793676188215613,
    "estimated_duration": 3600.0204952694917,
    "input_throughput": 4543.143301959358,
    "output_throughput": 3967.903243542696,
    "total_throughput": 8511.046545502053,
    "itl": 37.7431003923916,
    "ttft": 8776.491402771506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.230923008602343,
    "arrivals": 66148,
    "finished_requests": 65990,
    "scheduler_time": 40.79734458726259
}
#Debug simulation 
Total elapsed time: 4.793768846429884. Arrivals time: 0.1645077452994883 Scheduler time: 4.350849927403033 Scheduler overhead time: 0.10500935977324843 Adapter cache time: 0.023213470354676247 Engine time: 0.10201872838661075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.76397472107783,
    "estimated_duration": 3600.040161337202,
    "input_throughput": 4543.118483968505,
    "output_throughput": 3967.8815679362147,
    "total_throughput": 8511.000051904719,
    "itl": 37.76409015660714,
    "ttft": 8831.288225741257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.296009943429281,
    "arrivals": 66148,
    "finished_requests": 65990,
    "scheduler_time": 40.806425472021594
}
#Debug simulation 
Total elapsed time: 4.764046135824174. Arrivals time: 0.15970754623413086 Scheduler time: 4.327602934092283 Scheduler overhead time: 0.10374169144779444 Adapter cache time: 0.023260952904820442 Engine time: 0.10184422694146633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.755373433697969,
    "estimated_duration": 3600.0300262809956,
    "input_throughput": 4543.131274073268,
    "output_throughput": 3967.8927385937973,
    "total_throughput": 8511.024012667065,
    "itl": 37.767315024024,
    "ttft": 8777.018692593503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.59238467819512,
    "arrivals": 66148,
    "finished_requests": 65990,
    "scheduler_time": 40.8087728191098
}
#Debug simulation 
Total elapsed time: 4.755460706073791. Arrivals time: 0.1607045065611601 Scheduler time: 4.3177836812101305 Scheduler overhead time: 0.1047379495576024 Adapter cache time: 0.023329290095716715 Engine time: 0.10107337031513453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.779212875291705,
    "estimated_duration": 3600.0375061024324,
    "input_throughput": 4543.12183477975,
    "output_throughput": 3967.8844944771417,
    "total_throughput": 8511.006329256892,
    "itl": 37.7474317125906,
    "ttft": 8830.776543280497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.428371835108907,
    "arrivals": 66148,
    "finished_requests": 65990,
    "scheduler_time": 40.79910819730559
}
#Debug simulation 
Total elapsed time: 4.779305061325431. Arrivals time: 0.16043231403455138 Scheduler time: 4.339117481838912 Scheduler overhead time: 0.10458151483908296 Adapter cache time: 0.02336777886375785 Engine time: 0.10378007777035236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.756458780262619,
    "estimated_duration": 3600.0067012431086,
    "input_throughput": 4543.160709771,
    "output_throughput": 3967.9184472260695,
    "total_throughput": 8511.07915699707,
    "itl": 37.76717114556355,
    "ttft": 8668.3406374138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.462936442810939,
    "arrivals": 66148,
    "finished_requests": 65990,
    "scheduler_time": 40.80743302250365
}
#Debug simulation 
Total elapsed time: 4.756547609344125. Arrivals time: 0.16093781543895602 Scheduler time: 4.321344323456287 Scheduler overhead time: 0.10356894368305802 Adapter cache time: 0.02307233726605773 Engine time: 0.09974467707797885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.761367159895599,
    "estimated_duration": 3600.0281122190663,
    "input_throughput": 4543.13368956402,
    "output_throughput": 3967.8948482418873,
    "total_throughput": 8511.028537805907,
    "itl": 37.737980134623335,
    "ttft": 8776.282427576412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.911953633036545,
    "arrivals": 66148,
    "finished_requests": 65990,
    "scheduler_time": 40.79467210824245
}
#Debug simulation 
Total elapsed time: 4.7614381411112845. Arrivals time: 0.16015946306288242 Scheduler time: 4.324397878721356 Scheduler overhead time: 0.10431450000032783 Adapter cache time: 0.023242393042892218 Engine time: 0.10157786728814244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.756607921328396,
    "estimated_duration": 3600.0381870732936,
    "input_throughput": 4543.120975418425,
    "output_throughput": 3967.8837439257363,
    "total_throughput": 8511.00471934416,
    "itl": 37.764939825621354,
    "ttft": 8831.199892593844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.385888853110277,
    "arrivals": 66148,
    "finished_requests": 65990,
    "scheduler_time": 40.80712211427962
}
#Debug simulation 
Total elapsed time: 4.756752876099199. Arrivals time: 0.16191633651033044 Scheduler time: 4.318026928696781 Scheduler overhead time: 0.10417675226926804 Adapter cache time: 0.023349920753389597 Engine time: 0.1014695311896503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.709106068126857,
    "estimated_duration": 3600.000784387879,
    "input_throughput": 4520.619292800284,
    "output_throughput": 3960.7408036785896,
    "total_throughput": 8481.360096478873,
    "itl": 37.49264882336879,
    "ttft": 9151.829546846766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.166325154458368,
    "arrivals": 65922,
    "finished_requests": 65756,
    "scheduler_time": 40.563723489517784
}
#Debug simulation 
Total elapsed time: 4.709189121145755. Arrivals time: 0.15721645392477512 Scheduler time: 4.280132474377751 Scheduler overhead time: 0.10280353995040059 Adapter cache time: 0.022488894872367382 Engine time: 0.09923744853585958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.693454937078059,
    "estimated_duration": 3600.0342922820705,
    "input_throughput": 4520.577216414159,
    "output_throughput": 3960.703938450929,
    "total_throughput": 8481.281154865088,
    "itl": 37.510824213347036,
    "ttft": 9206.559485131334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.150757073191869,
    "arrivals": 65922,
    "finished_requests": 65756,
    "scheduler_time": 40.57245536913778
}
#Debug simulation 
Total elapsed time: 4.693543205969036. Arrivals time: 0.15843739826232195 Scheduler time: 4.259376617148519 Scheduler overhead time: 0.10323086613789201 Adapter cache time: 0.022529507987201214 Engine time: 0.1021289424970746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.686562140937895,
    "estimated_duration": 3600.011416314273,
    "input_throughput": 4520.605942039406,
    "output_throughput": 3960.7291064088254,
    "total_throughput": 8481.33504844823,
    "itl": 37.516255016635846,
    "ttft": 9152.059808839931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.41610684264446,
    "arrivals": 65922,
    "finished_requests": 65756,
    "scheduler_time": 40.57442839047852
}
#Debug simulation 
Total elapsed time: 4.686633015982807. Arrivals time: 0.157009182497859 Scheduler time: 4.256812109146267 Scheduler overhead time: 0.10300124203786254 Adapter cache time: 0.02254310855641961 Engine time: 0.0999972983263433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.700097133405507,
    "estimated_duration": 3600.0220665441066,
    "input_throughput": 4520.592568373529,
    "output_throughput": 3960.7173890708445,
    "total_throughput": 8481.309957444373,
    "itl": 37.49525230682308,
    "ttft": 9151.875418306421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.335314177931282,
    "arrivals": 65922,
    "finished_requests": 65756,
    "scheduler_time": 40.565353252807014
}
#Debug simulation 
Total elapsed time: 4.700186067260802. Arrivals time: 0.1609204956330359 Scheduler time: 4.266482002101839 Scheduler overhead time: 0.10256716050207615 Adapter cache time: 0.022464297711849213 Engine time: 0.10036892397329211 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.681476613972336,
    "estimated_duration": 3600.011158711655,
    "input_throughput": 4520.60626551616,
    "output_throughput": 3960.729389822999,
    "total_throughput": 8481.335655339159,
    "itl": 37.51395579687579,
    "ttft": 9151.969884997396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.301362430285616,
    "arrivals": 65922,
    "finished_requests": 65756,
    "scheduler_time": 40.573450177860934
}
#Debug simulation 
Total elapsed time: 4.681572780944407. Arrivals time: 0.1567489355802536 Scheduler time: 4.253181580454111 Scheduler overhead time: 0.10269683040678501 Adapter cache time: 0.022509445436298847 Engine time: 0.09896507253870368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.676228106021881,
    "estimated_duration": 3600.026514275251,
    "input_throughput": 4520.586983309008,
    "output_throughput": 3960.712495716305,
    "total_throughput": 8481.299479025312,
    "itl": 37.487463867953714,
    "ttft": 9151.734448154139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.877758440663998,
    "arrivals": 65922,
    "finished_requests": 65756,
    "scheduler_time": 40.561629727311555
}
#Debug simulation 
Total elapsed time: 4.676303792279214. Arrivals time: 0.1564465737901628 Scheduler time: 4.248145062476397 Scheduler overhead time: 0.10265551041811705 Adapter cache time: 0.022442131768912077 Engine time: 0.0992202372290194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.708787020295858,
    "estimated_duration": 3600.041265249181,
    "input_throughput": 4520.5684604489,
    "output_throughput": 3960.69626691156,
    "total_throughput": 8481.26472736046,
    "itl": 37.51222108713532,
    "ttft": 9206.468867165353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.226387508865402,
    "arrivals": 65922,
    "finished_requests": 65756,
    "scheduler_time": 40.57313490948457
}
#Debug simulation 
Total elapsed time: 4.708930829074234. Arrivals time: 0.1652559731155634 Scheduler time: 4.271408182103187 Scheduler overhead time: 0.10282413987442851 Adapter cache time: 0.02257272507995367 Engine time: 0.09939563786610961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.656095779966563,
    "estimated_duration": 3599.940929827853,
    "input_throughput": 4534.406068929744,
    "output_throughput": 3944.1680507460374,
    "total_throughput": 8478.57411967578,
    "itl": 37.18650093882336,
    "ttft": 8700.790950974737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.505084251263353,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.18540980907531
}
#Debug simulation 
Total elapsed time: 4.656168072018772. Arrivals time: 0.15596028231084347 Scheduler time: 4.2280087121762335 Scheduler overhead time: 0.10244414117187262 Adapter cache time: 0.0221946625970304 Engine time: 0.10023159440606833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.65236558765173,
    "estimated_duration": 3599.9582098489414,
    "input_throughput": 4534.38430350139,
    "output_throughput": 3944.1491184965166,
    "total_throughput": 8478.533421997905,
    "itl": 37.20274598642645,
    "ttft": 8701.028025412448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.440177488522595,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.19349821060591
}
#Debug simulation 
Total elapsed time: 4.652457887772471. Arrivals time: 0.15756527753546834 Scheduler time: 4.223114724270999 Scheduler overhead time: 0.10194394458085299 Adapter cache time: 0.02219699928537011 Engine time: 0.10049838107079268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.647890972904861,
    "estimated_duration": 3599.966844894382,
    "input_throughput": 4534.3734271194135,
    "output_throughput": 3944.1396578797026,
    "total_throughput": 8478.513084999115,
    "itl": 37.20703114173877,
    "ttft": 8701.108227524193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.698598285592157,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.195806019697734
}
#Debug simulation 
Total elapsed time: 4.6479598130099475. Arrivals time: 0.1539918784983456 Scheduler time: 4.22229117481038 Scheduler overhead time: 0.10313393361866474 Adapter cache time: 0.02208143100142479 Engine time: 0.0990517814643681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.657943695783615,
    "estimated_duration": 3599.95376891118,
    "input_throughput": 4534.389897161689,
    "output_throughput": 3944.153984036988,
    "total_throughput": 8478.543881198677,
    "itl": 37.18852187494517,
    "ttft": 8700.924431304637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.629456440107878,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.18659453331478
}
#Debug simulation 
Total elapsed time: 4.658034958876669. Arrivals time: 0.1569115505553782 Scheduler time: 4.229139047674835 Scheduler overhead time: 0.10320084448903799 Adapter cache time: 0.022132882848381996 Engine time: 0.0992509569041431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.665378202218562,
    "estimated_duration": 3599.939150019628,
    "input_throughput": 4534.408310737974,
    "output_throughput": 3944.170000740869,
    "total_throughput": 8478.578311478843,
    "itl": 37.20555478095498,
    "ttft": 8700.97497025265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.569772898252129,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.19438315759027
}
#Debug simulation 
Total elapsed time: 4.6654701279476285. Arrivals time: 0.15602634847164154 Scheduler time: 4.237906969618052 Scheduler overhead time: 0.10265749413520098 Adapter cache time: 0.022140401415526867 Engine time: 0.09936445532366633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.656112681608647,
    "estimated_duration": 3599.9743431483716,
    "input_throughput": 4534.363982639981,
    "output_throughput": 3944.131442776453,
    "total_throughput": 8478.495425416433,
    "itl": 37.18138524434961,
    "ttft": 8700.902367546467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1132,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.226598504725807,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.18339759914986
}
#Debug simulation 
Total elapsed time: 4.656233733985573. Arrivals time: 0.1572243352420628 Scheduler time: 4.227458817418665 Scheduler overhead time: 0.10242396919056773 Adapter cache time: 0.021963683888316154 Engine time: 0.09948649350553751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.657176950015128,
    "estimated_duration": 3599.9647280752065,
    "input_throughput": 4534.376093381265,
    "output_throughput": 3944.141977077553,
    "total_throughput": 8478.518070458817,
    "itl": 37.20425221708251,
    "ttft": 8701.018405101555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.511572971623401,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.19419337139522
}
#Debug simulation 
Total elapsed time: 4.657471000216901. Arrivals time: 0.15566869173198938 Scheduler time: 4.2308836481533945 Scheduler overhead time: 0.10232170671224594 Adapter cache time: 0.021961634047329426 Engine time: 0.09919194411486387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.56194007396698,
    "estimated_duration": 3599.9886196194607,
    "input_throughput": 4449.722122091956,
    "output_throughput": 3920.1896147915563,
    "total_throughput": 8369.911736883512,
    "itl": 36.5374115311952,
    "ttft": 9741.3538191945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.850455757100288,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.54341817849427
}
#Debug simulation 
Total elapsed time: 4.562025641091168. Arrivals time: 0.15248145861551166 Scheduler time: 4.1397109823301435 Scheduler overhead time: 0.10219575325027108 Adapter cache time: 0.02153215277940035 Engine time: 0.09882385469973087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.603636681102216,
    "estimated_duration": 3599.989066125657,
    "input_throughput": 4449.721570193476,
    "output_throughput": 3920.1891285709257,
    "total_throughput": 8369.9106987644,
    "itl": 36.55044876597797,
    "ttft": 9741.357291456834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.578555670883517,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.54961445460535
}
#Debug simulation 
Total elapsed time: 4.603707409929484. Arrivals time: 0.15405725315213203 Scheduler time: 4.176883751992136 Scheduler overhead time: 0.10326342890039086 Adapter cache time: 0.021433486603200436 Engine time: 0.10054615465924144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.5892916880548,
    "estimated_duration": 3599.9891840942882,
    "input_throughput": 4449.721424379825,
    "output_throughput": 3920.1890001096103,
    "total_throughput": 8369.910424489435,
    "itl": 36.55370199559817,
    "ttft": 9741.43757656472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.784109034561569,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.55140158844397
}
#Debug simulation 
Total elapsed time: 4.589383397251368. Arrivals time: 0.15394555777311325 Scheduler time: 4.164933226071298 Scheduler overhead time: 0.10182394878938794 Adapter cache time: 0.021568492520600557 Engine time: 0.0998383928090334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.592022996861488,
    "estimated_duration": 3600.002550413187,
    "input_throughput": 4449.704903170538,
    "output_throughput": 3920.174444982056,
    "total_throughput": 8369.879348152594,
    "itl": 36.53948384862579,
    "ttft": 9741.33339210257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.966350221652521,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.54461824378127
}
#Debug simulation 
Total elapsed time: 4.592094874940813. Arrivals time: 0.15061649214476347 Scheduler time: 4.172795470803976 Scheduler overhead time: 0.10155503964051604 Adapter cache time: 0.02141291182488203 Engine time: 0.09851775411516428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.572601129766554,
    "estimated_duration": 3600.003256885403,
    "input_throughput": 4449.704029951083,
    "output_throughput": 3920.173675678772,
    "total_throughput": 8369.877705629855,
    "itl": 36.55276511977681,
    "ttft": 9741.432869604178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.700639315871539,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.55085943409203
}
#Debug simulation 
Total elapsed time: 4.5726904780603945. Arrivals time: 0.15016662422567606 Scheduler time: 4.151629420928657 Scheduler overhead time: 0.10202557453885674 Adapter cache time: 0.021499196533113718 Engine time: 0.1000987901352346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.568236252292991,
    "estimated_duration": 3599.979356986027,
    "input_throughput": 4449.7335710867455,
    "output_throughput": 3920.199701315892,
    "total_throughput": 8369.933272402637,
    "itl": 36.533063420970166,
    "ttft": 9741.324516386048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.613742094431039,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.54129792487725
}
#Debug simulation 
Total elapsed time: 4.568309475202113. Arrivals time: 0.14922471158206463 Scheduler time: 4.150378464721143 Scheduler overhead time: 0.10137179773300886 Adapter cache time: 0.02136390609666705 Engine time: 0.0987551654689014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.592389051802456,
    "estimated_duration": 3599.98787908336,
    "input_throughput": 4449.72303742278,
    "output_throughput": 3920.1904211948076,
    "total_throughput": 8369.913458617588,
    "itl": 36.551571287860334,
    "ttft": 9741.429758512313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.648031553011405,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.55019542525459
}
#Debug simulation 
Total elapsed time: 4.592528861947358. Arrivals time: 0.1537558501586318 Scheduler time: 4.168615568429232 Scheduler overhead time: 0.10160643747076392 Adapter cache time: 0.02138252602890134 Engine time: 0.0998795717023313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.57391422893852,
    "estimated_duration": 3600.003615847084,
    "input_throughput": 4458.312744284129,
    "output_throughput": 3905.742188175972,
    "total_throughput": 8364.054932460102,
    "itl": 36.20425547679513,
    "ttft": 8100.813671806937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.792470311988264,
    "arrivals": 64957,
    "finished_requests": 64811,
    "scheduler_time": 39.160624091406746
}
#Debug simulation 
Total elapsed time: 4.5739881712943316. Arrivals time: 0.14932577777653933 Scheduler time: 4.155781036708504 Scheduler overhead time: 0.102626942563802 Adapter cache time: 0.02044486254453659 Engine time: 0.0982475751079619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.541270796675235,
    "estimated_duration": 3600.034208816887,
    "input_throughput": 4458.31985726455,
    "output_throughput": 3905.7998297802296,
    "total_throughput": 8364.11968704478,
    "itl": 36.21683764023527,
    "ttft": 8100.809664972549,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 877,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.496877634613788,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.1668471383236
}
#Debug simulation 
Total elapsed time: 4.541370756924152. Arrivals time: 0.14935567695647478 Scheduler time: 4.123763571958989 Scheduler overhead time: 0.10195888392627239 Adapter cache time: 0.020457657985389233 Engine time: 0.09845458529889584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.560791070107371,
    "estimated_duration": 3600.000392349099,
    "input_throughput": 4458.31284744027,
    "output_throughput": 3905.6245743421437,
    "total_throughput": 8363.937421782413,
    "itl": 36.2205672110916,
    "ttft": 8156.328886300072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.681614804575253,
    "arrivals": 64957,
    "finished_requests": 64810,
    "scheduler_time": 39.168080922860305
}
#Debug simulation 
Total elapsed time: 4.560873794835061. Arrivals time: 0.14855941757559776 Scheduler time: 4.1454129833728075 Scheduler overhead time: 0.10185101721435785 Adapter cache time: 0.020288049709051847 Engine time: 0.09775943774729967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.509190388955176,
    "estimated_duration": 3600.001614556525,
    "input_throughput": 4458.31133383454,
    "output_throughput": 3905.6232483751387,
    "total_throughput": 8363.934582209678,
    "itl": 36.206383142367145,
    "ttft": 8156.301767317072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 879,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.9252044467301905,
    "arrivals": 64957,
    "finished_requests": 64810,
    "scheduler_time": 39.16169640340411
}
#Debug simulation 
Total elapsed time: 4.5092628500424325. Arrivals time: 0.14668639609590173 Scheduler time: 4.097864055074751 Scheduler overhead time: 0.10095499129965901 Adapter cache time: 0.020208004396408796 Engine time: 0.09679332561790943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.524233477190137,
    "estimated_duration": 3600.027020213969,
    "input_throughput": 4458.283760060796,
    "output_throughput": 3905.7167963045727,
    "total_throughput": 8364.000556365369,
    "itl": 36.218894187635065,
    "ttft": 8156.120000310971,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 877,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.602287429417517,
    "arrivals": 64957,
    "finished_requests": 64811,
    "scheduler_time": 39.16772022608077
}
#Debug simulation 
Total elapsed time: 4.524327132850885. Arrivals time: 0.148037891369313 Scheduler time: 4.107901808805764 Scheduler overhead time: 0.10195648344233632 Adapter cache time: 0.020463844761252403 Engine time: 0.09879759466275573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.525996755808592,
    "estimated_duration": 3600.037407834736,
    "input_throughput": 4458.31589557105,
    "output_throughput": 3905.7963590598024,
    "total_throughput": 8364.112254630852,
    "itl": 36.201338384931354,
    "ttft": 8100.683234462009,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 878,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.605082585820901,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.159382589780634
}
#Debug simulation 
Total elapsed time: 4.526070267893374. Arrivals time: 0.14801989356055856 Scheduler time: 4.108807206619531 Scheduler overhead time: 0.10152864968404174 Adapter cache time: 0.020408338867127895 Engine time: 0.10030581196770072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.51908053457737,
    "estimated_duration": 3600.0193043865033,
    "input_throughput": 4458.293315384082,
    "output_throughput": 3905.725167325498,
    "total_throughput": 8364.01848270958,
    "itl": 36.21789137595867,
    "ttft": 8156.201575255532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 877,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.555686064679209,
    "arrivals": 64957,
    "finished_requests": 64811,
    "scheduler_time": 39.16722015159223
}
#Debug simulation 
Total elapsed time: 4.519198692869395. Arrivals time: 0.14923719828948379 Scheduler time: 4.102960950229317 Scheduler overhead time: 0.10180595749989152 Adapter cache time: 0.020454065408557653 Engine time: 0.09746943740174174 
