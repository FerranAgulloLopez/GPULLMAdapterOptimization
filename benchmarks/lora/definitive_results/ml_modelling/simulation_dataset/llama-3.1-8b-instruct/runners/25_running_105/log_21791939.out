INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 75.29503847798333,
    "estimated_duration": 3600.035279337696,
    "input_throughput": 7761.753658464325,
    "output_throughput": 6748.611642625245,
    "total_throughput": 14510.365301089569,
    "itl": 89.90330625129698,
    "ttft": 1553793.7216021344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.575279585779644,
    "arrivals": 291201,
    "finished_requests": 112793,
    "scheduler_time": 230.14811529184425
}
#Debug simulation 
Total elapsed time: 75.29526810604148. Arrivals time: 0.4719410331454128 Scheduler time: 74.61436549481004 Scheduler overhead time: 0.0801810477860272 Adapter cache time: 0.014828597661107779 Engine time: 0.08184025587979704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 75.51712164399214,
    "estimated_duration": 3600.045379206584,
    "input_throughput": 7720.7596216817055,
    "output_throughput": 6720.606951163553,
    "total_throughput": 14441.36657284526,
    "itl": 88.540014773926,
    "ttft": 1557121.9630518747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7894153716601433,
    "arrivals": 291201,
    "finished_requests": 112270,
    "scheduler_time": 231.36075307789608
}
#Debug simulation 
Total elapsed time: 75.5173154900549. Arrivals time: 0.47717878059484065 Scheduler time: 74.82834306615405 Scheduler overhead time: 0.08233407861553133 Adapter cache time: 0.015619418234564364 Engine time: 0.08115681493654847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 87.87810535402969,
    "estimated_duration": 3600.0352859534255,
    "input_throughput": 7664.087101494302,
    "output_throughput": 6655.9819825916,
    "total_throughput": 14320.069084085902,
    "itl": 86.2884483839084,
    "ttft": 1546890.8631624081,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6513541916711257,
    "arrivals": 291201,
    "finished_requests": 111437,
    "scheduler_time": 233.5628075697978
}
#Debug simulation 
Total elapsed time: 87.8782981429249. Arrivals time: 0.49740665371064097 Scheduler time: 87.15674474602565 Scheduler overhead time: 0.08731929329223931 Adapter cache time: 0.016203839797526598 Engine time: 0.08702955942135304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 84.4460525639588,
    "estimated_duration": 3600.0701642404083,
    "input_throughput": 7715.902394324844,
    "output_throughput": 6700.905510015234,
    "total_throughput": 14416.807904340078,
    "itl": 88.31041219152006,
    "ttft": 1546612.5494043645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6062081139162183,
    "arrivals": 291201,
    "finished_requests": 112082,
    "scheduler_time": 231.79077287938392
}
#Debug simulation 
Total elapsed time: 84.44622891105246. Arrivals time: 0.4815655016573146 Scheduler time: 83.7476464585634 Scheduler overhead time: 0.08490514568984509 Adapter cache time: 0.015501812798902392 Engine time: 0.08314575185067952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 88.14810526696965,
    "estimated_duration": 3600.0248011513077,
    "input_throughput": 7631.745478868233,
    "output_throughput": 6639.5309255524735,
    "total_throughput": 14271.276404420705,
    "itl": 86.2005574634274,
    "ttft": 1551530.8157270162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6697831308469178,
    "arrivals": 291201,
    "finished_requests": 110977,
    "scheduler_time": 234.0190190391505
}
#Debug simulation 
Total elapsed time: 88.14829564304091. Arrivals time: 0.5011821624357253 Scheduler time: 87.42376348073594 Scheduler overhead time: 0.0869852997129783 Adapter cache time: 0.015825323993340135 Engine time: 0.08710912428796291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.10745478700846,
    "estimated_duration": 3600.093287969907,
    "input_throughput": 7741.213566083834,
    "output_throughput": 6729.667556382087,
    "total_throughput": 14470.881122465922,
    "itl": 88.85846452775591,
    "ttft": 1549021.3243932428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5617850427702072,
    "arrivals": 291201,
    "finished_requests": 112523,
    "scheduler_time": 230.94177941096825
}
#Debug simulation 
Total elapsed time: 77.10764444700908. Arrivals time: 0.47316124150529504 Scheduler time: 76.42163490434177 Scheduler overhead time: 0.08289845020044595 Adapter cache time: 0.014497030759230256 Engine time: 0.08290533884428442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.08647610701155,
    "estimated_duration": 3600.093081625347,
    "input_throughput": 7637.4872472981815,
    "output_throughput": 6641.921044220493,
    "total_throughput": 14279.408291518675,
    "itl": 86.1616216036301,
    "ttft": 1560848.1480848407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6395485126040877,
    "arrivals": 291201,
    "finished_requests": 110968,
    "scheduler_time": 233.90630546225265
}
#Debug simulation 
Total elapsed time: 78.08665058901533. Arrivals time: 0.487476383917965 Scheduler time: 77.38414341083262 Scheduler overhead time: 0.08358413190580904 Adapter cache time: 0.015335352276451886 Engine time: 0.08311864838469774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 85.45311281492468,
    "estimated_duration": 3600.0165947929922,
    "input_throughput": 7729.582702548649,
    "output_throughput": 6735.035620410579,
    "total_throughput": 14464.618322959228,
    "itl": 89.33472052789595,
    "ttft": 1538897.0957978596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5620547677157441,
    "arrivals": 287382,
    "finished_requests": 112356,
    "scheduler_time": 230.14237304990937
}
#Debug simulation 
Total elapsed time: 85.45327977393754. Arrivals time: 0.49629309540614486 Scheduler time: 84.74108460324351 Scheduler overhead time: 0.08392424648627639 Adapter cache time: 0.015462868614122272 Engine time: 0.08362484944518656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.32318839395884,
    "estimated_duration": 3600.00437587949,
    "input_throughput": 7681.199552221228,
    "output_throughput": 6712.516007455243,
    "total_throughput": 14393.715559676471,
    "itl": 88.49053369408686,
    "ttft": 1553811.3682567235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.635917938281782,
    "arrivals": 287382,
    "finished_requests": 111824,
    "scheduler_time": 231.04225780212857
}
#Debug simulation 
Total elapsed time: 74.323354403954. Arrivals time: 0.45620222168508917 Scheduler time: 73.65864947380032 Scheduler overhead time: 0.08103339513763785 Adapter cache time: 0.01454476558137685 Engine time: 0.08104564074892551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.77130742603913,
    "estimated_duration": 3600.0070059387717,
    "input_throughput": 7606.821863075495,
    "output_throughput": 6636.296251809664,
    "total_throughput": 14243.118114885157,
    "itl": 86.05057136451072,
    "ttft": 1558675.160357425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6305435843393208,
    "arrivals": 287382,
    "finished_requests": 110633,
    "scheduler_time": 233.69437106999516
}
#Debug simulation 
Total elapsed time: 78.77161811396945. Arrivals time: 0.45308341819327325 Scheduler time: 78.10289235506207 Scheduler overhead time: 0.08327559160534292 Adapter cache time: 0.015308784786611795 Engine time: 0.0836048221681267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 75.05214956402779,
    "estimated_duration": 3600.040955671437,
    "input_throughput": 7680.522066406051,
    "output_throughput": 6712.110028063067,
    "total_throughput": 14392.632094469118,
    "itl": 88.49094653936687,
    "ttft": 1553743.9131029726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5984359720023349,
    "arrivals": 287382,
    "finished_requests": 111817,
    "scheduler_time": 231.04234702935815
}
#Debug simulation 
Total elapsed time: 75.05230809201021. Arrivals time: 0.4543691163416952 Scheduler time: 74.38741219474468 Scheduler overhead time: 0.08155563101172447 Adapter cache time: 0.014815469738095999 Engine time: 0.0815687543945387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 81.54048615903594,
    "estimated_duration": 3600.0308917546945,
    "input_throughput": 7584.486861636774,
    "output_throughput": 6628.329788683926,
    "total_throughput": 14212.8166503207,
    "itl": 85.75876300607771,
    "ttft": 1559279.3733184184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6109223145712168,
    "arrivals": 287382,
    "finished_requests": 110443,
    "scheduler_time": 233.9505250210142
}
#Debug simulation 
Total elapsed time: 81.54065919702407. Arrivals time: 0.4643757620360702 Scheduler time: 80.85502324998379 Scheduler overhead time: 0.0869416834320873 Adapter cache time: 0.01543858798686415 Engine time: 0.08524261973798275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 81.08332126506139,
    "estimated_duration": 3600.002755165009,
    "input_throughput": 7671.738573081757,
    "output_throughput": 6703.390980847702,
    "total_throughput": 14375.129553929459,
    "itl": 88.25986636897501,
    "ttft": 1546752.6121336843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 83,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5298654380673548,
    "arrivals": 287382,
    "finished_requests": 111682,
    "scheduler_time": 231.25531710574182
}
#Debug simulation 
Total elapsed time: 81.08349339303095. Arrivals time: 0.4557791529223323 Scheduler time: 80.4116565725999 Scheduler overhead time: 0.08445897744968534 Adapter cache time: 0.015184670453891158 Engine time: 0.08353724831249565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.91185067698825,
    "estimated_duration": 3600.0337090502417,
    "input_throughput": 7606.227944800016,
    "output_throughput": 6635.753976398838,
    "total_throughput": 14241.981921198854,
    "itl": 86.01050115491893,
    "ttft": 1558988.0459318145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6189450224488974,
    "arrivals": 287382,
    "finished_requests": 110617,
    "scheduler_time": 233.7464418292001
}
#Debug simulation 
Total elapsed time: 78.91201467905194. Arrivals time: 0.44876603968441486 Scheduler time: 78.24545315641444 Scheduler overhead time: 0.08476983092259616 Adapter cache time: 0.015114955371245742 Engine time: 0.08487185381818563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.89274758007377,
    "estimated_duration": 3600.095302820174,
    "input_throughput": 7735.818265195271,
    "output_throughput": 6761.933769067216,
    "total_throughput": 14497.752034262487,
    "itl": 89.806177005484,
    "ttft": 1530463.1840246413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5818919948115939,
    "arrivals": 285504,
    "finished_requests": 112621,
    "scheduler_time": 229.13869092752023
}
#Debug simulation 
Total elapsed time: 77.89292626001406. Arrivals time: 0.4695756734581664 Scheduler time: 77.21358647954185 Scheduler overhead time: 0.08187376463320106 Adapter cache time: 0.014684466761536896 Engine time: 0.0809264179551974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 75.69550079794135,
    "estimated_duration": 3600.034891013484,
    "input_throughput": 7679.558070121062,
    "output_throughput": 6714.715199105957,
    "total_throughput": 14394.27326922702,
    "itl": 88.46914572139364,
    "ttft": 1537198.7971173092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6670065059373157,
    "arrivals": 285504,
    "finished_requests": 111866,
    "scheduler_time": 231.65827568786372
}
#Debug simulation 
Total elapsed time: 75.69568792695645. Arrivals time: 0.4546396555379033 Scheduler time: 75.02956970501691 Scheduler overhead time: 0.08187090116553009 Adapter cache time: 0.015094868140295148 Engine time: 0.08177175547461957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.04045332502574,
    "estimated_duration": 3600.0151094132943,
    "input_throughput": 7606.373908931372,
    "output_throughput": 6664.568695076989,
    "total_throughput": 14270.94260400836,
    "itl": 86.44503184179561,
    "ttft": 1506631.4940627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6853672915603966,
    "arrivals": 285504,
    "finished_requests": 110785,
    "scheduler_time": 233.37196865644364
}
#Debug simulation 
Total elapsed time: 82.04063587600831. Arrivals time: 0.4506746541010216 Scheduler time: 81.37596420897171 Scheduler overhead time: 0.08399076096247882 Adapter cache time: 0.014281451585702598 Engine time: 0.08285985491238534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 80.16918350895867,
    "estimated_duration": 3600.044858564707,
    "input_throughput": 7710.361701177976,
    "output_throughput": 6742.804868736525,
    "total_throughput": 14453.166569914501,
    "itl": 88.6920803385138,
    "ttft": 1528455.25106261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6217523977439852,
    "arrivals": 285504,
    "finished_requests": 112338,
    "scheduler_time": 230.43692615623985
}
#Debug simulation 
Total elapsed time: 80.16937183204573. Arrivals time: 0.48830334411468357 Scheduler time: 79.46761932293884 Scheduler overhead time: 0.08328748401254416 Adapter cache time: 0.015090188826434314 Engine time: 0.08268819435033947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 79.53410321404226,
    "estimated_duration": 3600.0477492388686,
    "input_throughput": 7573.841209679702,
    "output_throughput": 6631.998979749039,
    "total_throughput": 14205.84018942874,
    "itl": 86.33724543741039,
    "ttft": 1526969.6517595628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6780663213972002,
    "arrivals": 285504,
    "finished_requests": 110333,
    "scheduler_time": 233.94537273905723
}
#Debug simulation 
Total elapsed time: 79.53428513905965. Arrivals time: 0.4515555716352537 Scheduler time: 78.86885835032444 Scheduler overhead time: 0.08342295745387673 Adapter cache time: 0.014999688719399273 Engine time: 0.08242371829692274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.18423277698457,
    "estimated_duration": 3600.010903428903,
    "input_throughput": 7708.965540511754,
    "output_throughput": 6733.60016129705,
    "total_throughput": 14442.565701808804,
    "itl": 88.66610554289414,
    "ttft": 1525881.7610476215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5617850427702072,
    "arrivals": 285504,
    "finished_requests": 112275,
    "scheduler_time": 230.29389745450874
}
#Debug simulation 
Total elapsed time: 82.18440265592653. Arrivals time: 0.47405524377245456 Scheduler time: 81.49553728697356 Scheduler overhead time: 0.08383564429823309 Adapter cache time: 0.015226890798658133 Engine time: 0.08316971105523407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.8166293380782,
    "estimated_duration": 3600.0098448475346,
    "input_throughput": 7606.723642490417,
    "output_throughput": 6665.012606657519,
    "total_throughput": 14271.736249147936,
    "itl": 86.44739454175044,
    "ttft": 1506435.8043011148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6725260266102852,
    "arrivals": 285504,
    "finished_requests": 110788,
    "scheduler_time": 233.36376029259057
}
#Debug simulation 
Total elapsed time: 81.81680232600775. Arrivals time: 0.45488861249759793 Scheduler time: 81.14723264437634 Scheduler overhead time: 0.08331541228108108 Adapter cache time: 0.014862089999951422 Engine time: 0.08316576015204191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 78.25881032506004,
    "estimated_duration": 3600.0591264319382,
    "input_throughput": 7716.1510476445155,
    "output_throughput": 6714.679440267524,
    "total_throughput": 14430.83048791204,
    "itl": 89.3130207964463,
    "ttft": 1528001.9321858264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6017292219074437,
    "arrivals": 284495,
    "finished_requests": 112258,
    "scheduler_time": 230.8817546167856
}
#Debug simulation 
Total elapsed time: 78.25898167910054. Arrivals time: 0.4540113875409588 Scheduler time: 77.59347339137457 Scheduler overhead time: 0.08206649939529598 Adapter cache time: 0.014760822639800608 Engine time: 0.08246950397733599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 85.8908549429616,
    "estimated_duration": 3600.004300418805,
    "input_throughput": 7712.772175513749,
    "output_throughput": 6703.853380728571,
    "total_throughput": 14416.62555624232,
    "itl": 88.2203030870216,
    "ttft": 1521724.2240962228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6656182849640027,
    "arrivals": 284495,
    "finished_requests": 112012,
    "scheduler_time": 231.07085780239865
}
#Debug simulation 
Total elapsed time: 85.89114887302276. Arrivals time: 0.470792155014351 Scheduler time: 85.20480473211501 Scheduler overhead time: 0.08376574597787112 Adapter cache time: 0.015038594137877226 Engine time: 0.08388095977716148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 73.34250966808759,
    "estimated_duration": 3600.0937821350203,
    "input_throughput": 7638.1060228082415,
    "output_throughput": 6644.385243157222,
    "total_throughput": 14282.491265965464,
    "itl": 86.19240070335006,
    "ttft": 1545073.6144399338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7304061190132053,
    "arrivals": 284495,
    "finished_requests": 111059,
    "scheduler_time": 233.2007548469294
}
#Debug simulation 
Total elapsed time: 73.34268668107688. Arrivals time: 0.4421199506614357 Scheduler time: 72.69092211057432 Scheduler overhead time: 0.08107077283784747 Adapter cache time: 0.014922288246452808 Engine time: 0.08093696343712509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 80.52660988003481,
    "estimated_duration": 3600.036993498657,
    "input_throughput": 7708.6255086035,
    "output_throughput": 6711.8426959600665,
    "total_throughput": 14420.468204563567,
    "itl": 88.43868739563491,
    "ttft": 1526661.5203325169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.62535987673793,
    "arrivals": 284495,
    "finished_requests": 112149,
    "scheduler_time": 230.76730339523695
}
#Debug simulation 
Total elapsed time: 80.52679510100279. Arrivals time: 0.4559468168299645 Scheduler time: 79.85852923174389 Scheduler overhead time: 0.08280967571772635 Adapter cache time: 0.01460622635204345 Engine time: 0.08248829375952482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 73.25021339592058,
    "estimated_duration": 3600.0718016951196,
    "input_throughput": 7640.03234242418,
    "output_throughput": 6646.04438965194,
    "total_throughput": 14286.076732076119,
    "itl": 86.24643883393158,
    "ttft": 1543214.940090003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7014660728117452,
    "arrivals": 284495,
    "finished_requests": 111034,
    "scheduler_time": 233.0331569077898
}
#Debug simulation 
Total elapsed time: 73.25039771292359. Arrivals time: 0.45182265411131084 Scheduler time: 72.58724371949211 Scheduler overhead time: 0.08182354411110282 Adapter cache time: 0.014606871060095727 Engine time: 0.08233952906448394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 86.036539823981,
    "estimated_duration": 3600.0561599454504,
    "input_throughput": 7716.851283903568,
    "output_throughput": 6706.877595033367,
    "total_throughput": 14423.728878936936,
    "itl": 88.31417303076714,
    "ttft": 1521370.3820079686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5809368055919187,
    "arrivals": 284495,
    "finished_requests": 112064,
    "scheduler_time": 230.9693530133161
}
#Debug simulation 
Total elapsed time: 86.03672026493587. Arrivals time: 0.4631940299877897 Scheduler time: 85.35841116460506 Scheduler overhead time: 0.08319561718963087 Adapter cache time: 0.015098639065399766 Engine time: 0.08375653752591461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.43303955893498,
    "estimated_duration": 3600.082724676609,
    "input_throughput": 7646.319572412812,
    "output_throughput": 6653.571829283925,
    "total_throughput": 14299.891401696737,
    "itl": 86.30154470514553,
    "ttft": 1541128.9741319811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7246053415536882,
    "arrivals": 284495,
    "finished_requests": 111197,
    "scheduler_time": 232.91361020328827
}
#Debug simulation 
Total elapsed time: 74.4332062829053. Arrivals time: 0.4505908527644351 Scheduler time: 73.77068721980322 Scheduler overhead time: 0.08193229348398745 Adapter cache time: 0.015015353448688984 Engine time: 0.08200952888000757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 81.90366016398184,
    "estimated_duration": 3600.067470021772,
    "input_throughput": 7781.061947661384,
    "output_throughput": 6761.774939693779,
    "total_throughput": 14542.836887355163,
    "itl": 89.93070282476855,
    "ttft": 1528341.382322569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6612409031949931,
    "arrivals": 284045,
    "finished_requests": 112971,
    "scheduler_time": 229.2256314861238
}
#Debug simulation 
Total elapsed time: 81.90384651604109. Arrivals time: 0.4744814446894452 Scheduler time: 81.2169267371064 Scheduler overhead time: 0.08204476209357381 Adapter cache time: 0.01505859603639692 Engine time: 0.08261117420624942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 79.2130211800104,
    "estimated_duration": 3600.020155786024,
    "input_throughput": 7753.753810277032,
    "output_throughput": 6737.8739424596,
    "total_throughput": 14491.627752736631,
    "itl": 88.85724445112338,
    "ttft": 1535362.907452091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8615839069057262,
    "arrivals": 284045,
    "finished_requests": 112630,
    "scheduler_time": 230.160294359351
}
#Debug simulation 
Total elapsed time: 79.21318128798157. Arrivals time: 0.45953570667188615 Scheduler time: 78.54262200789526 Scheduler overhead time: 0.08129766152705997 Adapter cache time: 0.014840947580523789 Engine time: 0.08268539828713983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.70749401603825,
    "estimated_duration": 3600.0536700372845,
    "input_throughput": 7669.9781533295945,
    "output_throughput": 6673.202458048686,
    "total_throughput": 14343.18061137828,
    "itl": 86.60562405312933,
    "ttft": 1520514.3328005802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.915219889408909,
    "arrivals": 284045,
    "finished_requests": 111548,
    "scheduler_time": 232.19459131807187
}
#Debug simulation 
Total elapsed time: 76.70765950903296. Arrivals time: 0.4539996103849262 Scheduler time: 76.0417415095726 Scheduler overhead time: 0.08163695165421814 Adapter cache time: 0.015211854013614357 Engine time: 0.08254822914022952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 77.97594947798643,
    "estimated_duration": 3600.0427224097734,
    "input_throughput": 7748.142494633711,
    "output_throughput": 6732.571491200534,
    "total_throughput": 14480.713985834245,
    "itl": 88.85258280221095,
    "ttft": 1535621.384003429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7549837004486472,
    "arrivals": 284045,
    "finished_requests": 112544,
    "scheduler_time": 230.27268168222406
}
#Debug simulation 
Total elapsed time: 77.9761385259917. Arrivals time: 0.4693011799827218 Scheduler time: 77.29603583493736 Scheduler overhead time: 0.08201439608819783 Adapter cache time: 0.015038400888442993 Engine time: 0.08153714833315462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 81.04503743897658,
    "estimated_duration": 3600.0265031162307,
    "input_throughput": 7636.942110343223,
    "output_throughput": 6636.500586681552,
    "total_throughput": 14273.442697024775,
    "itl": 86.26888995829646,
    "ttft": 1515382.5788253841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0102134945429866,
    "arrivals": 284045,
    "finished_requests": 110945,
    "scheduler_time": 233.6529076175495
}
#Debug simulation 
Total elapsed time: 81.04520046303514. Arrivals time: 0.4592219761107117 Scheduler time: 80.36891270591877 Scheduler overhead time: 0.083901162375696 Adapter cache time: 0.015697535942308605 Engine time: 0.08426172111649066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 81.53315134497825,
    "estimated_duration": 3600.040225210432,
    "input_throughput": 7745.215679741512,
    "output_throughput": 6726.950668609386,
    "total_throughput": 14472.166348350898,
    "itl": 88.78656609525966,
    "ttft": 1533180.210132344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6447760149976237,
    "arrivals": 284045,
    "finished_requests": 112468,
    "scheduler_time": 230.422951464497
}
#Debug simulation 
Total elapsed time: 81.53333245299291. Arrivals time: 0.4680736369919032 Scheduler time: 80.85230732045602 Scheduler overhead time: 0.0822640674887225 Adapter cache time: 0.015160090872086585 Engine time: 0.08285317418631166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 55.42965130193625,
    "estimated_duration": 3600.0181506427202,
    "input_throughput": 7647.722274701491,
    "output_throughput": 6661.526969167781,
    "total_throughput": 14309.249243869272,
    "itl": 86.18538639248773,
    "ttft": 1545272.7267021607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.566824867166581,
    "arrivals": 284045,
    "finished_requests": 111255,
    "scheduler_time": 233.04557942972806
}
#Debug simulation 
Total elapsed time: 55.42980475095101. Arrivals time: 0.4361631163628772 Scheduler time: 54.79160480340943 Scheduler overhead time: 0.07717867859173566 Adapter cache time: 0.014896416454575956 Engine time: 0.0780541041167453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.3380472509889,
    "estimated_duration": 3600.078944031497,
    "input_throughput": 7821.640146720528,
    "output_throughput": 6811.088973660426,
    "total_throughput": 14632.729120380955,
    "itl": 89.98366152069799,
    "ttft": 1530776.2239212424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7802642657700918,
    "arrivals": 283764,
    "finished_requests": 113795,
    "scheduler_time": 228.35543913414656
}
#Debug simulation 
Total elapsed time: 77.33836550998967. Arrivals time: 0.46804391546174884 Scheduler time: 76.65960465499666 Scheduler overhead time: 0.08160235942341387 Adapter cache time: 0.015319888829253614 Engine time: 0.08126623812131584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.56375682994258,
    "estimated_duration": 3600.06712521075,
    "input_throughput": 7750.099103601205,
    "output_throughput": 6748.50833470994,
    "total_throughput": 14498.607438311145,
    "itl": 88.83795609348756,
    "ttft": 1519127.3970202908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 79,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5779054658906534,
    "arrivals": 283764,
    "finished_requests": 112682,
    "scheduler_time": 229.44556249754126
}
#Debug simulation 
Total elapsed time: 74.56393231800757. Arrivals time: 0.45090898929629475 Scheduler time: 73.90290875907522 Scheduler overhead time: 0.08131879498250782 Adapter cache time: 0.01500801369547844 Engine time: 0.08166361099574715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.66782376100309,
    "estimated_duration": 3600.0381156501126,
    "input_throughput": 7666.92271396011,
    "output_throughput": 6673.4007330535,
    "total_throughput": 14340.323447013609,
    "itl": 86.41807387690389,
    "ttft": 1520414.1912727817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 81,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6080241706129165,
    "arrivals": 283764,
    "finished_requests": 111385,
    "scheduler_time": 232.06201072979138
}
#Debug simulation 
Total elapsed time: 78.66800291009713. Arrivals time: 0.46264217304997146 Scheduler time: 77.98823213309515 Scheduler overhead time: 0.08435994992032647 Adapter cache time: 0.015392483095638454 Engine time: 0.0846151567529887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 76.51787169999443,
    "estimated_duration": 3600.020138540873,
    "input_throughput": 7776.260388183981,
    "output_throughput": 6762.398559766723,
    "total_throughput": 14538.658947950704,
    "itl": 88.77721015858046,
    "ttft": 1524102.637572597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8002283309586344,
    "arrivals": 283764,
    "finished_requests": 112894,
    "scheduler_time": 229.7521191941674
}
#Debug simulation 
Total elapsed time: 76.51805745193269. Arrivals time: 0.4536981376586482 Scheduler time: 75.85323768202215 Scheduler overhead time: 0.08120792370755225 Adapter cache time: 0.015154448454268277 Engine time: 0.08257343654986471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 64.98887566104531,
    "estimated_duration": 3600.0657432469475,
    "input_throughput": 7673.502921945125,
    "output_throughput": 6687.46704005637,
    "total_throughput": 14360.969962001494,
    "itl": 86.63667299087598,
    "ttft": 1546241.0074798942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7955808367719875,
    "arrivals": 283764,
    "finished_requests": 111663,
    "scheduler_time": 231.74081251662548
}
#Debug simulation 
Total elapsed time: 64.98904688097537. Arrivals time: 0.4453736380673945 Scheduler time: 64.33406663453206 Scheduler overhead time: 0.08048024482559413 Adapter cache time: 0.01457253284752369 Engine time: 0.08201257849577814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.6411507120356,
    "estimated_duration": 3600.007639419312,
    "input_throughput": 7769.164902247669,
    "output_throughput": 6761.719262330913,
    "total_throughput": 14530.884164578581,
    "itl": 88.5975706317562,
    "ttft": 1533223.0445813728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7149991453438992,
    "arrivals": 283764,
    "finished_requests": 112908,
    "scheduler_time": 230.2816829742037
}
#Debug simulation 
Total elapsed time: 77.64133429701906. Arrivals time: 0.4611046585487202 Scheduler time: 76.96651638660114 Scheduler overhead time: 0.08310289564542472 Adapter cache time: 0.015021793777123094 Engine time: 0.08299371809698641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 68.36700586194638,
    "estimated_duration": 3600.058090099896,
    "input_throughput": 7677.819165199364,
    "output_throughput": 6688.617071546152,
    "total_throughput": 14366.436236745516,
    "itl": 86.70780156227099,
    "ttft": 1541995.2733892393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7909208796732129,
    "arrivals": 283764,
    "finished_requests": 111722,
    "scheduler_time": 231.6090672880816
}
#Debug simulation 
Total elapsed time: 68.36720136192162. Arrivals time: 0.4482825237791985 Scheduler time: 67.70790879428387 Scheduler overhead time: 0.08175202249549329 Adapter cache time: 0.015122030628845096 Engine time: 0.08190126367844641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 81.5489220970776,
    "estimated_duration": 3600.076366910533,
    "input_throughput": 7726.990809328937,
    "output_throughput": 6757.140549458944,
    "total_throughput": 14484.131358787881,
    "itl": 90.02612144223369,
    "ttft": 1487481.2357388372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8199387199617914,
    "arrivals": 264567,
    "finished_requests": 112714,
    "scheduler_time": 228.09749889493338
}
#Debug simulation 
Total elapsed time: 81.54911674303003. Arrivals time: 0.4632994288112968 Scheduler time: 80.87360303010792 Scheduler overhead time: 0.0817500950070098 Adapter cache time: 0.015071393107064068 Engine time: 0.0829716466832906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.92529299401212,
    "estimated_duration": 3600.0452178897144,
    "input_throughput": 7680.882135199639,
    "output_throughput": 6715.6884252059535,
    "total_throughput": 14396.570560405593,
    "itl": 88.68508174813655,
    "ttft": 1495790.1177859076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0100856403168297,
    "arrivals": 264567,
    "finished_requests": 112048,
    "scheduler_time": 229.53850222123046
}
#Debug simulation 
Total elapsed time: 81.92545998399146. Arrivals time: 0.4385600115638226 Scheduler time: 81.27775101945736 Scheduler overhead time: 0.0809760756092146 Adapter cache time: 0.015233795740641654 Engine time: 0.08019187743775547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.67509993899148,
    "estimated_duration": 3600.0198951532134,
    "input_throughput": 7631.558658047567,
    "output_throughput": 6671.736184662891,
    "total_throughput": 14303.294842710458,
    "itl": 86.58619325347084,
    "ttft": 1507663.4570879021,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0197408846672626,
    "arrivals": 264567,
    "finished_requests": 111354,
    "scheduler_time": 231.06109065662858
}
#Debug simulation 
Total elapsed time: 80.67526440799702. Arrivals time: 0.4447026976849884 Scheduler time: 80.02071491186507 Scheduler overhead time: 0.0811636132420972 Adapter cache time: 0.015248032985255122 Engine time: 0.08121830609161407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 82.46576002601068,
    "estimated_duration": 3600.023102005379,
    "input_throughput": 7711.253292940263,
    "output_throughput": 6738.85203305669,
    "total_throughput": 14450.105325996954,
    "itl": 88.87841561251454,
    "ttft": 1496601.4390373384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8896032241266214,
    "arrivals": 264567,
    "finished_requests": 112469,
    "scheduler_time": 228.69474531956203
}
#Debug simulation 
Total elapsed time: 82.46593025198672. Arrivals time: 0.430871186661534 Scheduler time: 81.82791659457143 Scheduler overhead time: 0.08000031288247555 Adapter cache time: 0.01447829173412174 Engine time: 0.0806599932257086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 80.13390748295933,
    "estimated_duration": 3600.029622114986,
    "input_throughput": 7631.538038250753,
    "output_throughput": 6671.718158221545,
    "total_throughput": 14303.256196472297,
    "itl": 86.58758047198685,
    "ttft": 1507663.5112363049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0106277288962158,
    "arrivals": 264567,
    "finished_requests": 111354,
    "scheduler_time": 231.06243507281224
}
#Debug simulation 
Total elapsed time: 80.13407108595129. Arrivals time: 0.44338855368550867 Scheduler time: 79.47909540496767 Scheduler overhead time: 0.0812691223109141 Adapter cache time: 0.015144384233281016 Engine time: 0.0823779086349532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 80.86547848698683,
    "estimated_duration": 3600.0282911217632,
    "input_throughput": 7685.629323590272,
    "output_throughput": 6725.6043680855255,
    "total_throughput": 14411.233691675798,
    "itl": 88.8712308962329,
    "ttft": 1492634.0609019182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8107579594524567,
    "arrivals": 264567,
    "finished_requests": 112131,
    "scheduler_time": 229.2044696783235
}
#Debug simulation 
Total elapsed time: 80.86566597106867. Arrivals time: 0.4564006223808974 Scheduler time: 80.19477904972155 Scheduler overhead time: 0.08285941183567047 Adapter cache time: 0.015327746630646288 Engine time: 0.08358862856402993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.71908290195279,
    "estimated_duration": 3600.0919573306014,
    "input_throughput": 7631.956996002051,
    "output_throughput": 6671.826799060367,
    "total_throughput": 14303.783795062418,
    "itl": 86.588063715659,
    "ttft": 1507670.3570613016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0011003387719395,
    "arrivals": 264567,
    "finished_requests": 111359,
    "scheduler_time": 231.06226013781134
}
#Debug simulation 
Total elapsed time: 80.71936071303207. Arrivals time: 0.44065236824098974 Scheduler time: 80.067001351621 Scheduler overhead time: 0.08084892213810235 Adapter cache time: 0.015214023413136601 Engine time: 0.08284841850399971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.07676356006414,
    "estimated_duration": 3600.075908016255,
    "input_throughput": 7788.744936617429,
    "output_throughput": 6772.326368372207,
    "total_throughput": 14561.071304989635,
    "itl": 90.0421119596132,
    "ttft": 1482016.9526303357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8199387199617914,
    "arrivals": 262719,
    "finished_requests": 113237,
    "scheduler_time": 227.11914392224242
}
#Debug simulation 
Total elapsed time: 83.07693024899345. Arrivals time: 0.4309973195195198 Scheduler time: 82.43909993511625 Scheduler overhead time: 0.08009096153546125 Adapter cache time: 0.014695875579491258 Engine time: 0.07992532395292073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.92325140303001,
    "estimated_duration": 3600.0718803293994,
    "input_throughput": 7756.780955563843,
    "output_throughput": 6744.422002423571,
    "total_throughput": 14501.202957987414,
    "itl": 88.9763246844854,
    "ttft": 1487326.8834601643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9420817679911858,
    "arrivals": 262719,
    "finished_requests": 112786,
    "scheduler_time": 228.1523043400421
}
#Debug simulation 
Total elapsed time: 82.92342536896467. Arrivals time: 0.4398804563097656 Scheduler time: 82.27537527342793 Scheduler overhead time: 0.08086616301443428 Adapter cache time: 0.015075882081873715 Engine time: 0.07999513729009777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 83.26521447696723,
    "estimated_duration": 3600.0524553988503,
    "input_throughput": 7673.875128838719,
    "output_throughput": 6673.836089240577,
    "total_throughput": 14347.711218079296,
    "itl": 86.53458211875618,
    "ttft": 1495114.3775750808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9734612094191836,
    "arrivals": 262719,
    "finished_requests": 111567,
    "scheduler_time": 230.57031920212088
}
#Debug simulation 
Total elapsed time: 83.26539608603343. Arrivals time: 0.4405642068013549 Scheduler time: 82.61278727499302 Scheduler overhead time: 0.08234252745751292 Adapter cache time: 0.015181617112830281 Engine time: 0.08151142380665988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 83.96998580603395,
    "estimated_duration": 3600.0089345856163,
    "input_throughput": 7757.955468300739,
    "output_throughput": 6745.037148857808,
    "total_throughput": 14502.992617158547,
    "itl": 88.96670151339585,
    "ttft": 1487003.0784401482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8321479356614871,
    "arrivals": 262719,
    "finished_requests": 112800,
    "scheduler_time": 228.0814837828554
}
#Debug simulation 
Total elapsed time: 83.9701560420217. Arrivals time: 0.4427319929236546 Scheduler time: 83.31822597887367 Scheduler overhead time: 0.08062375138979405 Adapter cache time: 0.015029996167868376 Engine time: 0.08011002454441041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 83.4355377049651,
    "estimated_duration": 3600.094398485608,
    "input_throughput": 7678.8372581643325,
    "output_throughput": 6676.62548240708,
    "total_throughput": 14355.462740571413,
    "itl": 86.55169845243931,
    "ttft": 1496621.01765485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9356684901192786,
    "arrivals": 262719,
    "finished_requests": 111638,
    "scheduler_time": 230.5030269248075
}
#Debug simulation 
Total elapsed time: 83.43570756306872. Arrivals time: 0.4385428340174258 Scheduler time: 82.78524356766138 Scheduler overhead time: 0.0818850677460432 Adapter cache time: 0.015420188079588115 Engine time: 0.08203119307290763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.77324502705596,
    "estimated_duration": 3600.0126967416445,
    "input_throughput": 7755.1279264289715,
    "output_throughput": 6744.55343504098,
    "total_throughput": 14499.681361469951,
    "itl": 88.9413193260406,
    "ttft": 1486437.3684423664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7979901175713157,
    "arrivals": 262719,
    "finished_requests": 112753,
    "scheduler_time": 228.07207063673854
}
#Debug simulation 
Total elapsed time: 83.77342179999687. Arrivals time: 0.455645554815419 Scheduler time: 83.11031175171956 Scheduler overhead time: 0.07977785798721015 Adapter cache time: 0.014795969473198056 Engine time: 0.08052576659247279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 85.67278003797401,
    "estimated_duration": 3600.0278955080917,
    "input_throughput": 7681.326590414428,
    "output_throughput": 6677.237426408151,
    "total_throughput": 14358.564016822578,
    "itl": 86.52404895272028,
    "ttft": 1493728.6178722694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8793415971659125,
    "arrivals": 262719,
    "finished_requests": 111652,
    "scheduler_time": 230.46899379187897
}
#Debug simulation 
Total elapsed time: 85.67294863797724. Arrivals time: 0.4627885934896767 Scheduler time: 84.99709173757583 Scheduler overhead time: 0.08200785156805068 Adapter cache time: 0.015319952624849975 Engine time: 0.08283452701289207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.66674107406288,
    "estimated_duration": 3600.0131005682606,
    "input_throughput": 7727.543545774525,
    "output_throughput": 6744.186013147437,
    "total_throughput": 14471.729558921963,
    "itl": 89.84202358481537,
    "ttft": 1486748.369206581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9059000373771404,
    "arrivals": 261754,
    "finished_requests": 112462,
    "scheduler_time": 228.13769353159807
}
#Debug simulation 
Total elapsed time: 83.66691667505074. Arrivals time: 0.4746882787439972 Scheduler time: 82.98329287220258 Scheduler overhead time: 0.0806789476191625 Adapter cache time: 0.015180452959612012 Engine time: 0.08109988644719124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 83.03849221090786,
    "estimated_duration": 3600.09032128703,
    "input_throughput": 7704.238928673438,
    "output_throughput": 6725.7948659865015,
    "total_throughput": 14430.03379465994,
    "itl": 88.79174143281128,
    "ttft": 1492353.5650608712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 141,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0347902870317929,
    "arrivals": 261754,
    "finished_requests": 112147,
    "scheduler_time": 228.7715328234399
}
#Debug simulation 
Total elapsed time: 83.03866666893009. Arrivals time: 0.45120331877842546 Scheduler time: 82.37691036309116 Scheduler overhead time: 0.08166252903174609 Adapter cache time: 0.015360583085566759 Engine time: 0.0813953954493627 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.31694944400806,
    "estimated_duration": 3600.0705751446353,
    "input_throughput": 7642.694615478364,
    "output_throughput": 6673.2188990586155,
    "total_throughput": 14315.913514536978,
    "itl": 86.68800310850051,
    "ttft": 1503071.1341558488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1022104247985418,
    "arrivals": 261754,
    "finished_requests": 111230,
    "scheduler_time": 230.55732406938617
}
#Debug simulation 
Total elapsed time: 81.31711382395588. Arrivals time: 0.4397162420209497 Scheduler time: 80.66509804455563 Scheduler overhead time: 0.08182316331658512 Adapter cache time: 0.015464114723727107 Engine time: 0.08214928256347775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 83.5879404990701,
    "estimated_duration": 3600.0341974427124,
    "input_throughput": 7700.663793608631,
    "output_throughput": 6724.518344074767,
    "total_throughput": 14425.182137683398,
    "itl": 88.79162127678661,
    "ttft": 1492165.5084783633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9384553336305538,
    "arrivals": 261754,
    "finished_requests": 112081,
    "scheduler_time": 228.87219839314787
}
#Debug simulation 
Total elapsed time: 83.5881134250667. Arrivals time: 0.4568958948366344 Scheduler time: 82.92161465797108 Scheduler overhead time: 0.08132020197808743 Adapter cache time: 0.01478144945576787 Engine time: 0.0811631710967049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 80.96841048600618,
    "estimated_duration": 3600.019360025894,
    "input_throughput": 7640.242801305981,
    "output_throughput": 6670.015518979678,
    "total_throughput": 14310.258320285659,
    "itl": 86.62280481072958,
    "ttft": 1503334.1653634734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0814483346696948,
    "arrivals": 261754,
    "finished_requests": 111203,
    "scheduler_time": 230.80110869018344
}
#Debug simulation 
Total elapsed time: 80.96857095404994. Arrivals time: 0.42828871216624975 Scheduler time: 80.32844688848127 Scheduler overhead time: 0.08107632282190025 Adapter cache time: 0.015120444353669882 Engine time: 0.08273062843363732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.06354132795241,
    "estimated_duration": 3600.075572312066,
    "input_throughput": 7703.878555579356,
    "output_throughput": 6724.836607930353,
    "total_throughput": 14428.715163509709,
    "itl": 88.78124368680879,
    "ttft": 1492484.1989219591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 140,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8937489316798731,
    "arrivals": 261754,
    "finished_requests": 112122,
    "scheduler_time": 228.85388872685164
}
#Debug simulation 
Total elapsed time: 83.06385541590862. Arrivals time: 0.6938064618734643 Scheduler time: 82.15955182921607 Scheduler overhead time: 0.08168292103800923 Adapter cache time: 0.015288963681086898 Engine time: 0.08102900418452919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.60249901202042,
    "estimated_duration": 3600.055042372955,
    "input_throughput": 7640.283461296734,
    "output_throughput": 6670.076350880516,
    "total_throughput": 14310.359812177248,
    "itl": 86.62216597115179,
    "ttft": 1503329.7344952484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0719209445454185,
    "arrivals": 261754,
    "finished_requests": 111205,
    "scheduler_time": 230.8019388955273
}
#Debug simulation 
Total elapsed time: 81.60266958002467. Arrivals time: 0.4327540078666061 Scheduler time: 80.95666542905383 Scheduler overhead time: 0.08224291226360947 Adapter cache time: 0.015150186489336193 Engine time: 0.0825702961301431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 84.66801200399641,
    "estimated_duration": 3600.0411412013045,
    "input_throughput": 7847.848925018769,
    "output_throughput": 6782.9294283708205,
    "total_throughput": 14630.778353389589,
    "itl": 90.04032371193632,
    "ttft": 1479847.0922458803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 132,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8728379922173908,
    "arrivals": 261307,
    "finished_requests": 113620,
    "scheduler_time": 226.61305343313566
}
#Debug simulation 
Total elapsed time: 84.66818337596487. Arrivals time: 0.443104860605672 Scheduler time: 84.01809505885467 Scheduler overhead time: 0.08025753602851182 Adapter cache time: 0.015099644777365029 Engine time: 0.07988076482433826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.77846347505692,
    "estimated_duration": 3600.08960186295,
    "input_throughput": 7811.336136036134,
    "output_throughput": 6755.910738281091,
    "total_throughput": 14567.246874317225,
    "itl": 88.92505216211124,
    "ttft": 1483298.8516475495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9096049793623396,
    "arrivals": 261307,
    "finished_requests": 113143,
    "scheduler_time": 227.65391992364493
}
#Debug simulation 
Total elapsed time: 82.77863049099687. Arrivals time: 0.4380688675446436 Scheduler time: 82.12743298639543 Scheduler overhead time: 0.08266285771969706 Adapter cache time: 0.015578211168758571 Engine time: 0.08198426477611065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.86854623595718,
    "estimated_duration": 3600.025204403099,
    "input_throughput": 7732.937804421788,
    "output_throughput": 6685.779024702897,
    "total_throughput": 14418.716829124685,
    "itl": 86.66149398303138,
    "ttft": 1497596.7050039195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.093666392825544,
    "arrivals": 261307,
    "finished_requests": 111985,
    "scheduler_time": 230.0664012929924
}
#Debug simulation 
Total elapsed time: 80.86871651397087. Arrivals time: 0.4339273312361911 Scheduler time: 80.22303175902925 Scheduler overhead time: 0.08230995887424797 Adapter cache time: 0.015083719976246357 Engine time: 0.08143156510777771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 85.536460667965,
    "estimated_duration": 3600.071726521693,
    "input_throughput": 7809.904950745314,
    "output_throughput": 6750.521613490292,
    "total_throughput": 14560.426564235606,
    "itl": 88.80479124921092,
    "ttft": 1483133.5700941987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.832978972708806,
    "arrivals": 261307,
    "finished_requests": 113080,
    "scheduler_time": 227.7795706772851
}
#Debug simulation 
Total elapsed time: 85.53662625595462. Arrivals time: 0.44599826890043914 Scheduler time: 84.87942899554037 Scheduler overhead time: 0.08159807499032468 Adapter cache time: 0.01507495332043618 Engine time: 0.08175516489427537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 82.38601647806354,
    "estimated_duration": 3600.0967793965146,
    "input_throughput": 7734.442073712147,
    "output_throughput": 6685.101949963235,
    "total_throughput": 14419.544023675382,
    "itl": 86.64702115467216,
    "ttft": 1497434.016460707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0809822313347839,
    "arrivals": 261307,
    "finished_requests": 111979,
    "scheduler_time": 230.09575552428365
}
#Debug simulation 
Total elapsed time: 82.386184633011. Arrivals time: 0.4442715961486101 Scheduler time: 81.72892424277961 Scheduler overhead time: 0.08196437580045313 Adapter cache time: 0.015597253106534481 Engine time: 0.08190783893223852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 79.95920478203334,
    "estimated_duration": 3600.005997442639,
    "input_throughput": 7816.417255968296,
    "output_throughput": 6754.068192462066,
    "total_throughput": 14570.485448430361,
    "itl": 88.93825849838883,
    "ttft": 1485516.7196028335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7916061966307452,
    "arrivals": 261307,
    "finished_requests": 113154,
    "scheduler_time": 227.63430460551206
}
#Debug simulation 
Total elapsed time: 79.95938679995015. Arrivals time: 0.4379275881219655 Scheduler time: 79.31426261330489 Scheduler overhead time: 0.0810993550112471 Adapter cache time: 0.014870151411741972 Engine time: 0.07925522571895272 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.00486021197867,
    "estimated_duration": 3600.0872806599814,
    "input_throughput": 7729.174831255449,
    "output_throughput": 6680.569698741012,
    "total_throughput": 14409.74452999646,
    "itl": 86.56489525782527,
    "ttft": 1496556.2515928592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0516779644973582,
    "arrivals": 261307,
    "finished_requests": 111924,
    "scheduler_time": 230.23590170637502
}
#Debug simulation 
Total elapsed time: 82.00503824197222. Arrivals time: 0.42782704648561776 Scheduler time: 81.36707043636125 Scheduler overhead time: 0.08153531851712614 Adapter cache time: 0.015130074927583337 Engine time: 0.080525744240731 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 83.29356853896752,
    "estimated_duration": 3600.0598654110076,
    "input_throughput": 7757.30266830207,
    "output_throughput": 6775.0820574800055,
    "total_throughput": 14532.384725782074,
    "itl": 89.90474664283,
    "ttft": 1490538.922103699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8133263109298414,
    "arrivals": 261056,
    "finished_requests": 112553,
    "scheduler_time": 227.3258858352218
}
#Debug simulation 
Total elapsed time: 83.29374929098412. Arrivals time: 0.42801381670869887 Scheduler time: 82.65829786215909 Scheduler overhead time: 0.0815044124610722 Adapter cache time: 0.014942883397452533 Engine time: 0.07924197323154658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 83.79997863504104,
    "estimated_duration": 3600.0206096159886,
    "input_throughput": 7720.871076614448,
    "output_throughput": 6742.702787634673,
    "total_throughput": 14463.573864249121,
    "itl": 88.7771059943036,
    "ttft": 1494262.0051280437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.847427844051272,
    "arrivals": 261056,
    "finished_requests": 112094,
    "scheduler_time": 228.31883625723975
}
#Debug simulation 
Total elapsed time: 83.80015344906133. Arrivals time: 0.43237752944696695 Scheduler time: 83.1591950270813 Scheduler overhead time: 0.08166644652374089 Adapter cache time: 0.014668591786175966 Engine time: 0.08039080060552806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.87657212291379,
    "estimated_duration": 3600.0091061827,
    "input_throughput": 7633.2179140341905,
    "output_throughput": 6668.708964866053,
    "total_throughput": 14301.926878900244,
    "itl": 86.44932636890769,
    "ttft": 1504459.2463803238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9458153765089812,
    "arrivals": 261056,
    "finished_requests": 110813,
    "scheduler_time": 230.87440991972167
}
#Debug simulation 
Total elapsed time: 82.87674368498847. Arrivals time: 0.4338909494690597 Scheduler time: 82.23134491662495 Scheduler overhead time: 0.08209537831135094 Adapter cache time: 0.015175330452620983 Engine time: 0.08136715390719473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 83.6391795630334,
    "estimated_duration": 3600.038919508941,
    "input_throughput": 7728.333949177158,
    "output_throughput": 6752.47172142124,
    "total_throughput": 14480.805670598396,
    "itl": 88.97886368393297,
    "ttft": 1487978.8874276709,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8343671936821192,
    "arrivals": 261056,
    "finished_requests": 112255,
    "scheduler_time": 227.89607076728927
}
#Debug simulation 
Total elapsed time: 83.6393467040034. Arrivals time: 0.47233718750067055 Scheduler time: 82.95254749455489 Scheduler overhead time: 0.0833232905715704 Adapter cache time: 0.015423103352077305 Engine time: 0.08336228819098324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 78.07663460599724,
    "estimated_duration": 3600.0808923104037,
    "input_throughput": 7635.433708923986,
    "output_throughput": 6674.341693633327,
    "total_throughput": 14309.775402557314,
    "itl": 86.45139835494966,
    "ttft": 1506538.463881483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9011874316306787,
    "arrivals": 261056,
    "finished_requests": 110860,
    "scheduler_time": 230.6931924626245
}
#Debug simulation 
Total elapsed time: 78.07693619502243. Arrivals time: 0.43863804230932146 Scheduler time: 77.42866935255006 Scheduler overhead time: 0.08130939037073404 Adapter cache time: 0.015055520925670862 Engine time: 0.08056301809847355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 81.17108495591674,
    "estimated_duration": 3600.085646673007,
    "input_throughput": 7734.014890932112,
    "output_throughput": 6756.7653626462225,
    "total_throughput": 14490.780253578336,
    "itl": 88.95641750468286,
    "ttft": 1492321.4812871725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 115,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7341509081656107,
    "arrivals": 261056,
    "finished_requests": 112370,
    "scheduler_time": 227.99137924254705
}
#Debug simulation 
Total elapsed time: 81.17125475290231. Arrivals time: 0.46992501989006996 Scheduler time: 80.4887563502416 Scheduler overhead time: 0.08304455445613712 Adapter cache time: 0.0151270852657035 Engine time: 0.0821737281512469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.34608241007663,
    "estimated_duration": 3600.0505414959684,
    "input_throughput": 7642.164098220566,
    "output_throughput": 6677.866247402772,
    "total_throughput": 14320.030345623338,
    "itl": 86.5715689024528,
    "ttft": 1507996.7448261213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9018072866462173,
    "arrivals": 261056,
    "finished_requests": 110938,
    "scheduler_time": 230.4979876355419
}
#Debug simulation 
Total elapsed time: 80.34624687803444. Arrivals time: 0.44602850324008614 Scheduler time: 79.69069012918044 Scheduler overhead time: 0.08127547637559474 Adapter cache time: 0.01457036635838449 Engine time: 0.08147004910279065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 85.59888977406081,
    "estimated_duration": 3600.0735195538505,
    "input_throughput": 7741.212741525828,
    "output_throughput": 6779.743210084437,
    "total_throughput": 14520.955951610265,
    "itl": 89.94563519825427,
    "ttft": 1474530.0161907696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0712102631758886,
    "arrivals": 258928,
    "finished_requests": 112832,
    "scheduler_time": 226.5110924882826
}
#Debug simulation 
Total elapsed time: 85.59905373305082. Arrivals time: 0.4426236409926787 Scheduler time: 84.94718243239913 Scheduler overhead time: 0.08176797453779727 Adapter cache time: 0.015362440957687795 Engine time: 0.08026277518365532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 80.73454604996368,
    "estimated_duration": 3600.049534659889,
    "input_throughput": 7715.905498679274,
    "output_throughput": 6754.249286266333,
    "total_throughput": 14470.154784945607,
    "itl": 89.09838820679,
    "ttft": 1484499.1415530767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2152116251457492,
    "arrivals": 258928,
    "finished_requests": 112443,
    "scheduler_time": 227.3690015985344
}
#Debug simulation 
Total elapsed time: 80.73470854095649. Arrivals time: 0.4355427585542202 Scheduler time: 80.09199997247197 Scheduler overhead time: 0.08015900221653283 Adapter cache time: 0.014652970829047263 Engine time: 0.08005681750364602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.71575176296756,
    "estimated_duration": 3600.0522915721285,
    "input_throughput": 7629.806951499852,
    "output_throughput": 6682.250715167986,
    "total_throughput": 14312.057666667837,
    "itl": 86.79694756456371,
    "ttft": 1496411.3298524707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.377101850099864,
    "arrivals": 258928,
    "finished_requests": 111200,
    "scheduler_time": 229.85860244651184
}
#Debug simulation 
Total elapsed time: 78.71590237505734. Arrivals time: 0.41968477109912783 Scheduler time: 78.08741102716886 Scheduler overhead time: 0.08118943613953888 Adapter cache time: 0.015306407352909446 Engine time: 0.07984999939799309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 81.26250419090502,
    "estimated_duration": 3600.0821588862377,
    "input_throughput": 7713.531184685808,
    "output_throughput": 6753.359208758068,
    "total_throughput": 14466.890393443875,
    "itl": 89.00228068647803,
    "ttft": 1482346.3937640542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.133863771646283,
    "arrivals": 258928,
    "finished_requests": 112363,
    "scheduler_time": 227.46115996319904
}
#Debug simulation 
Total elapsed time: 81.26267688395455. Arrivals time: 0.41689419257454574 Scheduler time: 80.63805886986665 Scheduler overhead time: 0.08080036542378366 Adapter cache time: 0.01471252168994397 Engine time: 0.08046581083908677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 78.96269816393033,
    "estimated_duration": 3600.038743440697,
    "input_throughput": 7631.104540209387,
    "output_throughput": 6681.085875484886,
    "total_throughput": 14312.190415694273,
    "itl": 86.74582112943716,
    "ttft": 1495226.2422920133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3071085752686509,
    "arrivals": 258928,
    "finished_requests": 111217,
    "scheduler_time": 229.9106326912058
}
#Debug simulation 
Total elapsed time: 78.96286650793627. Arrivals time: 0.41904589417390525 Scheduler time: 78.33578670676798 Scheduler overhead time: 0.0815130058908835 Adapter cache time: 0.014603739604353905 Engine time: 0.07928190636448562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 78.78236504003871,
    "estimated_duration": 3600.052952070739,
    "input_throughput": 7714.800412595226,
    "output_throughput": 6755.206471619183,
    "total_throughput": 14470.006884214408,
    "itl": 89.14705828092393,
    "ttft": 1486380.8104007363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1108022436592702,
    "arrivals": 258928,
    "finished_requests": 112443,
    "scheduler_time": 227.31193839615005
}
#Debug simulation 
Total elapsed time: 78.78252097102813. Arrivals time: 0.40234726609196514 Scheduler time: 78.17418209114112 Scheduler overhead time: 0.08028543507680297 Adapter cache time: 0.014443595078773797 Engine time: 0.07962928025517613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.06339209503494,
    "estimated_duration": 3600.0303227460886,
    "input_throughput": 7627.579641899792,
    "output_throughput": 6682.512324409876,
    "total_throughput": 14310.091966309667,
    "itl": 86.80674778025848,
    "ttft": 1497654.6208103627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3293141408264662,
    "arrivals": 258928,
    "finished_requests": 111190,
    "scheduler_time": 229.84659530095803
}
#Debug simulation 
Total elapsed time: 76.0635535250185. Arrivals time: 0.3944614011561498 Scheduler time: 75.4642829693621 Scheduler overhead time: 0.08017681771889329 Adapter cache time: 0.014613971929065883 Engine time: 0.07815820630639791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 79.11027616006322,
    "estimated_duration": 3600.0658469442546,
    "input_throughput": 7770.900641649622,
    "output_throughput": 6785.010341056196,
    "total_throughput": 14555.910982705818,
    "itl": 90.15843866586846,
    "ttft": 1480286.6393473523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1703963986551376,
    "arrivals": 257941,
    "finished_requests": 113035,
    "scheduler_time": 226.3273280797164
}
#Debug simulation 
Total elapsed time: 79.11043791600969. Arrivals time: 0.36009034095332026 Scheduler time: 78.55700809683185 Scheduler overhead time: 0.07540628884453326 Adapter cache time: 0.013521921820938587 Engine time: 0.07395725580863655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.5058892009547,
    "estimated_duration": 3600.050843178772,
    "input_throughput": 7733.393836020745,
    "output_throughput": 6750.856601386376,
    "total_throughput": 14484.25043740712,
    "itl": 88.99653441011594,
    "ttft": 1485746.3148831753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3573294376162826,
    "arrivals": 257941,
    "finished_requests": 112502,
    "scheduler_time": 227.51775840454874
}
#Debug simulation 
Total elapsed time: 78.50604390201624. Arrivals time: 0.37930612394120544 Scheduler time: 77.93118191196118 Scheduler overhead time: 0.07635240408126265 Adapter cache time: 0.01358008524402976 Engine time: 0.07471478485967964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.63892363710329,
    "estimated_duration": 3600.0781879873225,
    "input_throughput": 7654.339034065734,
    "output_throughput": 6681.6529375013415,
    "total_throughput": 14335.991971567075,
    "itl": 86.65868471312328,
    "ttft": 1496896.9748137954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.476964384773749,
    "arrivals": 257941,
    "finished_requests": 111335,
    "scheduler_time": 230.01272265435355
}
#Debug simulation 
Total elapsed time: 76.63906877802219. Arrivals time: 0.37483408593107015 Scheduler time: 76.06438083085231 Scheduler overhead time: 0.07779290212783962 Adapter cache time: 0.014093016041442752 Engine time: 0.07670438208151609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 77.53746039001271,
    "estimated_duration": 3600.012396971552,
    "input_throughput": 7732.865593301452,
    "output_throughput": 6751.171751643293,
    "total_throughput": 14484.037344944743,
    "itl": 88.99867301962236,
    "ttft": 1486169.0645373368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2373947276687236,
    "arrivals": 257941,
    "finished_requests": 112464,
    "scheduler_time": 227.51098087536724
}
#Debug simulation 
Total elapsed time: 77.53769404208288. Arrivals time: 0.3842897069407627 Scheduler time: 76.95409032073803 Scheduler overhead time: 0.07756239047739655 Adapter cache time: 0.01412542280741036 Engine time: 0.07645744062028825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 76.60311182891019,
    "estimated_duration": 3600.0514113704608,
    "input_throughput": 7653.000985758662,
    "output_throughput": 6681.079865702207,
    "total_throughput": 14334.080851460869,
    "itl": 86.65935503944287,
    "ttft": 1496734.904986141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4849855961371246,
    "arrivals": 257941,
    "finished_requests": 111323,
    "scheduler_time": 230.01482579435066
}
#Debug simulation 
Total elapsed time: 76.60327228798997. Arrivals time: 0.384587015141733 Scheduler time: 76.0157161122188 Scheduler overhead time: 0.07903468352742493 Adapter cache time: 0.01422474894206971 Engine time: 0.07784282055217773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.31127069296781,
    "estimated_duration": 3600.0796538236536,
    "input_throughput": 7733.5555535360345,
    "output_throughput": 6752.286154052618,
    "total_throughput": 14485.841707588652,
    "itl": 88.99999786611501,
    "ttft": 1486294.784001069,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1427218483621226,
    "arrivals": 257941,
    "finished_requests": 112480,
    "scheduler_time": 227.51916669079986
}
#Debug simulation 
Total elapsed time: 77.31141594098881. Arrivals time: 0.38306426943745464 Scheduler time: 76.73113959946204 Scheduler overhead time: 0.07728988234885037 Adapter cache time: 0.013718694681301713 Engine time: 0.07526750140823424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.66094192606397,
    "estimated_duration": 3600.0388460093045,
    "input_throughput": 7653.836299723302,
    "output_throughput": 6681.354571121712,
    "total_throughput": 14335.190870845014,
    "itl": 86.65688835968152,
    "ttft": 1497043.8794664552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4791848186776073,
    "arrivals": 257941,
    "finished_requests": 111328,
    "scheduler_time": 230.01215427986563
}
#Debug simulation 
Total elapsed time: 76.66109190904535. Arrivals time: 0.3795947984326631 Scheduler time: 76.08083380607422 Scheduler overhead time: 0.07871177711058408 Adapter cache time: 0.013862227206118405 Engine time: 0.076509925769642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 74.62514487199951,
    "estimated_duration": 3600.0842100078476,
    "input_throughput": 7788.850583564232,
    "output_throughput": 6790.6042119905605,
    "total_throughput": 14579.454795554793,
    "itl": 90.25001179864654,
    "ttft": 1476113.3801846495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0910474902717384,
    "arrivals": 257455,
    "finished_requests": 113404,
    "scheduler_time": 226.08894013422793
}
#Debug simulation 
Total elapsed time: 74.62529620504938. Arrivals time: 0.38445809425320476 Scheduler time: 74.04564496944658 Scheduler overhead time: 0.0765919117256999 Adapter cache time: 0.013746377662755549 Engine time: 0.0741461964789778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 75.27487523294985,
    "estimated_duration": 3600.032532091762,
    "input_throughput": 7747.276934687738,
    "output_throughput": 6756.972828205534,
    "total_throughput": 14504.24976289327,
    "itl": 89.10780649339733,
    "ttft": 1478429.1879585958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2576798137091112,
    "arrivals": 257455,
    "finished_requests": 112794,
    "scheduler_time": 227.3107136478025
}
#Debug simulation 
Total elapsed time: 75.2750168290222. Arrivals time: 0.3900162131758407 Scheduler time: 74.6875677498756 Scheduler overhead time: 0.0776869235560298 Adapter cache time: 0.013621747144497931 Engine time: 0.07532046001870185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.94326908304356,
    "estimated_duration": 3600.0759586726704,
    "input_throughput": 7658.9053443655375,
    "output_throughput": 6682.16011999631,
    "total_throughput": 14341.065464361847,
    "itl": 86.63151246542157,
    "ttft": 1488895.236899121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3222781428787875,
    "arrivals": 257455,
    "finished_requests": 111582,
    "scheduler_time": 229.96128116937024
}
#Debug simulation 
Total elapsed time: 74.94341334199999. Arrivals time: 0.3866066784830764 Scheduler time: 74.35570238728542 Scheduler overhead time: 0.07844035653397441 Adapter cache time: 0.014052673475816846 Engine time: 0.07695714803412557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 74.74263188592158,
    "estimated_duration": 3600.095158468559,
    "input_throughput": 7754.829461751246,
    "output_throughput": 6756.842508115176,
    "total_throughput": 14511.671969866422,
    "itl": 89.12309154728392,
    "ttft": 1479556.5922233905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1680026343697674,
    "arrivals": 257455,
    "finished_requests": 112887,
    "scheduler_time": 227.26371562981274
}
#Debug simulation 
Total elapsed time: 74.74275794194546. Arrivals time: 0.38906753866467625 Scheduler time: 74.15765818080399 Scheduler overhead time: 0.07624532270710915 Adapter cache time: 0.013759534223936498 Engine time: 0.07535227248445153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 74.780066634994,
    "estimated_duration": 3600.0730766123047,
    "input_throughput": 7662.146410082602,
    "output_throughput": 6684.813749015926,
    "total_throughput": 14346.960159098528,
    "itl": 86.74453856072554,
    "ttft": 1488505.9791726163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3090226435754466,
    "arrivals": 257455,
    "finished_requests": 111615,
    "scheduler_time": 229.82940897160512
}
#Debug simulation 
Total elapsed time: 74.7801999039948. Arrivals time: 0.39628802612423897 Scheduler time: 74.18051435344387 Scheduler overhead time: 0.0799619167810306 Adapter cache time: 0.01407816621940583 Engine time: 0.07764465385116637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 74.09660354501102,
    "estimated_duration": 3600.022431209439,
    "input_throughput": 7748.4631090558805,
    "output_throughput": 6756.619011351071,
    "total_throughput": 14505.08212040695,
    "itl": 89.10799579585245,
    "ttft": 1480891.3735405486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0916504808375587,
    "arrivals": 257455,
    "finished_requests": 112824,
    "scheduler_time": 227.28692065245679
}
#Debug simulation 
Total elapsed time: 74.09674874006305. Arrivals time: 0.3941628390457481 Scheduler time: 73.50654405343812 Scheduler overhead time: 0.07723209913820028 Adapter cache time: 0.013572806026786566 Engine time: 0.07418195786885917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 73.28439589007758,
    "estimated_duration": 3600.0608908556474,
    "input_throughput": 7660.170712680806,
    "output_throughput": 6682.227809286406,
    "total_throughput": 14342.398521967212,
    "itl": 86.74793838114596,
    "ttft": 1491008.1083258532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.327812451608481,
    "arrivals": 257455,
    "finished_requests": 111543,
    "scheduler_time": 229.83956487170136
}
#Debug simulation 
Total elapsed time: 73.2845319340704. Arrivals time: 0.38942966947797686 Scheduler time: 72.69678778224625 Scheduler overhead time: 0.07744008710142225 Adapter cache time: 0.013680081930942833 Engine time: 0.07576358574442565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 79.48050483700354,
    "estimated_duration": 3600.0761837536284,
    "input_throughput": 7799.693552796666,
    "output_throughput": 6788.158847938538,
    "total_throughput": 14587.852400735204,
    "itl": 90.07833094697645,
    "ttft": 1470836.3695245767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1439467625273378,
    "arrivals": 257175,
    "finished_requests": 113369,
    "scheduler_time": 226.23255666775822
}
#Debug simulation 
Total elapsed time: 79.48064823099412. Arrivals time: 0.3931178682250902 Scheduler time: 78.89091324654873 Scheduler overhead time: 0.07667279848828912 Adapter cache time: 0.013730657170526683 Engine time: 0.07560864894185215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 77.32226086501032,
    "estimated_duration": 3600.0773163467193,
    "input_throughput": 7758.31393208613,
    "output_throughput": 6754.886315796544,
    "total_throughput": 14513.200247882674,
    "itl": 89.0190419028712,
    "ttft": 1479521.355419209,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3587176585895955,
    "arrivals": 257175,
    "finished_requests": 112812,
    "scheduler_time": 227.33704318658204
}
#Debug simulation 
Total elapsed time: 77.32240834704135. Arrivals time: 0.3835564129985869 Scheduler time: 76.74112147698179 Scheduler overhead time: 0.07758947112597525 Adapter cache time: 0.013833059696480632 Engine time: 0.07520798628684133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.08226328610908,
    "estimated_duration": 3600.029481736955,
    "input_throughput": 7677.480737370118,
    "output_throughput": 6682.406941954531,
    "total_throughput": 14359.887679324649,
    "itl": 86.65595419697704,
    "ttft": 1488570.7464710628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.319328488688918,
    "arrivals": 257175,
    "finished_requests": 111590,
    "scheduler_time": 229.86702192273663
}
#Debug simulation 
Total elapsed time: 76.08245244703721. Arrivals time: 0.380256608244963 Scheduler time: 75.50007144059055 Scheduler overhead time: 0.07906438398640603 Adapter cache time: 0.013932230416685343 Engine time: 0.07731440232601017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 76.19903255300596,
    "estimated_duration": 3600.017519913681,
    "input_throughput": 7753.822542693987,
    "output_throughput": 6755.677678086173,
    "total_throughput": 14509.50022078016,
    "itl": 89.04832392056342,
    "ttft": 1479688.571213172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2559893065644416,
    "arrivals": 257175,
    "finished_requests": 112738,
    "scheduler_time": 227.32129197580156
}
#Debug simulation 
Total elapsed time: 76.19916458404623. Arrivals time: 0.3950618135277182 Scheduler time: 75.60815783357248 Scheduler overhead time: 0.07663251576013863 Adapter cache time: 0.013606886146590114 Engine time: 0.07487776933703572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 75.99783848703373,
    "estimated_duration": 3600.040769676638,
    "input_throughput": 7675.022525503735,
    "output_throughput": 6679.488522059125,
    "total_throughput": 14354.51104756286,
    "itl": 86.5357551813439,
    "ttft": 1489767.9253034133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3275568172289083,
    "arrivals": 257175,
    "finished_requests": 111502,
    "scheduler_time": 230.00995211710276
}
#Debug simulation 
Total elapsed time: 75.99798377405386. Arrivals time: 0.38223623926751316 Scheduler time: 75.41496055142488 Scheduler overhead time: 0.07877037557773292 Adapter cache time: 0.013888451154343784 Engine time: 0.0765808979049325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 76.76684734795708,
    "estimated_duration": 3600.04305155322,
    "input_throughput": 7758.682215744354,
    "output_throughput": 6755.145050142602,
    "total_throughput": 14513.827265886956,
    "itl": 88.99404104361089,
    "ttft": 1478845.086572664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1171861645998407,
    "arrivals": 257175,
    "finished_requests": 112786,
    "scheduler_time": 227.3626185514254
}
#Debug simulation 
Total elapsed time: 76.76698674599174. Arrivals time: 0.37861697713378817 Scheduler time: 76.19130966602825 Scheduler overhead time: 0.0775824167067185 Adapter cache time: 0.013664989150129259 Engine time: 0.07490435754880309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.45669532101601,
    "estimated_duration": 3600.040228488856,
    "input_throughput": 7672.7759266164285,
    "output_throughput": 6681.13006339825,
    "total_throughput": 14353.905990014679,
    "itl": 86.52092096776893,
    "ttft": 1488078.2742032113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.368658921811732,
    "arrivals": 257175,
    "finished_requests": 111501,
    "scheduler_time": 230.0303211133034
}
#Debug simulation 
Total elapsed time: 76.45683062600438. Arrivals time: 0.38100509834475815 Scheduler time: 75.87472432176583 Scheduler overhead time: 0.07833308214321733 Adapter cache time: 0.014290919993072748 Engine time: 0.07724892650730908 
