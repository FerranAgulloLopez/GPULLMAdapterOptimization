INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 66, 1080, 66, 66, 1080, 270, 1080, 270, 1080, 1080, 270, 66, 1080, 270, 66, 66, 270, 1080, 66, 270, 66, 66, 270, 270, 270, 66, 66, 1080, 270, 270, 66, 1080, 1080, 270, 1080, 1080, 270, 1080, 66, 1080, 270, 270, 270, 1080, 1080, 66, 66, 270, 66, 1080, 66, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 270, 1080, 1080, 66, 66, 1080, 270, 1080, 66, 270, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 66, 66, 270]
Prompts retrieved: 45312 . Total input tokens: 10038919 . Total output tokens: 8932037
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.474349979776889,
    "estimated_duration": 3599.7486230763598,
    "input_throughput": 1052.8193484691576,
    "output_throughput": 910.8861043740845,
    "total_throughput": 1963.705452843242,
    "itl": 25.86252098053122,
    "ttft": 6841.183179181862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.20395437858567,
    "arrivals": 15365,
    "finished_requests": 15336,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4744398049078882. Arrivals time: 0.04755719332024455 Scheduler time: 1.0622215657494962 Scheduler overhead time: 0.11871499242261052 Adapter cache time: 0.0652362983673811 Engine time: 0.12186336051672697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 66, 1080, 66, 66, 1080, 270, 1080, 270, 1080, 1080, 270, 66, 1080, 270, 66, 66, 270, 1080, 66, 270, 66, 66, 270, 270, 270, 66, 66, 1080, 270, 270, 66, 1080, 1080, 270, 1080, 1080, 270, 1080, 66, 1080, 270, 270, 270, 1080, 1080, 66, 66, 270, 66, 1080, 66, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 270, 1080, 1080, 66, 66, 1080, 270, 1080, 66, 270, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 66, 66, 270]
Prompts retrieved: 45312 . Total input tokens: 10038919 . Total output tokens: 8932037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.4777890010736883,
    "estimated_duration": 3599.736020715947,
    "input_throughput": 1052.8230342974523,
    "output_throughput": 910.8892933065274,
    "total_throughput": 1963.71232760398,
    "itl": 25.900850941730877,
    "ttft": 6841.491770431938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6680,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.146111600249135,
    "arrivals": 15365,
    "finished_requests": 15336,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4778707781806588. Arrivals time: 0.04768643993884325 Scheduler time: 1.0696751084178686 Scheduler overhead time: 0.11907650902867317 Adapter cache time: 0.06506529869511724 Engine time: 0.1174578401260078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 66, 1080, 66, 66, 1080, 270, 1080, 270, 1080, 1080, 270, 66, 1080, 270, 66, 66, 270, 1080, 66, 270, 66, 66, 270, 270, 270, 66, 66, 1080, 270, 270, 66, 1080, 1080, 270, 1080, 1080, 270, 1080, 66, 1080, 270, 270, 270, 1080, 1080, 66, 66, 270, 66, 1080, 66, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 270, 1080, 1080, 66, 66, 1080, 270, 1080, 66, 270, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 66, 66, 270]
Prompts retrieved: 45312 . Total input tokens: 10038919 . Total output tokens: 8932037
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.4939976260066032,
    "estimated_duration": 3599.7313163627778,
    "input_throughput": 1052.8244101921907,
    "output_throughput": 910.8904837134098,
    "total_throughput": 1963.7148939056005,
    "itl": 25.909241069822745,
    "ttft": 6841.584462251975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 49.311710428952594,
    "arrivals": 15365,
    "finished_requests": 15336,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4940735762938857. Arrivals time: 0.047510024625808 Scheduler time: 1.0857152938842773 Scheduler overhead time: 0.12055112747475505 Adapter cache time: 0.06534136366099119 Engine time: 0.1159043307416141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 66, 1080, 66, 66, 1080, 270, 1080, 270, 1080, 1080, 270, 66, 1080, 270, 66, 66, 270, 1080, 66, 270, 66, 66, 270, 270, 270, 66, 66, 1080, 270, 270, 66, 1080, 1080, 270, 1080, 1080, 270, 1080, 66, 1080, 270, 270, 270, 1080, 1080, 66, 66, 270, 66, 1080, 66, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 270, 1080, 1080, 66, 66, 1080, 270, 1080, 66, 270, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 66, 66, 270]
Prompts retrieved: 45312 . Total input tokens: 10038919 . Total output tokens: 8932037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.4797554602846503,
    "estimated_duration": 3599.7519611367698,
    "input_throughput": 1052.8183721867292,
    "output_throughput": 910.8852597067641,
    "total_throughput": 1963.7036318934934,
    "itl": 25.87986163762696,
    "ttft": 6841.265613479632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 45.57345480589835,
    "arrivals": 15365,
    "finished_requests": 15336,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4798488891683519. Arrivals time: 0.047968571074306965 Scheduler time: 1.0703491899184883 Scheduler overhead time: 0.11913651367649436 Adapter cache time: 0.06547230947762728 Engine time: 0.1182813853956759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 66, 1080, 66, 66, 1080, 270, 1080, 270, 1080, 1080, 270, 66, 1080, 270, 66, 66, 270, 1080, 66, 270, 66, 66, 270, 270, 270, 66, 66, 1080, 270, 270, 66, 1080, 1080, 270, 1080, 1080, 270, 1080, 66, 1080, 270, 270, 270, 1080, 1080, 66, 66, 270, 66, 1080, 66, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 270, 1080, 1080, 66, 66, 1080, 270, 1080, 66, 270, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 66, 66, 270]
Prompts retrieved: 45312 . Total input tokens: 10038919 . Total output tokens: 8932037
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5013766679912806,
    "estimated_duration": 3599.756245600486,
    "input_throughput": 1052.8171191124075,
    "output_throughput": 910.884175562567,
    "total_throughput": 1963.7012946749744,
    "itl": 25.9116260521945,
    "ttft": 6841.631011004057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.9422622618251,
    "arrivals": 15365,
    "finished_requests": 15336,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5014665382914245. Arrivals time: 0.04801734443753958 Scheduler time: 1.0930448737926781 Scheduler overhead time: 0.11909691011533141 Adapter cache time: 0.06497514108195901 Engine time: 0.1178696621209383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 66, 1080, 66, 66, 1080, 270, 1080, 270, 1080, 1080, 270, 66, 1080, 270, 66, 66, 270, 1080, 66, 270, 66, 66, 270, 270, 270, 66, 66, 1080, 270, 270, 66, 1080, 1080, 270, 1080, 1080, 270, 1080, 66, 1080, 270, 270, 270, 1080, 1080, 66, 66, 270, 66, 1080, 66, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 270, 1080, 1080, 66, 66, 1080, 270, 1080, 66, 270, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 66, 66, 270]
Prompts retrieved: 45312 . Total input tokens: 10038919 . Total output tokens: 8932037
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.501239208970219,
    "estimated_duration": 3599.7480812692224,
    "input_throughput": 1052.8195069316455,
    "output_throughput": 910.886241473843,
    "total_throughput": 1963.7057484054885,
    "itl": 25.847897884332838,
    "ttft": 6841.1521135459025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6686,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 42.68289540865917,
    "arrivals": 15365,
    "finished_requests": 15336,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.501312532927841. Arrivals time: 0.04788351617753506 Scheduler time: 1.0927734454162419 Scheduler overhead time: 0.11892968788743019 Adapter cache time: 0.06546391360461712 Engine time: 0.1174929104745388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.1-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 66, 1080, 66, 66, 1080, 270, 1080, 270, 1080, 1080, 270, 66, 1080, 270, 66, 66, 270, 1080, 66, 270, 66, 66, 270, 270, 270, 66, 66, 1080, 270, 270, 66, 1080, 1080, 270, 1080, 1080, 270, 1080, 66, 1080, 270, 270, 270, 1080, 1080, 66, 66, 270, 66, 1080, 66, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 270, 1080, 1080, 66, 66, 1080, 270, 1080, 66, 270, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 66, 66, 270]
Prompts retrieved: 45312 . Total input tokens: 10038919 . Total output tokens: 8932037
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5114743178710341,
    "estimated_duration": 3599.7594442717113,
    "input_throughput": 1052.8161836010556,
    "output_throughput": 910.8833661698708,
    "total_throughput": 1963.6995497709265,
    "itl": 25.904149965446916,
    "ttft": 6841.671726852644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.46879987865549,
    "arrivals": 15365,
    "finished_requests": 15336,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5115577969700098. Arrivals time: 0.048212836030870676 Scheduler time: 1.104091215878725 Scheduler overhead time: 0.117686633951962 Adapter cache time: 0.06617818167433143 Engine time: 0.11690154625102878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 33, 1080, 33, 33, 1080, 270, 1080, 270, 1080, 1080, 270, 33, 1080, 270, 33, 33, 270, 1080, 33, 270, 33, 33, 270, 270, 270, 33, 33, 1080, 270, 270, 33, 1080, 1080, 270, 1080, 1080, 270, 1080, 33, 1080, 270, 270, 270, 1080, 1080, 33, 33, 270, 33, 1080, 33, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 270, 1080, 1080, 33, 33, 1080, 270, 1080, 33, 270, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 33, 33, 270]
Prompts retrieved: 44256 . Total input tokens: 9794058 . Total output tokens: 8729348
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.5108093698509037,
    "estimated_duration": 3599.7618924409185,
    "input_throughput": 1016.6355746153187,
    "output_throughput": 896.1609396371892,
    "total_throughput": 1912.796514252508,
    "itl": 25.640649606130445,
    "ttft": 6764.011377213285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5947,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 39.32399651300515,
    "arrivals": 15003,
    "finished_requests": 14975,
    "scheduler_time": 1.943639629815115e-06
}
#Debug simulation 
Total elapsed time: 1.5108981360681355. Arrivals time: 0.04776755115017295 Scheduler time: 1.0988363297656178 Scheduler overhead time: 0.12053041392937303 Adapter cache time: 0.06408963399007916 Engine time: 0.12022203952074051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 33, 1080, 33, 33, 1080, 270, 1080, 270, 1080, 1080, 270, 33, 1080, 270, 33, 33, 270, 1080, 33, 270, 33, 33, 270, 270, 270, 33, 33, 1080, 270, 270, 33, 1080, 1080, 270, 1080, 1080, 270, 1080, 33, 1080, 270, 270, 270, 1080, 1080, 33, 33, 270, 33, 1080, 33, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 270, 1080, 1080, 33, 33, 1080, 270, 1080, 33, 270, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 33, 33, 270]
Prompts retrieved: 44256 . Total input tokens: 9794058 . Total output tokens: 8729348
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.4677293510176241,
    "estimated_duration": 3599.774203841126,
    "input_throughput": 1016.6320976729562,
    "output_throughput": 896.1578747238492,
    "total_throughput": 1912.7899723968055,
    "itl": 25.674574636745383,
    "ttft": 6764.257708753932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5951,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 42.805063852783526,
    "arrivals": 15003,
    "finished_requests": 14975,
    "scheduler_time": 1.943639629815115e-06
}
#Debug simulation 
Total elapsed time: 1.4678204730153084. Arrivals time: 0.04656755784526467 Scheduler time: 1.0608058767393231 Scheduler overhead time: 0.12012445088475943 Adapter cache time: 0.0628983206115663 Engine time: 0.11789829749614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 33, 1080, 33, 33, 1080, 270, 1080, 270, 1080, 1080, 270, 33, 1080, 270, 33, 33, 270, 1080, 33, 270, 33, 33, 270, 270, 270, 33, 33, 1080, 270, 270, 33, 1080, 1080, 270, 1080, 1080, 270, 1080, 33, 1080, 270, 270, 270, 1080, 1080, 33, 33, 270, 33, 1080, 33, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 270, 1080, 1080, 33, 33, 1080, 270, 1080, 33, 270, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 33, 33, 270]
Prompts retrieved: 44256 . Total input tokens: 9794058 . Total output tokens: 8729348
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.4751018672250211,
    "estimated_duration": 3599.7620049436373,
    "input_throughput": 1016.6355428425886,
    "output_throughput": 896.1609116296315,
    "total_throughput": 1912.7964544722201,
    "itl": 25.688192449176604,
    "ttft": 6764.428008953468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5951,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.82287278809064,
    "arrivals": 15003,
    "finished_requests": 14975,
    "scheduler_time": 1.943639629815115e-06
}
#Debug simulation 
Total elapsed time: 1.475194183178246. Arrivals time: 0.04709356231614947 Scheduler time: 1.0672862934879959 Scheduler overhead time: 0.11928146658465266 Adapter cache time: 0.06302754301577806 Engine time: 0.11788054229691625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 33, 1080, 33, 33, 1080, 270, 1080, 270, 1080, 1080, 270, 33, 1080, 270, 33, 33, 270, 1080, 33, 270, 33, 33, 270, 270, 270, 33, 33, 1080, 270, 270, 33, 1080, 1080, 270, 1080, 1080, 270, 1080, 33, 1080, 270, 270, 270, 1080, 1080, 33, 33, 270, 33, 1080, 33, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 270, 1080, 1080, 33, 33, 1080, 270, 1080, 33, 270, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 33, 33, 270]
Prompts retrieved: 44256 . Total input tokens: 9794058 . Total output tokens: 8729348
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.5112338769249618,
    "estimated_duration": 3599.7567888705976,
    "input_throughput": 1016.6370159546785,
    "output_throughput": 896.1622101731289,
    "total_throughput": 1912.7992261278073,
    "itl": 25.786287381102483,
    "ttft": 6764.562136042969,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5910,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 40.17085345083075,
    "arrivals": 15003,
    "finished_requests": 14975,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.511317831929773. Arrivals time: 0.04755662148818374 Scheduler time: 1.099793441593647 Scheduler overhead time: 0.11968073295429349 Adapter cache time: 0.06563252862542868 Engine time: 0.11916289990767837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 33, 1080, 33, 33, 1080, 270, 1080, 270, 1080, 1080, 270, 33, 1080, 270, 33, 33, 270, 1080, 33, 270, 33, 33, 270, 270, 270, 33, 33, 1080, 270, 270, 33, 1080, 1080, 270, 1080, 1080, 270, 1080, 33, 1080, 270, 270, 270, 1080, 1080, 33, 33, 270, 33, 1080, 33, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 270, 1080, 1080, 33, 33, 1080, 270, 1080, 33, 270, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 33, 33, 270]
Prompts retrieved: 44256 . Total input tokens: 9794058 . Total output tokens: 8729348
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.509903300087899,
    "estimated_duration": 3599.7599945156603,
    "input_throughput": 1016.6361106228131,
    "output_throughput": 896.1614121260455,
    "total_throughput": 1912.7975227488587,
    "itl": 25.680273600965947,
    "ttft": 6764.357503668955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5952,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.48967126463491,
    "arrivals": 15003,
    "finished_requests": 14975,
    "scheduler_time": 1.943639629815115e-06
}
#Debug simulation 
Total elapsed time: 1.509996334090829. Arrivals time: 0.047406671568751335 Scheduler time: 1.1003876673057675 Scheduler overhead time: 0.11992142256349325 Adapter cache time: 0.06313628004863858 Engine time: 0.11933199781924486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 33, 1080, 33, 33, 1080, 270, 1080, 270, 1080, 1080, 270, 33, 1080, 270, 33, 33, 270, 1080, 33, 270, 33, 33, 270, 270, 270, 33, 33, 1080, 270, 270, 33, 1080, 1080, 270, 1080, 1080, 270, 1080, 33, 1080, 270, 270, 270, 1080, 1080, 33, 33, 270, 33, 1080, 33, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 270, 1080, 1080, 33, 33, 1080, 270, 1080, 33, 270, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 33, 33, 270]
Prompts retrieved: 44256 . Total input tokens: 9794058 . Total output tokens: 8729348
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.506580798421055,
    "estimated_duration": 3599.7519543710623,
    "input_throughput": 1016.6383813074149,
    "output_throughput": 896.163413727108,
    "total_throughput": 1912.801795034523,
    "itl": 25.628752566251116,
    "ttft": 6763.868692152578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5952,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.99709743827977,
    "arrivals": 15003,
    "finished_requests": 14975,
    "scheduler_time": 1.943639629815115e-06
}
#Debug simulation 
Total elapsed time: 1.5066664251498878. Arrivals time: 0.047341372817754745 Scheduler time: 1.097878456581384 Scheduler overhead time: 0.11951993219554424 Adapter cache time: 0.06329660397022963 Engine time: 0.11967418482527137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 33, 1080, 33, 33, 1080, 270, 1080, 270, 1080, 1080, 270, 33, 1080, 270, 33, 33, 270, 1080, 33, 270, 33, 33, 270, 270, 270, 33, 33, 1080, 270, 270, 33, 1080, 1080, 270, 1080, 1080, 270, 1080, 33, 1080, 270, 270, 270, 1080, 1080, 33, 33, 270, 33, 1080, 33, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 270, 1080, 1080, 33, 33, 1080, 270, 1080, 33, 270, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 33, 33, 270]
Prompts retrieved: 44256 . Total input tokens: 9794058 . Total output tokens: 8729348
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.4975546719506383,
    "estimated_duration": 3599.764509761076,
    "input_throughput": 1016.6348354389711,
    "output_throughput": 896.1602880556522,
    "total_throughput": 1912.7951234946233,
    "itl": 25.676558368233426,
    "ttft": 6764.397530057543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5949,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.08672946644904,
    "arrivals": 15003,
    "finished_requests": 14975,
    "scheduler_time": 1.943639629815115e-06
}
#Debug simulation 
Total elapsed time: 1.497643091250211. Arrivals time: 0.0474390322342515 Scheduler time: 1.088419010862708 Scheduler overhead time: 0.11990773910656571 Adapter cache time: 0.06353091960772872 Engine time: 0.11910359095782042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 66, 1080, 66, 66, 1080, 135, 1080, 135, 1080, 1080, 135, 66, 1080, 135, 66, 66, 135, 1080, 66, 135, 66, 66, 135, 135, 135, 66, 66, 1080, 135, 135, 66, 1080, 1080, 135, 1080, 1080, 135, 1080, 66, 1080, 135, 135, 135, 1080, 1080, 66, 66, 135, 66, 1080, 66, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 135, 1080, 1080, 66, 66, 1080, 135, 1080, 66, 135, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 66, 66, 135]
Prompts retrieved: 40992 . Total input tokens: 9071342 . Total output tokens: 8100246
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.3881193213164806,
    "estimated_duration": 3599.944312262604,
    "input_throughput": 938.8234113754428,
    "output_throughput": 833.6806738306768,
    "total_throughput": 1772.5040852061195,
    "itl": 25.09258024936716,
    "ttft": 5224.963435994787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4934,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 32.62562616363785,
    "arrivals": 13896,
    "finished_requests": 13876,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3881987063214183. Arrivals time: 0.04487076587975025 Scheduler time: 0.9817886669188738 Scheduler overhead time: 0.12195704551413655 Adapter cache time: 0.05856509506702423 Engine time: 0.12098648166283965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 66, 1080, 66, 66, 1080, 135, 1080, 135, 1080, 1080, 135, 66, 1080, 135, 66, 66, 135, 1080, 66, 135, 66, 66, 135, 135, 135, 66, 66, 1080, 135, 135, 66, 1080, 1080, 135, 1080, 1080, 135, 1080, 66, 1080, 135, 135, 135, 1080, 1080, 66, 66, 135, 66, 1080, 66, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 135, 1080, 1080, 66, 66, 1080, 135, 1080, 66, 135, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 66, 66, 135]
Prompts retrieved: 40992 . Total input tokens: 9071342 . Total output tokens: 8100246
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.4031581557355821,
    "estimated_duration": 3599.9290023733624,
    "input_throughput": 938.8274040326413,
    "output_throughput": 833.6842193335939,
    "total_throughput": 1772.5116233662352,
    "itl": 25.12557234473334,
    "ttft": 5225.384811030324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4935,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.630442574400746,
    "arrivals": 13896,
    "finished_requests": 13876,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4032352287322283. Arrivals time: 0.044974920339882374 Scheduler time: 0.9950563283637166 Scheduler overhead time: 0.1224126536399126 Adapter cache time: 0.05870037991553545 Engine time: 0.12168372748419642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 66, 1080, 66, 66, 1080, 135, 1080, 135, 1080, 1080, 135, 66, 1080, 135, 66, 66, 135, 1080, 66, 135, 66, 66, 135, 135, 135, 66, 66, 1080, 135, 135, 66, 1080, 1080, 135, 1080, 1080, 135, 1080, 66, 1080, 135, 135, 135, 1080, 1080, 66, 66, 135, 66, 1080, 66, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 135, 1080, 1080, 66, 66, 1080, 135, 1080, 66, 135, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 66, 66, 135]
Prompts retrieved: 40992 . Total input tokens: 9071342 . Total output tokens: 8100246
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.411928768735379,
    "estimated_duration": 3599.9510099400773,
    "input_throughput": 938.8216647026696,
    "output_throughput": 833.6791227750504,
    "total_throughput": 1772.50078747772,
    "itl": 25.129074051158117,
    "ttft": 5225.41764788413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4933,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 36.49000941888969,
    "arrivals": 13896,
    "finished_requests": 13876,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4120236728340387. Arrivals time: 0.044694297946989536 Scheduler time: 1.0072790775448084 Scheduler overhead time: 0.12180187925696373 Adapter cache time: 0.05914388131350279 Engine time: 0.11914290813729167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 66, 1080, 66, 66, 1080, 135, 1080, 135, 1080, 1080, 135, 66, 1080, 135, 66, 66, 135, 1080, 66, 135, 66, 66, 135, 135, 135, 66, 66, 1080, 135, 135, 66, 1080, 1080, 135, 1080, 1080, 135, 1080, 66, 1080, 135, 135, 135, 1080, 1080, 66, 66, 135, 66, 1080, 66, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 135, 1080, 1080, 66, 66, 1080, 135, 1080, 66, 135, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 66, 66, 135]
Prompts retrieved: 40992 . Total input tokens: 9071342 . Total output tokens: 8100246
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.3859378220513463,
    "estimated_duration": 3599.9419883627916,
    "input_throughput": 938.8240174217503,
    "output_throughput": 833.6812120033384,
    "total_throughput": 1772.5052294250888,
    "itl": 25.106153955368622,
    "ttft": 5225.103496705592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.60642587299579,
    "arrivals": 13896,
    "finished_requests": 13876,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.386012643110007. Arrivals time: 0.044154281727969646 Scheduler time: 0.9819426224566996 Scheduler overhead time: 0.12266305321827531 Adapter cache time: 0.05864184722304344 Engine time: 0.11850971588864923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 66, 1080, 66, 66, 1080, 135, 1080, 135, 1080, 1080, 135, 66, 1080, 135, 66, 66, 135, 1080, 66, 135, 66, 66, 135, 135, 135, 66, 66, 1080, 135, 135, 66, 1080, 1080, 135, 1080, 1080, 135, 1080, 66, 1080, 135, 135, 135, 1080, 1080, 66, 66, 135, 66, 1080, 66, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 135, 1080, 1080, 66, 66, 1080, 135, 1080, 66, 135, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 66, 66, 135]
Prompts retrieved: 40992 . Total input tokens: 9071342 . Total output tokens: 8100246
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.396741135045886,
    "estimated_duration": 3599.942545270886,
    "input_throughput": 938.8238721864617,
    "output_throughput": 833.6810830335536,
    "total_throughput": 1772.5049552200153,
    "itl": 25.1294753569344,
    "ttft": 5225.473162698526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 36.184720197073986,
    "arrivals": 13896,
    "finished_requests": 13876,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.396818382665515. Arrivals time: 0.04476761259138584 Scheduler time: 0.9913785825483501 Scheduler overhead time: 0.12206547753885388 Adapter cache time: 0.058765868190675974 Engine time: 0.11961701232939959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 66, 1080, 66, 66, 1080, 135, 1080, 135, 1080, 1080, 135, 66, 1080, 135, 66, 66, 135, 1080, 66, 135, 66, 66, 135, 135, 135, 66, 66, 1080, 135, 135, 66, 1080, 1080, 135, 1080, 1080, 135, 1080, 66, 1080, 135, 135, 135, 1080, 1080, 66, 66, 135, 66, 1080, 66, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 135, 1080, 1080, 66, 66, 1080, 135, 1080, 66, 135, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 66, 66, 135]
Prompts retrieved: 40992 . Total input tokens: 9071342 . Total output tokens: 8100246
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.4035540684126318,
    "estimated_duration": 3599.935507153818,
    "input_throughput": 938.8257076505431,
    "output_throughput": 833.6827129363805,
    "total_throughput": 1772.5084205869236,
    "itl": 25.085491464836217,
    "ttft": 5224.889385129152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.47273023701582,
    "arrivals": 13896,
    "finished_requests": 13876,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4036744330078363. Arrivals time: 0.04504466336220503 Scheduler time: 0.9933575363829732 Scheduler overhead time: 0.1211282261647284 Adapter cache time: 0.058827118948102 Engine time: 0.12266131909564137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.1-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 66, 1080, 66, 66, 1080, 135, 1080, 135, 1080, 1080, 135, 66, 1080, 135, 66, 66, 135, 1080, 66, 135, 66, 66, 135, 135, 135, 66, 66, 1080, 135, 135, 66, 1080, 1080, 135, 1080, 1080, 135, 1080, 66, 1080, 135, 135, 135, 1080, 1080, 66, 66, 135, 66, 1080, 66, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 135, 1080, 1080, 66, 66, 1080, 135, 1080, 66, 135, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 66, 66, 135]
Prompts retrieved: 40992 . Total input tokens: 9071342 . Total output tokens: 8100246
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.4209233578294516,
    "estimated_duration": 3599.929324735873,
    "input_throughput": 938.8273199635579,
    "output_throughput": 833.6841446797566,
    "total_throughput": 1772.5114646433144,
    "itl": 25.12498342789395,
    "ttft": 5225.285285221652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.86638109661725,
    "arrivals": 13896,
    "finished_requests": 13876,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4209962389431894. Arrivals time: 0.04487305646762252 Scheduler time: 1.011817695107311 Scheduler overhead time: 0.12263202108442783 Adapter cache time: 0.05950092617422342 Engine time: 0.12112977216020226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 33, 1080, 33, 33, 1080, 135, 1080, 135, 1080, 1080, 135, 33, 1080, 135, 33, 33, 135, 1080, 33, 135, 33, 33, 135, 135, 135, 33, 33, 1080, 135, 135, 33, 1080, 1080, 135, 1080, 1080, 135, 1080, 33, 1080, 135, 135, 135, 1080, 1080, 33, 33, 135, 33, 1080, 33, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 135, 1080, 1080, 33, 33, 1080, 135, 1080, 33, 135, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 33, 33, 135]
Prompts retrieved: 39936 . Total input tokens: 8842285 . Total output tokens: 7885916
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.368682415690273,
    "estimated_duration": 3600.0050131820144,
    "input_throughput": 932.3309239042725,
    "output_throughput": 805.7519335052497,
    "total_throughput": 1738.0828574095221,
    "itl": 24.64942583021135,
    "ttft": 6163.895269805052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.58697048129285,
    "arrivals": 13527,
    "finished_requests": 13504,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3687452767044306. Arrivals time: 0.04371375869959593 Scheduler time: 0.9637730256654322 Scheduler overhead time: 0.1226101964712143 Adapter cache time: 0.056293980684131384 Engine time: 0.12161432346329093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 33, 1080, 33, 33, 1080, 135, 1080, 135, 1080, 1080, 135, 33, 1080, 135, 33, 33, 135, 1080, 33, 135, 33, 33, 135, 135, 135, 33, 33, 1080, 135, 135, 33, 1080, 1080, 135, 1080, 1080, 135, 1080, 33, 1080, 135, 135, 135, 1080, 1080, 33, 33, 135, 33, 1080, 33, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 135, 1080, 1080, 33, 33, 1080, 135, 1080, 33, 135, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 33, 33, 135]
Prompts retrieved: 39936 . Total input tokens: 8842285 . Total output tokens: 7885916
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.3633056310936809,
    "estimated_duration": 3599.990974879936,
    "input_throughput": 932.3345595642611,
    "output_throughput": 805.7550755656387,
    "total_throughput": 1738.0896351298998,
    "itl": 24.67108894922762,
    "ttft": 6164.085612624973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.053189752134028,
    "arrivals": 13527,
    "finished_requests": 13504,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3633657810278237. Arrivals time: 0.04346361476927996 Scheduler time: 0.9560348279774189 Scheduler overhead time: 0.1234968937933445 Adapter cache time: 0.056449204217642546 Engine time: 0.12241387227550149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 33, 1080, 33, 33, 1080, 135, 1080, 135, 1080, 1080, 135, 33, 1080, 135, 33, 33, 135, 1080, 33, 135, 33, 33, 135, 135, 135, 33, 33, 1080, 135, 135, 33, 1080, 1080, 135, 1080, 1080, 135, 1080, 33, 1080, 135, 135, 135, 1080, 1080, 33, 33, 135, 33, 1080, 33, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 135, 1080, 1080, 33, 33, 1080, 135, 1080, 33, 135, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 33, 33, 135]
Prompts retrieved: 39936 . Total input tokens: 8842285 . Total output tokens: 7885916
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.3801977587863803,
    "estimated_duration": 3599.9882881265157,
    "input_throughput": 932.3352553868211,
    "output_throughput": 805.7556769190409,
    "total_throughput": 1738.090932305862,
    "itl": 24.67968114335848,
    "ttft": 6164.310642132708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.806736155866172,
    "arrivals": 13527,
    "finished_requests": 13504,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.38027601595968. Arrivals time: 0.04360351338982582 Scheduler time: 0.9751564422622323 Scheduler overhead time: 0.12236436177045107 Adapter cache time: 0.056276360992342234 Engine time: 0.1222659982740879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 33, 1080, 33, 33, 1080, 135, 1080, 135, 1080, 1080, 135, 33, 1080, 135, 33, 33, 135, 1080, 33, 135, 33, 33, 135, 135, 135, 33, 33, 1080, 135, 135, 33, 1080, 1080, 135, 1080, 1080, 135, 1080, 33, 1080, 135, 135, 135, 1080, 1080, 33, 33, 135, 33, 1080, 33, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 135, 1080, 1080, 33, 33, 1080, 135, 1080, 33, 135, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 33, 33, 135]
Prompts retrieved: 39936 . Total input tokens: 8842285 . Total output tokens: 7885916
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.3686729250475764,
    "estimated_duration": 3599.999609019424,
    "input_throughput": 932.3323234788413,
    "output_throughput": 805.7531430649522,
    "total_throughput": 1738.0854665437935,
    "itl": 24.659327629481886,
    "ttft": 6163.981327394906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.402037830940493,
    "arrivals": 13527,
    "finished_requests": 13504,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3687583319842815. Arrivals time: 0.04364439379423857 Scheduler time: 0.9651963873766363 Scheduler overhead time: 0.12365812994539738 Adapter cache time: 0.056133586913347244 Engine time: 0.11917654424905777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 33, 1080, 33, 33, 1080, 135, 1080, 135, 1080, 1080, 135, 33, 1080, 135, 33, 33, 135, 1080, 33, 135, 33, 33, 135, 135, 135, 33, 33, 1080, 135, 135, 33, 1080, 1080, 135, 1080, 1080, 135, 1080, 33, 1080, 135, 135, 135, 1080, 1080, 33, 33, 135, 33, 1080, 33, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 135, 1080, 1080, 33, 33, 1080, 135, 1080, 33, 135, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 33, 33, 135]
Prompts retrieved: 39936 . Total input tokens: 8842285 . Total output tokens: 7885916
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.3602522159926593,
    "estimated_duration": 3599.9804147542136,
    "input_throughput": 932.3372944597411,
    "output_throughput": 805.7574391548584,
    "total_throughput": 1738.0947336145994,
    "itl": 24.67916349099239,
    "ttft": 6164.315695261256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.543286100240866,
    "arrivals": 13527,
    "finished_requests": 13504,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3603232959285378. Arrivals time: 0.04345309687778354 Scheduler time: 0.9558945060707629 Scheduler overhead time: 0.12242074264213443 Adapter cache time: 0.05604212312027812 Engine time: 0.12189202709123492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 33, 1080, 33, 33, 1080, 135, 1080, 135, 1080, 1080, 135, 33, 1080, 135, 33, 33, 135, 1080, 33, 135, 33, 33, 135, 135, 135, 33, 33, 1080, 135, 135, 33, 1080, 1080, 135, 1080, 1080, 135, 1080, 33, 1080, 135, 135, 135, 1080, 1080, 33, 33, 135, 33, 1080, 33, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 135, 1080, 1080, 33, 33, 1080, 135, 1080, 33, 135, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 33, 33, 135]
Prompts retrieved: 39936 . Total input tokens: 8842285 . Total output tokens: 7885916
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.362517329864204,
    "estimated_duration": 3599.982135235904,
    "input_throughput": 932.3368488827398,
    "output_throughput": 805.757054072136,
    "total_throughput": 1738.0939029548758,
    "itl": 24.64222722448294,
    "ttft": 6163.930277118417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.608182480300425,
    "arrivals": 13527,
    "finished_requests": 13504,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.362591716926545. Arrivals time: 0.043758781626820564 Scheduler time: 0.9555778931826353 Scheduler overhead time: 0.12363996775820851 Adapter cache time: 0.05623004771769047 Engine time: 0.12249554134905338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 1080, 135, 135, 135, 135, 1080, 135, 1080, 135, 33, 1080, 33, 33, 1080, 135, 1080, 135, 1080, 1080, 135, 33, 1080, 135, 33, 33, 135, 1080, 33, 135, 33, 33, 135, 135, 135, 33, 33, 1080, 135, 135, 33, 1080, 1080, 135, 1080, 1080, 135, 1080, 33, 1080, 135, 135, 135, 1080, 1080, 33, 33, 135, 33, 1080, 33, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 135, 1080, 1080, 33, 33, 1080, 135, 1080, 33, 135, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 33, 33, 135]
Prompts retrieved: 39936 . Total input tokens: 8842285 . Total output tokens: 7885916
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.3633254179731011,
    "estimated_duration": 3600.0019528491293,
    "input_throughput": 932.3317164713387,
    "output_throughput": 805.7526184685279,
    "total_throughput": 1738.0843349398667,
    "itl": 24.674433487520954,
    "ttft": 6164.141606375209,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.274190874101084,
    "arrivals": 13527,
    "finished_requests": 13504,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.363403205294162. Arrivals time: 0.04377964977174997 Scheduler time: 0.9578916965983808 Scheduler overhead time: 0.12325397692620754 Adapter cache time: 0.05622617527842522 Engine time: 0.12150819879025221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 1080, 66, 66, 66, 66, 1080, 66, 1080, 66, 33, 1080, 33, 33, 1080, 66, 1080, 66, 1080, 1080, 66, 33, 1080, 66, 33, 33, 66, 1080, 33, 66, 33, 33, 66, 66, 66, 33, 33, 1080, 66, 66, 33, 1080, 1080, 66, 1080, 1080, 66, 1080, 33, 1080, 66, 66, 66, 1080, 1080, 33, 33, 66, 33, 1080, 33, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 66, 1080, 1080, 33, 33, 1080, 66, 1080, 33, 66, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 33, 33, 66]
Prompts retrieved: 37728 . Total input tokens: 8351156 . Total output tokens: 7442954
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.3070851140655577,
    "estimated_duration": 3599.8337117287947,
    "input_throughput": 874.9979171919329,
    "output_throughput": 757.860006452858,
    "total_throughput": 1632.8579236447908,
    "itl": 24.247511662492663,
    "ttft": 5698.214218395389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2879,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.0371256029836,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3071526931598783. Arrivals time: 0.04142879555001855 Scheduler time: 0.9055655444972217 Scheduler overhead time: 0.12469070125371218 Adapter cache time: 0.052076703403145075 Engine time: 0.12159387208521366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 1080, 66, 66, 66, 66, 1080, 66, 1080, 66, 33, 1080, 33, 33, 1080, 66, 1080, 66, 1080, 1080, 66, 33, 1080, 66, 33, 33, 66, 1080, 33, 66, 33, 33, 66, 66, 66, 33, 33, 1080, 66, 66, 33, 1080, 1080, 66, 1080, 1080, 66, 1080, 33, 1080, 66, 66, 66, 1080, 1080, 33, 33, 66, 33, 1080, 33, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 66, 1080, 1080, 33, 33, 1080, 66, 1080, 33, 66, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 33, 33, 66]
Prompts retrieved: 37728 . Total input tokens: 8351156 . Total output tokens: 7442954
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.3033736729994416,
    "estimated_duration": 3599.8469120737227,
    "input_throughput": 874.9947086459584,
    "output_throughput": 757.8572274420454,
    "total_throughput": 1632.8519360880039,
    "itl": 24.263785435043665,
    "ttft": 5698.368762749524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2878,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.823134484858834,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3034472409635782. Arrivals time: 0.04124309029430151 Scheduler time: 0.9013959658332169 Scheduler overhead time: 0.12406766694039106 Adapter cache time: 0.052055610809475183 Engine time: 0.12290383316576481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 1080, 66, 66, 66, 66, 1080, 66, 1080, 66, 33, 1080, 33, 33, 1080, 66, 1080, 66, 1080, 1080, 66, 33, 1080, 66, 33, 33, 66, 1080, 33, 66, 33, 33, 66, 66, 66, 33, 33, 1080, 66, 66, 33, 1080, 1080, 66, 1080, 1080, 66, 1080, 33, 1080, 66, 66, 66, 1080, 1080, 33, 33, 66, 33, 1080, 33, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 66, 1080, 1080, 33, 33, 1080, 66, 1080, 33, 66, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 33, 33, 66]
Prompts retrieved: 37728 . Total input tokens: 8351156 . Total output tokens: 7442954
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.3093412988819182,
    "estimated_duration": 3599.8453792374785,
    "input_throughput": 874.9950812240726,
    "output_throughput": 757.8575501423015,
    "total_throughput": 1632.8526313663742,
    "itl": 24.26894271408994,
    "ttft": 5698.424126591289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2882,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.37163313932748,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3094235239550471. Arrivals time: 0.041507750283926725 Scheduler time: 0.9029558645561337 Scheduler overhead time: 0.12685527931898832 Adapter cache time: 0.05195728316903114 Engine time: 0.12376614427193999 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 1080, 66, 66, 66, 66, 1080, 66, 1080, 66, 33, 1080, 33, 33, 1080, 66, 1080, 66, 1080, 1080, 66, 33, 1080, 66, 33, 33, 66, 1080, 33, 66, 33, 33, 66, 66, 66, 33, 33, 1080, 66, 66, 33, 1080, 1080, 66, 1080, 1080, 66, 1080, 33, 1080, 66, 66, 66, 1080, 1080, 33, 33, 66, 33, 1080, 33, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 66, 1080, 1080, 33, 33, 1080, 66, 1080, 33, 66, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 33, 33, 66]
Prompts retrieved: 37728 . Total input tokens: 8351156 . Total output tokens: 7442954
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.3014344861730933,
    "estimated_duration": 3599.8412064753675,
    "input_throughput": 874.9960954761223,
    "output_throughput": 757.8584286141811,
    "total_throughput": 1632.8545240903034,
    "itl": 24.253112830446856,
    "ttft": 5698.196808623516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2879,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.666189230164083,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3015112597495317. Arrivals time: 0.041467360220849514 Scheduler time: 0.8989363382570446 Scheduler overhead time: 0.12465910194441676 Adapter cache time: 0.05217090202495456 Engine time: 0.12248098896816373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 1080, 66, 66, 66, 66, 1080, 66, 1080, 66, 33, 1080, 33, 33, 1080, 66, 1080, 66, 1080, 1080, 66, 33, 1080, 66, 33, 33, 66, 1080, 33, 66, 33, 33, 66, 66, 66, 33, 33, 1080, 66, 66, 33, 1080, 1080, 66, 1080, 1080, 66, 1080, 33, 1080, 66, 66, 66, 1080, 1080, 33, 33, 66, 33, 1080, 33, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 66, 1080, 1080, 33, 33, 1080, 66, 1080, 33, 66, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 33, 33, 66]
Prompts retrieved: 37728 . Total input tokens: 8351156 . Total output tokens: 7442954
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.3003043252974749,
    "estimated_duration": 3599.8287674747853,
    "input_throughput": 874.9991189746396,
    "output_throughput": 757.8610473502499,
    "total_throughput": 1632.8601663248896,
    "itl": 24.265354429773186,
    "ttft": 5698.393727949358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2880,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.181916798577003,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.300378831103444. Arrivals time: 0.04219313897192478 Scheduler time: 0.898485607933253 Scheduler overhead time: 0.12480202177539468 Adapter cache time: 0.051791489124298096 Engine time: 0.12137069273740053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 1080, 66, 66, 66, 66, 1080, 66, 1080, 66, 33, 1080, 33, 33, 1080, 66, 1080, 66, 1080, 1080, 66, 33, 1080, 66, 33, 33, 66, 1080, 33, 66, 33, 33, 66, 66, 66, 33, 33, 1080, 66, 66, 33, 1080, 1080, 66, 1080, 1080, 66, 1080, 33, 1080, 66, 66, 66, 1080, 1080, 33, 33, 66, 33, 1080, 33, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 66, 1080, 1080, 33, 33, 1080, 66, 1080, 33, 66, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 33, 33, 66]
Prompts retrieved: 37728 . Total input tokens: 8351156 . Total output tokens: 7442954
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.3148841150105,
    "estimated_duration": 3599.827738147984,
    "input_throughput": 874.9993691699573,
    "output_throughput": 757.8612640513658,
    "total_throughput": 1632.860633221323,
    "itl": 24.241579769195916,
    "ttft": 5698.246853246998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.360156625082194,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.315029283054173. Arrivals time: 0.041840741876512766 Scheduler time: 0.9113382706418633 Scheduler overhead time: 0.12575747817754745 Adapter cache time: 0.05212690867483616 Engine time: 0.12191323656588793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 1080, 66, 66, 66, 66, 1080, 66, 1080, 66, 33, 1080, 33, 33, 1080, 66, 1080, 66, 1080, 1080, 66, 33, 1080, 66, 33, 33, 66, 1080, 33, 66, 33, 33, 66, 66, 66, 33, 33, 1080, 66, 66, 33, 1080, 1080, 66, 1080, 1080, 66, 1080, 33, 1080, 66, 66, 66, 1080, 1080, 33, 33, 66, 33, 1080, 33, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 66, 1080, 1080, 33, 33, 1080, 66, 1080, 33, 66, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 33, 33, 66]
Prompts retrieved: 37728 . Total input tokens: 8351156 . Total output tokens: 7442954
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.317878970876336,
    "estimated_duration": 3599.847735884183,
    "input_throughput": 874.9945084069909,
    "output_throughput": 757.8570540095124,
    "total_throughput": 1632.8515624165034,
    "itl": 24.26488387901101,
    "ttft": 5698.494512181573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2881,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.997581014875593,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.317990896757692. Arrivals time: 0.04192114155739546 Scheduler time: 0.9122373103164136 Scheduler overhead time: 0.12684232695028186 Adapter cache time: 0.0524307731539011 Engine time: 0.12207526573911309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 135, 540, 135, 135, 540, 270, 540, 270, 540, 540, 270, 135, 540, 270, 135, 135, 270, 540, 135, 270, 135, 135, 270, 270, 270, 135, 135, 540, 270, 270, 135, 540, 540, 270, 540, 540, 270, 540, 135, 540, 270, 270, 270, 540, 540, 135, 135, 270, 135, 540, 135, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 270, 540, 540, 135, 135, 540, 270, 540, 135, 270, 540, 135, 540, 270, 135, 540, 135, 135, 540, 135, 135, 270]
Prompts retrieved: 30240 . Total input tokens: 6689008 . Total output tokens: 5959246
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.144430042244494,
    "estimated_duration": 3599.8891977008934,
    "input_throughput": 689.4509424304284,
    "output_throughput": 611.8271644045756,
    "total_throughput": 1301.2781068350039,
    "itl": 23.627117619549804,
    "ttft": 5703.947643721901,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5948,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 39.3306089220371,
    "arrivals": 10175,
    "finished_requests": 10159,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.144484519958496. Arrivals time: 0.035994790494441986 Scheduler time: 0.7403313647955656 Scheduler overhead time: 0.12563044019043446 Adapter cache time: 0.056978467386215925 Engine time: 0.12284357659518719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 135, 540, 135, 135, 540, 270, 540, 270, 540, 540, 270, 135, 540, 270, 135, 135, 270, 540, 135, 270, 135, 135, 270, 270, 270, 135, 135, 540, 270, 270, 135, 540, 540, 270, 540, 540, 270, 540, 135, 540, 270, 270, 270, 540, 540, 135, 135, 270, 135, 540, 135, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 270, 540, 540, 135, 135, 540, 270, 540, 135, 270, 540, 135, 540, 270, 135, 540, 135, 135, 540, 135, 135, 270]
Prompts retrieved: 30240 . Total input tokens: 6689008 . Total output tokens: 5959246
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.1465519978664815,
    "estimated_duration": 3599.9002590253936,
    "input_throughput": 689.4488239715678,
    "output_throughput": 611.825284458378,
    "total_throughput": 1301.2741084299457,
    "itl": 23.65962932116179,
    "ttft": 5704.322663742135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5952,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.18765565749151,
    "arrivals": 10175,
    "finished_requests": 10159,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1466284417547286. Arrivals time: 0.03584708785638213 Scheduler time: 0.7412327118217945 Scheduler overhead time: 0.1255605136975646 Adapter cache time: 0.057384326588362455 Engine time: 0.12330029672011733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 135, 540, 135, 135, 540, 270, 540, 270, 540, 540, 270, 135, 540, 270, 135, 135, 270, 540, 135, 270, 135, 135, 270, 270, 270, 135, 135, 540, 270, 270, 135, 540, 540, 270, 540, 540, 270, 540, 135, 540, 270, 270, 270, 540, 540, 135, 135, 270, 135, 540, 135, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 270, 540, 540, 135, 135, 540, 270, 540, 135, 270, 540, 135, 540, 270, 135, 540, 135, 135, 540, 135, 135, 270]
Prompts retrieved: 30240 . Total input tokens: 6689008 . Total output tokens: 5959246
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.142445211764425,
    "estimated_duration": 3599.882859783158,
    "input_throughput": 689.4521562708577,
    "output_throughput": 611.828241581358,
    "total_throughput": 1301.2803978522159,
    "itl": 23.88053618700625,
    "ttft": 5704.689725010121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6003,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.63072765509512,
    "arrivals": 10175,
    "finished_requests": 10159,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1425183047540486. Arrivals time: 0.03563092742115259 Scheduler time: 0.7423521121963859 Scheduler overhead time: 0.12402656069025397 Adapter cache time: 0.05697245988994837 Engine time: 0.12157385190948844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 135, 540, 135, 135, 540, 270, 540, 270, 540, 540, 270, 135, 540, 270, 135, 135, 270, 540, 135, 270, 135, 135, 270, 270, 270, 135, 135, 540, 270, 270, 135, 540, 540, 270, 540, 540, 270, 540, 135, 540, 270, 270, 270, 540, 540, 135, 135, 270, 135, 540, 135, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 270, 540, 540, 135, 135, 540, 270, 540, 135, 270, 540, 135, 540, 270, 135, 540, 135, 135, 540, 135, 135, 270]
Prompts retrieved: 30240 . Total input tokens: 6689008 . Total output tokens: 5959246
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.1515400791540742,
    "estimated_duration": 3599.8910256174154,
    "input_throughput": 689.450592347951,
    "output_throughput": 611.8268537371207,
    "total_throughput": 1301.2774460850717,
    "itl": 23.636811270743948,
    "ttft": 5704.167184213105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5948,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 40.63139313938238,
    "arrivals": 10175,
    "finished_requests": 10159,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1515954583883286. Arrivals time: 0.03614014899358153 Scheduler time: 0.7474864264950156 Scheduler overhead time: 0.12518837675452232 Adapter cache time: 0.05709898192435503 Engine time: 0.123211192432791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 135, 540, 135, 135, 540, 270, 540, 270, 540, 540, 270, 135, 540, 270, 135, 135, 270, 540, 135, 270, 135, 135, 270, 270, 270, 135, 135, 540, 270, 270, 135, 540, 540, 270, 540, 540, 270, 540, 135, 540, 270, 270, 270, 540, 540, 135, 135, 270, 135, 540, 135, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 270, 540, 540, 135, 135, 540, 270, 540, 135, 270, 540, 135, 540, 270, 135, 540, 135, 135, 540, 135, 135, 270]
Prompts retrieved: 30240 . Total input tokens: 6689008 . Total output tokens: 5959246
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.1602430348284543,
    "estimated_duration": 3599.8958173527635,
    "input_throughput": 689.4496746367333,
    "output_throughput": 611.8260393490077,
    "total_throughput": 1301.275713985741,
    "itl": 23.879076908393955,
    "ttft": 5704.684161848724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6005,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.275984807507264,
    "arrivals": 10175,
    "finished_requests": 10159,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1603092742152512. Arrivals time: 0.03629699582234025 Scheduler time: 0.7540382901206613 Scheduler overhead time: 0.12448398442938924 Adapter cache time: 0.05774916894733906 Engine time: 0.12396974628791213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 135, 540, 135, 135, 540, 270, 540, 270, 540, 540, 270, 135, 540, 270, 135, 135, 270, 540, 135, 270, 135, 135, 270, 270, 270, 135, 135, 540, 270, 270, 135, 540, 540, 270, 540, 540, 270, 540, 135, 540, 270, 270, 270, 540, 540, 135, 135, 270, 135, 540, 135, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 270, 540, 540, 135, 135, 540, 270, 540, 135, 270, 540, 135, 540, 270, 135, 540, 135, 135, 540, 135, 135, 270]
Prompts retrieved: 30240 . Total input tokens: 6689008 . Total output tokens: 5959246
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.1612284602597356,
    "estimated_duration": 3599.8950227334667,
    "input_throughput": 689.449826821731,
    "output_throughput": 611.8261743998283,
    "total_throughput": 1301.2760012215592,
    "itl": 23.61517148048352,
    "ttft": 5703.6641532000685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5948,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.97156175451749,
    "arrivals": 10175,
    "finished_requests": 10159,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.161305817309767. Arrivals time: 0.03635471919551492 Scheduler time: 0.7527669710107148 Scheduler overhead time: 0.12586709996685386 Adapter cache time: 0.057958771008998156 Engine time: 0.12514909775927663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.05-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 135, 540, 135, 135, 540, 270, 540, 270, 540, 540, 270, 135, 540, 270, 135, 135, 270, 540, 135, 270, 135, 135, 270, 270, 270, 135, 135, 540, 270, 270, 135, 540, 540, 270, 540, 540, 270, 540, 135, 540, 270, 270, 270, 540, 540, 135, 135, 270, 135, 540, 135, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 270, 540, 540, 135, 135, 540, 270, 540, 135, 270, 540, 135, 540, 270, 135, 540, 135, 135, 540, 135, 135, 270]
Prompts retrieved: 30240 . Total input tokens: 6689008 . Total output tokens: 5959246
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.1637647161260247,
    "estimated_duration": 3599.887359373761,
    "input_throughput": 689.4512945071041,
    "output_throughput": 611.827476841706,
    "total_throughput": 1301.27877134881,
    "itl": 23.660158622877198,
    "ttft": 5704.264598796683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5952,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.51278904545987,
    "arrivals": 10175,
    "finished_requests": 10159,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1638305513188243. Arrivals time: 0.03641415620222688 Scheduler time: 0.7560122404247522 Scheduler overhead time: 0.12619394483044744 Adapter cache time: 0.05768760573118925 Engine time: 0.12484697252511978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 66, 540, 66, 66, 540, 270, 540, 270, 540, 540, 270, 66, 540, 270, 66, 66, 270, 540, 66, 270, 66, 66, 270, 270, 270, 66, 66, 540, 270, 270, 66, 540, 540, 270, 540, 540, 270, 540, 66, 540, 270, 270, 270, 540, 540, 66, 66, 270, 66, 540, 66, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 270, 540, 540, 66, 66, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 540, 66, 66, 540, 66, 66, 270]
Prompts retrieved: 28032 . Total input tokens: 6199612 . Total output tokens: 5515691
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.1387111032381654,
    "estimated_duration": 3600.0065635764295,
    "input_throughput": 648.1485405076436,
    "output_throughput": 576.8697815753052,
    "total_throughput": 1225.0183220829488,
    "itl": 23.5115135089526,
    "ttft": 6867.515141513516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5091,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.6637743816543,
    "arrivals": 9494,
    "finished_requests": 9476,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1387843349948525. Arrivals time: 0.03491684375330806 Scheduler time: 0.7302121724933386 Scheduler overhead time: 0.13085483107715845 Adapter cache time: 0.05407864134758711 Engine time: 0.12463444750756025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 66, 540, 66, 66, 540, 270, 540, 270, 540, 540, 270, 66, 540, 270, 66, 66, 270, 540, 66, 270, 66, 66, 270, 270, 270, 66, 66, 540, 270, 270, 66, 540, 540, 270, 540, 540, 270, 540, 66, 540, 270, 270, 270, 540, 540, 66, 66, 270, 66, 540, 66, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 270, 540, 540, 66, 66, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 540, 66, 66, 540, 66, 66, 270]
Prompts retrieved: 28032 . Total input tokens: 6199612 . Total output tokens: 5515691
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.1179305389523506,
    "estimated_duration": 3600.007120347051,
    "input_throughput": 648.1484402661569,
    "output_throughput": 576.8696923576631,
    "total_throughput": 1225.0181326238198,
    "itl": 23.537396281596955,
    "ttft": 6867.889214690297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5093,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 36.82928835635465,
    "arrivals": 9494,
    "finished_requests": 9476,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1180410659871995. Arrivals time: 0.03458905778825283 Scheduler time: 0.7130912216380239 Scheduler overhead time: 0.12758701806887984 Adapter cache time: 0.05425835261121392 Engine time: 0.12550476007163525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 66, 540, 66, 66, 540, 270, 540, 270, 540, 540, 270, 66, 540, 270, 66, 66, 270, 540, 66, 270, 66, 66, 270, 270, 270, 66, 66, 540, 270, 270, 66, 540, 540, 270, 540, 540, 270, 540, 66, 540, 270, 270, 270, 540, 540, 66, 66, 270, 66, 540, 66, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 270, 540, 540, 66, 66, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 540, 66, 66, 540, 66, 66, 270]
Prompts retrieved: 28032 . Total input tokens: 6199612 . Total output tokens: 5515691
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.1103604040108621,
    "estimated_duration": 3599.9959650205806,
    "input_throughput": 648.1504486871447,
    "output_throughput": 576.8714799068192,
    "total_throughput": 1225.021928593964,
    "itl": 23.544016261038564,
    "ttft": 6868.045452357436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5094,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.7509546849742,
    "arrivals": 9494,
    "finished_requests": 9476,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.110421541146934. Arrivals time: 0.034431368578225374 Scheduler time: 0.70848637400195 Scheduler overhead time: 0.12629793537780643 Adapter cache time: 0.05399734899401665 Engine time: 0.12407907703891397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 66, 540, 66, 66, 540, 270, 540, 270, 540, 540, 270, 66, 540, 270, 66, 66, 270, 540, 66, 270, 66, 66, 270, 270, 270, 66, 66, 540, 270, 270, 66, 540, 540, 270, 540, 540, 270, 540, 66, 540, 270, 270, 270, 540, 540, 66, 66, 270, 66, 540, 66, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 270, 540, 540, 66, 66, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 540, 66, 66, 540, 66, 66, 270]
Prompts retrieved: 28032 . Total input tokens: 6199612 . Total output tokens: 5515691
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.1218430530279875,
    "estimated_duration": 3599.9951528071433,
    "input_throughput": 648.1505949197038,
    "output_throughput": 576.8716100577632,
    "total_throughput": 1225.022204977467,
    "itl": 23.519899712107932,
    "ttft": 6867.609595616175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5095,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.75972473826845,
    "arrivals": 9494,
    "finished_requests": 9476,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.121905884705484. Arrivals time: 0.035048266407102346 Scheduler time: 0.7179128979332745 Scheduler overhead time: 0.1269079721532762 Adapter cache time: 0.054188264068216085 Engine time: 0.12458688952028751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 66, 540, 66, 66, 540, 270, 540, 270, 540, 540, 270, 66, 540, 270, 66, 66, 270, 540, 66, 270, 66, 66, 270, 270, 270, 66, 66, 540, 270, 270, 66, 540, 540, 270, 540, 540, 270, 540, 66, 540, 270, 270, 270, 540, 540, 66, 66, 270, 66, 540, 66, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 270, 540, 540, 66, 66, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 540, 66, 66, 540, 66, 66, 270]
Prompts retrieved: 28032 . Total input tokens: 6199612 . Total output tokens: 5515691
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.1127662500366569,
    "estimated_duration": 3600.007327876332,
    "input_throughput": 648.1484029024052,
    "output_throughput": 576.8696591029106,
    "total_throughput": 1225.0180620053159,
    "itl": 23.545490474793706,
    "ttft": 6867.955910646344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5092,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.422625084086896,
    "arrivals": 9494,
    "finished_requests": 9476,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1128412350080907. Arrivals time: 0.03448890522122383 Scheduler time: 0.7082871170714498 Scheduler overhead time: 0.1278698048554361 Adapter cache time: 0.05389422923326492 Engine time: 0.12487306585535407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 66, 540, 66, 66, 540, 270, 540, 270, 540, 540, 270, 66, 540, 270, 66, 66, 270, 540, 66, 270, 66, 66, 270, 270, 270, 66, 66, 540, 270, 270, 66, 540, 540, 270, 540, 540, 270, 540, 66, 540, 270, 270, 270, 540, 540, 66, 66, 270, 66, 540, 66, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 270, 540, 540, 66, 66, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 540, 66, 66, 540, 66, 66, 270]
Prompts retrieved: 28032 . Total input tokens: 6199612 . Total output tokens: 5515691
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.1222526929341257,
    "estimated_duration": 3600.0012628684053,
    "input_throughput": 648.149494853467,
    "output_throughput": 576.870630968974,
    "total_throughput": 1225.0201258224408,
    "itl": 23.502658176754316,
    "ttft": 6867.391948846756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 32.53246111315067,
    "arrivals": 9494,
    "finished_requests": 9476,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.122309467755258. Arrivals time: 0.03437040047720075 Scheduler time: 0.71868826309219 Scheduler overhead time: 0.1265487400814891 Adapter cache time: 0.054643474984914064 Engine time: 0.12503291852772236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.05-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 66, 540, 66, 66, 540, 270, 540, 270, 540, 540, 270, 66, 540, 270, 66, 66, 270, 540, 66, 270, 66, 66, 270, 270, 270, 66, 66, 540, 270, 270, 66, 540, 540, 270, 540, 540, 270, 540, 66, 540, 270, 270, 270, 540, 540, 66, 66, 270, 66, 540, 66, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 270, 540, 540, 66, 66, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 540, 66, 66, 540, 66, 66, 270]
Prompts retrieved: 28032 . Total input tokens: 6199612 . Total output tokens: 5515691
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.1380576430819929,
    "estimated_duration": 3599.996431076197,
    "input_throughput": 648.1503647775735,
    "output_throughput": 576.8714052250248,
    "total_throughput": 1225.0217700025983,
    "itl": 23.538287424258,
    "ttft": 6867.947590863707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5093,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.09724250311096,
    "arrivals": 9494,
    "finished_requests": 9476,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1381366499699652. Arrivals time: 0.03475821204483509 Scheduler time: 0.7334889261983335 Scheduler overhead time: 0.12719849310815334 Adapter cache time: 0.05457623768597841 Engine time: 0.12430554255843163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 33, 540, 33, 33, 540, 270, 540, 270, 540, 540, 270, 33, 540, 270, 33, 33, 270, 540, 33, 270, 33, 33, 270, 270, 270, 33, 33, 540, 270, 270, 33, 540, 540, 270, 540, 540, 270, 540, 33, 540, 270, 270, 270, 540, 540, 33, 33, 270, 33, 540, 33, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 270, 540, 540, 33, 33, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 540, 33, 33, 540, 33, 33, 270]
Prompts retrieved: 26976 . Total input tokens: 5970497 . Total output tokens: 5305411
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.0792864807881415,
    "estimated_duration": 3599.9432535026604,
    "input_throughput": 614.0274566409541,
    "output_throughput": 541.1793638981483,
    "total_throughput": 1155.2068205391024,
    "itl": 23.168399881225454,
    "ttft": 5186.60434697957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.701415135340532,
    "arrivals": 9096,
    "finished_requests": 9083,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0793561660684645. Arrivals time: 0.03339321305975318 Scheduler time: 0.6764363385736942 Scheduler overhead time: 0.12741034990176558 Adapter cache time: 0.05229659331962466 Engine time: 0.12594131100922823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 33, 540, 33, 33, 540, 270, 540, 270, 540, 540, 270, 33, 540, 270, 33, 33, 270, 540, 33, 270, 33, 33, 270, 270, 270, 33, 33, 540, 270, 270, 33, 540, 540, 270, 540, 540, 270, 540, 33, 540, 270, 270, 270, 540, 540, 33, 33, 270, 33, 540, 33, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 270, 540, 540, 33, 33, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 540, 33, 33, 540, 33, 33, 270]
Prompts retrieved: 26976 . Total input tokens: 5970497 . Total output tokens: 5305411
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.0823862701654434,
    "estimated_duration": 3599.9417228954835,
    "input_throughput": 614.0277177104114,
    "output_throughput": 541.1795939943781,
    "total_throughput": 1155.2073117047896,
    "itl": 23.18709588050757,
    "ttft": 5186.891324841399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.38180045014665,
    "arrivals": 9096,
    "finished_requests": 9083,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.082447465043515. Arrivals time: 0.034434244967997074 Scheduler time: 0.6779235107824206 Scheduler overhead time: 0.12824111711233854 Adapter cache time: 0.05383645463734865 Engine time: 0.12455301452428102 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 33, 540, 33, 33, 540, 270, 540, 270, 540, 540, 270, 33, 540, 270, 33, 33, 270, 540, 33, 270, 33, 33, 270, 270, 270, 33, 33, 540, 270, 270, 33, 540, 540, 270, 540, 540, 270, 540, 33, 540, 270, 270, 270, 540, 540, 33, 33, 270, 33, 540, 33, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 270, 540, 540, 33, 33, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 540, 33, 33, 540, 33, 33, 270]
Prompts retrieved: 26976 . Total input tokens: 5970497 . Total output tokens: 5305411
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.0769438869319856,
    "estimated_duration": 3599.94233768672,
    "input_throughput": 614.0276128479375,
    "output_throughput": 541.1795015727668,
    "total_throughput": 1155.2071144207043,
    "itl": 23.190443136202934,
    "ttft": 5187.113677996011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.17016262365623,
    "arrivals": 9096,
    "finished_requests": 9083,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0770216532982886. Arrivals time: 0.03359005134552717 Scheduler time: 0.6735335732810199 Scheduler overhead time: 0.12707680091261864 Adapter cache time: 0.05284622497856617 Engine time: 0.12600120482966304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 33, 540, 33, 33, 540, 270, 540, 270, 540, 540, 270, 33, 540, 270, 33, 33, 270, 540, 33, 270, 33, 33, 270, 270, 270, 33, 33, 540, 270, 270, 33, 540, 540, 270, 540, 540, 270, 540, 33, 540, 270, 270, 270, 540, 540, 33, 33, 270, 33, 540, 33, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 270, 540, 540, 33, 33, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 540, 33, 33, 540, 33, 33, 270]
Prompts retrieved: 26976 . Total input tokens: 5970497 . Total output tokens: 5305411
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.0697247218340635,
    "estimated_duration": 3599.938215704701,
    "input_throughput": 614.0283159185535,
    "output_throughput": 541.1801212312278,
    "total_throughput": 1155.2084371497813,
    "itl": 23.172353657550595,
    "ttft": 5186.650290025982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.556847054167886,
    "arrivals": 9096,
    "finished_requests": 9083,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0698370458558202. Arrivals time: 0.03348720958456397 Scheduler time: 0.6683509615249932 Scheduler overhead time: 0.12659776583313942 Adapter cache time: 0.052379678934812546 Engine time: 0.12531560566276312 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 33, 540, 33, 33, 540, 270, 540, 270, 540, 540, 270, 33, 540, 270, 33, 33, 270, 540, 33, 270, 33, 33, 270, 270, 270, 33, 33, 540, 270, 270, 33, 540, 540, 270, 540, 540, 270, 540, 33, 540, 270, 270, 270, 540, 540, 33, 33, 270, 33, 540, 33, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 270, 540, 540, 33, 33, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 540, 33, 33, 540, 33, 33, 270]
Prompts retrieved: 26976 . Total input tokens: 5970497 . Total output tokens: 5305411
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.093067869078368,
    "estimated_duration": 3599.9360268873575,
    "input_throughput": 614.0286892573621,
    "output_throughput": 541.180450277196,
    "total_throughput": 1155.209139534558,
    "itl": 23.191832613335762,
    "ttft": 5186.870368742825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.9052063892701,
    "arrivals": 9096,
    "finished_requests": 9083,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0931339347735047. Arrivals time: 0.034260965418070555 Scheduler time: 0.6840692632831633 Scheduler overhead time: 0.12760983314365149 Adapter cache time: 0.05246076174080372 Engine time: 0.13096579629927874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 33, 540, 33, 33, 540, 270, 540, 270, 540, 540, 270, 33, 540, 270, 33, 33, 270, 540, 33, 270, 33, 33, 270, 270, 270, 33, 33, 540, 270, 270, 33, 540, 540, 270, 540, 540, 270, 540, 33, 540, 270, 270, 270, 540, 540, 33, 33, 270, 33, 540, 33, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 270, 540, 540, 33, 33, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 540, 33, 33, 540, 33, 33, 270]
Prompts retrieved: 26976 . Total input tokens: 5970497 . Total output tokens: 5305411
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.097721449099481,
    "estimated_duration": 3599.9431889467614,
    "input_throughput": 614.0274676519874,
    "output_throughput": 541.1793736028349,
    "total_throughput": 1155.2068412548222,
    "itl": 23.156321554947244,
    "ttft": 5186.373168864653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4640,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.62139316425012,
    "arrivals": 9096,
    "finished_requests": 9083,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0978232668712735. Arrivals time: 0.03439981583505869 Scheduler time: 0.6866742144338787 Scheduler overhead time: 0.1287297671660781 Adapter cache time: 0.05431473208591342 Engine time: 0.12875452730804682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.05-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 540, 270, 270, 270, 270, 540, 270, 540, 270, 33, 540, 33, 33, 540, 270, 540, 270, 540, 540, 270, 33, 540, 270, 33, 33, 270, 540, 33, 270, 33, 33, 270, 270, 270, 33, 33, 540, 270, 270, 33, 540, 540, 270, 540, 540, 270, 540, 33, 540, 270, 270, 270, 540, 540, 33, 33, 270, 33, 540, 33, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 270, 540, 540, 33, 33, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 540, 33, 33, 540, 33, 33, 270]
Prompts retrieved: 26976 . Total input tokens: 5970497 . Total output tokens: 5305411
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.0762607851065695,
    "estimated_duration": 3599.927494562165,
    "input_throughput": 614.0301445901326,
    "output_throughput": 541.1817329495822,
    "total_throughput": 1155.2118775397148,
    "itl": 23.188646425182892,
    "ttft": 5186.820042760223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4638,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.57693500217181,
    "arrivals": 9096,
    "finished_requests": 9083,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0763120609335601. Arrivals time: 0.03361624851822853 Scheduler time: 0.6742906305007637 Scheduler overhead time: 0.12811341881752014 Adapter cache time: 0.05174659471958876 Engine time: 0.12466682679951191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 66, 540, 66, 66, 540, 135, 540, 135, 540, 540, 135, 66, 540, 135, 66, 66, 135, 540, 66, 135, 66, 66, 135, 135, 135, 66, 66, 540, 135, 135, 66, 540, 540, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 540, 66, 66, 135, 66, 540, 66, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 135, 540, 540, 66, 66, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 540, 66, 66, 540, 66, 66, 135]
Prompts retrieved: 23712 . Total input tokens: 5259212 . Total output tokens: 4661994
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.0126722566783428,
    "estimated_duration": 3599.8389180221957,
    "input_throughput": 538.6882702699986,
    "output_throughput": 486.7931704462992,
    "total_throughput": 1025.4814407162978,
    "itl": 22.681995649692585,
    "ttft": 6746.8335657482785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3810,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.193278411727537,
    "arrivals": 8052,
    "finished_requests": 8037,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0127306268550456. Arrivals time: 0.03137818770483136 Scheduler time: 0.61457927711308 Scheduler overhead time: 0.12808535853400826 Adapter cache time: 0.04813382914289832 Engine time: 0.1260810741223395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 66, 540, 66, 66, 540, 135, 540, 135, 540, 540, 135, 66, 540, 135, 66, 66, 135, 540, 66, 135, 66, 66, 135, 135, 135, 66, 66, 540, 135, 135, 66, 540, 540, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 540, 66, 66, 135, 66, 540, 66, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 135, 540, 540, 66, 66, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 540, 66, 66, 540, 66, 66, 135]
Prompts retrieved: 23712 . Total input tokens: 5259212 . Total output tokens: 4661994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.019977882038802,
    "estimated_duration": 3599.8259295484027,
    "input_throughput": 538.6902139024459,
    "output_throughput": 486.79492683687494,
    "total_throughput": 1025.4851407393207,
    "itl": 22.700159035581045,
    "ttft": 6747.185030908178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3811,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.5442424787064,
    "arrivals": 8052,
    "finished_requests": 8037,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.020058928988874. Arrivals time: 0.03153769997879863 Scheduler time: 0.6186265242286026 Scheduler overhead time: 0.12918938556686044 Adapter cache time: 0.048737077973783016 Engine time: 0.12670169584453106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 66, 540, 66, 66, 540, 135, 540, 135, 540, 540, 135, 66, 540, 135, 66, 66, 135, 540, 66, 135, 66, 66, 135, 135, 135, 66, 66, 540, 135, 135, 66, 540, 540, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 540, 66, 66, 135, 66, 540, 66, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 135, 540, 540, 66, 66, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 540, 66, 66, 540, 66, 66, 135]
Prompts retrieved: 23712 . Total input tokens: 5259212 . Total output tokens: 4661994
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.0285557033494115,
    "estimated_duration": 3599.840602189047,
    "input_throughput": 538.688018247471,
    "output_throughput": 486.7929427026262,
    "total_throughput": 1025.4809609500971,
    "itl": 22.707165800118283,
    "ttft": 6747.121098556814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3810,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.21517399374971,
    "arrivals": 8052,
    "finished_requests": 8037,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.028623467311263. Arrivals time: 0.03182450449094176 Scheduler time: 0.6285432898439467 Scheduler overhead time: 0.12883353605866432 Adapter cache time: 0.04830677853897214 Engine time: 0.1263196226209402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 66, 540, 66, 66, 540, 135, 540, 135, 540, 540, 135, 66, 540, 135, 66, 66, 135, 540, 66, 135, 66, 66, 135, 135, 135, 66, 66, 540, 135, 135, 66, 540, 540, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 540, 66, 66, 135, 66, 540, 66, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 135, 540, 540, 66, 66, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 540, 66, 66, 540, 66, 66, 135]
Prompts retrieved: 23712 . Total input tokens: 5259212 . Total output tokens: 4661994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.021170490887016,
    "estimated_duration": 3599.842783210383,
    "input_throughput": 538.6140775489208,
    "output_throughput": 486.72653377307256,
    "total_throughput": 1025.3406113219933,
    "itl": 22.849636082815977,
    "ttft": 7194.174966418757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.75738509903277,
    "arrivals": 8052,
    "finished_requests": 8036,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.021248759701848. Arrivals time: 0.0307908421382308 Scheduler time: 0.6224694079719484 Scheduler overhead time: 0.12801933893933892 Adapter cache time: 0.04798385454341769 Engine time: 0.1277792425826192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 66, 540, 66, 66, 540, 135, 540, 135, 540, 540, 135, 66, 540, 135, 66, 66, 135, 540, 66, 135, 66, 66, 135, 135, 135, 66, 66, 540, 135, 135, 66, 540, 540, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 540, 66, 66, 135, 66, 540, 66, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 135, 540, 540, 66, 66, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 540, 66, 66, 540, 66, 66, 135]
Prompts retrieved: 23712 . Total input tokens: 5259212 . Total output tokens: 4661994
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.0279977400787175,
    "estimated_duration": 3599.8379845885033,
    "input_throughput": 538.6884099512241,
    "output_throughput": 486.79329667118725,
    "total_throughput": 1025.4817066224114,
    "itl": 22.703251872237843,
    "ttft": 6747.109934206984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3810,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.975953654759714,
    "arrivals": 8052,
    "finished_requests": 8037,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.02805000776425. Arrivals time: 0.031774168368428946 Scheduler time: 0.626312103588134 Scheduler overhead time: 0.12863500928506255 Adapter cache time: 0.04857296636328101 Engine time: 0.12782652443274856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 66, 540, 66, 66, 540, 135, 540, 135, 540, 540, 135, 66, 540, 135, 66, 66, 135, 540, 66, 135, 66, 66, 135, 135, 135, 66, 66, 540, 135, 135, 66, 540, 540, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 540, 66, 66, 135, 66, 540, 66, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 135, 540, 540, 66, 66, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 540, 66, 66, 540, 66, 66, 135]
Prompts retrieved: 23712 . Total input tokens: 5259212 . Total output tokens: 4661994
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.033765418920666,
    "estimated_duration": 3599.8464413956494,
    "input_throughput": 538.6871444572457,
    "output_throughput": 486.7921530898993,
    "total_throughput": 1025.4792975471448,
    "itl": 22.67576584108152,
    "ttft": 6746.6556289458595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.33550662545701,
    "arrivals": 8052,
    "finished_requests": 8037,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0338678788393736. Arrivals time: 0.03191951150074601 Scheduler time: 0.6297222403809428 Scheduler overhead time: 0.12818753300234675 Adapter cache time: 0.04866918129846454 Engine time: 0.13056036923080683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.05-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 66, 540, 66, 66, 540, 135, 540, 135, 540, 540, 135, 66, 540, 135, 66, 66, 135, 540, 66, 135, 66, 66, 135, 135, 135, 66, 66, 540, 135, 135, 66, 540, 540, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 540, 66, 66, 135, 66, 540, 66, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 135, 540, 540, 66, 66, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 540, 66, 66, 540, 66, 66, 135]
Prompts retrieved: 23712 . Total input tokens: 5259212 . Total output tokens: 4661994
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.04042794322595,
    "estimated_duration": 3599.8321316549573,
    "input_throughput": 538.6892857996943,
    "output_throughput": 486.7940881438759,
    "total_throughput": 1025.4833739435703,
    "itl": 22.702269019701745,
    "ttft": 6747.213847156021,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3810,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.73569772988664,
    "arrivals": 8052,
    "finished_requests": 8037,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.04048020998016. Arrivals time: 0.03182078339159489 Scheduler time: 0.6353745139203966 Scheduler overhead time: 0.12982578482478857 Adapter cache time: 0.048508172389119864 Engine time: 0.12946729455143213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 33, 540, 33, 33, 540, 135, 540, 135, 540, 540, 135, 33, 540, 135, 33, 33, 135, 540, 33, 135, 33, 33, 135, 135, 135, 33, 33, 540, 135, 135, 33, 540, 540, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 540, 33, 33, 135, 33, 540, 33, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 135, 540, 540, 33, 33, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 540, 33, 33, 540, 33, 33, 135]
Prompts retrieved: 22656 . Total input tokens: 5036055 . Total output tokens: 4457254
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.9898239998146892,
    "estimated_duration": 3599.445993822488,
    "input_throughput": 518.0408327282041,
    "output_throughput": 458.11466620974693,
    "total_throughput": 976.155498937951,
    "itl": 22.366179141126867,
    "ttft": 6639.8527903946315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.796026405481868,
    "arrivals": 7636,
    "finished_requests": 7622,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9899019529111683. Arrivals time: 0.030598504934459925 Scheduler time: 0.5892115118913352 Scheduler overhead time: 0.13010160299018025 Adapter cache time: 0.045903935097157955 Engine time: 0.12907490227371454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 33, 540, 33, 33, 540, 135, 540, 135, 540, 540, 135, 33, 540, 135, 33, 33, 135, 540, 33, 135, 33, 33, 135, 135, 135, 33, 33, 540, 135, 135, 33, 540, 540, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 540, 33, 33, 135, 33, 540, 33, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 135, 540, 540, 33, 33, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 540, 33, 33, 540, 33, 33, 135]
Prompts retrieved: 22656 . Total input tokens: 5036055 . Total output tokens: 4457254
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.9890181948430836,
    "estimated_duration": 3599.428199250688,
    "input_throughput": 518.0433937779828,
    "output_throughput": 458.1169310012275,
    "total_throughput": 976.1603247792103,
    "itl": 22.38153511112902,
    "ttft": 6640.0425778435165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.73614837510942,
    "arrivals": 7636,
    "finished_requests": 7622,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9891643901355565. Arrivals time: 0.0302883954718709 Scheduler time: 0.5918708802200854 Scheduler overhead time: 0.1290813460946083 Adapter cache time: 0.04552427865564823 Engine time: 0.12705398444086313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 33, 540, 33, 33, 540, 135, 540, 135, 540, 540, 135, 33, 540, 135, 33, 33, 135, 540, 33, 135, 33, 33, 135, 135, 135, 33, 33, 540, 135, 135, 33, 540, 540, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 540, 33, 33, 135, 33, 540, 33, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 135, 540, 540, 33, 33, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 540, 33, 33, 540, 33, 33, 135]
Prompts retrieved: 22656 . Total input tokens: 5036055 . Total output tokens: 4457254
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.010018571279943,
    "estimated_duration": 3599.4473029643227,
    "input_throughput": 518.0406443134646,
    "output_throughput": 458.1144995905346,
    "total_throughput": 976.1551439039991,
    "itl": 22.384962378107875,
    "ttft": 6640.181407987252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.302850244609285,
    "arrivals": 7636,
    "finished_requests": 7622,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0100845079869032. Arrivals time: 0.030771573539823294 Scheduler time: 0.6089219944551587 Scheduler overhead time: 0.12947583943605423 Adapter cache time: 0.04647353384643793 Engine time: 0.12916113762184978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 33, 540, 33, 33, 540, 135, 540, 135, 540, 540, 135, 33, 540, 135, 33, 33, 135, 540, 33, 135, 33, 33, 135, 135, 135, 33, 33, 540, 135, 135, 33, 540, 540, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 540, 33, 33, 135, 33, 540, 33, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 135, 540, 540, 33, 33, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 540, 33, 33, 540, 33, 33, 135]
Prompts retrieved: 22656 . Total input tokens: 5036055 . Total output tokens: 4457254
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 0.9948020279407501,
    "estimated_duration": 3599.4402880252815,
    "input_throughput": 518.0416539214175,
    "output_throughput": 458.11539240859275,
    "total_throughput": 976.1570463300102,
    "itl": 22.3694473951173,
    "ttft": 6639.984978930044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.47147906842236,
    "arrivals": 7636,
    "finished_requests": 7622,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9948613941669464. Arrivals time: 0.03074789373204112 Scheduler time: 0.5934612364508212 Scheduler overhead time: 0.12938840128481388 Adapter cache time: 0.046251178719103336 Engine time: 0.129149517044425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 33, 540, 33, 33, 540, 135, 540, 135, 540, 540, 135, 33, 540, 135, 33, 33, 135, 540, 33, 135, 33, 33, 135, 135, 135, 33, 33, 540, 135, 135, 33, 540, 540, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 540, 33, 33, 135, 33, 540, 33, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 135, 540, 540, 33, 33, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 540, 33, 33, 540, 33, 33, 135]
Prompts retrieved: 22656 . Total input tokens: 5036055 . Total output tokens: 4457254
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 0.9831943521276116,
    "estimated_duration": 3599.4403751482832,
    "input_throughput": 518.0416413824282,
    "output_throughput": 458.11538132009457,
    "total_throughput": 976.1570227025228,
    "itl": 22.384945337840907,
    "ttft": 6640.156697880639,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.10458873412162,
    "arrivals": 7636,
    "finished_requests": 7622,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9832683890126646. Arrivals time: 0.03020226862281561 Scheduler time: 0.5849770097993314 Scheduler overhead time: 0.12986151734367013 Adapter cache time: 0.04583482351154089 Engine time: 0.12731624860316515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 33, 540, 33, 33, 540, 135, 540, 135, 540, 540, 135, 33, 540, 135, 33, 33, 135, 540, 33, 135, 33, 33, 135, 135, 135, 33, 33, 540, 135, 135, 33, 540, 540, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 540, 33, 33, 135, 33, 540, 33, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 135, 540, 540, 33, 33, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 540, 33, 33, 540, 33, 33, 135]
Prompts retrieved: 22656 . Total input tokens: 5036055 . Total output tokens: 4457254
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.991555601824075,
    "estimated_duration": 3599.4278200628937,
    "input_throughput": 518.043448352138,
    "output_throughput": 458.11697926232824,
    "total_throughput": 976.1604276144661,
    "itl": 22.360973800996515,
    "ttft": 6639.843040050224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.077431358095897,
    "arrivals": 7636,
    "finished_requests": 7622,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9916226449422538. Arrivals time: 0.030686932615935802 Scheduler time: 0.5930957980453968 Scheduler overhead time: 0.1287227924913168 Adapter cache time: 0.045998979825526476 Engine time: 0.12747551873326302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 540, 135, 135, 135, 135, 540, 135, 540, 135, 33, 540, 33, 33, 540, 135, 540, 135, 540, 540, 135, 33, 540, 135, 33, 33, 135, 540, 33, 135, 33, 33, 135, 135, 135, 33, 33, 540, 135, 135, 33, 540, 540, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 540, 33, 33, 135, 33, 540, 33, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 135, 540, 540, 33, 33, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 540, 33, 33, 540, 33, 33, 135]
Prompts retrieved: 22656 . Total input tokens: 5036055 . Total output tokens: 4457254
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.9898931002244353,
    "estimated_duration": 3599.430456861002,
    "input_throughput": 518.0430688543254,
    "output_throughput": 458.11664366423884,
    "total_throughput": 976.1597125185642,
    "itl": 22.38222120787382,
    "ttft": 6640.119373673399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.906376099587025,
    "arrivals": 7636,
    "finished_requests": 7622,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9899466512724757. Arrivals time: 0.030483942944556475 Scheduler time: 0.591007393784821 Scheduler overhead time: 0.12949700932949781 Adapter cache time: 0.04589085653424263 Engine time: 0.12779290368780494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 540, 66, 66, 66, 66, 540, 66, 540, 66, 33, 540, 33, 33, 540, 66, 540, 66, 540, 540, 66, 33, 540, 66, 33, 33, 66, 540, 33, 66, 33, 33, 66, 66, 66, 33, 33, 540, 66, 66, 33, 540, 540, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 540, 33, 33, 66, 33, 540, 33, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 66, 540, 540, 33, 33, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 540, 33, 33, 540, 33, 33, 66]
Prompts retrieved: 20448 . Total input tokens: 4545729 . Total output tokens: 4043414
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.9597817100584507,
    "estimated_duration": 3599.3510122818793,
    "input_throughput": 468.7858989696832,
    "output_throughput": 420.6916732580723,
    "total_throughput": 889.4775722277554,
    "itl": 22.055811110507094,
    "ttft": 5741.141596737712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.578835679274487,
    "arrivals": 6944,
    "finished_requests": 6933,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9598415391519666. Arrivals time: 0.02900165691971779 Scheduler time: 0.5580096519552171 Scheduler overhead time: 0.13375823851674795 Adapter cache time: 0.042643419466912746 Engine time: 0.12929933052510023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 540, 66, 66, 66, 66, 540, 66, 540, 66, 33, 540, 33, 33, 540, 66, 540, 66, 540, 540, 66, 33, 540, 66, 33, 33, 66, 540, 33, 66, 33, 33, 66, 66, 66, 33, 33, 540, 66, 66, 33, 540, 540, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 540, 33, 33, 66, 33, 540, 33, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 66, 540, 540, 33, 33, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 540, 33, 33, 540, 33, 33, 66]
Prompts retrieved: 20448 . Total input tokens: 4545729 . Total output tokens: 4043414
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.9606527290306985,
    "estimated_duration": 3599.3648009474555,
    "input_throughput": 468.78410311615204,
    "output_throughput": 420.6900616468258,
    "total_throughput": 889.4741647629778,
    "itl": 22.063314723856035,
    "ttft": 5741.489097837724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.037336679533794,
    "arrivals": 6944,
    "finished_requests": 6933,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9607445569708943. Arrivals time: 0.028792879078537226 Scheduler time: 0.559247812256217 Scheduler overhead time: 0.13118779193609953 Adapter cache time: 0.04325989447534084 Engine time: 0.1321397041901946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 540, 66, 66, 66, 66, 540, 66, 540, 66, 33, 540, 33, 33, 540, 66, 540, 66, 540, 540, 66, 33, 540, 66, 33, 33, 66, 540, 33, 66, 33, 33, 66, 66, 66, 33, 33, 540, 66, 66, 33, 540, 540, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 540, 33, 33, 66, 33, 540, 33, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 66, 540, 540, 33, 33, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 540, 33, 33, 540, 33, 33, 66]
Prompts retrieved: 20448 . Total input tokens: 4545729 . Total output tokens: 4043414
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.9536589100025594,
    "estimated_duration": 3599.3529189826454,
    "input_throughput": 468.7856506377044,
    "output_throughput": 420.6914504032554,
    "total_throughput": 889.4771010409597,
    "itl": 22.06561523173447,
    "ttft": 5741.42297737728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.460721363113127,
    "arrivals": 6944,
    "finished_requests": 6933,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9537328667938709. Arrivals time: 0.02897724835202098 Scheduler time: 0.5549033908173442 Scheduler overhead time: 0.13055274402722716 Adapter cache time: 0.04341826820746064 Engine time: 0.12980101304128766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 540, 66, 66, 66, 66, 540, 66, 540, 66, 33, 540, 33, 33, 540, 66, 540, 66, 540, 540, 66, 33, 540, 66, 33, 33, 66, 540, 33, 66, 33, 33, 66, 66, 66, 33, 33, 540, 66, 66, 33, 540, 540, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 540, 33, 33, 66, 33, 540, 33, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 66, 540, 540, 33, 33, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 540, 33, 33, 540, 33, 33, 66]
Prompts retrieved: 20448 . Total input tokens: 4545729 . Total output tokens: 4043414
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 0.9550703503191471,
    "estimated_duration": 3599.3587670583292,
    "input_throughput": 468.7848889759358,
    "output_throughput": 420.6907668827728,
    "total_throughput": 889.4756558587086,
    "itl": 22.05594868577613,
    "ttft": 5741.169888735701,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.051699788482278,
    "arrivals": 6944,
    "finished_requests": 6933,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9551209989003837. Arrivals time: 0.029013984836637974 Scheduler time: 0.5562460096552968 Scheduler overhead time: 0.13050486892461777 Adapter cache time: 0.04257357493042946 Engine time: 0.130640662740916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 540, 66, 66, 66, 66, 540, 66, 540, 66, 33, 540, 33, 33, 540, 66, 540, 66, 540, 540, 66, 33, 540, 66, 33, 33, 66, 540, 33, 66, 33, 33, 66, 66, 66, 33, 33, 540, 66, 66, 33, 540, 540, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 540, 33, 33, 66, 33, 540, 33, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 66, 540, 540, 33, 33, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 540, 33, 33, 540, 33, 33, 66]
Prompts retrieved: 20448 . Total input tokens: 4545729 . Total output tokens: 4043414
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 0.9565098518505692,
    "estimated_duration": 3599.3643572693127,
    "input_throughput": 468.7841609011495,
    "output_throughput": 420.6901135034779,
    "total_throughput": 889.4742744046274,
    "itl": 22.067562699201787,
    "ttft": 5741.552473991467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.31366816771667,
    "arrivals": 6944,
    "finished_requests": 6933,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9565872312523425. Arrivals time: 0.02921054046601057 Scheduler time: 0.5573456804268062 Scheduler overhead time: 0.13092307792976499 Adapter cache time: 0.04293579515069723 Engine time: 0.13012414891272783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 540, 66, 66, 66, 66, 540, 66, 540, 66, 33, 540, 33, 33, 540, 66, 540, 66, 540, 540, 66, 33, 540, 66, 33, 33, 66, 540, 33, 66, 33, 33, 66, 66, 66, 33, 33, 540, 66, 66, 33, 540, 540, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 540, 33, 33, 66, 33, 540, 33, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 66, 540, 540, 33, 33, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 540, 33, 33, 540, 33, 33, 66]
Prompts retrieved: 20448 . Total input tokens: 4545729 . Total output tokens: 4043414
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.9586270339787006,
    "estimated_duration": 3599.3649201624016,
    "input_throughput": 468.78408758950417,
    "output_throughput": 420.6900477131058,
    "total_throughput": 889.47413530261,
    "itl": 22.049077045779256,
    "ttft": 5741.085563228815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.034133815044504,
    "arrivals": 6944,
    "finished_requests": 6933,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9586791968904436. Arrivals time: 0.029313100967556238 Scheduler time: 0.5571124199777842 Scheduler overhead time: 0.1294423146173358 Adapter cache time: 0.04293695371598005 Engine time: 0.13432036899030209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 540, 66, 66, 66, 66, 540, 66, 540, 66, 33, 540, 33, 33, 540, 66, 540, 66, 540, 540, 66, 33, 540, 66, 33, 33, 66, 540, 33, 66, 33, 33, 66, 66, 66, 33, 33, 540, 66, 66, 33, 540, 540, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 540, 33, 33, 66, 33, 540, 33, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 66, 540, 540, 33, 33, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 540, 33, 33, 540, 33, 33, 66]
Prompts retrieved: 20448 . Total input tokens: 4545729 . Total output tokens: 4043414
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.9586948067881167,
    "estimated_duration": 3599.3717700670313,
    "input_throughput": 468.78319545429366,
    "output_throughput": 420.6892471048637,
    "total_throughput": 889.4724425591573,
    "itl": 22.065083668570654,
    "ttft": 5741.456892019198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.16972023345513,
    "arrivals": 6944,
    "finished_requests": 6933,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9587718290276825. Arrivals time: 0.02897906629368663 Scheduler time: 0.5551065858453512 Scheduler overhead time: 0.1318877707235515 Adapter cache time: 0.04296536976471543 Engine time: 0.13337690895423293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 66, 270, 66, 66, 270, 135, 270, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 66, 135, 66, 66, 135, 135, 135, 66, 66, 270, 135, 135, 66, 270, 270, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 270, 66, 66, 135, 66, 270, 66, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 135, 270, 270, 66, 66, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 270, 66, 66, 270, 66, 66, 135]
Prompts retrieved: 15072 . Total input tokens: 3332730 . Total output tokens: 3000837
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.8296891171485186,
    "estimated_duration": 3599.4884170358982,
    "input_throughput": 348.83623852135804,
    "output_throughput": 300.5188167519562,
    "total_throughput": 649.3550552733142,
    "itl": 21.317105593732332,
    "ttft": 5719.404345482833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2966,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.61240518876311,
    "arrivals": 5070,
    "finished_requests": 5062,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8297386420890689. Arrivals time: 0.025246459525078535 Scheduler time: 0.4329731147736311 Scheduler overhead time: 0.13213441474363208 Adapter cache time: 0.04021526826545596 Engine time: 0.13160015642642975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 66, 270, 66, 66, 270, 135, 270, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 66, 135, 66, 66, 135, 135, 135, 66, 66, 270, 135, 135, 66, 270, 270, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 270, 66, 66, 135, 66, 270, 66, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 135, 270, 270, 66, 66, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 270, 66, 66, 270, 66, 66, 135]
Prompts retrieved: 15072 . Total input tokens: 3332730 . Total output tokens: 3000837
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.8329045278951526,
    "estimated_duration": 3599.4997392705523,
    "input_throughput": 348.83514125617273,
    "output_throughput": 300.51787146933145,
    "total_throughput": 649.3530127255042,
    "itl": 21.325231558977947,
    "ttft": 5719.893087050527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2967,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.539843092714023,
    "arrivals": 5070,
    "finished_requests": 5062,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8329682159237564. Arrivals time: 0.02492796629667282 Scheduler time: 0.43447416042909026 Scheduler overhead time: 0.1320504667237401 Adapter cache time: 0.040469685569405556 Engine time: 0.13367609726265073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 66, 270, 66, 66, 270, 135, 270, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 66, 135, 66, 66, 135, 135, 135, 66, 66, 270, 135, 135, 66, 270, 270, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 270, 66, 66, 135, 66, 270, 66, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 135, 270, 270, 66, 66, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 270, 66, 66, 270, 66, 66, 135]
Prompts retrieved: 15072 . Total input tokens: 3332730 . Total output tokens: 3000837
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.8274094369262457,
    "estimated_duration": 3599.498205381863,
    "input_throughput": 348.83528990863675,
    "output_throughput": 300.517999531894,
    "total_throughput": 649.3532894405308,
    "itl": 21.3333513303986,
    "ttft": 5720.0297953562485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2967,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.08885789119212,
    "arrivals": 5070,
    "finished_requests": 5062,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8274662420153618. Arrivals time: 0.025228133890777826 Scheduler time: 0.431250280700624 Scheduler overhead time: 0.13165025785565376 Adapter cache time: 0.03999435529112816 Engine time: 0.1320048519410193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 66, 270, 66, 66, 270, 135, 270, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 66, 135, 66, 66, 135, 135, 135, 66, 66, 270, 135, 135, 66, 270, 270, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 270, 66, 66, 135, 66, 270, 66, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 135, 270, 270, 66, 66, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 270, 66, 66, 270, 66, 66, 135]
Prompts retrieved: 15072 . Total input tokens: 3332730 . Total output tokens: 3000837
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 0.8300139009952545,
    "estimated_duration": 3599.4887918260692,
    "input_throughput": 348.83620219942424,
    "output_throughput": 300.5187854609854,
    "total_throughput": 649.3549876604096,
    "itl": 21.320036462808503,
    "ttft": 5719.675448259683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2965,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.224923977866382,
    "arrivals": 5070,
    "finished_requests": 5062,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8300667656585574. Arrivals time: 0.025166056118905544 Scheduler time: 0.4321245509199798 Scheduler overhead time: 0.13182886457070708 Adapter cache time: 0.04060494899749756 Engine time: 0.13337963167577982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 66, 270, 66, 66, 270, 135, 270, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 66, 135, 66, 66, 135, 135, 135, 66, 66, 270, 135, 135, 66, 270, 270, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 270, 66, 66, 135, 66, 270, 66, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 135, 270, 270, 66, 66, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 270, 66, 66, 270, 66, 66, 135]
Prompts retrieved: 15072 . Total input tokens: 3332730 . Total output tokens: 3000837
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 0.8238473981618881,
    "estimated_duration": 3599.4977760743864,
    "input_throughput": 348.8353315137738,
    "output_throughput": 300.5180353742898,
    "total_throughput": 649.3533668880636,
    "itl": 21.326227372045835,
    "ttft": 5720.025780792234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2967,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.894996213880738,
    "arrivals": 5070,
    "finished_requests": 5062,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8239243370480835. Arrivals time: 0.024842746555805206 Scheduler time: 0.4286162042990327 Scheduler overhead time: 0.1319996784441173 Adapter cache time: 0.040102016646414995 Engine time: 0.13080967031419277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 66, 270, 66, 66, 270, 135, 270, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 66, 135, 66, 66, 135, 135, 135, 66, 66, 270, 135, 135, 66, 270, 270, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 270, 66, 66, 135, 66, 270, 66, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 135, 270, 270, 66, 66, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 270, 66, 66, 270, 66, 66, 135]
Prompts retrieved: 15072 . Total input tokens: 3332730 . Total output tokens: 3000837
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.8311101351864636,
    "estimated_duration": 3599.4876500835753,
    "input_throughput": 348.836312848815,
    "output_throughput": 300.5188807842927,
    "total_throughput": 649.3551936331078,
    "itl": 21.31197450407037,
    "ttft": 5719.060398309234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2966,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.93470950973362,
    "arrivals": 5070,
    "finished_requests": 5062,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8312658262439072. Arrivals time: 0.025115810334682465 Scheduler time: 0.43201927468180656 Scheduler overhead time: 0.1317869871854782 Adapter cache time: 0.04035980999469757 Engine time: 0.13456331146880984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 66, 270, 66, 66, 270, 135, 270, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 66, 135, 66, 66, 135, 135, 135, 66, 66, 270, 135, 135, 66, 270, 270, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 270, 66, 66, 135, 66, 270, 66, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 135, 270, 270, 66, 66, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 270, 66, 66, 270, 66, 66, 135]
Prompts retrieved: 15072 . Total input tokens: 3332730 . Total output tokens: 3000837
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.8209064058028162,
    "estimated_duration": 3599.492595638438,
    "input_throughput": 348.83583356206066,
    "output_throughput": 300.5184678837039,
    "total_throughput": 649.3543014457646,
    "itl": 21.325308299850526,
    "ttft": 5719.869213377876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2967,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.702843342963963,
    "arrivals": 5070,
    "finished_requests": 5062,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.820967435836792. Arrivals time: 0.02477175509557128 Scheduler time: 0.4267356786876917 Scheduler overhead time: 0.13183143641799688 Adapter cache time: 0.03951759869232774 Engine time: 0.13085864437744021 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 33, 270, 33, 33, 270, 135, 270, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 33, 135, 33, 33, 135, 135, 135, 33, 33, 270, 135, 135, 33, 270, 270, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 270, 33, 33, 135, 33, 270, 33, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 135, 270, 270, 33, 33, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 270, 33, 33, 270, 33, 33, 135]
Prompts retrieved: 14016 . Total input tokens: 3084908 . Total output tokens: 2787633
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.8116249907761812,
    "estimated_duration": 3598.5871443945007,
    "input_throughput": 318.6050396989664,
    "output_throughput": 284.90764815770814,
    "total_throughput": 603.5126878566745,
    "itl": 21.160858649204556,
    "ttft": 7685.105226629505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.650045942450237,
    "arrivals": 4708,
    "finished_requests": 4698,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8116898676380515. Arrivals time: 0.0238176672719419 Scheduler time: 0.41615203069522977 Scheduler overhead time: 0.13186271442100406 Adapter cache time: 0.03891843790188432 Engine time: 0.1332881129346788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 33, 270, 33, 33, 270, 135, 270, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 33, 135, 33, 33, 135, 135, 135, 33, 33, 270, 135, 135, 33, 270, 270, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 270, 33, 33, 135, 33, 270, 33, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 135, 270, 270, 33, 33, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 270, 33, 33, 270, 33, 33, 135]
Prompts retrieved: 14016 . Total input tokens: 3084908 . Total output tokens: 2787633
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.8051818227395415,
    "estimated_duration": 3598.588059687603,
    "input_throughput": 318.60495866246254,
    "output_throughput": 284.907575692063,
    "total_throughput": 603.5125343545255,
    "itl": 21.170020216371146,
    "ttft": 7685.253579705528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.155656167352355,
    "arrivals": 4708,
    "finished_requests": 4698,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8052412141114473. Arrivals time: 0.024079028982669115 Scheduler time: 0.4115434866398573 Scheduler overhead time: 0.13164478540420532 Adapter cache time: 0.03822001302614808 Engine time: 0.1323025757446885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 33, 270, 33, 33, 270, 135, 270, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 33, 135, 33, 33, 135, 135, 135, 33, 33, 270, 135, 135, 33, 270, 270, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 270, 33, 33, 135, 33, 270, 33, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 135, 270, 270, 33, 33, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 270, 33, 33, 270, 33, 33, 135]
Prompts retrieved: 14016 . Total input tokens: 3084908 . Total output tokens: 2787633
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.8142799101769924,
    "estimated_duration": 3598.5846650864596,
    "input_throughput": 318.6052592074983,
    "output_throughput": 284.9078444498309,
    "total_throughput": 603.5131036573292,
    "itl": 21.175031379468003,
    "ttft": 7685.4330613571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.594279038799026,
    "arrivals": 4708,
    "finished_requests": 4698,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.814339985139668. Arrivals time: 0.024163398891687393 Scheduler time: 0.4172884183935821 Scheduler overhead time: 0.13226708257570863 Adapter cache time: 0.038592622615396976 Engine time: 0.13414566963911057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 33, 270, 33, 33, 270, 135, 270, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 33, 135, 33, 33, 135, 135, 135, 33, 33, 270, 135, 135, 33, 270, 270, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 270, 33, 33, 135, 33, 270, 33, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 135, 270, 270, 33, 33, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 270, 33, 33, 270, 33, 33, 135]
Prompts retrieved: 14016 . Total input tokens: 3084908 . Total output tokens: 2787633
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 0.811702401842922,
    "estimated_duration": 3598.582031039861,
    "input_throughput": 318.60549241632674,
    "output_throughput": 284.9080529932328,
    "total_throughput": 603.5135454095596,
    "itl": 21.16436764169326,
    "ttft": 7684.983412477799,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2517,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.14142381978729,
    "arrivals": 4708,
    "finished_requests": 4698,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8117640120908618. Arrivals time: 0.024048566818237305 Scheduler time: 0.41713363816961646 Scheduler overhead time: 0.13211702462285757 Adapter cache time: 0.038634137250483036 Engine time: 0.13258496997877955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 33, 270, 33, 33, 270, 135, 270, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 33, 135, 33, 33, 135, 135, 135, 33, 33, 270, 135, 135, 33, 270, 270, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 270, 33, 33, 135, 33, 270, 33, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 135, 270, 270, 33, 33, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 270, 33, 33, 270, 33, 33, 135]
Prompts retrieved: 14016 . Total input tokens: 3084908 . Total output tokens: 2787633
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 0.8165037683211267,
    "estimated_duration": 3598.577651360306,
    "input_throughput": 318.6058801778526,
    "output_throughput": 284.9083997430033,
    "total_throughput": 603.514279920856,
    "itl": 21.16980465954658,
    "ttft": 7685.338696403892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.444119085753353,
    "arrivals": 4708,
    "finished_requests": 4698,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8165553850121796. Arrivals time: 0.024019985925406218 Scheduler time: 0.41906248638406396 Scheduler overhead time: 0.13200153037905693 Adapter cache time: 0.03858134336769581 Engine time: 0.13534654956310987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 33, 270, 33, 33, 270, 135, 270, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 33, 135, 33, 33, 135, 135, 135, 33, 33, 270, 135, 135, 33, 270, 270, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 270, 33, 33, 135, 33, 270, 33, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 135, 270, 270, 33, 33, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 270, 33, 33, 270, 33, 33, 135]
Prompts retrieved: 14016 . Total input tokens: 3084908 . Total output tokens: 2787633
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.8094099159352481,
    "estimated_duration": 3598.583579092198,
    "input_throughput": 318.60535535740723,
    "output_throughput": 284.90793043040566,
    "total_throughput": 603.5132857878128,
    "itl": 21.157450114319442,
    "ttft": 7684.9050597373625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2517,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.068329007417066,
    "arrivals": 4708,
    "finished_requests": 4698,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8094913410022855. Arrivals time: 0.02452024957165122 Scheduler time: 0.41480028815567493 Scheduler overhead time: 0.13205995177850127 Adapter cache time: 0.03821661742404103 Engine time: 0.13231028011068702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 270, 135, 135, 135, 135, 270, 135, 270, 135, 33, 270, 33, 33, 270, 135, 270, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 33, 135, 33, 33, 135, 135, 135, 33, 33, 270, 135, 135, 33, 270, 270, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 270, 33, 33, 135, 33, 270, 33, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 135, 270, 270, 33, 33, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 270, 33, 33, 270, 33, 33, 135]
Prompts retrieved: 14016 . Total input tokens: 3084908 . Total output tokens: 2787633
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.8081361297518015,
    "estimated_duration": 3598.593147454661,
    "input_throughput": 318.6045082120374,
    "output_throughput": 284.90717288371025,
    "total_throughput": 603.5116810957476,
    "itl": 21.170294570654754,
    "ttft": 7685.3227131885305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.283810391053557,
    "arrivals": 4708,
    "finished_requests": 4698,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8081971490755677. Arrivals time: 0.024095460306853056 Scheduler time: 0.41363465785980225 Scheduler overhead time: 0.13226272258907557 Adapter cache time: 0.03823729045689106 Engine time: 0.1325661800801754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 270, 66, 66, 66, 66, 270, 66, 270, 66, 33, 270, 33, 33, 270, 66, 270, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 33, 66, 33, 33, 66, 66, 66, 33, 33, 270, 66, 66, 33, 270, 270, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 270, 33, 33, 66, 33, 270, 33, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 66, 270, 270, 33, 33, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 270, 33, 33, 270, 33, 33, 66]
Prompts retrieved: 11808 . Total input tokens: 2580649 . Total output tokens: 2350802
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.7606166359037161,
    "estimated_duration": 3599.5821837667067,
    "input_throughput": 263.39112474656235,
    "output_throughput": 238.32766032370168,
    "total_throughput": 501.71878507026406,
    "itl": 20.87977943534946,
    "ttft": 3704.178877385771,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1858,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.285855981363312,
    "arrivals": 3927,
    "finished_requests": 3923,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.76066734502092. Arrivals time: 0.022495543118566275 Scheduler time: 0.36808278877288103 Scheduler overhead time: 0.13181831734254956 Adapter cache time: 0.03529130481183529 Engine time: 0.1347785904072225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 270, 66, 66, 66, 66, 270, 66, 270, 66, 33, 270, 33, 33, 270, 66, 270, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 33, 66, 33, 33, 66, 66, 66, 33, 33, 270, 66, 66, 33, 270, 270, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 270, 33, 33, 66, 33, 270, 33, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 66, 270, 270, 33, 33, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 270, 33, 33, 270, 33, 33, 66]
Prompts retrieved: 11808 . Total input tokens: 2580649 . Total output tokens: 2350802
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.7553270640783012,
    "estimated_duration": 3599.5800308443354,
    "input_throughput": 263.3912822817859,
    "output_throughput": 238.3278028683728,
    "total_throughput": 501.7190851501587,
    "itl": 20.88395045505659,
    "ttft": 3704.7561558296625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1858,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.470273215649497,
    "arrivals": 3927,
    "finished_requests": 3923,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7553751259110868. Arrivals time: 0.022274501156061888 Scheduler time: 0.36386334151029587 Scheduler overhead time: 0.13307825103402138 Adapter cache time: 0.03521566465497017 Engine time: 0.13307752320542932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 270, 66, 66, 66, 66, 270, 66, 270, 66, 33, 270, 33, 33, 270, 66, 270, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 33, 66, 33, 33, 66, 66, 66, 33, 33, 270, 66, 66, 33, 270, 270, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 270, 33, 33, 66, 33, 270, 33, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 66, 270, 270, 33, 33, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 270, 33, 33, 270, 33, 33, 66]
Prompts retrieved: 11808 . Total input tokens: 2580649 . Total output tokens: 2350802
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.7582666240632534,
    "estimated_duration": 3599.5830070894262,
    "input_throughput": 263.39106450183493,
    "output_throughput": 238.32760581167153,
    "total_throughput": 501.71867031350644,
    "itl": 20.88532342758699,
    "ttft": 3704.842962459902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1858,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.81088865844977,
    "arrivals": 3927,
    "finished_requests": 3923,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7583669307641685. Arrivals time: 0.022479522973299026 Scheduler time: 0.3660809719003737 Scheduler overhead time: 0.1320893494412303 Adapter cache time: 0.03550522495061159 Engine time: 0.13427904667332768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 270, 66, 66, 66, 66, 270, 66, 270, 66, 33, 270, 33, 33, 270, 66, 270, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 33, 66, 33, 33, 66, 66, 66, 33, 33, 270, 66, 66, 33, 270, 270, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 270, 33, 33, 66, 33, 270, 33, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 66, 270, 270, 33, 33, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 270, 33, 33, 270, 33, 33, 66]
Prompts retrieved: 11808 . Total input tokens: 2580649 . Total output tokens: 2350802
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 0.7547445921227336,
    "estimated_duration": 3599.5891517452446,
    "input_throughput": 263.39061488178976,
    "output_throughput": 238.32719897604446,
    "total_throughput": 501.7178138578342,
    "itl": 20.882046657249816,
    "ttft": 3704.1672181197637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1858,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.6984223544879,
    "arrivals": 3927,
    "finished_requests": 3923,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7547962279058993. Arrivals time: 0.02229103399440646 Scheduler time: 0.3636878151446581 Scheduler overhead time: 0.1320546166971326 Adapter cache time: 0.03538395371288061 Engine time: 0.13369423104450107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 270, 66, 66, 66, 66, 270, 66, 270, 66, 33, 270, 33, 33, 270, 66, 270, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 33, 66, 33, 33, 66, 66, 66, 33, 33, 270, 66, 66, 33, 270, 270, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 270, 33, 33, 66, 33, 270, 33, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 66, 270, 270, 33, 33, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 270, 33, 33, 270, 33, 33, 66]
Prompts retrieved: 11808 . Total input tokens: 2580649 . Total output tokens: 2350802
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 0.7536105890758336,
    "estimated_duration": 3599.596609370195,
    "input_throughput": 263.39006919052645,
    "output_throughput": 238.3267052110318,
    "total_throughput": 501.7167744015582,
    "itl": 20.886874190491827,
    "ttft": 3704.691517340304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1857,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.689364241272928,
    "arrivals": 3927,
    "finished_requests": 3923,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.753665201831609. Arrivals time: 0.022298292256891727 Scheduler time: 0.3636148045770824 Scheduler overhead time: 0.13237830204889178 Adapter cache time: 0.035367611330002546 Engine time: 0.13199166022241116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 270, 66, 66, 66, 66, 270, 66, 270, 66, 33, 270, 33, 33, 270, 66, 270, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 33, 66, 33, 33, 66, 66, 66, 33, 33, 270, 66, 66, 33, 270, 270, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 270, 33, 33, 66, 33, 270, 33, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 66, 270, 270, 33, 33, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 270, 33, 33, 270, 33, 33, 66]
Prompts retrieved: 11808 . Total input tokens: 2580649 . Total output tokens: 2350802
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.7571661379188299,
    "estimated_duration": 3599.5950517299243,
    "input_throughput": 263.3901831663967,
    "output_throughput": 238.32680834131958,
    "total_throughput": 501.7169915077163,
    "itl": 20.87300121372571,
    "ttft": 3704.0484742027597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1858,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.861325107580525,
    "arrivals": 3927,
    "finished_requests": 3923,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7572378078475595. Arrivals time: 0.022282151505351067 Scheduler time: 0.36726030241698027 Scheduler overhead time: 0.13179431948810816 Adapter cache time: 0.03508251765742898 Engine time: 0.1330758254043758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 270, 66, 66, 66, 66, 270, 66, 270, 66, 33, 270, 33, 33, 270, 66, 270, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 33, 66, 33, 33, 66, 66, 66, 33, 33, 270, 66, 66, 33, 270, 270, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 270, 33, 33, 66, 33, 270, 33, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 66, 270, 270, 33, 33, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 270, 33, 33, 270, 33, 33, 66]
Prompts retrieved: 11808 . Total input tokens: 2580649 . Total output tokens: 2350802
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.7531564608216286,
    "estimated_duration": 3599.5871146895192,
    "input_throughput": 263.3907639381518,
    "output_throughput": 238.3273338486756,
    "total_throughput": 501.71809778682734,
    "itl": 20.884721382012952,
    "ttft": 3704.8462692693015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1858,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.570839850753334,
    "arrivals": 3927,
    "finished_requests": 3923,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7532112807966769. Arrivals time: 0.022244491148740053 Scheduler time: 0.3629784043878317 Scheduler overhead time: 0.1325701717287302 Adapter cache time: 0.035240840166807175 Engine time: 0.13226937223225832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 135, 66, 66, 66, 66, 135, 66, 135, 66, 33, 135, 33, 33, 135, 66, 135, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 33, 66, 33, 33, 66, 66, 66, 33, 33, 135, 66, 66, 33, 135, 135, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 135, 33, 33, 66, 33, 135, 33, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 66, 135, 135, 33, 33, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 135, 33, 33, 135, 33, 33, 66]
Prompts retrieved: 7488 . Total input tokens: 1633565 . Total output tokens: 1492335
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.6436884361319244,
    "estimated_duration": 3599.023247235332,
    "input_throughput": 167.93139651550587,
    "output_throughput": 146.67701866208097,
    "total_throughput": 314.60841517758683,
    "itl": 20.26176876594656,
    "ttft": 10428.651470889876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.449132506656698,
    "arrivals": 2425,
    "finished_requests": 2418,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6437393850646913. Arrivals time: 0.01907687122002244 Scheduler time: 0.26578413089737296 Scheduler overhead time: 0.13054238306358457 Adapter cache time: 0.030331446323543787 Engine time: 0.13126866007223725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 135, 66, 66, 66, 66, 135, 66, 135, 66, 33, 135, 33, 33, 135, 66, 135, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 33, 66, 33, 33, 66, 66, 66, 33, 33, 135, 66, 66, 33, 135, 135, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 135, 33, 33, 66, 33, 135, 33, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 66, 135, 135, 33, 33, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 135, 33, 33, 135, 33, 33, 66]
Prompts retrieved: 7488 . Total input tokens: 1633565 . Total output tokens: 1492335
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.6609832649119198,
    "estimated_duration": 3599.029520168351,
    "input_throughput": 167.93110381926755,
    "output_throughput": 146.6767630111872,
    "total_throughput": 314.6078668304547,
    "itl": 20.266250883401955,
    "ttft": 10428.772656140269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.3720219000568,
    "arrivals": 2425,
    "finished_requests": 2418,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6610314981080592. Arrivals time: 0.018741711508482695 Scheduler time: 0.27368066366761923 Scheduler overhead time: 0.13712162803858519 Adapter cache time: 0.03053327277302742 Engine time: 0.13309329468756914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 135, 66, 66, 66, 66, 135, 66, 135, 66, 33, 135, 33, 33, 135, 66, 135, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 33, 66, 33, 33, 66, 66, 66, 33, 33, 135, 66, 66, 33, 135, 135, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 135, 33, 33, 66, 33, 135, 33, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 66, 135, 135, 33, 33, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 135, 33, 33, 135, 33, 33, 66]
Prompts retrieved: 7488 . Total input tokens: 1633565 . Total output tokens: 1492335
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.6444524098187685,
    "estimated_duration": 3599.0312895683246,
    "input_throughput": 167.9310212589154,
    "output_throughput": 146.67669090015517,
    "total_throughput": 314.60771215907056,
    "itl": 20.269617455112396,
    "ttft": 10429.107100635978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.636750268228264,
    "arrivals": 2425,
    "finished_requests": 2418,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6445152857340872. Arrivals time: 0.01863261079415679 Scheduler time: 0.2655882891267538 Scheduler overhead time: 0.1302365530282259 Adapter cache time: 0.030144128017127514 Engine time: 0.1326605137437582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 135, 66, 66, 66, 66, 135, 66, 135, 66, 33, 135, 33, 33, 135, 66, 135, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 33, 66, 33, 33, 66, 66, 66, 33, 33, 135, 66, 66, 33, 135, 135, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 135, 33, 33, 66, 33, 135, 33, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 66, 135, 135, 33, 33, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 135, 33, 33, 135, 33, 33, 66]
Prompts retrieved: 7488 . Total input tokens: 1633565 . Total output tokens: 1492335
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 0.6428594100289047,
    "estimated_duration": 3599.028131947378,
    "input_throughput": 167.93116859383218,
    "output_throughput": 146.6768195875048,
    "total_throughput": 314.60798818133696,
    "itl": 20.26421201849468,
    "ttft": 10428.6451781175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.738993136226275,
    "arrivals": 2425,
    "finished_requests": 2418,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6429111771285534. Arrivals time: 0.018806673120707273 Scheduler time: 0.26537017803639174 Scheduler overhead time: 0.13001982029527426 Adapter cache time: 0.03040253510698676 Engine time: 0.13116206554695964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 135, 66, 66, 66, 66, 135, 66, 135, 66, 33, 135, 33, 33, 135, 66, 135, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 33, 66, 33, 33, 66, 66, 66, 33, 33, 135, 66, 66, 33, 135, 135, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 135, 33, 33, 66, 33, 135, 33, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 66, 135, 135, 33, 33, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 135, 33, 33, 135, 33, 33, 66]
Prompts retrieved: 7488 . Total input tokens: 1633565 . Total output tokens: 1492335
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 0.6512031299062073,
    "estimated_duration": 3599.031082451148,
    "input_throughput": 167.93103092301615,
    "output_throughput": 146.67669934111092,
    "total_throughput": 314.60773026412704,
    "itl": 20.269010897767554,
    "ttft": 10429.020187291342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.54230483569196,
    "arrivals": 2425,
    "finished_requests": 2418,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6512536639347672. Arrivals time: 0.018893275409936905 Scheduler time: 0.27184852911159396 Scheduler overhead time: 0.13096747547388077 Adapter cache time: 0.03053476568311453 Engine time: 0.13145338045433164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 135, 66, 66, 66, 66, 135, 66, 135, 66, 33, 135, 33, 33, 135, 66, 135, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 33, 66, 33, 33, 66, 66, 66, 33, 33, 135, 66, 66, 33, 135, 135, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 135, 33, 33, 66, 33, 135, 33, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 66, 135, 135, 33, 33, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 135, 33, 33, 135, 33, 33, 66]
Prompts retrieved: 7488 . Total input tokens: 1633565 . Total output tokens: 1492335
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 0.6521039349026978,
    "estimated_duration": 3599.0211908425117,
    "input_throughput": 167.93149246740492,
    "output_throughput": 146.67710246974755,
    "total_throughput": 314.6085949371525,
    "itl": 20.259585765464575,
    "ttft": 10428.41720860935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.1226230240754,
    "arrivals": 2425,
    "finished_requests": 2418,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6521662836894393. Arrivals time: 0.01910815155133605 Scheduler time: 0.26974780298769474 Scheduler overhead time: 0.13007386401295662 Adapter cache time: 0.03055365616455674 Engine time: 0.13502155849710107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 135, 66, 66, 66, 66, 135, 66, 135, 66, 33, 135, 33, 33, 135, 66, 135, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 33, 66, 33, 33, 66, 66, 66, 33, 33, 135, 66, 66, 33, 135, 135, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 135, 33, 33, 66, 33, 135, 33, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 66, 135, 135, 33, 33, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 135, 33, 33, 135, 33, 33, 66]
Prompts retrieved: 7488 . Total input tokens: 1633565 . Total output tokens: 1492335
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 0.646053820848465,
    "estimated_duration": 3599.030046865265,
    "input_throughput": 167.9310792435366,
    "output_throughput": 146.6767415459042,
    "total_throughput": 314.6078207894408,
    "itl": 20.268429961076574,
    "ttft": 10428.946990041191,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.450344809275038,
    "arrivals": 2425,
    "finished_requests": 2418,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6461140979081392. Arrivals time: 0.019003117457032204 Scheduler time: 0.2681087711825967 Scheduler overhead time: 0.12965815886855125 Adapter cache time: 0.03032927867025137 Engine time: 0.13196646329015493 
