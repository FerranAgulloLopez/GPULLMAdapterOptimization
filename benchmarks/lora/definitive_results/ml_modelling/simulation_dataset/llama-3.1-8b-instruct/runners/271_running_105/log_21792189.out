INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.35860962606966,
    "estimated_duration": 3600.031186423137,
    "input_throughput": 6469.671453913466,
    "output_throughput": 5658.635702053505,
    "total_throughput": 12128.307155966972,
    "itl": 96.26707850944356,
    "ttft": 2117392.252719474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.896930768205792,
    "arrivals": 2257312,
    "finished_requests": 94316,
    "scheduler_time": 260.5804433981692
}
#Debug simulation 
Total elapsed time: 75.35882708709687. Arrivals time: 0.5118679306469858 Scheduler time: 74.64799469383433 Scheduler overhead time: 0.0747474730014801 Adapter cache time: 0.020039426628500223 Engine time: 0.07428033091127872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 71.52521012583748,
    "estimated_duration": 3600.0391319047667,
    "input_throughput": 6332.712830245726,
    "output_throughput": 5531.950978950311,
    "total_throughput": 11864.663809196036,
    "itl": 90.67149010933271,
    "ttft": 2125387.7569695413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.1221597542614035,
    "arrivals": 2257312,
    "finished_requests": 92211,
    "scheduler_time": 267.1293119735206
}
#Debug simulation 
Total elapsed time: 71.5254115127027. Arrivals time: 0.48613381339237094 Scheduler time: 70.83235827833414 Scheduler overhead time: 0.0769674377515912 Adapter cache time: 0.02058229921385646 Engine time: 0.07842212496325374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 75.05623334599659,
    "estimated_duration": 3600.0136338996404,
    "input_throughput": 6487.390430991649,
    "output_throughput": 5663.517717824396,
    "total_throughput": 12150.908148816045,
    "itl": 96.25928212306859,
    "ttft": 2121780.293977294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 672,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.610673916898661,
    "arrivals": 2257312,
    "finished_requests": 94601,
    "scheduler_time": 260.3868121420596
}
#Debug simulation 
Total elapsed time: 75.05640277499333. Arrivals time: 0.5255713374353945 Scheduler time: 74.33160190982744 Scheduler overhead time: 0.0748209748417139 Adapter cache time: 0.020363718271255493 Engine time: 0.0745105305686593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 72.02266304194927,
    "estimated_duration": 3600.0519585394163,
    "input_throughput": 6321.2095997727365,
    "output_throughput": 5524.426655238856,
    "total_throughput": 11845.636255011592,
    "itl": 90.36729586404446,
    "ttft": 2125069.5639452403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 682,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.081718539325549,
    "arrivals": 2257312,
    "finished_requests": 92125,
    "scheduler_time": 267.58672155862604
}
#Debug simulation 
Total elapsed time: 72.02283073682338. Arrivals time: 0.49855792289599776 Scheduler time: 71.3189806365408 Scheduler overhead time: 0.07749157305806875 Adapter cache time: 0.02029178896918893 Engine time: 0.07711183791980147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 74.72623613476753,
    "estimated_duration": 3600.1087689634505,
    "input_throughput": 6498.079225182849,
    "output_throughput": 5675.505466987027,
    "total_throughput": 12173.584692169876,
    "itl": 96.13646462977565,
    "ttft": 2118314.407053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3474501605285125,
    "arrivals": 2257312,
    "finished_requests": 94614,
    "scheduler_time": 259.92961727548163
}
#Debug simulation 
Total elapsed time: 74.72640481078997. Arrivals time: 0.5167492553591728 Scheduler time: 74.01139321736991 Scheduler overhead time: 0.07466389425098896 Adapter cache time: 0.020011943764984608 Engine time: 0.07399595016613603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 71.02585517801344,
    "estimated_duration": 3600.064590892934,
    "input_throughput": 6322.479895938323,
    "output_throughput": 5518.348768034875,
    "total_throughput": 11840.828663973198,
    "itl": 90.40852148598778,
    "ttft": 2130036.443321759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.93882564563307,
    "arrivals": 2257312,
    "finished_requests": 92037,
    "scheduler_time": 267.87270040185086
}
#Debug simulation 
Total elapsed time: 71.02602472202852. Arrivals time: 0.5069376560859382 Scheduler time: 70.31544466363266 Scheduler overhead time: 0.07639744225889444 Adapter cache time: 0.020294864661991596 Engine time: 0.07650232315063477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 78.30906015215442,
    "estimated_duration": 3600.068284161044,
    "input_throughput": 6548.82661635241,
    "output_throughput": 5726.7466538636745,
    "total_throughput": 12275.573270216086,
    "itl": 98.51166979425476,
    "ttft": 2113791.6626045047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.324515506895331,
    "arrivals": 2233996,
    "finished_requests": 95591,
    "scheduler_time": 257.68685050511914
}
#Debug simulation 
Total elapsed time: 78.30923117091879. Arrivals time: 0.9778264411725104 Scheduler time: 77.13452369207516 Scheduler overhead time: 0.07442210568115115 Adapter cache time: 0.020129951648414135 Engine time: 0.07356383837759495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 74.00141234695911,
    "estimated_duration": 3600.082715325369,
    "input_throughput": 6499.299002324542,
    "output_throughput": 5683.919125772261,
    "total_throughput": 12183.218128096803,
    "itl": 96.67045501187636,
    "ttft": 2115775.5755177774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.910255794012926,
    "arrivals": 2233996,
    "finished_requests": 94813,
    "scheduler_time": 259.363824998937
}
#Debug simulation 
Total elapsed time: 74.00158052658662. Arrivals time: 0.5182354203425348 Scheduler time: 73.28574218973517 Scheduler overhead time: 0.07443883595988154 Adapter cache time: 0.019657904747873545 Engine time: 0.07391939871013165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 72.83800471527502,
    "estimated_duration": 3600.039215241676,
    "input_throughput": 6347.725297893986,
    "output_throughput": 5550.753423850833,
    "total_throughput": 11898.47872174482,
    "itl": 91.3152647651102,
    "ttft": 2124335.0193529394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.884699041638556,
    "arrivals": 2233996,
    "finished_requests": 92741,
    "scheduler_time": 266.09050895269166
}
#Debug simulation 
Total elapsed time: 72.83818400232121. Arrivals time: 0.6573031768202782 Scheduler time: 71.97743393853307 Scheduler overhead time: 0.07645170809701085 Adapter cache time: 0.020246743224561214 Engine time: 0.07609652262181044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 76.78901913203299,
    "estimated_duration": 3600.102198302307,
    "input_throughput": 6535.529188892283,
    "output_throughput": 5710.915931690879,
    "total_throughput": 12246.445120583161,
    "itl": 97.04789231964749,
    "ttft": 2113020.5364394584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.577366091222495,
    "arrivals": 2233996,
    "finished_requests": 95404,
    "scheduler_time": 257.906752376284
}
#Debug simulation 
Total elapsed time: 76.78918585414067. Arrivals time: 0.5295734242536128 Scheduler time: 76.06165301380679 Scheduler overhead time: 0.07485078135505319 Adapter cache time: 0.02020028419792652 Engine time: 0.07359151029959321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 72.33875981485471,
    "estimated_duration": 3600.008389483442,
    "input_throughput": 6332.116354670748,
    "output_throughput": 5545.6551318939155,
    "total_throughput": 11877.771486564663,
    "itl": 91.31855447332852,
    "ttft": 2124662.0839474667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 656,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.896906624194267,
    "arrivals": 2233996,
    "finished_requests": 92487,
    "scheduler_time": 266.5243860337028
}
#Debug simulation 
Total elapsed time: 72.33893487788737. Arrivals time: 0.4838596675544977 Scheduler time: 71.65162046207115 Scheduler overhead time: 0.07702836534008384 Adapter cache time: 0.01982714608311653 Engine time: 0.07588282786309719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 75.82444063993171,
    "estimated_duration": 3600.0720208906087,
    "input_throughput": 6476.356824170451,
    "output_throughput": 5671.587924218481,
    "total_throughput": 12147.944748388933,
    "itl": 96.26741004160893,
    "ttft": 2115356.1894358643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2963787930039485,
    "arrivals": 2233996,
    "finished_requests": 94660,
    "scheduler_time": 260.10457434812906
}
#Debug simulation 
Total elapsed time: 75.82460541790351. Arrivals time: 0.5079964506439865 Scheduler time: 75.1183657720685 Scheduler overhead time: 0.0742928464896977 Adapter cache time: 0.019848155323415995 Engine time: 0.0743979113176465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 72.64530188310891,
    "estimated_duration": 3600.0054676136574,
    "input_throughput": 6341.221757958142,
    "output_throughput": 5544.126857460076,
    "total_throughput": 11885.34861541822,
    "itl": 91.07522292076423,
    "ttft": 2124028.9347432726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.8545494856872,
    "arrivals": 2233996,
    "finished_requests": 92534,
    "scheduler_time": 266.65089083796533
}
#Debug simulation 
Total elapsed time: 72.64546770928428. Arrivals time: 0.48834092170000076 Scheduler time: 71.95373590709642 Scheduler overhead time: 0.0765955108217895 Adapter cache time: 0.019988499581813812 Engine time: 0.07586664101108909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 80.20399568602443,
    "estimated_duration": 3600.1042735629703,
    "input_throughput": 6567.231169835298,
    "output_throughput": 5705.878618807373,
    "total_throughput": 12273.10978864267,
    "itl": 98.34193897947621,
    "ttft": 2105160.189549923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.060019145617325,
    "arrivals": 2222461,
    "finished_requests": 95616,
    "scheduler_time": 258.2316178815717
}
#Debug simulation 
Total elapsed time: 80.20415565697476. Arrivals time: 0.5298068919219077 Scheduler time: 79.47546464251354 Scheduler overhead time: 0.07486014999449253 Adapter cache time: 0.019606985617429018 Engine time: 0.07500443933531642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 72.7205902361311,
    "estimated_duration": 3600.000235570599,
    "input_throughput": 6583.18956922058,
    "output_throughput": 5722.931569957106,
    "total_throughput": 12306.121139177685,
    "itl": 97.334482839418,
    "ttft": 2105629.638550121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 689,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.039879617723643,
    "arrivals": 2222461,
    "finished_requests": 95789,
    "scheduler_time": 257.186181711684
}
#Debug simulation 
Total elapsed time: 72.72075611492619. Arrivals time: 0.6961205508559942 Scheduler time: 71.82662095455453 Scheduler overhead time: 0.07399425841867924 Adapter cache time: 0.020115674939006567 Engine time: 0.07413502037525177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 71.73938363511115,
    "estimated_duration": 3600.0006286301514,
    "input_throughput": 6374.859164603144,
    "output_throughput": 5541.163476849323,
    "total_throughput": 11916.022641452468,
    "itl": 90.9944539417818,
    "ttft": 2116158.725988227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.874446203270958,
    "arrivals": 2222461,
    "finished_requests": 92857,
    "scheduler_time": 266.7060381000724
}
#Debug simulation 
Total elapsed time: 71.73954760702327. Arrivals time: 0.4992198827676475 Scheduler time: 71.03882258757949 Scheduler overhead time: 0.07597468793392181 Adapter cache time: 0.020071751438081264 Engine time: 0.0748878438025713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 76.43485787278041,
    "estimated_duration": 3600.036896712971,
    "input_throughput": 6549.429540993917,
    "output_throughput": 5705.560967654066,
    "total_throughput": 12254.990508647983,
    "itl": 96.601729108299,
    "ttft": 2106610.370655793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.357253006491804,
    "arrivals": 2222461,
    "finished_requests": 95360,
    "scheduler_time": 259.225219643462
}
#Debug simulation 
Total elapsed time: 76.43502363096923. Arrivals time: 0.5080386493355036 Scheduler time: 75.72692603245378 Scheduler overhead time: 0.07558241626247764 Adapter cache time: 0.020298431627452374 Engine time: 0.07441376615315676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 68.89883741270751,
    "estimated_duration": 3600.000729366956,
    "input_throughput": 6418.505088487356,
    "output_throughput": 5579.319147396938,
    "total_throughput": 11997.824235884294,
    "itl": 91.42281637088094,
    "ttft": 2117670.6346160537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.031348030776748,
    "arrivals": 2222461,
    "finished_requests": 93409,
    "scheduler_time": 264.7114422272461
}
#Debug simulation 
Total elapsed time: 68.89900534180924. Arrivals time: 0.50189169915393 Scheduler time: 68.19633697206154 Scheduler overhead time: 0.07580633787438273 Adapter cache time: 0.019807950127869844 Engine time: 0.07481577293947339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 74.90272058919072,
    "estimated_duration": 3600.0128159235514,
    "input_throughput": 6555.046664173076,
    "output_throughput": 5700.146374266621,
    "total_throughput": 12255.193038439698,
    "itl": 96.51280760471592,
    "ttft": 2103349.5892962813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 640,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.085709401965122,
    "arrivals": 2222461,
    "finished_requests": 95329,
    "scheduler_time": 259.162929218279
}
#Debug simulation 
Total elapsed time: 74.90289759775624. Arrivals time: 0.5058411075733602 Scheduler time: 74.19801942864433 Scheduler overhead time: 0.07434134557843208 Adapter cache time: 0.020001523196697235 Engine time: 0.0748922904022038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 70.52367443265393,
    "estimated_duration": 3600.0963211915837,
    "input_throughput": 6384.118353920261,
    "output_throughput": 5545.163578676827,
    "total_throughput": 11929.281932597087,
    "itl": 90.70287581266517,
    "ttft": 2117392.7168275174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.699813230074975,
    "arrivals": 2222461,
    "finished_requests": 92937,
    "scheduler_time": 266.58104265528084
}
#Debug simulation 
Total elapsed time: 70.52383854892105. Arrivals time: 0.49513045186176896 Scheduler time: 69.82644217321649 Scheduler overhead time: 0.07584443548694253 Adapter cache time: 0.020016466733068228 Engine time: 0.07582983607426286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 79.59863479202613,
    "estimated_duration": 3600.0021987923305,
    "input_throughput": 6649.6559385520895,
    "output_throughput": 5744.932324468574,
    "total_throughput": 12394.588263020663,
    "itl": 98.24641965527461,
    "ttft": 2108205.4024162204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 591,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.907933737882472,
    "arrivals": 2216625,
    "finished_requests": 96303,
    "scheduler_time": 256.23097902051205
}
#Debug simulation 
Total elapsed time: 79.59880644408986. Arrivals time: 0.5348218693397939 Scheduler time: 78.86498252907768 Scheduler overhead time: 0.07501452695578337 Adapter cache time: 0.01928083784878254 Engine time: 0.07526630582287908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.54654805082828,
    "estimated_duration": 3600.00134830739,
    "input_throughput": 6589.62697643257,
    "output_throughput": 5703.928697041885,
    "total_throughput": 12293.555673474455,
    "itl": 96.4112028813255,
    "ttft": 2109808.297472894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5982652272889455,
    "arrivals": 2216625,
    "finished_requests": 95616,
    "scheduler_time": 258.2349208733205
}
#Debug simulation 
Total elapsed time: 75.54671650286764. Arrivals time: 0.6694789230823517 Scheduler time: 74.6770797665231 Scheduler overhead time: 0.07646470842882991 Adapter cache time: 0.01969187008216977 Engine time: 0.07409183168783784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.15247573703527,
    "estimated_duration": 3600.061194604579,
    "input_throughput": 6412.239057101787,
    "output_throughput": 5559.28188942862,
    "total_throughput": 11971.520946530407,
    "itl": 90.7148944415421,
    "ttft": 2118469.2621886665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 606,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.567718443074296,
    "arrivals": 2216625,
    "finished_requests": 93096,
    "scheduler_time": 265.8771485427837
}
#Debug simulation 
Total elapsed time: 73.15264796093106. Arrivals time: 0.5060957227833569 Scheduler time: 72.44216995825991 Scheduler overhead time: 0.07743921177461743 Adapter cache time: 0.019812773447483778 Engine time: 0.07647902518510818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 75.4449604828842,
    "estimated_duration": 3600.0691158525724,
    "input_throughput": 6648.196806724898,
    "output_throughput": 5738.352885791093,
    "total_throughput": 12386.549692515991,
    "itl": 96.97534009102178,
    "ttft": 2106754.7740667514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4419155304972024,
    "arrivals": 2216625,
    "finished_requests": 96254,
    "scheduler_time": 256.56137710775704
}
#Debug simulation 
Total elapsed time: 75.44512144988403. Arrivals time: 0.506240934599191 Scheduler time: 74.73838425939903 Scheduler overhead time: 0.07589809456840158 Adapter cache time: 0.019998878240585327 Engine time: 0.07458512345328927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 71.61049405112863,
    "estimated_duration": 3600.0457980712463,
    "input_throughput": 6453.2709590657905,
    "output_throughput": 5580.696504128861,
    "total_throughput": 12033.967463194651,
    "itl": 91.05904304491519,
    "ttft": 2121558.085308742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.497460196027565,
    "arrivals": 2216625,
    "finished_requests": 93533,
    "scheduler_time": 264.7898168551095
}
#Debug simulation 
Total elapsed time: 71.61066752206534. Arrivals time: 0.66657575359568 Scheduler time: 70.74109529750422 Scheduler overhead time: 0.07673672633245587 Adapter cache time: 0.019556873943656683 Engine time: 0.07610193500295281 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 77.08937019202858,
    "estimated_duration": 3600.002670569425,
    "input_throughput": 6621.413143626808,
    "output_throughput": 5727.219918072666,
    "total_throughput": 12348.633061699475,
    "itl": 96.65983664586973,
    "ttft": 2107764.472664983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9388792203320007,
    "arrivals": 2216625,
    "finished_requests": 95960,
    "scheduler_time": 257.16111326803264
}
#Debug simulation 
Total elapsed time: 77.08953263377771. Arrivals time: 0.5151053252629936 Scheduler time: 76.37606116430834 Scheduler overhead time: 0.07482971251010895 Adapter cache time: 0.019772530999034643 Engine time: 0.07392754592001438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.57914026407525,
    "estimated_duration": 3600.0128018469727,
    "input_throughput": 6445.032636577348,
    "output_throughput": 5595.828434183528,
    "total_throughput": 12040.861070760875,
    "itl": 91.1295481277804,
    "ttft": 2118056.663040794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.413947587441685,
    "arrivals": 2216625,
    "finished_requests": 93532,
    "scheduler_time": 264.7949293154016
}
#Debug simulation 
Total elapsed time: 73.57930694334209. Arrivals time: 0.4918478187173605 Scheduler time: 72.88423263886943 Scheduler overhead time: 0.07691384432837367 Adapter cache time: 0.0196780557744205 Engine time: 0.0760438428260386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 75.82589671900496,
    "estimated_duration": 3600.0523249271455,
    "input_throughput": 6626.067858745008,
    "output_throughput": 5788.256980520888,
    "total_throughput": 12414.324839265895,
    "itl": 99.36558396232103,
    "ttft": 2108465.4805392604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9806702372339235,
    "arrivals": 2213766,
    "finished_requests": 96655,
    "scheduler_time": 253.967431314324
}
#Debug simulation 
Total elapsed time: 75.82606200082228. Arrivals time: 0.5250441101379693 Scheduler time: 75.10660084988922 Scheduler overhead time: 0.07306826487183571 Adapter cache time: 0.019114668015390635 Engine time: 0.07326148683205247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 72.90397506719455,
    "estimated_duration": 3600.009924711081,
    "input_throughput": 6580.145748320716,
    "output_throughput": 5734.171413890396,
    "total_throughput": 12314.317162211111,
    "itl": 97.20432315412431,
    "ttft": 2114522.5864077862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.423113442263572,
    "arrivals": 2213766,
    "finished_requests": 95862,
    "scheduler_time": 256.68343015867896
}
#Debug simulation 
Total elapsed time: 72.90414804918692. Arrivals time: 0.504307250957936 Scheduler time: 72.20168301323429 Scheduler overhead time: 0.07491061696782708 Adapter cache time: 0.019330673851072788 Engine time: 0.07374607818201184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 70.58409443264827,
    "estimated_duration": 3600.0411333435345,
    "input_throughput": 6404.956539645092,
    "output_throughput": 5585.406737096084,
    "total_throughput": 11990.363276741176,
    "itl": 91.29939049924315,
    "ttft": 2122031.0547668845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.37342472350694,
    "arrivals": 2213766,
    "finished_requests": 93317,
    "scheduler_time": 264.5272938198974
}
#Debug simulation 
Total elapsed time: 70.5842568888329. Arrivals time: 0.5082541643641889 Scheduler time: 69.87483522295952 Scheduler overhead time: 0.07602122612297535 Adapter cache time: 0.019367178436368704 Engine time: 0.07557231048122048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 72.75038680899888,
    "estimated_duration": 3600.0944219631524,
    "input_throughput": 6568.726879975717,
    "output_throughput": 5725.353166920622,
    "total_throughput": 12294.080046896339,
    "itl": 96.9947638960754,
    "ttft": 2113735.06902655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1537985734408664,
    "arrivals": 2213766,
    "finished_requests": 95640,
    "scheduler_time": 257.1488686397929
}
#Debug simulation 
Total elapsed time: 72.7505635949783. Arrivals time: 0.4957014163956046 Scheduler time: 72.05838656704873 Scheduler overhead time: 0.07435375638306141 Adapter cache time: 0.01920033898204565 Engine time: 0.07336907600983977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 69.88686456810683,
    "estimated_duration": 3600.030377376142,
    "input_throughput": 6411.372566478878,
    "output_throughput": 5595.710004725668,
    "total_throughput": 12007.082571204546,
    "itl": 91.4738753693053,
    "ttft": 2123220.8354264637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5099816882657215,
    "arrivals": 2213766,
    "finished_requests": 93452,
    "scheduler_time": 263.9400851540015
}
#Debug simulation 
Total elapsed time: 69.88703231001273. Arrivals time: 0.49302251171320677 Scheduler time: 69.19453518977389 Scheduler overhead time: 0.07576053217053413 Adapter cache time: 0.019211788196116686 Engine time: 0.07451303256675601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 75.00736476154998,
    "estimated_duration": 3600.0300824511023,
    "input_throughput": 6574.350063176582,
    "output_throughput": 5730.229339071139,
    "total_throughput": 12304.579402247722,
    "itl": 97.13160588012612,
    "ttft": 2114514.4153407244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7537455130554562,
    "arrivals": 2213766,
    "finished_requests": 95798,
    "scheduler_time": 257.00113313658125
}
#Debug simulation 
Total elapsed time: 75.00753753678873. Arrivals time: 0.5200701979920268 Scheduler time: 74.29005691083148 Scheduler overhead time: 0.07411135965958238 Adapter cache time: 0.01915215328335762 Engine time: 0.07443687599152327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 72.05672276299447,
    "estimated_duration": 3600.087828171018,
    "input_throughput": 6394.807876588941,
    "output_throughput": 5581.391610161984,
    "total_throughput": 11976.199486750926,
    "itl": 91.37479790095561,
    "ttft": 2122785.258665734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3075065993331725,
    "arrivals": 2213766,
    "finished_requests": 93246,
    "scheduler_time": 264.6605587573161
}
#Debug simulation 
Total elapsed time: 72.05688803317025. Arrivals time: 0.4853002028539777 Scheduler time: 71.37063707690686 Scheduler overhead time: 0.07571637630462646 Adapter cache time: 0.01895523490384221 Engine time: 0.07615055842325091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 75.64165791729465,
    "estimated_duration": 3600.073209312744,
    "input_throughput": 6632.113185431015,
    "output_throughput": 5811.147380526116,
    "total_throughput": 12443.26056595713,
    "itl": 99.85787576014643,
    "ttft": 2095265.5829772225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6764994217642166,
    "arrivals": 2212338,
    "finished_requests": 97417,
    "scheduler_time": 252.80083666302562
}
#Debug simulation 
Total elapsed time: 75.64182298909873. Arrivals time: 0.5187606564722955 Scheduler time: 74.92856281623244 Scheduler overhead time: 0.07429919298738241 Adapter cache time: 0.018502144142985344 Engine time: 0.07246765587478876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 72.8480028999038,
    "estimated_duration": 3600.047347344988,
    "input_throughput": 6585.968936626855,
    "output_throughput": 5770.3413304606465,
    "total_throughput": 12356.310267087501,
    "itl": 97.6238733338841,
    "ttft": 2100006.2659264468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 552,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.041730782240634,
    "arrivals": 2212338,
    "finished_requests": 96698,
    "scheduler_time": 255.03937393342537
}
#Debug simulation 
Total elapsed time: 72.84817353403196. Arrivals time: 0.6881058998405933 Scheduler time: 71.96424925047904 Scheduler overhead time: 0.07416755752637982 Adapter cache time: 0.018675556406378746 Engine time: 0.07391585409641266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 69.32257733587176,
    "estimated_duration": 3600.023934914368,
    "input_throughput": 6404.571029761344,
    "output_throughput": 5613.724343330155,
    "total_throughput": 12018.295373091498,
    "itl": 91.7017926746095,
    "ttft": 2111268.448653193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 555,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.16950915217403,
    "arrivals": 2212338,
    "finished_requests": 94049,
    "scheduler_time": 263.1302127066822
}
#Debug simulation 
Total elapsed time: 69.3227459359914. Arrivals time: 0.505327045917511 Scheduler time: 68.61723474552855 Scheduler overhead time: 0.07564081065356731 Adapter cache time: 0.018810343462973833 Engine time: 0.0754432575777173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 73.51327908085659,
    "estimated_duration": 3600.0042885975126,
    "input_throughput": 6578.439663255562,
    "output_throughput": 5767.58424032016,
    "total_throughput": 12346.02390357572,
    "itl": 97.49496828982568,
    "ttft": 2097695.7679346306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 554,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8046188489254513,
    "arrivals": 2212338,
    "finished_requests": 96652,
    "scheduler_time": 255.17275403273544
}
#Debug simulation 
Total elapsed time: 73.51344248000532. Arrivals time: 0.5276778689585626 Scheduler time: 72.79012596746907 Scheduler overhead time: 0.07445109961554408 Adapter cache time: 0.01830450724810362 Engine time: 0.07360040862113237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 68.23531952919438,
    "estimated_duration": 3600.0258722935214,
    "input_throughput": 6400.891220628761,
    "output_throughput": 5612.989938632438,
    "total_throughput": 12013.881159261198,
    "itl": 91.70337958383672,
    "ttft": 2110116.7304462115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.204030527793835,
    "arrivals": 2212338,
    "finished_requests": 94004,
    "scheduler_time": 263.1052947956123
}
#Debug simulation 
Total elapsed time: 68.23548407200724. Arrivals time: 0.6646539838984609 Scheduler time: 67.37061289185658 Scheduler overhead time: 0.0755598209798336 Adapter cache time: 0.01898721233010292 Engine time: 0.07541160378605127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 71.69651413476095,
    "estimated_duration": 3600.077618302843,
    "input_throughput": 6575.5968370370365,
    "output_throughput": 5762.1279870569115,
    "total_throughput": 12337.724824093948,
    "itl": 97.41280504820294,
    "ttft": 2099130.977990713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7345937502337447,
    "arrivals": 2212338,
    "finished_requests": 96500,
    "scheduler_time": 255.49565406318547
}
#Debug simulation 
Total elapsed time: 71.6966795148328. Arrivals time: 0.5091378847137094 Scheduler time: 70.99354120157659 Scheduler overhead time: 0.07344135828316212 Adapter cache time: 0.01886586146429181 Engine time: 0.07266574632376432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_384_slots_32_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 70.56431777728721,
    "estimated_duration": 3600.002958431911,
    "input_throughput": 6398.916685900189,
    "output_throughput": 5608.308168945041,
    "total_throughput": 12007.22485484523,
    "itl": 91.56704661794275,
    "ttft": 2108752.3736461066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.077504319082981,
    "arrivals": 2212338,
    "finished_requests": 94084,
    "scheduler_time": 263.36337472862186
}
#Debug simulation 
Total elapsed time: 70.564482008107. Arrivals time: 0.517168689519167 Scheduler time: 69.8472878751345 Scheduler overhead time: 0.07580118300393224 Adapter cache time: 0.018943872768431902 Engine time: 0.07494457997381687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 80.1685667540878,
    "estimated_duration": 3600.0462929605474,
    "input_throughput": 6571.3985529183465,
    "output_throughput": 5766.888064910647,
    "total_throughput": 12338.286617828993,
    "itl": 98.14776706035384,
    "ttft": 2095672.531264867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 698,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6154615043011376,
    "arrivals": 2026382,
    "finished_requests": 96040,
    "scheduler_time": 255.0674017961147
}
#Debug simulation 
Total elapsed time: 80.16873311297968. Arrivals time: 0.5152669944800436 Scheduler time: 79.45622078794986 Scheduler overhead time: 0.07466977322474122 Adapter cache time: 0.020161921624094248 Engine time: 0.07333540637046099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 77.31994128739461,
    "estimated_duration": 3600.0389726458466,
    "input_throughput": 6537.107564339454,
    "output_throughput": 5727.60271671327,
    "total_throughput": 12264.710281052723,
    "itl": 96.30850845004404,
    "ttft": 2101029.941166338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 696,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.092896390147514,
    "arrivals": 2026382,
    "finished_requests": 95463,
    "scheduler_time": 256.9452566735841
}
#Debug simulation 
Total elapsed time: 77.32011108240113. Arrivals time: 0.5342879220843315 Scheduler time: 76.58623632369563 Scheduler overhead time: 0.07519322540611029 Adapter cache time: 0.02052525756880641 Engine time: 0.07442802144214511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.98272475926206,
    "estimated_duration": 3600.022184432145,
    "input_throughput": 6384.138714307746,
    "output_throughput": 5597.532172757593,
    "total_throughput": 11981.67088706534,
    "itl": 91.26339933849557,
    "ttft": 2111395.1763423826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.151046434966874,
    "arrivals": 2026382,
    "finished_requests": 93249,
    "scheduler_time": 263.82218514606643
}
#Debug simulation 
Total elapsed time: 73.98289585998282. Arrivals time: 0.9682613587938249 Scheduler time: 72.81117322482169 Scheduler overhead time: 0.07651925925165415 Adapter cache time: 0.020226124674081802 Engine time: 0.07634100411087275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 78.43876828625798,
    "estimated_duration": 3600.057529192429,
    "input_throughput": 6575.269369461993,
    "output_throughput": 5763.723727119054,
    "total_throughput": 12338.993096581047,
    "itl": 97.01493217662956,
    "ttft": 2099716.494954115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 697,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.777213045279488,
    "arrivals": 2026382,
    "finished_requests": 96063,
    "scheduler_time": 255.1275340316021
}
#Debug simulation 
Total elapsed time: 78.43893454223871. Arrivals time: 0.5467785941436887 Scheduler time: 77.69122636551037 Scheduler overhead time: 0.07582858204841614 Adapter cache time: 0.02054234640672803 Engine time: 0.07434542570263147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 73.07395532075316,
    "estimated_duration": 3600.009226262533,
    "input_throughput": 6394.550556166052,
    "output_throughput": 5602.5434192954335,
    "total_throughput": 11997.093975461486,
    "itl": 91.19829915211193,
    "ttft": 2110051.026837182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 701,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.218541972814154,
    "arrivals": 2026382,
    "finished_requests": 93368,
    "scheduler_time": 263.43131883277135
}
#Debug simulation 
Total elapsed time: 73.0741229057312. Arrivals time: 0.5048644132912159 Scheduler time: 72.366080859676 Scheduler overhead time: 0.07622258411720395 Adapter cache time: 0.02064863545820117 Engine time: 0.07581685902550817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 79.22240124410018,
    "estimated_duration": 3600.0168704051407,
    "input_throughput": 6529.665789414638,
    "output_throughput": 5733.863963164422,
    "total_throughput": 12263.529752579061,
    "itl": 96.31219417040148,
    "ttft": 2099824.3267074265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.194236057954821,
    "arrivals": 2026382,
    "finished_requests": 95438,
    "scheduler_time": 257.05756357012956
}
#Debug simulation 
Total elapsed time: 79.22256921883672. Arrivals time: 0.5411495328880847 Scheduler time: 78.48136419337243 Scheduler overhead time: 0.07522981520742178 Adapter cache time: 0.02014429261907935 Engine time: 0.07500692270696163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.911666165106,
    "estimated_duration": 3600.1035107557095,
    "input_throughput": 6387.132461970013,
    "output_throughput": 5606.950450089357,
    "total_throughput": 11994.08291205937,
    "itl": 91.36247798349694,
    "ttft": 2108926.5440100445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 702,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.18312379784885,
    "arrivals": 2026382,
    "finished_requests": 93350,
    "scheduler_time": 263.25016813276017
}
#Debug simulation 
Total elapsed time: 73.91183476615697. Arrivals time: 0.5025538424961269 Scheduler time: 73.20596120459959 Scheduler overhead time: 0.07660449296236038 Adapter cache time: 0.020696301478892565 Engine time: 0.07576390914618969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.93624392291531,
    "estimated_duration": 3600.0757774099397,
    "input_throughput": 6614.855762045341,
    "output_throughput": 5783.891864348652,
    "total_throughput": 12398.747626393993,
    "itl": 99.02214663273645,
    "ttft": 2089571.9471785233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0005074643297736,
    "arrivals": 1888011,
    "finished_requests": 96535,
    "scheduler_time": 254.55334714866186
}
#Debug simulation 
Total elapsed time: 82.93641708791256. Arrivals time: 0.5186009802855551 Scheduler time: 82.21943078795448 Scheduler overhead time: 0.07483063591644168 Adapter cache time: 0.019794567953795195 Engine time: 0.07432191073894501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 78.93894955795258,
    "estimated_duration": 3600.0756676680903,
    "input_throughput": 6512.174233044885,
    "output_throughput": 5696.589708983515,
    "total_throughput": 12208.763942028401,
    "itl": 96.23718815028624,
    "ttft": 2098894.66915487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.428666326156823,
    "arrivals": 1888011,
    "finished_requests": 95020,
    "scheduler_time": 258.5752070978488
}
#Debug simulation 
Total elapsed time: 78.9391188910231. Arrivals time: 0.5221621091477573 Scheduler time: 78.21618774114177 Scheduler overhead time: 0.07549430895596743 Adapter cache time: 0.01968839531764388 Engine time: 0.07560797547921538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.04598480416462,
    "estimated_duration": 3600.0641463339366,
    "input_throughput": 6369.636780875555,
    "output_throughput": 5570.427965963201,
    "total_throughput": 11940.064746838756,
    "itl": 90.96833874569857,
    "ttft": 2108955.825732202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.570200138664838,
    "arrivals": 1888011,
    "finished_requests": 92885,
    "scheduler_time": 265.43743025013254
}
#Debug simulation 
Total elapsed time: 75.0461544492282. Arrivals time: 0.4836445115506649 Scheduler time: 74.35770173138008 Scheduler overhead time: 0.07750393077731133 Adapter cache time: 0.020059064496308565 Engine time: 0.07650922192260623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 79.18736289488152,
    "estimated_duration": 3600.0915997399175,
    "input_throughput": 6503.5509101189145,
    "output_throughput": 5696.108121660421,
    "total_throughput": 12199.659031779336,
    "itl": 96.01196746592082,
    "ttft": 2097355.5041660094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 629,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.308400896987872,
    "arrivals": 1888011,
    "finished_requests": 94840,
    "scheduler_time": 259.36043487510256
}
#Debug simulation 
Total elapsed time: 79.18752532312647. Arrivals time: 0.5058243302628398 Scheduler time: 78.48150921147317 Scheduler overhead time: 0.07476865313947201 Adapter cache time: 0.02020605094730854 Engine time: 0.07541619660332799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 74.67100186506286,
    "estimated_duration": 3600.0604531535964,
    "input_throughput": 6350.611690415569,
    "output_throughput": 5548.359051166131,
    "total_throughput": 11898.9707415817,
    "itl": 90.52218412032386,
    "ttft": 2107865.1728619467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.597681385544161,
    "arrivals": 1888011,
    "finished_requests": 92556,
    "scheduler_time": 266.2423912653606
}
#Debug simulation 
Total elapsed time: 74.67117440933362. Arrivals time: 0.47960258927196264 Scheduler time: 73.98515093605965 Scheduler overhead time: 0.07777402782812715 Adapter cache time: 0.020146958529949188 Engine time: 0.0771556505933404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 77.57859605969861,
    "estimated_duration": 3600.0682022279475,
    "input_throughput": 6549.959799485752,
    "output_throughput": 5720.514124497696,
    "total_throughput": 12270.473923983447,
    "itl": 96.66298448805539,
    "ttft": 2098190.0039116284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 629,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.015486271618847,
    "arrivals": 1888011,
    "finished_requests": 95494,
    "scheduler_time": 257.3482931596238
}
#Debug simulation 
Total elapsed time: 77.57876714598387. Arrivals time: 0.5173504343256354 Scheduler time: 76.86167231667787 Scheduler overhead time: 0.07544945552945137 Adapter cache time: 0.01979751093313098 Engine time: 0.07511999970301986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.07897804724053,
    "estimated_duration": 3600.00068280478,
    "input_throughput": 6371.6654581656585,
    "output_throughput": 5575.2625536622445,
    "total_throughput": 11946.928011827902,
    "itl": 91.00252796161269,
    "ttft": 2106952.2901067133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.597156289406155,
    "arrivals": 1888011,
    "finished_requests": 92995,
    "scheduler_time": 264.854994027472
}
#Debug simulation 
Total elapsed time: 75.07914436608553. Arrivals time: 0.9634596267715096 Scheduler time: 73.91148429363966 Scheduler overhead time: 0.07648768974468112 Adapter cache time: 0.020037146285176277 Engine time: 0.07662088051438332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 78.34565709112212,
    "estimated_duration": 3600.0444685751454,
    "input_throughput": 6646.619287308545,
    "output_throughput": 5780.6460952513125,
    "total_throughput": 12427.265382559857,
    "itl": 98.9279302245392,
    "ttft": 2092998.1732008778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.053406736585375,
    "arrivals": 1864865,
    "finished_requests": 96702,
    "scheduler_time": 254.19620671471566
}
#Debug simulation 
Total elapsed time: 78.34582906682044. Arrivals time: 0.6532144346274436 Scheduler time: 77.49480083351955 Scheduler overhead time: 0.07504602754488587 Adapter cache time: 0.019586103036999702 Engine time: 0.0738560245372355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.70442143222317,
    "estimated_duration": 3600.0843335174836,
    "input_throughput": 6559.613001322852,
    "output_throughput": 5705.924110930342,
    "total_throughput": 12265.537112253194,
    "itl": 96.4262278376211,
    "ttft": 2097609.387985101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.46419340985362,
    "arrivals": 1864865,
    "finished_requests": 95485,
    "scheduler_time": 258.04069492946144
}
#Debug simulation 
Total elapsed time: 75.70458823209628. Arrivals time: 0.4924087324179709 Scheduler time: 75.01252498431131 Scheduler overhead time: 0.07507444638758898 Adapter cache time: 0.019648061133921146 Engine time: 0.07492943620309234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.99585714191198,
    "estimated_duration": 3600.040632314521,
    "input_throughput": 6425.654141888862,
    "output_throughput": 5591.041895285336,
    "total_throughput": 12016.696037174197,
    "itl": 91.1125390593051,
    "ttft": 2106625.0129038137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.502173938886292,
    "arrivals": 1864865,
    "finished_requests": 93553,
    "scheduler_time": 264.0157363149739
}
#Debug simulation 
Total elapsed time: 75.99601617688313. Arrivals time: 0.5023816819302738 Scheduler time: 75.28787494823337 Scheduler overhead time: 0.0776228797622025 Adapter cache time: 0.01968532893806696 Engine time: 0.07746046269312501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 76.21668169181794,
    "estimated_duration": 3600.0181637986643,
    "input_throughput": 6556.3641420893755,
    "output_throughput": 5708.2361990963755,
    "total_throughput": 12264.600341185751,
    "itl": 96.26463830875144,
    "ttft": 2096555.401151164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.233455919795664,
    "arrivals": 1864865,
    "finished_requests": 95504,
    "scheduler_time": 257.82322979346566
}
#Debug simulation 
Total elapsed time: 76.216851035133. Arrivals time: 0.47850241605192423 Scheduler time: 75.54094525333494 Scheduler overhead time: 0.07444398198276758 Adapter cache time: 0.019534584134817123 Engine time: 0.07393727777525783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 75.19708532607183,
    "estimated_duration": 3600.0706194994177,
    "input_throughput": 6392.230995513038,
    "output_throughput": 5566.274975679888,
    "total_throughput": 11958.505971192924,
    "itl": 90.52782187063363,
    "ttft": 2104510.4206106383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.569929959406173,
    "arrivals": 1864865,
    "finished_requests": 93143,
    "scheduler_time": 265.2951414279834
}
#Debug simulation 
Total elapsed time: 75.19724538596347. Arrivals time: 0.48700846266001463 Scheduler time: 74.50791170308366 Scheduler overhead time: 0.07593814609572291 Adapter cache time: 0.019535602536052465 Engine time: 0.07609874429181218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 78.39906727010384,
    "estimated_duration": 3600.0629554349407,
    "input_throughput": 6530.153025382238,
    "output_throughput": 5691.466858674573,
    "total_throughput": 12221.61988405681,
    "itl": 95.86090298547,
    "ttft": 2097495.98876636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8367364852828727,
    "arrivals": 1864865,
    "finished_requests": 95210,
    "scheduler_time": 258.93568268063905
}
#Debug simulation 
Total elapsed time: 78.39923128299415. Arrivals time: 0.48597465734928846 Scheduler time: 77.71358926221728 Scheduler overhead time: 0.07564061274752021 Adapter cache time: 0.019500635098665953 Engine time: 0.07457732688635588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.12824926897883,
    "estimated_duration": 3600.033207547555,
    "input_throughput": 6385.199989768804,
    "output_throughput": 5561.559253960165,
    "total_throughput": 11946.75924372897,
    "itl": 90.75870265541259,
    "ttft": 2106721.687362901,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.421756894849268,
    "arrivals": 1864865,
    "finished_requests": 93058,
    "scheduler_time": 265.519398711969
}
#Debug simulation 
Total elapsed time: 75.12841602414846. Arrivals time: 0.6227381676435471 Scheduler time: 74.30165321240202 Scheduler overhead time: 0.07706251507624984 Adapter cache time: 0.019643509760499 Engine time: 0.07643560459837317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.44000713294372,
    "estimated_duration": 3600.0584295885997,
    "input_throughput": 6590.744140425364,
    "output_throughput": 5749.213076623656,
    "total_throughput": 12339.95721704902,
    "itl": 98.38295794525732,
    "ttft": 2089682.4493284484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.020344691425624,
    "arrivals": 1853406,
    "finished_requests": 96174,
    "scheduler_time": 255.89153709227568
}
#Debug simulation 
Total elapsed time: 81.44016714580357. Arrivals time: 0.9606151771731675 Scheduler time: 80.28189172549173 Scheduler overhead time: 0.0751108773984015 Adapter cache time: 0.019313122611492872 Engine time: 0.07426206301897764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 76.59373309835792,
    "estimated_duration": 3600.025109600207,
    "input_throughput": 6555.05094591428,
    "output_throughput": 5712.934319584229,
    "total_throughput": 12267.98526549851,
    "itl": 96.51936040081802,
    "ttft": 2091634.6763099374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.48195695170202,
    "arrivals": 1853406,
    "finished_requests": 95517,
    "scheduler_time": 257.63953792709947
}
#Debug simulation 
Total elapsed time: 76.5938908783719. Arrivals time: 0.5019236961379647 Scheduler time: 75.89253862807527 Scheduler overhead time: 0.07508439011871815 Adapter cache time: 0.01953688869252801 Engine time: 0.07535431766882539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 76.27332951221615,
    "estimated_duration": 3600.0352770080913,
    "input_throughput": 6354.74495100304,
    "output_throughput": 5546.8025903889275,
    "total_throughput": 11901.547541391968,
    "itl": 90.54904980130935,
    "ttft": 2099688.5062598493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.300740063143928,
    "arrivals": 1853406,
    "finished_requests": 92571,
    "scheduler_time": 266.8706042900336
}
#Debug simulation 
Total elapsed time: 76.27349083311856. Arrivals time: 0.48337932443246245 Scheduler time: 75.58327997103333 Scheduler overhead time: 0.07812574505805969 Adapter cache time: 0.01952084479853511 Engine time: 0.07812004303559661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 77.304092773702,
    "estimated_duration": 3600.0744840692287,
    "input_throughput": 6532.967332780255,
    "output_throughput": 5708.010790035877,
    "total_throughput": 12240.978122816132,
    "itl": 96.59487878771134,
    "ttft": 2092660.6928102705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.244561687582166,
    "arrivals": 1853406,
    "finished_requests": 95354,
    "scheduler_time": 258.25772786834324
}
#Debug simulation 
Total elapsed time: 77.30425446061417. Arrivals time: 0.4818490156903863 Scheduler time: 76.62155504105613 Scheduler overhead time: 0.07582283532246947 Adapter cache time: 0.01950138621032238 Engine time: 0.07575581967830658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 75.77480682171881,
    "estimated_duration": 3600.002284062477,
    "input_throughput": 6385.733726273669,
    "output_throughput": 5573.887852469974,
    "total_throughput": 11959.621578743643,
    "itl": 91.1917505306399,
    "ttft": 2102189.7312854175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.35918101775929,
    "arrivals": 1853406,
    "finished_requests": 93141,
    "scheduler_time": 264.91838583008706
}
#Debug simulation 
Total elapsed time: 75.77496682479978. Arrivals time: 0.49047770351171494 Scheduler time: 75.0772913666442 Scheduler overhead time: 0.07782711880281568 Adapter cache time: 0.020008396822959185 Engine time: 0.07828503008931875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 77.04360748967156,
    "estimated_duration": 3600.0552313922894,
    "input_throughput": 6577.253535869381,
    "output_throughput": 5732.555939709465,
    "total_throughput": 12309.809475578846,
    "itl": 96.83704534458742,
    "ttft": 2091796.451654483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9261113784508597,
    "arrivals": 1853406,
    "finished_requests": 95930,
    "scheduler_time": 256.70023655341424
}
#Debug simulation 
Total elapsed time: 77.04377256287262. Arrivals time: 0.4833206352777779 Scheduler time: 76.36076862970367 Scheduler overhead time: 0.07551636546850204 Adapter cache time: 0.019534078892320395 Engine time: 0.0744909318163991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 75.20784886507317,
    "estimated_duration": 3600.0994587755804,
    "input_throughput": 6370.070122395907,
    "output_throughput": 5556.804257514557,
    "total_throughput": 11926.874379910465,
    "itl": 90.67919390410951,
    "ttft": 2101593.3458012817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.441999874897333,
    "arrivals": 1853406,
    "finished_requests": 92883,
    "scheduler_time": 265.8174696425008
}
#Debug simulation 
Total elapsed time: 75.20801401976496. Arrivals time: 0.9602089859545231 Scheduler time: 74.04274196410552 Scheduler overhead time: 0.07705497182905674 Adapter cache time: 0.01970585947856307 Engine time: 0.07762496219947934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 78.47268812730908,
    "estimated_duration": 3600.0444924083636,
    "input_throughput": 6681.212982428784,
    "output_throughput": 5802.384954977528,
    "total_throughput": 12483.597937406312,
    "itl": 99.2819002324572,
    "ttft": 2082302.1561796395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.060019145617325,
    "arrivals": 1847695,
    "finished_requests": 97343,
    "scheduler_time": 253.48202605852526
}
#Debug simulation 
Total elapsed time: 78.47284820117056. Arrivals time: 0.48500261548906565 Scheduler time: 77.79209157219157 Scheduler overhead time: 0.07379255583509803 Adapter cache time: 0.01903864834457636 Engine time: 0.07418156927451491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 78.47309858212247,
    "estimated_duration": 3600.0547597964915,
    "input_throughput": 6646.551121170343,
    "output_throughput": 5767.306162080073,
    "total_throughput": 12413.857283250416,
    "itl": 97.24563328717346,
    "ttft": 2088940.2328840191,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.456704598744407,
    "arrivals": 1847695,
    "finished_requests": 96771,
    "scheduler_time": 254.94232473540538
}
#Debug simulation 
Total elapsed time: 78.47327184909955. Arrivals time: 0.4959290763363242 Scheduler time: 77.77629508683458 Scheduler overhead time: 0.07557287765666842 Adapter cache time: 0.01972700236365199 Engine time: 0.07587624620646238 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.29607237316668,
    "estimated_duration": 3600.091911379636,
    "input_throughput": 6455.764344942348,
    "output_throughput": 5592.285557033093,
    "total_throughput": 12048.04990197544,
    "itl": 91.09111671336765,
    "ttft": 2100335.310052788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.520970809226886,
    "arrivals": 1847695,
    "finished_requests": 93994,
    "scheduler_time": 263.95651780550685
}
#Debug simulation 
Total elapsed time: 73.2962337359786. Arrivals time: 0.46427230490371585 Scheduler time: 72.62726179976016 Scheduler overhead time: 0.07770142098888755 Adapter cache time: 0.019489622209221125 Engine time: 0.07667021080851555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 75.87873956607655,
    "estimated_duration": 3600.0201978317295,
    "input_throughput": 6625.911714152818,
    "output_throughput": 5749.141911055303,
    "total_throughput": 12375.05362520812,
    "itl": 96.86377776548136,
    "ttft": 2091098.9176855474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.25871775043662,
    "arrivals": 1847695,
    "finished_requests": 96378,
    "scheduler_time": 255.8749118157859
}
#Debug simulation 
Total elapsed time: 75.8789112251252. Arrivals time: 0.5014513460919261 Scheduler time: 75.17842166684568 Scheduler overhead time: 0.07525423215702176 Adapter cache time: 0.019360987469553947 Engine time: 0.0748622240498662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 71.20941194659099,
    "estimated_duration": 3600.0911235526173,
    "input_throughput": 6451.115042077507,
    "output_throughput": 5598.185242631245,
    "total_throughput": 12049.300284708752,
    "itl": 91.18001956044287,
    "ttft": 2101724.527805881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 628,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.688900219560632,
    "arrivals": 1847695,
    "finished_requests": 93972,
    "scheduler_time": 263.516338681306
}
#Debug simulation 
Total elapsed time: 71.20957424957305. Arrivals time: 0.4732963480055332 Scheduler time: 70.53308745473623 Scheduler overhead time: 0.07695874944329262 Adapter cache time: 0.019498296082019806 Engine time: 0.0761144426651299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 75.56609970983118,
    "estimated_duration": 3600.0230216944215,
    "input_throughput": 6658.58991888267,
    "output_throughput": 5778.888877829489,
    "total_throughput": 12437.478796712157,
    "itl": 97.27371425701182,
    "ttft": 2084881.1016451456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.002718429737706,
    "arrivals": 1847695,
    "finished_requests": 96885,
    "scheduler_time": 255.12487513498078
}
#Debug simulation 
Total elapsed time: 75.5662573450245. Arrivals time: 0.4876791266724467 Scheduler time: 74.88248251518235 Scheduler overhead time: 0.07417018339037895 Adapter cache time: 0.019221633207052946 Engine time: 0.07323448685929179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 74.37175347423181,
    "estimated_duration": 3600.0094333031657,
    "input_throughput": 6435.665080672284,
    "output_throughput": 5578.974547733816,
    "total_throughput": 12014.6396284061,
    "itl": 90.79695659215713,
    "ttft": 2099823.1117817992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 579,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.285821578856593,
    "arrivals": 1847695,
    "finished_requests": 93673,
    "scheduler_time": 264.59945820449315
}
#Debug simulation 
Total elapsed time: 74.37191414507106. Arrivals time: 0.4927016953006387 Scheduler time: 73.67629870958626 Scheduler overhead time: 0.07724117813631892 Adapter cache time: 0.019184258300811052 Engine time: 0.07601214107125998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 78.18678951496258,
    "estimated_duration": 3600.081194658839,
    "input_throughput": 6701.8441239035465,
    "output_throughput": 5825.791938003091,
    "total_throughput": 12527.636061906636,
    "itl": 99.90393399124152,
    "ttft": 2089758.6689822825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9277709649783223,
    "arrivals": 1844823,
    "finished_requests": 97246,
    "scheduler_time": 251.9481624522628
}
#Debug simulation 
Total elapsed time: 78.1869569667615. Arrivals time: 0.6296494794078171 Scheduler time: 77.36122452607378 Scheduler overhead time: 0.07373069226741791 Adapter cache time: 0.019245514180511236 Engine time: 0.07401045691221952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 77.02228481508791,
    "estimated_duration": 3600.046934406085,
    "input_throughput": 6588.025776367476,
    "output_throughput": 5733.780246785971,
    "total_throughput": 12321.806023153447,
    "itl": 96.6370586666138,
    "ttft": 2099646.3877273602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.21576819941402,
    "arrivals": 1844823,
    "finished_requests": 95695,
    "scheduler_time": 256.6961916964426
}
#Debug simulation 
Total elapsed time: 77.02244700863957. Arrivals time: 0.9777391399256885 Scheduler time: 75.84859238471836 Scheduler overhead time: 0.07411374431103468 Adapter cache time: 0.018772279378026724 Engine time: 0.0736443055793643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.81934128608555,
    "estimated_duration": 3600.032441633946,
    "input_throughput": 6436.658384523571,
    "output_throughput": 5614.866345711488,
    "total_throughput": 12051.52473023506,
    "itl": 91.30462581792894,
    "ttft": 2102960.7282727025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 538,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.04588635597847,
    "arrivals": 1844823,
    "finished_requests": 93542,
    "scheduler_time": 263.49727688256246
}
#Debug simulation 
Total elapsed time: 73.8195015010424. Arrivals time: 0.46855244413018227 Scheduler time: 73.15032922197133 Scheduler overhead time: 0.07623287476599216 Adapter cache time: 0.01892625680193305 Engine time: 0.07524811616167426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 76.07363301469013,
    "estimated_duration": 3600.0108692137173,
    "input_throughput": 6588.399274800063,
    "output_throughput": 5733.950465685262,
    "total_throughput": 12322.349740485326,
    "itl": 96.6310675116621,
    "ttft": 2099525.3421838465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.954782656431192,
    "arrivals": 1844823,
    "finished_requests": 95700,
    "scheduler_time": 256.71101802662486
}
#Debug simulation 
Total elapsed time: 76.07379502803087. Arrivals time: 0.4954665214754641 Scheduler time: 75.38233452895656 Scheduler overhead time: 0.07434794679284096 Adapter cache time: 0.018669755663722754 Engine time: 0.07363057509064674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 73.21076036198065,
    "estimated_duration": 3600.0458841849463,
    "input_throughput": 6433.028284927892,
    "output_throughput": 5604.160793791575,
    "total_throughput": 12037.189078719466,
    "itl": 91.3237831312576,
    "ttft": 2107322.8005754077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.263359302668865,
    "arrivals": 1844823,
    "finished_requests": 93529,
    "scheduler_time": 263.47649692961875
}
#Debug simulation 
Total elapsed time: 73.21092693693936. Arrivals time: 0.4712616200558841 Scheduler time: 72.53834139928222 Scheduler overhead time: 0.0760224792174995 Adapter cache time: 0.01882825279608369 Engine time: 0.07579734828323126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 74.89925451390445,
    "estimated_duration": 3600.004836967797,
    "input_throughput": 6596.8589142238525,
    "output_throughput": 5750.555329096961,
    "total_throughput": 12347.414243320813,
    "itl": 97.03267930526228,
    "ttft": 2094060.93561653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7409776711743152,
    "arrivals": 1844823,
    "finished_requests": 95867,
    "scheduler_time": 255.92974793580143
}
#Debug simulation 
Total elapsed time: 74.89941205270588. Arrivals time: 0.5073832422494888 Scheduler time: 74.1974094118923 Scheduler overhead time: 0.0738311787135899 Adapter cache time: 0.01834995485842228 Engine time: 0.07347741769626737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.27805414935574,
    "estimated_duration": 3600.009966078197,
    "input_throughput": 6423.549161779653,
    "output_throughput": 5603.733931319291,
    "total_throughput": 12027.283093098944,
    "itl": 91.04977325145295,
    "ttft": 2102747.336261352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.147544255852733,
    "arrivals": 1844823,
    "finished_requests": 93424,
    "scheduler_time": 263.7726613152886
}
#Debug simulation 
Total elapsed time: 73.27821292402223. Arrivals time: 0.4675196991302073 Scheduler time: 72.60938649531454 Scheduler overhead time: 0.07634318061172962 Adapter cache time: 0.018360255751758814 Engine time: 0.07590984692797065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 76.91155907092616,
    "estimated_duration": 3600.058707133637,
    "input_throughput": 6667.82876413491,
    "output_throughput": 5827.01025359176,
    "total_throughput": 12494.83901772667,
    "itl": 99.90147452447269,
    "ttft": 2087032.161312496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4979643779015626,
    "arrivals": 1843393,
    "finished_requests": 97215,
    "scheduler_time": 251.83191702486178
}
#Debug simulation 
Total elapsed time: 76.91171508328989. Arrivals time: 0.49177942145615816 Scheduler time: 76.22874198528007 Scheduler overhead time: 0.07242755172774196 Adapter cache time: 0.017538761720061302 Engine time: 0.07253033807501197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 74.91142699494958,
    "estimated_duration": 3600.0488586120414,
    "input_throughput": 6614.5613393649155,
    "output_throughput": 5781.407646790759,
    "total_throughput": 12395.968986155674,
    "itl": 97.52650037854339,
    "ttft": 2089709.556018803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 528,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8635287021473097,
    "arrivals": 1843393,
    "finished_requests": 96454,
    "scheduler_time": 254.1926488813352
}
#Debug simulation 
Total elapsed time: 74.9115762100555. Arrivals time: 0.4798482726328075 Scheduler time: 74.239593393635 Scheduler overhead time: 0.07286378648132086 Adapter cache time: 0.017554302234202623 Engine time: 0.07293799566105008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 72.49406219320372,
    "estimated_duration": 3600.0761592964313,
    "input_throughput": 6446.06029793971,
    "output_throughput": 5633.196938801245,
    "total_throughput": 12079.257236740954,
    "itl": 91.86681621342808,
    "ttft": 2103300.4928868595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9239723661775097,
    "arrivals": 1843393,
    "finished_requests": 94071,
    "scheduler_time": 261.929964477561
}
#Debug simulation 
Total elapsed time: 72.49420736683533. Arrivals time: 0.3990826057270169 Scheduler time: 71.90501508209854 Scheduler overhead time: 0.07244788995012641 Adapter cache time: 0.016959527041763067 Engine time: 0.07141631748527288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 74.88989586103708,
    "estimated_duration": 3600.007260466866,
    "input_throughput": 6612.162497948131,
    "output_throughput": 5775.701962696465,
    "total_throughput": 12387.864460644596,
    "itl": 97.4985833649785,
    "ttft": 2088542.5882282897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 538,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.702476113876323,
    "arrivals": 1843393,
    "finished_requests": 96453,
    "scheduler_time": 254.48224151942802
}
#Debug simulation 
Total elapsed time: 74.8900424069725. Arrivals time: 0.42156187212094665 Scheduler time: 74.28503773687407 Scheduler overhead time: 0.06943979067727923 Adapter cache time: 0.016717505175620317 Engine time: 0.0694927154108882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 71.07131688669324,
    "estimated_duration": 3600.069766602057,
    "input_throughput": 6453.082997312913,
    "output_throughput": 5636.0829971223275,
    "total_throughput": 12089.165994435241,
    "itl": 91.88676233996172,
    "ttft": 2100795.730281605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9913675178401515,
    "arrivals": 1843393,
    "finished_requests": 94111,
    "scheduler_time": 261.77554273897664
}
#Debug simulation 
Total elapsed time: 71.07146200491115. Arrivals time: 0.41174477245658636 Scheduler time: 70.47114232880995 Scheduler overhead time: 0.07098791794851422 Adapter cache time: 0.01721841422840953 Engine time: 0.07110137352719903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 74.13126581674442,
    "estimated_duration": 3600.059069908813,
    "input_throughput": 6605.323006714531,
    "output_throughput": 5770.780033486012,
    "total_throughput": 12376.103040200544,
    "itl": 97.45135672339208,
    "ttft": 2089998.3456652712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.41539770320522,
    "arrivals": 1843393,
    "finished_requests": 96286,
    "scheduler_time": 254.80359226687435
}
#Debug simulation 
Total elapsed time: 74.13140783691779. Arrivals time: 0.428280642721802 Scheduler time: 73.51987206051126 Scheduler overhead time: 0.06915424531325698 Adapter cache time: 0.016943856608122587 Engine time: 0.06908622104674578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_384_slots_32_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.25792700611055,
    "estimated_duration": 3600.057715117273,
    "input_throughput": 6451.95128468749,
    "output_throughput": 5634.552444762464,
    "total_throughput": 12086.503729449954,
    "itl": 91.58975345642571,
    "ttft": 2102190.8431100585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8616786132194427,
    "arrivals": 1843393,
    "finished_requests": 94107,
    "scheduler_time": 261.94691529587783
}
#Debug simulation 
Total elapsed time: 73.2580729899928. Arrivals time: 0.41559871612116694 Scheduler time: 72.65201052138582 Scheduler overhead time: 0.07229546783491969 Adapter cache time: 0.01707106502726674 Engine time: 0.07166295731440187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 75.95183716807514,
    "estimated_duration": 3600.083024513612,
    "input_throughput": 6592.16352467492,
    "output_throughput": 5722.824684795572,
    "total_throughput": 12314.988209470492,
    "itl": 97.6699004287133,
    "ttft": 2098852.131970145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 591,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.907933737882472,
    "arrivals": 1703822,
    "finished_requests": 95692,
    "scheduler_time": 257.08294391787695
}
#Debug simulation 
Total elapsed time: 75.95197847206146. Arrivals time: 0.4177882014773786 Scheduler time: 75.34766979515553 Scheduler overhead time: 0.07021746272221208 Adapter cache time: 0.01795376930385828 Engine time: 0.06977087818086147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.53001787280664,
    "estimated_duration": 3600.0478712791596,
    "input_throughput": 6528.34179997979,
    "output_throughput": 5671.597359272093,
    "total_throughput": 12199.939159251882,
    "itl": 95.76422405332153,
    "ttft": 2095914.156618439,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.259624608950695,
    "arrivals": 1703822,
    "finished_requests": 94838,
    "scheduler_time": 259.78406226640914
}
#Debug simulation 
Total elapsed time: 73.5301635665819. Arrivals time: 0.42326471535488963 Scheduler time: 72.91790110524744 Scheduler overhead time: 0.07124244747683406 Adapter cache time: 0.01782873272895813 Engine time: 0.07082671346142888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 71.53906726185232,
    "estimated_duration": 3600.028870405117,
    "input_throughput": 6385.663789805402,
    "output_throughput": 5552.1940849743,
    "total_throughput": 11937.857874779702,
    "itl": 90.2742933909587,
    "ttft": 2108242.4373300406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.436792462701012,
    "arrivals": 1703822,
    "finished_requests": 92846,
    "scheduler_time": 265.901761087476
}
#Debug simulation 
Total elapsed time: 71.53921960713342. Arrivals time: 0.4270394057966769 Scheduler time: 70.9155214256607 Scheduler overhead time: 0.07425698870792985 Adapter cache time: 0.018540424294769764 Engine time: 0.07370468322187662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 74.89330978086218,
    "estimated_duration": 3600.0010894448765,
    "input_throughput": 6545.9394079332915,
    "output_throughput": 5694.290498995664,
    "total_throughput": 12240.229906928957,
    "itl": 95.93561017426013,
    "ttft": 2095691.0499670121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1141068268241305,
    "arrivals": 1703822,
    "finished_requests": 95130,
    "scheduler_time": 258.6537765499755
}
#Debug simulation 
Total elapsed time: 74.89345047809184. Arrivals time: 0.42853772221133113 Scheduler time: 74.27577513549477 Scheduler overhead time: 0.07150014955550432 Adapter cache time: 0.018151392694562674 Engine time: 0.07051762193441391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_384_slots_32_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 75.67208128096536,
    "estimated_duration": 3600.0227100568973,
    "input_throughput": 6354.966577318736,
    "output_throughput": 5531.707048505053,
    "total_throughput": 11886.673625823789,
    "itl": 89.87518620472738,
    "ttft": 2109011.6187504306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.24446276364386,
    "arrivals": 1703822,
    "finished_requests": 92373,
    "scheduler_time": 267.01554305982404
}
#Debug simulation 
Total elapsed time: 75.67223176220432. Arrivals time: 0.41693312721326947 Scheduler time: 75.05850230529904 Scheduler overhead time: 0.07478644652292132 Adapter cache time: 0.01825902471318841 Engine time: 0.0738581083714962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_384_slots_32_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_384_slots_32_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 76.18471137108281,
    "estimated_duration": 3600.0414813560965,
    "input_throughput": 6505.877813157143,
    "output_throughput": 5656.032327808032,
    "total_throughput": 12161.910140965174,
    "itl": 95.21546184149041,
    "ttft": 2098376.1372794197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8622721690451547,
    "arrivals": 1703822,
    "finished_requests": 94448,
    "scheduler_time": 260.5329325846449
}
#Debug simulation 
Total elapsed time: 76.18484395602718. Arrivals time: 0.42489775037392974 Scheduler time: 75.57030313229188 Scheduler overhead time: 0.0720710358582437 Adapter cache time: 0.01794039783999324 Engine time: 0.07056891778483987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_384_slots_32_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_384_slots_32_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 71.44959004502743,
    "estimated_duration": 3600.021251392482,
    "input_throughput": 6370.6660040212155,
    "output_throughput": 5535.2248246568815,
    "total_throughput": 11905.890828678097,
    "itl": 90.17672625098405,
    "ttft": 2105949.52312509,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.539430727697943,
    "arrivals": 1703822,
    "finished_requests": 92560,
    "scheduler_time": 266.7457374697021
}
#Debug simulation 
Total elapsed time: 71.44971094001085. Arrivals time: 0.4200575025752187 Scheduler time: 70.83314760075882 Scheduler overhead time: 0.07409150199964643 Adapter cache time: 0.018558653537184 Engine time: 0.07353075174614787 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 76.27411394007504,
    "estimated_duration": 3600.040393928697,
    "input_throughput": 6648.199848080381,
    "output_throughput": 5774.249376495113,
    "total_throughput": 12422.449224575494,
    "itl": 98.87210358585459,
    "ttft": 2084773.3056181627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9740578282019734,
    "arrivals": 1680953,
    "finished_requests": 96508,
    "scheduler_time": 254.3742480669556
}
#Debug simulation 
Total elapsed time: 76.27424057479948. Arrivals time: 0.4264530115760863 Scheduler time: 75.66090221935883 Scheduler overhead time: 0.07127540512010455 Adapter cache time: 0.017672013491392136 Engine time: 0.0694985962472856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 73.3734648716636,
    "estimated_duration": 3600.07020287597,
    "input_throughput": 6569.350225755805,
    "output_throughput": 5722.966453137752,
    "total_throughput": 12292.316678893558,
    "itl": 96.51660342418513,
    "ttft": 2091512.1549177615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.385640953667467,
    "arrivals": 1680953,
    "finished_requests": 95491,
    "scheduler_time": 257.05502478578927
}
#Debug simulation 
Total elapsed time: 73.3735982356593. Arrivals time: 0.4198854654096067 Scheduler time: 72.76731957960874 Scheduler overhead time: 0.0709598558023572 Adapter cache time: 0.01761479815468192 Engine time: 0.06921523809432983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 70.91221117926762,
    "estimated_duration": 3600.069068637949,
    "input_throughput": 6411.033944060456,
    "output_throughput": 5562.504945933278,
    "total_throughput": 11973.538889993733,
    "itl": 90.55877390975726,
    "ttft": 2100991.9304280505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 688,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.180401074271679,
    "arrivals": 1680953,
    "finished_requests": 93068,
    "scheduler_time": 265.2245149705762
}
#Debug simulation 
Total elapsed time: 70.91234040306881. Arrivals time: 0.5353373237885535 Scheduler time: 70.18499390827492 Scheduler overhead time: 0.07320440327748656 Adapter cache time: 0.018634357023984194 Engine time: 0.07077046111226082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 75.3816790622659,
    "estimated_duration": 3600.0420019275966,
    "input_throughput": 6590.460052215016,
    "output_throughput": 5725.858473029719,
    "total_throughput": 12316.318525244735,
    "itl": 96.65323010361105,
    "ttft": 2089482.3347693027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.071912491382094,
    "arrivals": 1680953,
    "finished_requests": 95649,
    "scheduler_time": 256.9578369152998
}
#Debug simulation 
Total elapsed time: 75.38180764811113. Arrivals time: 0.43959579383954406 Scheduler time: 74.75264698173851 Scheduler overhead time: 0.07275911141186953 Adapter cache time: 0.017789805307984352 Engine time: 0.07032470172271132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_384_slots_32_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 67.4692770279944,
    "estimated_duration": 3600.0055029908895,
    "input_throughput": 6412.414920149754,
    "output_throughput": 5575.7214769043085,
    "total_throughput": 11988.136397054062,
    "itl": 90.94939582428532,
    "ttft": 2102984.6994331097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.406548147872122,
    "arrivals": 1680953,
    "finished_requests": 93127,
    "scheduler_time": 264.74941226332936
}
#Debug simulation 
Total elapsed time: 67.46940132789314. Arrivals time: 0.42435075202956796 Scheduler time: 66.85289309732616 Scheduler overhead time: 0.07324542058631778 Adapter cache time: 0.01803648518398404 Engine time: 0.0714490539394319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_384_slots_32_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_384_slots_32_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 73.33481007115915,
    "estimated_duration": 3600.0222770754763,
    "input_throughput": 6573.338768121148,
    "output_throughput": 5715.312133211177,
    "total_throughput": 12288.650901332325,
    "itl": 96.33441353178699,
    "ttft": 2091949.9832482103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7665133549365972,
    "arrivals": 1680953,
    "finished_requests": 95471,
    "scheduler_time": 257.52873180133787
}
#Debug simulation 
Total elapsed time: 73.33493525302038. Arrivals time: 0.4263246553018689 Scheduler time: 72.71940406318754 Scheduler overhead time: 0.0722108231857419 Adapter cache time: 0.017743324860930443 Engine time: 0.07034683180972934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_384_slots_32_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_384_slots_32_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 70.84266927884892,
    "estimated_duration": 3600.0341785399623,
    "input_throughput": 6389.069341927271,
    "output_throughput": 5551.648403545329,
    "total_throughput": 11940.717745472599,
    "itl": 90.31629903275382,
    "ttft": 2100893.6297132983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 633,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.673202983047844,
    "arrivals": 1680953,
    "finished_requests": 92776,
    "scheduler_time": 265.9894652735268
}
#Debug simulation 
Total elapsed time: 70.84280072897673. Arrivals time: 0.4288166556507349 Scheduler time: 70.21916812332347 Scheduler overhead time: 0.07423311052843928 Adapter cache time: 0.018567829858511686 Engine time: 0.07198834884911776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_384_slots_32_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_384_slots_32_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 4320, 270, 34560, 34560, 270, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 270, 270, 4320, 270, 34560, 270, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 34560, 34560, 270, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 34560, 4320, 4320, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 4320, 270, 34560, 4320, 4320, 270, 34560, 34560, 34560, 270, 4320, 34560, 270, 34560, 4320, 4320, 34560, 34560, 270, 4320, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 34560, 270, 4320, 4320, 270, 270, 4320, 270, 34560, 34560, 270, 34560, 34560, 270, 4320, 34560, 270, 270, 4320, 4320, 270, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 4320, 270, 34560, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 270, 34560, 34560, 4320, 270, 34560, 4320, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 4320, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 34560, 270, 270, 270, 4320, 270, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 4320, 4320, 4320, 270, 4320, 270, 270, 34560, 270, 4320, 270, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5011200 . Total input tokens: 1117859278 . Total output tokens: 983942946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 76.15288394223899,
    "estimated_duration": 3600.0439199657344,
    "input_throughput": 6612.239330743102,
    "output_throughput": 5762.669973258954,
    "total_throughput": 12374.909304002056,
    "itl": 98.99583626139673,
    "ttft": 2084604.928509003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.81536001143517,
    "arrivals": 1669342,
    "finished_requests": 96212,
    "scheduler_time": 255.1075105883455
}
#Debug simulation 
Total elapsed time: 76.15301343938336. Arrivals time: 0.5654070852324367 Scheduler time: 75.39904549252242 Scheduler overhead time: 0.07252286979928613 Adapter cache time: 0.01766102248802781 Engine time: 0.06997591443359852 
