INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.91150169074535,
    "estimated_duration": 3600.0767314928084,
    "input_throughput": 6855.595266650305,
    "output_throughput": 5995.677761859704,
    "total_throughput": 12851.27302851001,
    "itl": 100.88361177602391,
    "ttft": 1865464.7123687656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 665,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.397252006246783,
    "arrivals": 501405,
    "finished_requests": 100449,
    "scheduler_time": 209.1726396608528
}
#Debug simulation 
Total elapsed time: 81.91171959089115. Arrivals time: 0.5334093002602458 Scheduler time: 81.18019660236314 Scheduler overhead time: 0.07488746242597699 Adapter cache time: 0.01837338414043188 Engine time: 0.07556065311655402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 80.23897926369682,
    "estimated_duration": 3600.059773489907,
    "input_throughput": 6786.199823653704,
    "output_throughput": 5932.7531607330075,
    "total_throughput": 12718.952984386711,
    "itl": 98.31850067130058,
    "ttft": 1872660.8693301808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 695,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.049030502927495,
    "arrivals": 501405,
    "finished_requests": 99399,
    "scheduler_time": 211.43245948435072
}
#Debug simulation 
Total elapsed time: 80.23917838884518. Arrivals time: 0.5133163267746568 Scheduler time: 79.52496761828661 Scheduler overhead time: 0.07636259403079748 Adapter cache time: 0.018516588024795055 Engine time: 0.07564436737447977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.92222671722993,
    "estimated_duration": 3600.036275827811,
    "input_throughput": 6600.733209149633,
    "output_throughput": 5770.562963347658,
    "total_throughput": 12371.29617249729,
    "itl": 91.9002683486961,
    "ttft": 1890701.5715839877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 768,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.72908497967294,
    "arrivals": 501405,
    "finished_requests": 96596,
    "scheduler_time": 217.71927301160054
}
#Debug simulation 
Total elapsed time: 78.92247227020562. Arrivals time: 0.532485369592905 Scheduler time: 78.18219692306593 Scheduler overhead time: 0.078077407553792 Adapter cache time: 0.01974073378369212 Engine time: 0.07900386163964868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 80.43415334494784,
    "estimated_duration": 3600.0808866970992,
    "input_throughput": 6785.255045314002,
    "output_throughput": 5932.339209076474,
    "total_throughput": 12717.594254390477,
    "itl": 98.25057373157513,
    "ttft": 1872615.4392572688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 696,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.727794274166217,
    "arrivals": 501405,
    "finished_requests": 99378,
    "scheduler_time": 211.5053705366557
}
#Debug simulation 
Total elapsed time: 80.43431598460302. Arrivals time: 0.5311907771974802 Scheduler time: 79.70268866093829 Scheduler overhead time: 0.07591369701549411 Adapter cache time: 0.01847756188362837 Engine time: 0.07600389327853918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 78.5560003737919,
    "estimated_duration": 3600.033438097568,
    "input_throughput": 6597.135945645717,
    "output_throughput": 5767.567539864965,
    "total_throughput": 12364.703485510681,
    "itl": 91.90467186154515,
    "ttft": 1890299.9058570396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 735,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.425138935856547,
    "arrivals": 501405,
    "finished_requests": 96534,
    "scheduler_time": 217.66116121574157
}
#Debug simulation 
Total elapsed time: 78.5561647200957. Arrivals time: 0.523248930927366 Scheduler time: 77.8246672572568 Scheduler overhead time: 0.0787314991466701 Adapter cache time: 0.01961221033707261 Engine time: 0.07900552777573466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 80.67733593378216,
    "estimated_duration": 3600.0171714883404,
    "input_throughput": 6785.739577431102,
    "output_throughput": 5932.646424341971,
    "total_throughput": 12718.386001773073,
    "itl": 98.25475952543061,
    "ttft": 1872460.539737159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 695,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4368250536964995,
    "arrivals": 501405,
    "finished_requests": 99393,
    "scheduler_time": 211.50902912207675
}
#Debug simulation 
Total elapsed time: 80.6774964928627. Arrivals time: 0.5391340185888112 Scheduler time: 79.93609118787572 Scheduler overhead time: 0.07618230348452926 Adapter cache time: 0.018952151760458946 Engine time: 0.07749681221321225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.9493598449044,
    "estimated_duration": 3600.0132518660116,
    "input_throughput": 6600.75959100509,
    "output_throughput": 5770.533758238839,
    "total_throughput": 12371.29334924393,
    "itl": 91.88450093270859,
    "ttft": 1890670.0911356788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 774,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.671600804701472,
    "arrivals": 501405,
    "finished_requests": 96595,
    "scheduler_time": 217.7357289038726
}
#Debug simulation 
Total elapsed time: 78.94952202728018. Arrivals time: 0.535198416095227 Scheduler time: 78.20629633357748 Scheduler overhead time: 0.079427779186517 Adapter cache time: 0.019533897750079632 Engine time: 0.07832342619076371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.68608131771907,
    "estimated_duration": 3600.0064754293585,
    "input_throughput": 6840.9701949391,
    "output_throughput": 6007.183916918028,
    "total_throughput": 12848.154111857128,
    "itl": 100.68725691837639,
    "ttft": 1865041.1512368585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.430314051406533,
    "arrivals": 500396,
    "finished_requests": 100103,
    "scheduler_time": 209.79695771588928
}
#Debug simulation 
Total elapsed time: 81.68624702980742. Arrivals time: 0.49372145207598805 Scheduler time: 80.99650812055916 Scheduler overhead time: 0.07402969105169177 Adapter cache time: 0.018107736483216286 Engine time: 0.07472007581964135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 81.01689985720441,
    "estimated_duration": 3600.003525252274,
    "input_throughput": 6763.141710616474,
    "output_throughput": 5938.898906634201,
    "total_throughput": 12702.040617250675,
    "itl": 98.00396101135988,
    "ttft": 1871880.0196863525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.950221393750987,
    "arrivals": 500396,
    "finished_requests": 98971,
    "scheduler_time": 212.14524108208488
}
#Debug simulation 
Total elapsed time: 81.01706233294681. Arrivals time: 0.48933232203125954 Scheduler time: 80.32677010586485 Scheduler overhead time: 0.07628098083660007 Adapter cache time: 0.018386092502623796 Engine time: 0.07640183111652732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.56905821384862,
    "estimated_duration": 3600.091434330815,
    "input_throughput": 6597.826592261306,
    "output_throughput": 5798.208845737296,
    "total_throughput": 12396.035437998602,
    "itl": 91.40976224841586,
    "ttft": 1892238.661835419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.123705532653286,
    "arrivals": 500396,
    "finished_requests": 96614,
    "scheduler_time": 219.10811563720813
}
#Debug simulation 
Total elapsed time: 79.56922426680103. Arrivals time: 0.5044975085183978 Scheduler time: 78.85630610398948 Scheduler overhead time: 0.07896472280845046 Adapter cache time: 0.019068461377173662 Engine time: 0.07860518153756857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 81.55770489480346,
    "estimated_duration": 3600.0188187574427,
    "input_throughput": 6773.389037009624,
    "output_throughput": 5947.453076756429,
    "total_throughput": 12720.842113766053,
    "itl": 97.88574433473956,
    "ttft": 1872280.8329911807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6400909327762125,
    "arrivals": 500396,
    "finished_requests": 99123,
    "scheduler_time": 212.4348273564974
}
#Debug simulation 
Total elapsed time: 81.5578704290092. Arrivals time: 0.4890373549424112 Scheduler time: 80.86605352442712 Scheduler overhead time: 0.0766729530878365 Adapter cache time: 0.019009826239198446 Engine time: 0.07694375514984131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 79.28614957910031,
    "estimated_duration": 3600.0732715655745,
    "input_throughput": 6598.945134710789,
    "output_throughput": 5799.1639128280785,
    "total_throughput": 12398.109047538868,
    "itl": 91.47728766282329,
    "ttft": 1892434.8490506036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 686,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.086474813330017,
    "arrivals": 500396,
    "finished_requests": 96637,
    "scheduler_time": 219.01725913519925
}
#Debug simulation 
Total elapsed time: 79.28630654420704. Arrivals time: 0.47335787396878004 Scheduler time: 78.60349009372294 Scheduler overhead time: 0.07933034701272845 Adapter cache time: 0.019052816554903984 Engine time: 0.07966458704322577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 80.43141785170883,
    "estimated_duration": 3600.0507806762266,
    "input_throughput": 6755.4655424698685,
    "output_throughput": 5932.943533643152,
    "total_throughput": 12688.409076113021,
    "itl": 98.10697264563348,
    "ttft": 1871763.4644605424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2963787930039485,
    "arrivals": 500396,
    "finished_requests": 98873,
    "scheduler_time": 211.91026792507319
}
#Debug simulation 
Total elapsed time: 80.43158006761223. Arrivals time: 0.4885857063345611 Scheduler time: 79.74550485983491 Scheduler overhead time: 0.0751951071433723 Adapter cache time: 0.017970059532672167 Engine time: 0.07504203543066978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.1928926371038,
    "estimated_duration": 3600.022951416597,
    "input_throughput": 6597.394605679986,
    "output_throughput": 5797.884147318209,
    "total_throughput": 12395.278752998194,
    "itl": 91.40799442634632,
    "ttft": 1892281.9573210718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.031331271883129,
    "arrivals": 500396,
    "finished_requests": 96608,
    "scheduler_time": 219.10709592361488
}
#Debug simulation 
Total elapsed time: 79.19305456010625. Arrivals time: 0.5023407437838614 Scheduler time: 78.48178122751415 Scheduler overhead time: 0.0787546462379396 Adapter cache time: 0.019412565510720015 Engine time: 0.07977978745475411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 84.30060959002003,
    "estimated_duration": 3600.008696318625,
    "input_throughput": 6907.728313387114,
    "output_throughput": 6032.312372524879,
    "total_throughput": 12940.040685911994,
    "itl": 100.27917930301687,
    "ttft": 1872720.9553879888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 555,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6698870127322665,
    "arrivals": 499947,
    "finished_requests": 100829,
    "scheduler_time": 210.51773677013725
}
#Debug simulation 
Total elapsed time: 84.30076781008393. Arrivals time: 0.5027410709299147 Scheduler time: 83.59794500330463 Scheduler overhead time: 0.0764673794619739 Adapter cache time: 0.017873331904411316 Engine time: 0.07588433101773262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 83.5230100019835,
    "estimated_duration": 3600.020622800889,
    "input_throughput": 6837.931661860234,
    "output_throughput": 5970.016911536091,
    "total_throughput": 12807.948573396325,
    "itl": 97.4111783353401,
    "ttft": 1879718.164905192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 554,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.033675309522082,
    "arrivals": 499947,
    "finished_requests": 99804,
    "scheduler_time": 213.22385539739986
}
#Debug simulation 
Total elapsed time: 83.52316939085722. Arrivals time: 0.49088800279423594 Scheduler time: 82.83100183773786 Scheduler overhead time: 0.07607070496305823 Adapter cache time: 0.018045062199234962 Engine time: 0.07740987278521061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 81.3055474627763,
    "estimated_duration": 3600.0547426291664,
    "input_throughput": 6658.717356751393,
    "output_throughput": 5817.714589724338,
    "total_throughput": 12476.431946475732,
    "itl": 91.00203458631832,
    "ttft": 1898911.502289442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.004570071911473,
    "arrivals": 499947,
    "finished_requests": 97203,
    "scheduler_time": 219.8866132093811
}
#Debug simulation 
Total elapsed time: 81.30571282003075. Arrivals time: 0.5016663437709212 Scheduler time: 80.59573718113825 Scheduler overhead time: 0.07932815561071038 Adapter cache time: 0.018303395714610815 Engine time: 0.07940923282876611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 83.63126411987469,
    "estimated_duration": 3600.0534619595187,
    "input_throughput": 6832.549366256214,
    "output_throughput": 5964.318648843844,
    "total_throughput": 12796.868015100059,
    "itl": 97.61220731567717,
    "ttft": 1879559.719286706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7660319924773593,
    "arrivals": 499947,
    "finished_requests": 99715,
    "scheduler_time": 212.88626171098656
}
#Debug simulation 
Total elapsed time: 83.6314310086891. Arrivals time: 0.49798724614083767 Scheduler time: 82.93150021554902 Scheduler overhead time: 0.07703546900302172 Adapter cache time: 0.01817003032192588 Engine time: 0.0766517031006515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 81.76388060022146,
    "estimated_duration": 3600.0990797469804,
    "input_throughput": 6659.616713016969,
    "output_throughput": 5818.16126057067,
    "total_throughput": 12477.77797358764,
    "itl": 91.02871079944182,
    "ttft": 1898899.9168604936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 538,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.988513821805862,
    "arrivals": 499947,
    "finished_requests": 97214,
    "scheduler_time": 219.8566380101198
}
#Debug simulation 
Total elapsed time: 81.76404540799558. Arrivals time: 0.46978768380358815 Scheduler time: 81.08322293311357 Scheduler overhead time: 0.0801221732981503 Adapter cache time: 0.018449732568114996 Engine time: 0.08094446826726198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 83.42628142191097,
    "estimated_duration": 3600.049679470075,
    "input_throughput": 6832.156550578638,
    "output_throughput": 5964.315471102233,
    "total_throughput": 12796.472021680871,
    "itl": 97.52049450559397,
    "ttft": 1879460.5441621416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.504772596373207,
    "arrivals": 499947,
    "finished_requests": 99710,
    "scheduler_time": 213.00275644765094
}
#Debug simulation 
Total elapsed time: 83.42644772585481. Arrivals time: 0.49001949978992343 Scheduler time: 82.7325787562877 Scheduler overhead time: 0.07774980831891298 Adapter cache time: 0.017882853746414185 Engine time: 0.07777894334867597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 81.66735168593004,
    "estimated_duration": 3600.047993004748,
    "input_throughput": 6657.924573942864,
    "output_throughput": 5817.1266162818565,
    "total_throughput": 12475.05119022472,
    "itl": 90.94363050493914,
    "ttft": 1898908.990646293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 536,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9384463270753964,
    "arrivals": 499947,
    "finished_requests": 97190,
    "scheduler_time": 219.96686174219025
}
#Debug simulation 
Total elapsed time: 81.66751271905378. Arrivals time: 0.47819102369248867 Scheduler time: 80.97922177938744 Scheduler overhead time: 0.08024230971932411 Adapter cache time: 0.01807964313775301 Engine time: 0.08038179529830813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 82.43606370873749,
    "estimated_duration": 3600.077532760181,
    "input_throughput": 6996.499595020776,
    "output_throughput": 6063.9541235831075,
    "total_throughput": 13060.453718603883,
    "itl": 98.92060616076012,
    "ttft": 1872113.6182880385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 467,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0879950179206532,
    "arrivals": 498492,
    "finished_requests": 101376,
    "scheduler_time": 212.47537750559104
}
#Debug simulation 
Total elapsed time: 82.43623547488824. Arrivals time: 0.4982048333622515 Scheduler time: 81.74070441816002 Scheduler overhead time: 0.07523407973349094 Adapter cache time: 0.01690283278003335 Engine time: 0.07562216278165579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 82.51652577286586,
    "estimated_duration": 3600.098699518174,
    "input_throughput": 6896.461756263209,
    "output_throughput": 5980.003549036398,
    "total_throughput": 12876.465305299607,
    "itl": 96.33098539172106,
    "ttft": 1878670.6973113236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 474,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4632681324239867,
    "arrivals": 498492,
    "finished_requests": 99952,
    "scheduler_time": 214.58179862649698
}
#Debug simulation 
Total elapsed time: 82.5166890937835. Arrivals time: 0.5001455238088965 Scheduler time: 81.81488300906494 Scheduler overhead time: 0.07674632873386145 Adapter cache time: 0.017362010665237904 Engine time: 0.07712060725316405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.92640985222533,
    "estimated_duration": 3600.0344731525347,
    "input_throughput": 6717.951780839871,
    "output_throughput": 5824.401448477901,
    "total_throughput": 12542.353229317772,
    "itl": 89.97265875421228,
    "ttft": 1898371.4248870937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.250871649975912,
    "arrivals": 498492,
    "finished_requests": 97335,
    "scheduler_time": 221.28120378221948
}
#Debug simulation 
Total elapsed time: 79.92657364532351. Arrivals time: 0.47753263683989644 Scheduler time: 79.24033028911799 Scheduler overhead time: 0.07987510645762086 Adapter cache time: 0.017381410114467144 Engine time: 0.07992877438664436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 81.48955108784139,
    "estimated_duration": 3600.0604934579355,
    "input_throughput": 6931.334916550774,
    "output_throughput": 6006.739342101744,
    "total_throughput": 12938.07425865252,
    "itl": 96.21908111786892,
    "ttft": 1880439.158655084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 468,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1972963671572456,
    "arrivals": 498492,
    "finished_requests": 100438,
    "scheduler_time": 215.0756806827956
}
#Debug simulation 
Total elapsed time: 81.48972042091191. Arrivals time: 0.5272725783288479 Scheduler time: 80.76180520839989 Scheduler overhead time: 0.07678031735122204 Adapter cache time: 0.017189289443194866 Engine time: 0.07681840751320124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 80.55041763326153,
    "estimated_duration": 3600.0132504826456,
    "input_throughput": 6718.0541618166435,
    "output_throughput": 5824.511339559327,
    "total_throughput": 12542.56550137597,
    "itl": 89.97203300710673,
    "ttft": 1898369.0295983232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.219804073483706,
    "arrivals": 498492,
    "finished_requests": 97336,
    "scheduler_time": 221.28134587235726
}
#Debug simulation 
Total elapsed time: 80.55058608530089. Arrivals time: 0.47783053247258067 Scheduler time: 79.86196442553774 Scheduler overhead time: 0.08068676479160786 Adapter cache time: 0.017758365254849195 Engine time: 0.0804810905829072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.54257121030241,
    "estimated_duration": 3600.0313700564734,
    "input_throughput": 6929.823503068155,
    "output_throughput": 6006.758768788389,
    "total_throughput": 12936.582271856545,
    "itl": 96.21186428514918,
    "ttft": 1880110.0428140415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 466,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9749071583058555,
    "arrivals": 498492,
    "finished_requests": 100420,
    "scheduler_time": 215.09446436311134
}
#Debug simulation 
Total elapsed time: 81.54272834211588. Arrivals time: 0.49368759198114276 Scheduler time: 80.8465594346635 Scheduler overhead time: 0.0775259886868298 Adapter cache time: 0.017503002658486366 Engine time: 0.07746377913281322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 79.99758544797078,
    "estimated_duration": 3600.0871549289413,
    "input_throughput": 6717.949860432578,
    "output_throughput": 5824.225941667199,
    "total_throughput": 12542.175802099777,
    "itl": 89.968831883363,
    "ttft": 1898414.4474002628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.198883742131316,
    "arrivals": 498492,
    "finished_requests": 97335,
    "scheduler_time": 221.28957713703855
}
#Debug simulation 
Total elapsed time: 79.99775350512937. Arrivals time: 0.5090705156326294 Scheduler time: 79.281189548783 Scheduler overhead time: 0.07915977574884892 Adapter cache time: 0.016970366705209017 Engine time: 0.07965822983533144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 86.13893588585779,
    "estimated_duration": 3600.055475654709,
    "input_throughput": 6946.2371258188,
    "output_throughput": 6080.673519626507,
    "total_throughput": 13026.910645445309,
    "itl": 99.32243006223874,
    "ttft": 1870698.6573538098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.631738794716093,
    "arrivals": 497997,
    "finished_requests": 101420,
    "scheduler_time": 212.09592137888708
}
#Debug simulation 
Total elapsed time: 86.13909425120801. Arrivals time: 0.5030270544812083 Scheduler time: 85.43840744066983 Scheduler overhead time: 0.07568679517135024 Adapter cache time: 0.01657351804897189 Engine time: 0.07595254713669419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 84.14300799602643,
    "estimated_duration": 3600.026520323621,
    "input_throughput": 6886.786488942668,
    "output_throughput": 6030.453630671704,
    "total_throughput": 12917.240119614373,
    "itl": 96.50341037479055,
    "ttft": 1878570.3280924102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.87009223582689,
    "arrivals": 497997,
    "finished_requests": 100546,
    "scheduler_time": 215.00369460068907
}
#Debug simulation 
Total elapsed time: 84.14316760795191. Arrivals time: 0.4819283024407923 Scheduler time: 83.46027135290205 Scheduler overhead time: 0.07673691725358367 Adapter cache time: 0.01680553937330842 Engine time: 0.07684894790872931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 82.91656657168642,
    "estimated_duration": 3600.036785561986,
    "input_throughput": 6708.15173246351,
    "output_throughput": 5872.32249536579,
    "total_throughput": 12580.4742278293,
    "itl": 90.09702924377146,
    "ttft": 1898809.1424449238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.733230064865219,
    "arrivals": 497997,
    "finished_requests": 97870,
    "scheduler_time": 221.7817805927726
}
#Debug simulation 
Total elapsed time: 82.91672853892669. Arrivals time: 0.4888118742965162 Scheduler time: 82.21852424228564 Scheduler overhead time: 0.08054895978420973 Adapter cache time: 0.01726917689666152 Engine time: 0.0800960105843842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 84.75083629693836,
    "estimated_duration": 3600.090400880063,
    "input_throughput": 6875.500402420208,
    "output_throughput": 6019.536896824155,
    "total_throughput": 12895.037299244363,
    "itl": 96.60334168023786,
    "ttft": 1877675.8087267957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7193238559784323,
    "arrivals": 497997,
    "finished_requests": 100359,
    "scheduler_time": 214.73190316905752
}
#Debug simulation 
Total elapsed time: 84.75100064929575. Arrivals time: 0.4832864343188703 Scheduler time: 84.06575165688992 Scheduler overhead time: 0.07704238593578339 Adapter cache time: 0.016977827064692974 Engine time: 0.07751968875527382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 83.02372820163146,
    "estimated_duration": 3600.0098714793053,
    "input_throughput": 6709.630490561519,
    "output_throughput": 5873.454172310747,
    "total_throughput": 12583.084662872265,
    "itl": 90.14331022402686,
    "ttft": 1898894.6442088867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.694139780495332,
    "arrivals": 497997,
    "finished_requests": 97893,
    "scheduler_time": 221.71688887966224
}
#Debug simulation 
Total elapsed time: 83.0238988106139. Arrivals time: 0.4786682827398181 Scheduler time: 82.334929601755 Scheduler overhead time: 0.08071543695405126 Adapter cache time: 0.017168829683214426 Engine time: 0.08059871336445212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 84.43797668628395,
    "estimated_duration": 3600.0391617729965,
    "input_throughput": 6876.357974897207,
    "output_throughput": 6020.028123618108,
    "total_throughput": 12896.386098515315,
    "itl": 96.6340728054584,
    "ttft": 1877825.02444787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 400,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5535683762282027,
    "arrivals": 497997,
    "finished_requests": 100377,
    "scheduler_time": 214.6827791219163
}
#Debug simulation 
Total elapsed time: 84.43816468724981. Arrivals time: 0.5261357598938048 Scheduler time: 83.70946635305882 Scheduler overhead time: 0.07768809050321579 Adapter cache time: 0.01702031260356307 Engine time: 0.07752784015610814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 83.10212069610134,
    "estimated_duration": 3600.0715576668563,
    "input_throughput": 6707.32723314232,
    "output_throughput": 5871.653288386137,
    "total_throughput": 12578.980521528458,
    "itl": 90.0655867315095,
    "ttft": 1898793.7430438679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.685178879890607,
    "arrivals": 497997,
    "finished_requests": 97859,
    "scheduler_time": 221.82756887605936
}
#Debug simulation 
Total elapsed time: 83.10228198999539. Arrivals time: 0.4778688265942037 Scheduler time: 82.41504610469565 Scheduler overhead time: 0.08059710310772061 Adapter cache time: 0.017184554133564234 Engine time: 0.08003421034663916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 82.3938901158981,
    "estimated_duration": 3600.0099769590433,
    "input_throughput": 7007.179191572277,
    "output_throughput": 6131.389674271206,
    "total_throughput": 13138.568865843483,
    "itl": 98.50870235422616,
    "ttft": 1873222.929650386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9771103005530288,
    "arrivals": 497033,
    "finished_requests": 102421,
    "scheduler_time": 214.22490857379125
}
#Debug simulation 
Total elapsed time: 82.39405294880271. Arrivals time: 0.7847818844020367 Scheduler time: 81.41216726601124 Scheduler overhead time: 0.07533158315345645 Adapter cache time: 0.01606555934995413 Engine time: 0.07618985418230295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 82.05478674732149,
    "estimated_duration": 3600.042357810181,
    "input_throughput": 6929.2745808618365,
    "output_throughput": 6069.998857816829,
    "total_throughput": 12999.273438678665,
    "itl": 95.7727641804662,
    "ttft": 1880982.213931087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1772761930059668,
    "arrivals": 497033,
    "finished_requests": 101348,
    "scheduler_time": 216.91465353323608
}
#Debug simulation 
Total elapsed time: 82.05495284637436. Arrivals time: 0.7906234571710229 Scheduler time: 81.06313068279997 Scheduler overhead time: 0.07727334974333644 Adapter cache time: 0.016263315454125404 Engine time: 0.07715357607230544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.93775211973116,
    "estimated_duration": 3600.0567757076296,
    "input_throughput": 6746.248604711562,
    "output_throughput": 5912.242868952066,
    "total_throughput": 12658.491473663627,
    "itl": 89.47111976226952,
    "ttft": 1902044.4122094184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1971176654193627,
    "arrivals": 497033,
    "finished_requests": 98700,
    "scheduler_time": 223.6391231433625
}
#Debug simulation 
Total elapsed time: 78.93791128974408. Arrivals time: 0.7848311578854918 Scheduler time: 77.94468316016719 Scheduler overhead time: 0.07962258253246546 Adapter cache time: 0.01674006972461939 Engine time: 0.0801986763253808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 81.22062247199938,
    "estimated_duration": 3600.0523443228303,
    "input_throughput": 6931.210052917607,
    "output_throughput": 6071.444498430314,
    "total_throughput": 13002.65455134792,
    "itl": 95.81884299392846,
    "ttft": 1880962.429231801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0342894327547367,
    "arrivals": 497033,
    "finished_requests": 101372,
    "scheduler_time": 216.86260019953974
}
#Debug simulation 
Total elapsed time: 81.22078101709485. Arrivals time: 0.7737835138104856 Scheduler time: 80.24451342457905 Scheduler overhead time: 0.07883061515167356 Adapter cache time: 0.015962558798491955 Engine time: 0.07726678252220154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 78.80374711193144,
    "estimated_duration": 3600.0760529890163,
    "input_throughput": 6749.177140251412,
    "output_throughput": 5913.6245142166035,
    "total_throughput": 12662.801654468016,
    "itl": 89.52651335231262,
    "ttft": 1902173.2204989728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.176405947757892,
    "arrivals": 497033,
    "finished_requests": 98730,
    "scheduler_time": 223.56540080758387
}
#Debug simulation 
Total elapsed time: 78.80391299864277. Arrivals time: 0.49523503007367253 Scheduler time: 78.09867032710463 Scheduler overhead time: 0.08082600589841604 Adapter cache time: 0.016884158831089735 Engine time: 0.08042555022984743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.48165757209063,
    "estimated_duration": 3600.068103703169,
    "input_throughput": 6934.629646122498,
    "output_throughput": 6073.381216735662,
    "total_throughput": 13008.01086285816,
    "itl": 95.90217248814248,
    "ttft": 1881352.3126558377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9087923612305824,
    "arrivals": 497033,
    "finished_requests": 101422,
    "scheduler_time": 216.76034369392698
}
#Debug simulation 
Total elapsed time: 81.48183317808434. Arrivals time: 0.7947864811867476 Scheduler time: 80.48689344292507 Scheduler overhead time: 0.07673152908682823 Adapter cache time: 0.016121266409754753 Engine time: 0.07692927354946733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.58338947920129,
    "estimated_duration": 3600.0617141077664,
    "input_throughput": 6751.005935470046,
    "output_throughput": 5915.2830398847245,
    "total_throughput": 12666.288975354772,
    "itl": 89.57288688524685,
    "ttft": 1902463.19849773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1633042001351783,
    "arrivals": 497033,
    "finished_requests": 98762,
    "scheduler_time": 223.4934369856996
}
#Debug simulation 
Total elapsed time: 78.58354407502338. Arrivals time: 0.778854968957603 Scheduler time: 77.59668195759878 Scheduler overhead time: 0.07997910771518946 Adapter cache time: 0.0162806399166584 Engine time: 0.08013442065566778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.9083842607215,
    "estimated_duration": 3600.0751069648077,
    "input_throughput": 6633.244388096439,
    "output_throughput": 5801.18035859749,
    "total_throughput": 12434.424746693929,
    "itl": 94.04218160869075,
    "ttft": 1838659.0516050335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4216679418692348,
    "arrivals": 432423,
    "finished_requests": 96666,
    "scheduler_time": 216.08942072270682
}
#Debug simulation 
Total elapsed time: 81.90855654375628. Arrivals time: 0.6002099509350955 Scheduler time: 81.10139855835587 Scheduler overhead time: 0.07990459073334932 Adapter cache time: 0.015787044074386358 Engine time: 0.08048235578462481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.66730789886788,
    "estimated_duration": 3600.010702568245,
    "input_throughput": 6761.688231269514,
    "output_throughput": 5903.6905042637445,
    "total_throughput": 12665.378735533259,
    "itl": 97.19508212428983,
    "ttft": 1831790.238212289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8150452958047407,
    "arrivals": 432423,
    "finished_requests": 98454,
    "scheduler_time": 212.40461564165136
}
#Debug simulation 
Total elapsed time: 74.66748506808653. Arrivals time: 0.5554508650675416 Scheduler time: 73.91544031631202 Scheduler overhead time: 0.07546834787353873 Adapter cache time: 0.015046308748424053 Engine time: 0.07592801284044981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.87652098201215,
    "estimated_duration": 3600.0004718619766,
    "input_throughput": 6590.994413877875,
    "output_throughput": 5757.220356496284,
    "total_throughput": 12348.21477037416,
    "itl": 91.31417151971455,
    "ttft": 1849781.2501591924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8546679989388262,
    "arrivals": 432423,
    "finished_requests": 95965,
    "scheduler_time": 217.81043303138327
}
#Debug simulation 
Total elapsed time: 71.87668662564829. Arrivals time: 0.5184501870535314 Scheduler time: 71.15643050801009 Scheduler overhead time: 0.07707562390714884 Adapter cache time: 0.015456423163414001 Engine time: 0.07826872449368238 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 75.0182430986315,
    "estimated_duration": 3600.0718355742106,
    "input_throughput": 6748.235343510886,
    "output_throughput": 5892.513529974175,
    "total_throughput": 12640.748873485061,
    "itl": 97.07643680018376,
    "ttft": 1830294.4336553952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7139790178742238,
    "arrivals": 432423,
    "finished_requests": 98274,
    "scheduler_time": 212.51571773440872
}
#Debug simulation 
Total elapsed time: 75.01840928383172. Arrivals time: 0.5440600202418864 Scheduler time: 74.27558309119195 Scheduler overhead time: 0.07666883943602443 Adapter cache time: 0.015099047217518091 Engine time: 0.07702348008751869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 71.6541612627916,
    "estimated_duration": 3600.053320044574,
    "input_throughput": 6580.7103100647055,
    "output_throughput": 5749.037628071504,
    "total_throughput": 12329.74793813621,
    "itl": 91.16391332515752,
    "ttft": 1847331.166443028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8183754723426007,
    "arrivals": 432423,
    "finished_requests": 95787,
    "scheduler_time": 218.08892265387564
}
#Debug simulation 
Total elapsed time: 71.6543271667324. Arrivals time: 0.5171618503518403 Scheduler time: 70.93350824946538 Scheduler overhead time: 0.07884052721783519 Adapter cache time: 0.01569932233542204 Engine time: 0.07798683224245906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 76.34755937429145,
    "estimated_duration": 3600.0524496444946,
    "input_throughput": 6753.91465543816,
    "output_throughput": 5915.952141781477,
    "total_throughput": 12669.866797219636,
    "itl": 96.96184661353148,
    "ttft": 1822011.742872482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.429998290687795,
    "arrivals": 432423,
    "finished_requests": 98494,
    "scheduler_time": 212.75524121118497
}
#Debug simulation 
Total elapsed time: 76.34771807305515. Arrivals time: 0.45927236368879676 Scheduler time: 75.70136617636308 Scheduler overhead time: 0.07321635307744145 Adapter cache time: 0.013936096802353859 Engine time: 0.07110414654016495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.44014298217371,
    "estimated_duration": 3600.0588790550223,
    "input_throughput": 6574.96940889289,
    "output_throughput": 5747.126837389645,
    "total_throughput": 12322.096246282536,
    "itl": 91.22304680272319,
    "ttft": 1845043.6973480117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7463091704808218,
    "arrivals": 432423,
    "finished_requests": 95762,
    "scheduler_time": 218.09813359896592
}
#Debug simulation 
Total elapsed time: 71.44029791187495. Arrivals time: 0.458819595631212 Scheduler time: 70.78990152524784 Scheduler overhead time: 0.07460553525015712 Adapter cache time: 0.014349351171404123 Engine time: 0.07267199596390128 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 69.10677311941981,
    "estimated_duration": 3600.030705995805,
    "input_throughput": 6821.783480651406,
    "output_throughput": 5966.623830242694,
    "total_throughput": 12788.4073108941,
    "itl": 99.57279087036918,
    "ttft": 1787805.4922029409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.86469934700988,
    "arrivals": 386952,
    "finished_requests": 99767,
    "scheduler_time": 208.78709388062884
}
#Debug simulation 
Total elapsed time: 69.1069424902089. Arrivals time: 0.45028620585799217 Scheduler time: 68.47397398995236 Scheduler overhead time: 0.0710821826942265 Adapter cache time: 0.014501156285405159 Engine time: 0.06864717975258827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.83040913008153,
    "estimated_duration": 3600.00206605601,
    "input_throughput": 6742.270575025377,
    "output_throughput": 5909.991608229533,
    "total_throughput": 12652.26218325491,
    "itl": 96.91989954306722,
    "ttft": 1793640.535313739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8516867473535263,
    "arrivals": 386952,
    "finished_requests": 98762,
    "scheduler_time": 211.22758950267297
}
#Debug simulation 
Total elapsed time: 71.83057070011273. Arrivals time: 0.45769405039027333 Scheduler time: 71.18734687007964 Scheduler overhead time: 0.07204303843900561 Adapter cache time: 0.014386691153049469 Engine time: 0.07039482984691858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 67.263495168183,
    "estimated_duration": 3600.0054257004685,
    "input_throughput": 6577.183142826206,
    "output_throughput": 5755.339937013724,
    "total_throughput": 12332.52307983993,
    "itl": 90.88629499853398,
    "ttft": 1810019.8298427945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7332219677371958,
    "arrivals": 386952,
    "finished_requests": 96196,
    "scheduler_time": 217.16867078879426
}
#Debug simulation 
Total elapsed time: 67.26365867629647. Arrivals time: 0.45188144827261567 Scheduler time: 66.61776607390493 Scheduler overhead time: 0.07553443126380444 Adapter cache time: 0.01488073030486703 Engine time: 0.07322434103116393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 68.42664199089631,
    "estimated_duration": 3600.0824559057965,
    "input_throughput": 6745.24522630418,
    "output_throughput": 5909.022712827734,
    "total_throughput": 12654.267939131912,
    "itl": 97.01015416793597,
    "ttft": 1793472.7251045888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8047421320155241,
    "arrivals": 386952,
    "finished_requests": 98719,
    "scheduler_time": 210.9454223184746
}
#Debug simulation 
Total elapsed time: 68.42681127693504. Arrivals time: 0.45680183731019497 Scheduler time: 67.78520595561713 Scheduler overhead time: 0.07204186171293259 Adapter cache time: 0.014626326505094767 Engine time: 0.06920602591708302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 64.77001816919073,
    "estimated_duration": 3600.057494666713,
    "input_throughput": 6563.20045860474,
    "output_throughput": 5750.780377999674,
    "total_throughput": 12313.980836604414,
    "itl": 90.95389877397669,
    "ttft": 1812642.071018769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9527690792223504,
    "arrivals": 386952,
    "finished_requests": 96067,
    "scheduler_time": 217.0907949248892
}
#Debug simulation 
Total elapsed time: 64.77018014807254. Arrivals time: 0.4490985064767301 Scheduler time: 64.1293286215514 Scheduler overhead time: 0.074213947635144 Adapter cache time: 0.014958641957491636 Engine time: 0.07227855361998081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 71.62449598498642,
    "estimated_duration": 3600.0692417843347,
    "input_throughput": 6737.6734642964075,
    "output_throughput": 5904.224216938918,
    "total_throughput": 12641.897681235327,
    "itl": 96.8286880907291,
    "ttft": 1796444.2949573814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5895963142020575,
    "arrivals": 386952,
    "finished_requests": 98667,
    "scheduler_time": 211.1903974427739
}
#Debug simulation 
Total elapsed time: 71.62464976031333. Arrivals time: 0.46572430105879903 Scheduler time: 70.97321533458307 Scheduler overhead time: 0.07230016170069575 Adapter cache time: 0.01454230397939682 Engine time: 0.07004474010318518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 64.77771189901978,
    "estimated_duration": 3600.087849374983,
    "input_throughput": 6552.524268010693,
    "output_throughput": 5737.477768378908,
    "total_throughput": 12290.002036389602,
    "itl": 90.56256868254555,
    "ttft": 1815590.831555631,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9239312744699486,
    "arrivals": 386952,
    "finished_requests": 95863,
    "scheduler_time": 217.57434015394233
}
#Debug simulation 
Total elapsed time: 64.7778689591214. Arrivals time: 0.4570058682002127 Scheduler time: 64.1285769417882 Scheduler overhead time: 0.07480150833725929 Adapter cache time: 0.014685909729450941 Engine time: 0.07247010059654713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 66.57239628396928,
    "estimated_duration": 3600.0329349621725,
    "input_throughput": 6808.741320656156,
    "output_throughput": 5962.489062680074,
    "total_throughput": 12771.23038333623,
    "itl": 99.86498381664573,
    "ttft": 1784758.7207087097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9771103005530288,
    "arrivals": 379429,
    "finished_requests": 99280,
    "scheduler_time": 208.83746617187626
}
#Debug simulation 
Total elapsed time: 66.57255238108337. Arrivals time: 0.4678754983469844 Scheduler time: 65.92208720184863 Scheduler overhead time: 0.07043931307271123 Adapter cache time: 0.0145210400223732 Engine time: 0.069060233887285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 67.88120197877288,
    "estimated_duration": 3600.0628559857787,
    "input_throughput": 6730.778869516416,
    "output_throughput": 5890.588817009192,
    "total_throughput": 12621.367686525607,
    "itl": 96.96799736615162,
    "ttft": 1785553.59560878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.03710378543474,
    "arrivals": 379429,
    "finished_requests": 98064,
    "scheduler_time": 211.40581394493375
}
#Debug simulation 
Total elapsed time: 67.88135729916394. Arrivals time: 0.46242943592369556 Scheduler time: 67.23212312115356 Scheduler overhead time: 0.07261817017570138 Adapter cache time: 0.014883853495121002 Engine time: 0.07042368687689304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 63.86160597810522,
    "estimated_duration": 3600.048288649504,
    "input_throughput": 6573.560436567802,
    "output_throughput": 5752.001456560478,
    "total_throughput": 12325.561893128279,
    "itl": 91.21387048701388,
    "ttft": 1809384.5503289646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.579947698768241,
    "arrivals": 379429,
    "finished_requests": 95827,
    "scheduler_time": 216.49805314899533
}
#Debug simulation 
Total elapsed time: 63.861739192157984. Arrivals time: 0.45309371827170253 Scheduler time: 63.2159990943037 Scheduler overhead time: 0.0739692272618413 Adapter cache time: 0.015948018059134483 Engine time: 0.0723966951481998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 68.4490356259048,
    "estimated_duration": 3600.0887254261884,
    "input_throughput": 6753.502720164692,
    "output_throughput": 5904.720028110829,
    "total_throughput": 12658.22274827552,
    "itl": 97.43710353452869,
    "ttft": 1783093.5154013373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.076757621318099,
    "arrivals": 379429,
    "finished_requests": 98377,
    "scheduler_time": 210.88743381036664
}
#Debug simulation 
Total elapsed time: 68.44917529122904. Arrivals time: 0.4835517844185233 Scheduler time: 67.77792591741309 Scheduler overhead time: 0.07250315137207508 Adapter cache time: 0.014934205450117588 Engine time: 0.0710841272957623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 64.1317842840217,
    "estimated_duration": 3600.012411407218,
    "input_throughput": 6559.957383804882,
    "output_throughput": 5745.959356821823,
    "total_throughput": 12305.916740626704,
    "itl": 91.16748471877112,
    "ttft": 1812597.0830562937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6371489433525275,
    "arrivals": 379429,
    "finished_requests": 95664,
    "scheduler_time": 216.6751773739305
}
#Debug simulation 
Total elapsed time: 64.1319214142859. Arrivals time: 0.47596876230090857 Scheduler time: 63.46387916011736 Scheduler overhead time: 0.07451496412977576 Adapter cache time: 0.015628888737410307 Engine time: 0.07178062153980136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 68.19156281091273,
    "estimated_duration": 3600.0748391844686,
    "input_throughput": 6714.685132900192,
    "output_throughput": 5875.047032297608,
    "total_throughput": 12589.7321651978,
    "itl": 96.76065632143738,
    "ttft": 1784471.2699334985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.813033547122025,
    "arrivals": 379429,
    "finished_requests": 97875,
    "scheduler_time": 212.04879796401184
}
#Debug simulation 
Total elapsed time: 68.19170620292425. Arrivals time: 0.45890475949272513 Scheduler time: 67.54565615812317 Scheduler overhead time: 0.07227833848446608 Adapter cache time: 0.014828096609562635 Engine time: 0.07084476621821523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 66.89650865178555,
    "estimated_duration": 3600.0616884968617,
    "input_throughput": 6557.823738253029,
    "output_throughput": 5747.448180155827,
    "total_throughput": 12305.271918408855,
    "itl": 91.07335268959461,
    "ttft": 1803964.4145783945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1314678652398356,
    "arrivals": 379429,
    "finished_requests": 95657,
    "scheduler_time": 216.67076045846494
}
#Debug simulation 
Total elapsed time: 66.89664793666452. Arrivals time: 0.458310900721699 Scheduler time: 66.24428481515497 Scheduler overhead time: 0.07458153879269958 Adapter cache time: 0.015774738509207964 Engine time: 0.07321013603359461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 71.02206461597234,
    "estimated_duration": 3600.0223271482882,
    "input_throughput": 6818.814654253907,
    "output_throughput": 5964.727173514416,
    "total_throughput": 12783.541827768322,
    "itl": 99.68350777571814,
    "ttft": 1767865.8228690028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9043738012015796,
    "arrivals": 375670,
    "finished_requests": 99621,
    "scheduler_time": 208.81692154898172
}
#Debug simulation 
Total elapsed time: 71.02220121305436. Arrivals time: 0.47390341851860285 Scheduler time: 70.36420094501227 Scheduler overhead time: 0.07130112731829286 Adapter cache time: 0.01476375013589859 Engine time: 0.06950248684734106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 69.45662436215207,
    "estimated_duration": 3600.087848556854,
    "input_throughput": 6712.183984534338,
    "output_throughput": 5872.989185104352,
    "total_throughput": 12585.17316963869,
    "itl": 96.8713499287206,
    "ttft": 1773269.595270813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0717998320842184,
    "arrivals": 375670,
    "finished_requests": 98069,
    "scheduler_time": 212.10244696024685
}
#Debug simulation 
Total elapsed time: 69.45676007913426. Arrivals time: 0.4547509402036667 Scheduler time: 68.81608264055103 Scheduler overhead time: 0.07214922225102782 Adapter cache time: 0.014754804316908121 Engine time: 0.07010807003825903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 68.73681220086291,
    "estimated_duration": 3600.072143934712,
    "input_throughput": 6571.737469166967,
    "output_throughput": 5752.7083269405675,
    "total_throughput": 12324.445796107533,
    "itl": 91.24486416676979,
    "ttft": 1792102.210666602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3194996138196537,
    "arrivals": 375670,
    "finished_requests": 96013,
    "scheduler_time": 216.4632495752171
}
#Debug simulation 
Total elapsed time: 68.73694215714931. Arrivals time: 0.46100658690556884 Scheduler time: 68.08185896556824 Scheduler overhead time: 0.07474271254613996 Adapter cache time: 0.015887357760220766 Engine time: 0.07314727082848549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 69.18780483491719,
    "estimated_duration": 3600.046090614996,
    "input_throughput": 6726.555546921677,
    "output_throughput": 5884.851323217598,
    "total_throughput": 12611.406870139275,
    "itl": 96.93710909499832,
    "ttft": 1773242.0623798952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1758500612992764,
    "arrivals": 375670,
    "finished_requests": 98317,
    "scheduler_time": 211.66136499771702
}
#Debug simulation 
Total elapsed time: 69.18794690165669. Arrivals time: 0.45749287074431777 Scheduler time: 68.54369945032522 Scheduler overhead time: 0.07208244130015373 Adapter cache time: 0.015421382151544094 Engine time: 0.07033104170113802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 62.49307589279488,
    "estimated_duration": 3600.07425173994,
    "input_throughput": 6579.981784695476,
    "output_throughput": 5752.581350229333,
    "total_throughput": 12332.563134924809,
    "itl": 91.24196949883606,
    "ttft": 1801636.2177903967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7049661706900388,
    "arrivals": 375670,
    "finished_requests": 96096,
    "scheduler_time": 216.51362770849124
}
#Debug simulation 
Total elapsed time: 62.49321838468313. Arrivals time: 0.4401283492334187 Scheduler time: 61.863570543471724 Scheduler overhead time: 0.07282830821350217 Adapter cache time: 0.015682135708630085 Engine time: 0.07109281700104475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 69.15964651899412,
    "estimated_duration": 3600.0154698587885,
    "input_throughput": 6747.523504656725,
    "output_throughput": 5902.542969026049,
    "total_throughput": 12650.066473682775,
    "itl": 97.45189233910205,
    "ttft": 1772959.6583072182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9726315706362874,
    "arrivals": 375670,
    "finished_requests": 98615,
    "scheduler_time": 210.89127807410122
}
#Debug simulation 
Total elapsed time: 69.15977876121178. Arrivals time: 0.46136748464778066 Scheduler time: 68.5115933441557 Scheduler overhead time: 0.07222241070121527 Adapter cache time: 0.01508409483358264 Engine time: 0.07072148285806179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 63.72272758977488,
    "estimated_duration": 3600.025421093593,
    "input_throughput": 6577.729663032946,
    "output_throughput": 5754.2960331949935,
    "total_throughput": 12332.02569622794,
    "itl": 91.19921619955353,
    "ttft": 1799800.588461858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.640247500929997,
    "arrivals": 375670,
    "finished_requests": 96085,
    "scheduler_time": 216.4320040982882
}
#Debug simulation 
Total elapsed time: 63.72286348184571. Arrivals time: 0.4493215284310281 Scheduler time: 63.083704326767474 Scheduler overhead time: 0.0731709054671228 Adapter cache time: 0.01556485053151846 Engine time: 0.07124768430367112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 73.68855288997293,
    "estimated_duration": 3600.0641369390382,
    "input_throughput": 6839.908696998021,
    "output_throughput": 5988.393867429885,
    "total_throughput": 12828.302564427906,
    "itl": 100.27874397628244,
    "ttft": 1761900.599291034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4796533869812394,
    "arrivals": 373765,
    "finished_requests": 100050,
    "scheduler_time": 207.6971012034102
}
#Debug simulation 
Total elapsed time: 73.68869110709056. Arrivals time: 0.468789700884372 Scheduler time: 73.0342090013437 Scheduler overhead time: 0.07213035598397255 Adapter cache time: 0.015285855624824762 Engine time: 0.06981623033061624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.93876154301688,
    "estimated_duration": 3600.0423489793343,
    "input_throughput": 6754.597486025182,
    "output_throughput": 5912.293783442429,
    "total_throughput": 12666.891269467611,
    "itl": 97.39712840983155,
    "ttft": 1766723.700087861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3604739730665485,
    "arrivals": 373765,
    "finished_requests": 98866,
    "scheduler_time": 210.42813699029816
}
#Debug simulation 
Total elapsed time: 77.93891140492633. Arrivals time: 0.4846432670019567 Scheduler time: 77.26310523226857 Scheduler overhead time: 0.07394819427281618 Adapter cache time: 0.015715693589299917 Engine time: 0.07221769820898771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 72.12396719679236,
    "estimated_duration": 3600.055170296705,
    "input_throughput": 6568.899331076355,
    "output_throughput": 5741.967003882423,
    "total_throughput": 12310.866334958779,
    "itl": 90.83015311869032,
    "ttft": 1790335.3788810486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.762279773573409,
    "arrivals": 373765,
    "finished_requests": 96070,
    "scheduler_time": 216.72653873030436
}
#Debug simulation 
Total elapsed time: 72.1241079159081. Arrivals time: 0.4978027315810323 Scheduler time: 71.43051167111844 Scheduler overhead time: 0.07668516971170902 Adapter cache time: 0.015413162764161825 Engine time: 0.07306337310001254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 74.95056010922417,
    "estimated_duration": 3600.0199270253415,
    "input_throughput": 6762.327290814641,
    "output_throughput": 5914.853648489293,
    "total_throughput": 12677.180939303935,
    "itl": 97.40524098042481,
    "ttft": 1767322.5360923174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4564687295630536,
    "arrivals": 373765,
    "finished_requests": 98905,
    "scheduler_time": 210.4453808566719
}
#Debug simulation 
Total elapsed time: 74.95070240227506. Arrivals time: 0.47332477010786533 Scheduler time: 74.28696864517406 Scheduler overhead time: 0.0736918831244111 Adapter cache time: 0.01554500637575984 Engine time: 0.07219478487968445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 73.38805138599128,
    "estimated_duration": 3600.014820040088,
    "input_throughput": 6572.886830431577,
    "output_throughput": 5747.17884071644,
    "total_throughput": 12320.065671148019,
    "itl": 91.08590358662545,
    "ttft": 1788168.8743250552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.500737888952731,
    "arrivals": 373765,
    "finished_requests": 96162,
    "scheduler_time": 216.52222045095874
}
#Debug simulation 
Total elapsed time: 73.38819570513442. Arrivals time: 0.512317712418735 Scheduler time: 72.67855304386467 Scheduler overhead time: 0.0771376509219408 Adapter cache time: 0.01590669807046652 Engine time: 0.0738246738910675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 74.42639014357701,
    "estimated_duration": 3600.1047176127354,
    "input_throughput": 6775.732350412148,
    "output_throughput": 5923.427142458453,
    "total_throughput": 12699.1594928706,
    "itl": 97.62157592140879,
    "ttft": 1767343.4738501266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4514256411790747,
    "arrivals": 373765,
    "finished_requests": 98989,
    "scheduler_time": 210.08162139826774
}
#Debug simulation 
Total elapsed time: 74.42653399799019. Arrivals time: 0.4825987075455487 Scheduler time: 73.75415355013683 Scheduler overhead time: 0.07373904762789607 Adapter cache time: 0.01532821822911501 Engine time: 0.07138953218236566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 68.12035573925823,
    "estimated_duration": 3600.0753660728783,
    "input_throughput": 6537.491193045084,
    "output_throughput": 5721.5027202247265,
    "total_throughput": 12258.993913269811,
    "itl": 90.5748013722796,
    "ttft": 1792167.4001126664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.617781811449692,
    "arrivals": 373765,
    "finished_requests": 95602,
    "scheduler_time": 217.57381444476522
}
#Debug simulation 
Total elapsed time: 68.12049528909847. Arrivals time: 0.46687616035342216 Scheduler time: 67.45760389277712 Scheduler overhead time: 0.07583716092631221 Adapter cache time: 0.015945002902299166 Engine time: 0.07375469757243991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 71.10250326618552,
    "estimated_duration": 3600.074724292432,
    "input_throughput": 6831.230705866406,
    "output_throughput": 5969.0888789047385,
    "total_throughput": 12800.319584771145,
    "itl": 99.64185008541472,
    "ttft": 1767896.0265517973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9175986192654795,
    "arrivals": 372804,
    "finished_requests": 99369,
    "scheduler_time": 208.64175425213898
}
#Debug simulation 
Total elapsed time: 71.10264768730849. Arrivals time: 0.4758063475601375 Scheduler time: 70.44077037135139 Scheduler overhead time: 0.07199065526947379 Adapter cache time: 0.014946207404136658 Engine time: 0.07017247099429369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 69.30742939701304,
    "estimated_duration": 3600.014798290786,
    "input_throughput": 6768.823564716836,
    "output_throughput": 5913.956523208798,
    "total_throughput": 12682.780087925634,
    "itl": 96.98272238602635,
    "ttft": 1776958.0367887863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0731880530575317,
    "arrivals": 372804,
    "finished_requests": 98501,
    "scheduler_time": 211.19211394415922
}
#Debug simulation 
Total elapsed time: 69.30757432803512. Arrivals time: 0.4644886436872184 Scheduler time: 68.65685595571995 Scheduler overhead time: 0.0722254142165184 Adapter cache time: 0.01452304795384407 Engine time: 0.07062243483960629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 64.03594051487744,
    "estimated_duration": 3600.001911056109,
    "input_throughput": 6591.389556523534,
    "output_throughput": 5747.422782320162,
    "total_throughput": 12338.812338843696,
    "itl": 90.72084070113283,
    "ttft": 1798598.6229709566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.141521069002347,
    "arrivals": 372804,
    "finished_requests": 95655,
    "scheduler_time": 217.1557003468736
}
#Debug simulation 
Total elapsed time: 64.0360809257254. Arrivals time: 0.4665820971131325 Scheduler time: 63.377670410089195 Scheduler overhead time: 0.07387936487793922 Adapter cache time: 0.015095409005880356 Engine time: 0.07288743136450648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 67.86426758533344,
    "estimated_duration": 3600.0999818713094,
    "input_throughput": 6795.356274323189,
    "output_throughput": 5924.604624150808,
    "total_throughput": 12719.960898473997,
    "itl": 96.84616523846425,
    "ttft": 1777907.8317709134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.942969134687443,
    "arrivals": 372804,
    "finished_requests": 98705,
    "scheduler_time": 211.5359115405671
}
#Debug simulation 
Total elapsed time: 67.86440407531336. Arrivals time: 0.40903306379914284 Scheduler time: 67.27176231797785 Scheduler overhead time: 0.07087781513109803 Adapter cache time: 0.014623471535742283 Engine time: 0.06933162920176983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 63.13195017678663,
    "estimated_duration": 3600.0175141007767,
    "input_throughput": 6581.337148277205,
    "output_throughput": 5751.409241455261,
    "total_throughput": 12332.746389732465,
    "itl": 90.84403150464566,
    "ttft": 1798796.9420259607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0866410032566716,
    "arrivals": 372804,
    "finished_requests": 95673,
    "scheduler_time": 217.049595470614
}
#Debug simulation 
Total elapsed time: 63.132091405801475. Arrivals time: 0.3965508472174406 Scheduler time: 62.54528517508879 Scheduler overhead time: 0.07328431494534016 Adapter cache time: 0.014767579734325409 Engine time: 0.0722774276509881 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 68.33539855899289,
    "estimated_duration": 3600.093726832113,
    "input_throughput": 6778.392134105125,
    "output_throughput": 5911.954136463109,
    "total_throughput": 12690.346270568234,
    "itl": 96.78245329639022,
    "ttft": 1778089.489081911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.813033547122025,
    "arrivals": 372804,
    "finished_requests": 98520,
    "scheduler_time": 211.37388898513078
}
#Debug simulation 
Total elapsed time: 68.33552897162735. Arrivals time: 0.3987464611418545 Scheduler time: 67.75459999032319 Scheduler overhead time: 0.07033489178866148 Adapter cache time: 0.01441947789862752 Engine time: 0.06875950610265136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 64.40910823689774,
    "estimated_duration": 3600.0134499267956,
    "input_throughput": 6601.584780324436,
    "output_throughput": 5761.355975051081,
    "total_throughput": 12362.940755375517,
    "itl": 90.95226669022216,
    "ttft": 1795843.6655748142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.063290127795197,
    "arrivals": 372804,
    "finished_requests": 95857,
    "scheduler_time": 216.85323889990366
}
#Debug simulation 
Total elapsed time: 64.4092463310808. Arrivals time: 0.38143832981586456 Scheduler time: 63.83958769682795 Scheduler overhead time: 0.07285411981865764 Adapter cache time: 0.014471000991761684 Engine time: 0.0705016003921628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 77.50260092318058,
    "estimated_duration": 3600.046480682905,
    "input_throughput": 6790.70148987704,
    "output_throughput": 5932.313128342941,
    "total_throughput": 12723.014618219982,
    "itl": 99.19210050367101,
    "ttft": 1760657.4935213777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.507629259284584,
    "arrivals": 372307,
    "finished_requests": 98901,
    "scheduler_time": 209.7480565199336
}
#Debug simulation 
Total elapsed time: 77.50272471318021. Arrivals time: 0.4183652540668845 Scheduler time: 76.90029481099918 Scheduler overhead time: 0.07158710155636072 Adapter cache time: 0.013881754130125046 Engine time: 0.06960400380194187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 76.25696075195447,
    "estimated_duration": 3600.0026775692745,
    "input_throughput": 6789.017728315202,
    "output_throughput": 5924.991704284517,
    "total_throughput": 12714.009432599718,
    "itl": 96.76514109561663,
    "ttft": 1769211.5888453894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5272021918697298,
    "arrivals": 372307,
    "finished_requests": 98818,
    "scheduler_time": 211.537342126134
}
#Debug simulation 
Total elapsed time: 76.2571017681621. Arrivals time: 0.4221368255093694 Scheduler time: 75.64979552850127 Scheduler overhead time: 0.07180457701906562 Adapter cache time: 0.014018136076629162 Engine time: 0.07030592998489738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 69.75967414397746,
    "estimated_duration": 3600.0799863672396,
    "input_throughput": 6603.190231889211,
    "output_throughput": 5769.463755987011,
    "total_throughput": 12372.653987876221,
    "itl": 90.66623408307792,
    "ttft": 1791169.1738883362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5555483535164996,
    "arrivals": 372307,
    "finished_requests": 96104,
    "scheduler_time": 217.4300777834715
}
#Debug simulation 
Total elapsed time: 69.75980616826564. Arrivals time: 0.3833398697897792 Scheduler time: 69.18522493960336 Scheduler overhead time: 0.07408045837655663 Adapter cache time: 0.014498255681246519 Engine time: 0.07227643672376871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 74.20910279080272,
    "estimated_duration": 3600.07447760456,
    "input_throughput": 6787.738740410621,
    "output_throughput": 5933.259195852183,
    "total_throughput": 12720.997936262804,
    "itl": 96.81877971128804,
    "ttft": 1770117.9403101483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4555624075001086,
    "arrivals": 372307,
    "finished_requests": 98873,
    "scheduler_time": 211.6619708912319
}
#Debug simulation 
Total elapsed time: 74.20923651009798. Arrivals time: 0.3948052441701293 Scheduler time: 73.62785265175626 Scheduler overhead time: 0.07231441186740994 Adapter cache time: 0.014122920110821724 Engine time: 0.07077093841508031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 69.58924239315093,
    "estimated_duration": 3600.0278095613025,
    "input_throughput": 6618.728037799136,
    "output_throughput": 5780.571734676661,
    "total_throughput": 12399.299772475797,
    "itl": 90.53615333143307,
    "ttft": 1794199.7189540388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5274871484888743,
    "arrivals": 372307,
    "finished_requests": 96291,
    "scheduler_time": 217.71598218230503
}
#Debug simulation 
Total elapsed time: 69.5893793781288. Arrivals time: 0.38874222431331873 Scheduler time: 69.00745008327067 Scheduler overhead time: 0.0745798759162426 Adapter cache time: 0.014325309079140425 Engine time: 0.07358574355021119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 74.6060621230863,
    "estimated_duration": 3600.024563706514,
    "input_throughput": 6772.942953171406,
    "output_throughput": 5919.6890529154025,
    "total_throughput": 12692.63200608681,
    "itl": 96.88801593915137,
    "ttft": 1769452.7577491044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.340623397519808,
    "arrivals": 372307,
    "finished_requests": 98587,
    "scheduler_time": 211.41352622680898
}
#Debug simulation 
Total elapsed time: 74.60619074804708. Arrivals time: 0.3978335112333298 Scheduler time: 74.02322897873819 Scheduler overhead time: 0.07206869218498468 Adapter cache time: 0.014227877371013165 Engine time: 0.06974906427785754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_128_slots_32_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 67.53497258713469,
    "estimated_duration": 3600.099139594359,
    "input_throughput": 6604.281737274316,
    "output_throughput": 5773.386008014747,
    "total_throughput": 12377.667745289062,
    "itl": 90.59831867755453,
    "ttft": 1795484.9526166755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5488642453402308,
    "arrivals": 372307,
    "finished_requests": 96132,
    "scheduler_time": 217.8367224155225
}
#Debug simulation 
Total elapsed time: 67.53510004002601. Arrivals time: 0.41696072882041335 Scheduler time: 66.92681277822703 Scheduler overhead time: 0.07417473336681724 Adapter cache time: 0.01444911491125822 Engine time: 0.07251474680379033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 78.7171685649082,
    "estimated_duration": 3600.0327558370736,
    "input_throughput": 6802.350050925004,
    "output_throughput": 5976.555898030207,
    "total_throughput": 12778.905948955211,
    "itl": 99.73138983864797,
    "ttft": 1695329.3436080902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1770088076870875,
    "arrivals": 324936,
    "finished_requests": 99742,
    "scheduler_time": 206.8185015425327
}
#Debug simulation 
Total elapsed time: 78.7173157390207. Arrivals time: 0.4008977389894426 Scheduler time: 78.13200823869556 Scheduler overhead time: 0.07190377870574594 Adapter cache time: 0.013925664592534304 Engine time: 0.06955688912421465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.09321490162984,
    "estimated_duration": 3600.002268954947,
    "input_throughput": 6724.6024283833485,
    "output_throughput": 5910.967107856094,
    "total_throughput": 12635.569536239444,
    "itl": 97.0769172631146,
    "ttft": 1706112.729272666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.293764081331902,
    "arrivals": 324936,
    "finished_requests": 98578,
    "scheduler_time": 209.1664579889961
}
#Debug simulation 
Total elapsed time: 75.09336067689583. Arrivals time: 0.4200180461630225 Scheduler time: 74.48201641999185 Scheduler overhead time: 0.07472530379891396 Adapter cache time: 0.014457147102802992 Engine time: 0.07178610656410456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 60.385207881685346,
    "estimated_duration": 3600.061577842069,
    "input_throughput": 6531.902994308058,
    "output_throughput": 5748.1347339631,
    "total_throughput": 12280.037728271158,
    "itl": 90.22717416754261,
    "ttft": 1752133.8946066126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.004227821649989,
    "arrivals": 324936,
    "finished_requests": 95693,
    "scheduler_time": 216.19350679723024
}
#Debug simulation 
Total elapsed time: 60.385337044950575. Arrivals time: 0.3711773189716041 Scheduler time: 59.82532605109736 Scheduler overhead time: 0.07262150337919593 Adapter cache time: 0.014531701803207397 Engine time: 0.07151551870629191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 70.3422265779227,
    "estimated_duration": 3600.0415642934345,
    "input_throughput": 6715.189135530084,
    "output_throughput": 5912.017019774945,
    "total_throughput": 12627.206155305028,
    "itl": 96.83843829597744,
    "ttft": 1721521.1004609752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4419635285716486,
    "arrivals": 324936,
    "finished_requests": 98516,
    "scheduler_time": 209.5548810572974
}
#Debug simulation 
Total elapsed time: 70.34235861198977. Arrivals time: 0.39938840037211776 Scheduler time: 69.7580958395265 Scheduler overhead time: 0.07170603331178427 Adapter cache time: 0.0141364517621696 Engine time: 0.07001792825758457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_128_slots_32_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 64.80895704869181,
    "estimated_duration": 3600.0610702718204,
    "input_throughput": 6538.362972332227,
    "output_throughput": 5761.643370798115,
    "total_throughput": 12300.006343130342,
    "itl": 90.67292858707658,
    "ttft": 1743260.7332502678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7685188668314396,
    "arrivals": 324936,
    "finished_requests": 95946,
    "scheduler_time": 215.7118828223388
}
#Debug simulation 
Total elapsed time: 64.8090947298333. Arrivals time: 0.39356347918510437 Scheduler time: 64.22561941156164 Scheduler overhead time: 0.07392452517524362 Adapter cache time: 0.014735901728272438 Engine time: 0.07113961176946759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_32_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_32_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 61.12689198786393,
    "estimated_duration": 3600.05480469551,
    "input_throughput": 6719.577982104093,
    "output_throughput": 5914.1194106906905,
    "total_throughput": 12633.697392794784,
    "itl": 97.06618325387878,
    "ttft": 1721714.719091155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3597751603415196,
    "arrivals": 324936,
    "finished_requests": 98545,
    "scheduler_time": 209.15456440129492
}
#Debug simulation 
Total elapsed time: 61.127032273914665. Arrivals time: 0.3998617120087147 Scheduler time: 60.54689674498513 Scheduler overhead time: 0.06991366017609835 Adapter cache time: 0.013686845544725657 Engine time: 0.06811929401010275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_128_slots_32_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_128_slots_32_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 60.558836549986154,
    "estimated_duration": 3600.026270531734,
    "input_throughput": 6525.010717916352,
    "output_throughput": 5747.817222718125,
    "total_throughput": 12272.827940634477,
    "itl": 90.28788464653945,
    "ttft": 1745741.1787482991,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8665662228688642,
    "arrivals": 324936,
    "finished_requests": 95659,
    "scheduler_time": 216.1042870581974
}
#Debug simulation 
Total elapsed time: 60.55897388700396. Arrivals time: 0.4019585023634136 Scheduler time: 59.96687860926613 Scheduler overhead time: 0.07356347376480699 Adapter cache time: 0.014583170879632235 Engine time: 0.07153349323198199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 73.21810962678865,
    "estimated_duration": 3600.070673223237,
    "input_throughput": 6842.681223795092,
    "output_throughput": 5967.10813478742,
    "total_throughput": 12809.789358582511,
    "itl": 99.08600285226316,
    "ttft": 1696793.2527659773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3423190334858357,
    "arrivals": 317347,
    "finished_requests": 99443,
    "scheduler_time": 207.4969594663283
}
#Debug simulation 
Total elapsed time: 73.21823578886688. Arrivals time: 0.41886109160259366 Scheduler time: 72.61507271090522 Scheduler overhead time: 0.07183305593207479 Adapter cache time: 0.014176422730088234 Engine time: 0.06953127263113856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 67.21889019897208,
    "estimated_duration": 3600.0291581358233,
    "input_throughput": 6781.235075507394,
    "output_throughput": 5907.582707194732,
    "total_throughput": 12688.817782702126,
    "itl": 96.973168156797,
    "ttft": 1708875.8761060187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3170805070735523,
    "arrivals": 317347,
    "finished_requests": 98454,
    "scheduler_time": 209.1368386325332
}
#Debug simulation 
Total elapsed time: 67.21901615103707. Arrivals time: 0.40682769287377596 Scheduler time: 66.62836355669424 Scheduler overhead time: 0.0713034369982779 Adapter cache time: 0.013666131068021059 Engine time: 0.06991483364254236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 57.624312387779355,
    "estimated_duration": 3600.065901782555,
    "input_throughput": 6587.692460923306,
    "output_throughput": 5746.2039763652865,
    "total_throughput": 12333.896437288593,
    "itl": 90.79588545999604,
    "ttft": 1739961.5849285393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.740830082511534,
    "arrivals": 317347,
    "finished_requests": 95659,
    "scheduler_time": 215.2498213063652
}
#Debug simulation 
Total elapsed time: 57.62444650800899. Arrivals time: 0.3799229096621275 Scheduler time: 57.06234715972096 Scheduler overhead time: 0.07002446008846164 Adapter cache time: 0.013800527900457382 Engine time: 0.06892023934051394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 67.53615150414407,
    "estimated_duration": 3600.071065075673,
    "input_throughput": 6784.537737864515,
    "output_throughput": 5913.313269429175,
    "total_throughput": 12697.85100729369,
    "itl": 97.12075030190653,
    "ttft": 1708166.2651660806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2429476115619753,
    "arrivals": 317347,
    "finished_requests": 98531,
    "scheduler_time": 208.95025001542945
}
#Debug simulation 
Total elapsed time: 67.53628827491775. Arrivals time: 0.3914195648394525 Scheduler time: 66.9614802156575 Scheduler overhead time: 0.07134629040956497 Adapter cache time: 0.013356898911297321 Engine time: 0.06972450437024236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_128_slots_32_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 57.602863108273596,
    "estimated_duration": 3600.0439953340397,
    "input_throughput": 6583.87342785813,
    "output_throughput": 5742.733707364512,
    "total_throughput": 12326.60713522264,
    "itl": 90.7149919214421,
    "ttft": 1740652.521849335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7314564440678852,
    "arrivals": 317347,
    "finished_requests": 95608,
    "scheduler_time": 215.31531325997963
}
#Debug simulation 
Total elapsed time: 57.6029940652661. Arrivals time: 0.3685529613867402 Scheduler time: 57.05041599087417 Scheduler overhead time: 0.070997120346874 Adapter cache time: 0.013772749807685614 Engine time: 0.06943789962679148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_32_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_32_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 67.18105857726187,
    "estimated_duration": 3600.0371810268994,
    "input_throughput": 6777.974718872126,
    "output_throughput": 5908.758418415085,
    "total_throughput": 12686.73313728721,
    "itl": 96.97375048090636,
    "ttft": 1709084.8885076358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1618736111838341,
    "arrivals": 317347,
    "finished_requests": 98449,
    "scheduler_time": 209.12040421325986
}
#Debug simulation 
Total elapsed time: 67.18119354220107. Arrivals time: 0.400614598300308 Scheduler time: 66.59674542583525 Scheduler overhead time: 0.07151667913421988 Adapter cache time: 0.013788446318358183 Engine time: 0.06986171659082174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_128_slots_32_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_128_slots_32_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 62.5456476979889,
    "estimated_duration": 3600.0575027809055,
    "input_throughput": 6594.924103756715,
    "output_throughput": 5750.712310569432,
    "total_throughput": 12345.636414326147,
    "itl": 90.97662082637922,
    "ttft": 1733229.2133197621,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.371662300247702,
    "arrivals": 317347,
    "finished_requests": 95785,
    "scheduler_time": 214.9856213769551
}
#Debug simulation 
Total elapsed time: 62.545773163903505. Arrivals time: 0.3809095351025462 Scheduler time: 61.9769277186133 Scheduler overhead time: 0.07320642424747348 Adapter cache time: 0.013663195073604584 Engine time: 0.07104872493073344 
