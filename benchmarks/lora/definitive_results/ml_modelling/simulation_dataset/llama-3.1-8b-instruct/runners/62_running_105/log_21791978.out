INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.975747651886195,
    "estimated_duration": 3600.022025992019,
    "input_throughput": 3788.9279291954645,
    "output_throughput": 3277.742167910325,
    "total_throughput": 7066.670097105789,
    "itl": 41.60638834947923,
    "ttft": 44652.46188860739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2115,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.9852451025745,
    "arrivals": 55037,
    "finished_requests": 54635,
    "scheduler_time": 35.28704636309448
}
#Debug simulation 
Total elapsed time: 6.975864768959582. Arrivals time: 0.1763016707263887 Scheduler time: 6.518814770039171 Scheduler overhead time: 0.10709018167108297 Adapter cache time: 0.030057364143431187 Engine time: 0.09740397008135915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.91686040116474,
    "estimated_duration": 3600.004259379241,
    "input_throughput": 3785.5616321826024,
    "output_throughput": 3274.2702926780794,
    "total_throughput": 7059.831924860682,
    "itl": 41.590345572077204,
    "ttft": 47254.784333238385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.625375783815683,
    "arrivals": 55037,
    "finished_requests": 54593,
    "scheduler_time": 35.2374724762417
}
#Debug simulation 
Total elapsed time: 6.916995225008577. Arrivals time: 0.16936816973611712 Scheduler time: 6.467986039817333 Scheduler overhead time: 0.10603498062118888 Adapter cache time: 0.030009588226675987 Engine time: 0.0971926897764206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.865629893727601,
    "estimated_duration": 3599.997132943025,
    "input_throughput": 3788.209405836722,
    "output_throughput": 3275.9534423181017,
    "total_throughput": 7064.162848154824,
    "itl": 41.67237981771259,
    "ttft": 45349.54412419212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.147355559030213,
    "arrivals": 55037,
    "finished_requests": 54619,
    "scheduler_time": 35.222056304396745
}
#Debug simulation 
Total elapsed time: 6.865737187676132. Arrivals time: 0.1683306209743023 Scheduler time: 6.416686199139804 Scheduler overhead time: 0.10725892102345824 Adapter cache time: 0.02999892784282565 Engine time: 0.09703640406951308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.88740779273212,
    "estimated_duration": 3600.0258422157326,
    "input_throughput": 3787.230035995661,
    "output_throughput": 3275.7778740670797,
    "total_throughput": 7063.007910062741,
    "itl": 41.58333693825003,
    "ttft": 46089.082223187135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.686088413442757,
    "arrivals": 55037,
    "finished_requests": 54609,
    "scheduler_time": 35.21814858208925
}
#Debug simulation 
Total elapsed time: 6.887512928806245. Arrivals time: 0.17516929982230067 Scheduler time: 6.433019922114909 Scheduler overhead time: 0.10587631119415164 Adapter cache time: 0.03020582627505064 Engine time: 0.09733650274574757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_96_slots_32_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.888395548798144,
    "estimated_duration": 3600.0211396642912,
    "input_throughput": 3788.3966429352126,
    "output_throughput": 3275.9871518698296,
    "total_throughput": 7064.383794805042,
    "itl": 41.668737291398415,
    "ttft": 45282.38745192865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.997402723161159,
    "arrivals": 55037,
    "finished_requests": 54620,
    "scheduler_time": 35.22114752518781
}
#Debug simulation 
Total elapsed time: 6.888534275814891. Arrivals time: 0.17517702700570226 Scheduler time: 6.431524289306253 Scheduler overhead time: 0.10643442301079631 Adapter cache time: 0.030454090796411037 Engine time: 0.09870026027783751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_96_slots_32_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_96_slots_32_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.880296334158629,
    "estimated_duration": 3600.023411790524,
    "input_throughput": 3788.4578626161424,
    "output_throughput": 3277.3403532206044,
    "total_throughput": 7065.798215836747,
    "itl": 41.60693034204631,
    "ttft": 44951.82789882034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.68074257564337,
    "arrivals": 55037,
    "finished_requests": 54627,
    "scheduler_time": 35.23745315041144
}
#Debug simulation 
Total elapsed time: 6.880396995227784. Arrivals time: 0.1674716556444764 Scheduler time: 6.43299876851961 Scheduler overhead time: 0.10562403639778495 Adapter cache time: 0.030102909542620182 Engine time: 0.09836233826354146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_96_slots_32_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_96_slots_32_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.9460828313604,
    "estimated_duration": 3600.0317914514794,
    "input_throughput": 3788.5523767835934,
    "output_throughput": 3278.1035512027797,
    "total_throughput": 7066.655927986373,
    "itl": 41.65117915352436,
    "ttft": 44874.71520642598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.77450809603523,
    "arrivals": 55037,
    "finished_requests": 54629,
    "scheduler_time": 35.25930586911145
}
#Debug simulation 
Total elapsed time: 6.9462038022466. Arrivals time: 0.1727308896370232 Scheduler time: 6.493543504271656 Scheduler overhead time: 0.10594936227425933 Adapter cache time: 0.03022393025457859 Engine time: 0.09793124487623572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.972015280742198,
    "estimated_duration": 3600.031494804804,
    "input_throughput": 3672.0717635601613,
    "output_throughput": 3190.788474094399,
    "total_throughput": 6862.8602376545605,
    "itl": 40.81791073450774,
    "ttft": 34758.772082406125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.473037134763285,
    "arrivals": 53479,
    "finished_requests": 53154,
    "scheduler_time": 32.7597423218031
}
#Debug simulation 
Total elapsed time: 5.972167730797082. Arrivals time: 0.1704029100947082 Scheduler time: 5.525049964897335 Scheduler overhead time: 0.10167558351531625 Adapter cache time: 0.031316506676375866 Engine time: 0.09717902960255742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.031456814147532,
    "estimated_duration": 3600.0063193624164,
    "input_throughput": 3672.5438310734835,
    "output_throughput": 3191.3266202362493,
    "total_throughput": 6863.870451309733,
    "itl": 40.84789636336314,
    "ttft": 34149.3800813594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.034040964394503,
    "arrivals": 53479,
    "finished_requests": 53163,
    "scheduler_time": 32.78262148892925
}
#Debug simulation 
Total elapsed time: 6.031587222125381. Arrivals time: 0.1638891533948481 Scheduler time: 5.590529227629304 Scheduler overhead time: 0.10177844855934381 Adapter cache time: 0.03149392735213041 Engine time: 0.09768646070733666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.046877727843821,
    "estimated_duration": 3600.034124027281,
    "input_throughput": 3671.2463117474176,
    "output_throughput": 3190.5264239970184,
    "total_throughput": 6861.7727357444355,
    "itl": 40.85593160616433,
    "ttft": 35234.82757224012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.709718012716504,
    "arrivals": 53479,
    "finished_requests": 53146,
    "scheduler_time": 32.759669780308585
}
#Debug simulation 
Total elapsed time: 6.046998707111925. Arrivals time: 0.1608794485218823 Scheduler time: 5.609521686565131 Scheduler overhead time: 0.10152023984119296 Adapter cache time: 0.031783745158463717 Engine time: 0.09714286495000124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.043377291411161,
    "estimated_duration": 3600.0230307637844,
    "input_throughput": 3672.2992844840646,
    "output_throughput": 3191.0734742057216,
    "total_throughput": 6863.372758689786,
    "itl": 40.83968979834894,
    "ttft": 34574.412635113964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.00564307629163,
    "arrivals": 53479,
    "finished_requests": 53157,
    "scheduler_time": 32.768444186451475
}
#Debug simulation 
Total elapsed time: 6.0434878072701395. Arrivals time: 0.1692156596109271 Scheduler time: 5.5982600706629455 Scheduler overhead time: 0.10127513017505407 Adapter cache time: 0.03188151912763715 Engine time: 0.09694927046075463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.085574090946466,
    "estimated_duration": 3600.0182024830856,
    "input_throughput": 3672.197265803162,
    "output_throughput": 3191.792194849045,
    "total_throughput": 6863.989460652207,
    "itl": 40.83318035901691,
    "ttft": 33473.9608068469,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.3742785446903,
    "arrivals": 53479,
    "finished_requests": 53173,
    "scheduler_time": 32.78797907781745
}
#Debug simulation 
Total elapsed time: 6.085687561891973. Arrivals time: 0.17829782841727138 Scheduler time: 5.630092337727547 Scheduler overhead time: 0.10140692442655563 Adapter cache time: 0.032011869829148054 Engine time: 0.09783229557797313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.043108274694532,
    "estimated_duration": 3600.0315732743106,
    "input_throughput": 3671.176969145142,
    "output_throughput": 3190.871181596915,
    "total_throughput": 6862.048150742056,
    "itl": 40.774069170278,
    "ttft": 35376.52612778972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.040517735985075,
    "arrivals": 53479,
    "finished_requests": 53144,
    "scheduler_time": 32.738152955260695
}
#Debug simulation 
Total elapsed time: 6.043304683640599. Arrivals time: 0.16665574721992016 Scheduler time: 5.602275638841093 Scheduler overhead time: 0.10006494726985693 Adapter cache time: 0.03181699151173234 Engine time: 0.09665657440200448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.050221688114107,
    "estimated_duration": 3600.006850479095,
    "input_throughput": 3672.315789688181,
    "output_throughput": 3191.0878165332283,
    "total_throughput": 6863.403606221409,
    "itl": 40.86210586574514,
    "ttft": 34516.0718226578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.176385744381513,
    "arrivals": 53479,
    "finished_requests": 53157,
    "scheduler_time": 32.77676802731948
}
#Debug simulation 
Total elapsed time: 6.050362554844469. Arrivals time: 0.16355440765619278 Scheduler time: 5.609225898049772 Scheduler overhead time: 0.10154504887759686 Adapter cache time: 0.031636583153158426 Engine time: 0.09798176493495703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.650577660650015,
    "estimated_duration": 3600.0421562271094,
    "input_throughput": 3639.265161753147,
    "output_throughput": 3138.2126957757996,
    "total_throughput": 6777.4778575289465,
    "itl": 40.31968198447557,
    "ttft": 28023.928622426254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.970494048335073,
    "arrivals": 52821,
    "finished_requests": 52567,
    "scheduler_time": 31.44682657866268
}
#Debug simulation 
Total elapsed time: 5.65068848663941. Arrivals time: 0.17212305404245853 Scheduler time: 5.200526338070631 Scheduler overhead time: 0.10230045299977064 Adapter cache time: 0.0313142454251647 Engine time: 0.09732256038114429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.661780742928386,
    "estimated_duration": 3600.0003126144184,
    "input_throughput": 3638.545239593971,
    "output_throughput": 3137.6072275385395,
    "total_throughput": 6776.1524671325105,
    "itl": 40.339068440585635,
    "ttft": 28450.339798794637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.340648782280947,
    "arrivals": 52821,
    "finished_requests": 52559,
    "scheduler_time": 31.453180073115686
}
#Debug simulation 
Total elapsed time: 5.661891569383442. Arrivals time: 0.17284469911828637 Scheduler time: 5.212387413717806 Scheduler overhead time: 0.10177160706371069 Adapter cache time: 0.03130604978650808 Engine time: 0.0969431889243424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.641513334121555,
    "estimated_duration": 3600.0282834274353,
    "input_throughput": 3639.12779805361,
    "output_throughput": 3138.0884011401167,
    "total_throughput": 6777.216199193726,
    "itl": 40.33393116146827,
    "ttft": 28189.230949885157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.816223522429134,
    "arrivals": 52821,
    "finished_requests": 52565,
    "scheduler_time": 31.46083996117098
}
#Debug simulation 
Total elapsed time: 5.64161202730611. Arrivals time: 0.16691787261515856 Scheduler time: 5.2006732588633895 Scheduler overhead time: 0.10106032295152545 Adapter cache time: 0.030952593311667442 Engine time: 0.09574562730267644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.656341800000519,
    "estimated_duration": 3600.0313698207437,
    "input_throughput": 3638.6769042660467,
    "output_throughput": 3137.700714136403,
    "total_throughput": 6776.37761840245,
    "itl": 40.320361961585654,
    "ttft": 28383.966663105606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.252595303058929,
    "arrivals": 52821,
    "finished_requests": 52563,
    "scheduler_time": 31.462786634142063
}
#Debug simulation 
Total elapsed time: 5.6564474436454475. Arrivals time: 0.16324359131976962 Scheduler time: 5.216112401802093 Scheduler overhead time: 0.10198406223207712 Adapter cache time: 0.031074393540620804 Engine time: 0.09740198496729136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.6671879552304745,
    "estimated_duration": 3600.002499298425,
    "input_throughput": 3638.7513626873474,
    "output_throughput": 3138.1205991389247,
    "total_throughput": 6776.871961826272,
    "itl": 40.340643956167675,
    "ttft": 28310.137418414357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.636140944044794,
    "arrivals": 52821,
    "finished_requests": 52563,
    "scheduler_time": 31.481655685342147
}
#Debug simulation 
Total elapsed time: 5.667296049185097. Arrivals time: 0.16351171676069498 Scheduler time: 5.2281612092629075 Scheduler overhead time: 0.10119172697886825 Adapter cache time: 0.031109153758734465 Engine time: 0.09683538740500808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.66475494671613,
    "estimated_duration": 3600.0296195613732,
    "input_throughput": 3638.6081183398683,
    "output_throughput": 3137.9597375024914,
    "total_throughput": 6776.56785584236,
    "itl": 40.31639785944525,
    "ttft": 28353.043021194775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2263,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.446813088511936,
    "arrivals": 52821,
    "finished_requests": 52563,
    "scheduler_time": 31.453619613161372
}
#Debug simulation 
Total elapsed time: 5.664859137963504. Arrivals time: 0.16891288198530674 Scheduler time: 5.220391302835196 Scheduler overhead time: 0.10157898953184485 Adapter cache time: 0.031202984508126974 Engine time: 0.09663904784247279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.668858776334673,
    "estimated_duration": 3600.0398607636425,
    "input_throughput": 3638.482490919281,
    "output_throughput": 3137.9483108289055,
    "total_throughput": 6776.430801748186,
    "itl": 40.35525269351122,
    "ttft": 28297.377050780073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.58951826935609,
    "arrivals": 52821,
    "finished_requests": 52563,
    "scheduler_time": 31.459504705925745
}
#Debug simulation 
Total elapsed time: 5.668986067175865. Arrivals time: 0.17624157760292292 Scheduler time: 5.213813990820199 Scheduler overhead time: 0.10290682269260287 Adapter cache time: 0.03142797900363803 Engine time: 0.09797327499836683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.4467808292247355,
    "estimated_duration": 3600.0196966581902,
    "input_throughput": 3607.8072050707406,
    "output_throughput": 3129.93454187477,
    "total_throughput": 6737.74174694551,
    "itl": 40.11454890015853,
    "ttft": 28232.90360819387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2086,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.793485240647946,
    "arrivals": 52471,
    "finished_requests": 52197,
    "scheduler_time": 31.097796547548374
}
#Debug simulation 
Total elapsed time: 5.446875578258187. Arrivals time: 0.1359518188983202 Scheduler time: 5.037301029078662 Scheduler overhead time: 0.10110341291874647 Adapter cache time: 0.029727541841566563 Engine time: 0.09645314188674092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.4178300821222365,
    "estimated_duration": 3600.013561210653,
    "input_throughput": 3608.0383529532924,
    "output_throughput": 3130.792095185802,
    "total_throughput": 6738.830448139094,
    "itl": 40.17430308000402,
    "ttft": 27727.27314736564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2099,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.15872602744477,
    "arrivals": 52471,
    "finished_requests": 52204,
    "scheduler_time": 31.10256951834167
}
#Debug simulation 
Total elapsed time: 5.417912870179862. Arrivals time: 0.13642408745363355 Scheduler time: 5.008925512898713 Scheduler overhead time: 0.10073500033468008 Adapter cache time: 0.02983403578400612 Engine time: 0.09561422746628523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.43610472092405,
    "estimated_duration": 3600.019960983983,
    "input_throughput": 3608.1924935909524,
    "output_throughput": 3129.9568119395026,
    "total_throughput": 6738.149305530455,
    "itl": 40.15437651170996,
    "ttft": 27789.284941762086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.76151249526508,
    "arrivals": 52471,
    "finished_requests": 52202,
    "scheduler_time": 31.098636314228198
}
#Debug simulation 
Total elapsed time: 5.436182550620288. Arrivals time: 0.13621018454432487 Scheduler time: 5.026038175448775 Scheduler overhead time: 0.1007990287616849 Adapter cache time: 0.030126283410936594 Engine time: 0.09667694941163063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.423916782718152,
    "estimated_duration": 3600.0368916689945,
    "input_throughput": 3608.330800737355,
    "output_throughput": 3130.5848631942968,
    "total_throughput": 6738.915663931652,
    "itl": 40.15109284353153,
    "ttft": 27575.580671973046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.327446039868827,
    "arrivals": 52471,
    "finished_requests": 52206,
    "scheduler_time": 31.097734637183283
}
#Debug simulation 
Total elapsed time: 5.423991524614394. Arrivals time: 0.13609115919098258 Scheduler time: 5.014884412288666 Scheduler overhead time: 0.10114635014906526 Adapter cache time: 0.029977896716445684 Engine time: 0.09507230157032609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.442191377747804,
    "estimated_duration": 3600.0421374268726,
    "input_throughput": 3608.922202584611,
    "output_throughput": 3130.6164121888514,
    "total_throughput": 6739.5386147734625,
    "itl": 40.148982814223906,
    "ttft": 27360.791111522085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2081,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.259431485389111,
    "arrivals": 52471,
    "finished_requests": 52211,
    "scheduler_time": 31.128353909284986
}
#Debug simulation 
Total elapsed time: 5.442268096841872. Arrivals time: 0.13574828812852502 Scheduler time: 5.03509020106867 Scheduler overhead time: 0.10048907622694969 Adapter cache time: 0.02955401176586747 Engine time: 0.09522358514368534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.441692705266178,
    "estimated_duration": 3600.013790058006,
    "input_throughput": 3608.438122063607,
    "output_throughput": 3130.5096750249977,
    "total_throughput": 6738.947797088605,
    "itl": 40.123685365579256,
    "ttft": 27461.149561989365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.4509214217828,
    "arrivals": 52471,
    "finished_requests": 52208,
    "scheduler_time": 31.0902258106295
}
#Debug simulation 
Total elapsed time: 5.44179358612746. Arrivals time: 0.13572760205715895 Scheduler time: 5.031477796379477 Scheduler overhead time: 0.10082536470144987 Adapter cache time: 0.030123287811875343 Engine time: 0.09689834108576179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.442570535000414,
    "estimated_duration": 3600.029635764362,
    "input_throughput": 3607.644190196355,
    "output_throughput": 3130.1953428523366,
    "total_throughput": 6737.839533048692,
    "itl": 40.157504154323554,
    "ttft": 28339.537268637665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2092,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.213952624089771,
    "arrivals": 52471,
    "finished_requests": 52196,
    "scheduler_time": 31.10717490542742
}
#Debug simulation 
Total elapsed time: 5.442672615870833. Arrivals time: 0.13537490740418434 Scheduler time: 5.029924412257969 Scheduler overhead time: 0.1029542675241828 Adapter cache time: 0.029775660950690508 Engine time: 0.0979979825206101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.169812075793743,
    "estimated_duration": 3599.9154264081567,
    "input_throughput": 3484.5107493305245,
    "output_throughput": 3056.569029172643,
    "total_throughput": 6541.079778503167,
    "itl": 39.635690309766055,
    "ttft": 26089.890731883956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.180564901181903,
    "arrivals": 50642,
    "finished_requests": 50392,
    "scheduler_time": 29.582325655025876
}
#Debug simulation 
Total elapsed time: 5.16988516272977. Arrivals time: 0.13265340588986874 Scheduler time: 4.759073596447706 Scheduler overhead time: 0.10144329909235239 Adapter cache time: 0.032207232899963856 Engine time: 0.09782422240823507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.180508111137897,
    "estimated_duration": 3599.9265733020925,
    "input_throughput": 3484.0846735646696,
    "output_throughput": 3056.118722418389,
    "total_throughput": 6540.203395983059,
    "itl": 39.66350251161523,
    "ttft": 26593.6200474176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.81207671514678,
    "arrivals": 50642,
    "finished_requests": 50386,
    "scheduler_time": 29.60112655739679
}
#Debug simulation 
Total elapsed time: 5.180583184119314. Arrivals time: 0.1324382359161973 Scheduler time: 4.769930026959628 Scheduler overhead time: 0.10166715830564499 Adapter cache time: 0.032132904045283794 Engine time: 0.09768154285848141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.182525438722223,
    "estimated_duration": 3599.9447069027633,
    "input_throughput": 3484.067123572845,
    "output_throughput": 3056.103328171803,
    "total_throughput": 6540.170451744648,
    "itl": 39.66984353209513,
    "ttft": 26596.13727070587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.29274107401253,
    "arrivals": 50642,
    "finished_requests": 50386,
    "scheduler_time": 29.60469937460752
}
#Debug simulation 
Total elapsed time: 5.182598564773798. Arrivals time: 0.13207149831578135 Scheduler time: 4.774077993351966 Scheduler overhead time: 0.10158308502286673 Adapter cache time: 0.03190302010625601 Engine time: 0.0963625954464078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.179508958943188,
    "estimated_duration": 3599.9068928242086,
    "input_throughput": 3484.5190093677647,
    "output_throughput": 3056.576274773482,
    "total_throughput": 6541.095284141247,
    "itl": 39.64131247173602,
    "ttft": 26130.528508492436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.69372779458331,
    "arrivals": 50642,
    "finished_requests": 50392,
    "scheduler_time": 29.591987058801973
}
#Debug simulation 
Total elapsed time: 5.179584548342973. Arrivals time: 0.1324562388472259 Scheduler time: 4.763636611402035 Scheduler overhead time: 0.10057300981134176 Adapter cache time: 0.031935188453644514 Engine time: 0.10452910931780934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.172138795722276,
    "estimated_duration": 3599.905498900195,
    "input_throughput": 3484.1059032888047,
    "output_throughput": 3056.037721923824,
    "total_throughput": 6540.143625212629,
    "itl": 39.666437513510864,
    "ttft": 26925.725690787836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.093241350008093,
    "arrivals": 50642,
    "finished_requests": 50381,
    "scheduler_time": 29.59768353381236
}
#Debug simulation 
Total elapsed time: 5.172211662866175. Arrivals time: 0.13239173591136932 Scheduler time: 4.762727935798466 Scheduler overhead time: 0.10155930044129491 Adapter cache time: 0.03191055543720722 Engine time: 0.09700784971937537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.173149562906474,
    "estimated_duration": 3599.900757858256,
    "input_throughput": 3484.1096584734937,
    "output_throughput": 3056.1406383173385,
    "total_throughput": 6540.250296790832,
    "itl": 39.62451633871862,
    "ttft": 26550.324009055897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.6597580672205,
    "arrivals": 50642,
    "finished_requests": 50386,
    "scheduler_time": 29.579934644476428
}
#Debug simulation 
Total elapsed time: 5.173222489655018. Arrivals time: 0.131787464953959 Scheduler time: 4.762996565084904 Scheduler overhead time: 0.10141125973314047 Adapter cache time: 0.032218207605183125 Engine time: 0.09824498556554317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.159999887924641,
    "estimated_duration": 3599.939812139887,
    "input_throughput": 3484.07186078605,
    "output_throughput": 3056.107483491585,
    "total_throughput": 6540.179344277635,
    "itl": 39.66562286671759,
    "ttft": 26594.369844214085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.960939357075738,
    "arrivals": 50642,
    "finished_requests": 50386,
    "scheduler_time": 29.602191988432068
}
#Debug simulation 
Total elapsed time: 5.160076620988548. Arrivals time: 0.13197056390345097 Scheduler time: 4.749307201709598 Scheduler overhead time: 0.10259033879265189 Adapter cache time: 0.03204087680205703 Engine time: 0.09672153228893876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.740756060928106,
    "estimated_duration": 3599.9423097982094,
    "input_throughput": 3438.810384906951,
    "output_throughput": 2988.4326120223404,
    "total_throughput": 6427.242996929292,
    "itl": 39.051324627577664,
    "ttft": 20031.15369813565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.02847949344709,
    "arrivals": 49929,
    "finished_requests": 49734,
    "scheduler_time": 27.989075853331656
}
#Debug simulation 
Total elapsed time: 4.740831557661295. Arrivals time: 0.13042052555829287 Scheduler time: 4.329236369114369 Scheduler overhead time: 0.10285602556541562 Adapter cache time: 0.032017544843256474 Engine time: 0.09897579252719879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.745336453896016,
    "estimated_duration": 3599.9407210485574,
    "input_throughput": 3438.8119025454976,
    "output_throughput": 2988.4339308971885,
    "total_throughput": 6427.2458334426865,
    "itl": 39.07753183831948,
    "ttft": 20044.169722143397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.5580796654473,
    "arrivals": 49929,
    "finished_requests": 49734,
    "scheduler_time": 28.001447405061263
}
#Debug simulation 
Total elapsed time: 4.745412221644074. Arrivals time: 0.1309676975943148 Scheduler time: 4.331776381004602 Scheduler overhead time: 0.10451315948739648 Adapter cache time: 0.03199346084147692 Engine time: 0.09851485956460238 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.731953562702984,
    "estimated_duration": 3599.9158058069356,
    "input_throughput": 3438.8357027769657,
    "output_throughput": 2988.4546140346497,
    "total_throughput": 6427.290316811615,
    "itl": 39.08934770802939,
    "ttft": 20046.019554909122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.953219936313037,
    "arrivals": 49929,
    "finished_requests": 49734,
    "scheduler_time": 28.0044278115397
}
#Debug simulation 
Total elapsed time: 4.7320282948203385. Arrivals time: 0.12981496658176184 Scheduler time: 4.323845651000738 Scheduler overhead time: 0.1019052336923778 Adapter cache time: 0.031854871194809675 Engine time: 0.09752743178978562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.74928819341585,
    "estimated_duration": 3599.929301421548,
    "input_throughput": 3438.8228110789696,
    "output_throughput": 2988.4434107502566,
    "total_throughput": 6427.266221829226,
    "itl": 39.05393279827551,
    "ttft": 20028.85597047231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.54078754513095,
    "arrivals": 49929,
    "finished_requests": 49734,
    "scheduler_time": 27.99206689517501
}
#Debug simulation 
Total elapsed time: 4.749387150164694. Arrivals time: 0.12992694461718202 Scheduler time: 4.338996586855501 Scheduler overhead time: 0.10325768543407321 Adapter cache time: 0.032075977884233 Engine time: 0.09775276109576225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.747292669955641,
    "estimated_duration": 3599.9361264368454,
    "input_throughput": 3438.8162915137705,
    "output_throughput": 2988.4377450464003,
    "total_throughput": 6427.254036560171,
    "itl": 39.07835846274015,
    "ttft": 20070.844685067448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.74362782917907,
    "arrivals": 49929,
    "finished_requests": 49734,
    "scheduler_time": 28.006999010093317
}
#Debug simulation 
Total elapsed time: 4.747367640025914. Arrivals time: 0.1304725557565689 Scheduler time: 4.336880539543927 Scheduler overhead time: 0.10286893462762237 Adapter cache time: 0.03220447478815913 Engine time: 0.09745019813999534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.74909106688574,
    "estimated_duration": 3599.940701955121,
    "input_throughput": 3439.2330388319683,
    "output_throughput": 2989.045623489313,
    "total_throughput": 6428.278662321281,
    "itl": 39.04179297169219,
    "ttft": 19816.383730794358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.455472597122215,
    "arrivals": 49929,
    "finished_requests": 49737,
    "scheduler_time": 27.98437538159457
}
#Debug simulation 
Total elapsed time: 4.7491674749180675. Arrivals time: 0.12980565475299954 Scheduler time: 4.340104724280536 Scheduler overhead time: 0.1023908369243145 Adapter cache time: 0.031900194473564625 Engine time: 0.09771831193938851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.4-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.747912188060582,
    "estimated_duration": 3599.9375603887415,
    "input_throughput": 3438.8149217408063,
    "output_throughput": 2988.436554671318,
    "total_throughput": 6427.251476412124,
    "itl": 39.08225175107143,
    "ttft": 20039.725008910667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.612723787501345,
    "arrivals": 49929,
    "finished_requests": 49734,
    "scheduler_time": 28.00133246867593
}
#Debug simulation 
Total elapsed time: 4.74798393715173. Arrivals time: 0.130094182677567 Scheduler time: 4.338365416973829 Scheduler overhead time: 0.10219206986948848 Adapter cache time: 0.032170165330171585 Engine time: 0.09803804801777005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.60084324516356,
    "estimated_duration": 3600.0413392613177,
    "input_throughput": 3406.5831039919067,
    "output_throughput": 2982.0343680283336,
    "total_throughput": 6388.617472020241,
    "itl": 39.01121438196787,
    "ttft": 18800.35390311617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.216679418692756,
    "arrivals": 49606,
    "finished_requests": 49417,
    "scheduler_time": 27.72720644221725
}
#Debug simulation 
Total elapsed time: 4.60091769695282. Arrivals time: 0.12907163612544537 Scheduler time: 4.192381547763944 Scheduler overhead time: 0.1031065583229065 Adapter cache time: 0.030744743067771196 Engine time: 0.09829715639352798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.605910758022219,
    "estimated_duration": 3600.0065910975577,
    "input_throughput": 3406.0143196187046,
    "output_throughput": 2980.682598341724,
    "total_throughput": 6386.696917960428,
    "itl": 39.03920013107199,
    "ttft": 19557.08331636825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.724968541622925,
    "arrivals": 49606,
    "finished_requests": 49406,
    "scheduler_time": 27.723132635189014
}
#Debug simulation 
Total elapsed time: 4.60598569503054. Arrivals time: 0.12908007949590683 Scheduler time: 4.20005247509107 Scheduler overhead time: 0.10220339056104422 Adapter cache time: 0.030579232145100832 Engine time: 0.09695262275636196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.583304368425161,
    "estimated_duration": 3600.012918719197,
    "input_throughput": 3406.4369425549103,
    "output_throughput": 2981.856799508145,
    "total_throughput": 6388.293742063055,
    "itl": 39.05027828966415,
    "ttft": 18700.423920650373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.38334901962874,
    "arrivals": 49606,
    "finished_requests": 49416,
    "scheduler_time": 27.715270723258143
}
#Debug simulation 
Total elapsed time: 4.583379567135125. Arrivals time: 0.12953038280829787 Scheduler time: 4.172719039954245 Scheduler overhead time: 0.10403748787939548 Adapter cache time: 0.030859160237014294 Engine time: 0.09878846444189548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.567569334991276,
    "estimated_duration": 3600.0197475693085,
    "input_throughput": 3406.430480910551,
    "output_throughput": 2981.8511432466335,
    "total_throughput": 6388.2816241571845,
    "itl": 39.02290181024663,
    "ttft": 18662.341486005294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.15876293848302,
    "arrivals": 49606,
    "finished_requests": 49416,
    "scheduler_time": 27.701046743322895
}
#Debug simulation 
Total elapsed time: 4.567666575778276. Arrivals time: 0.1290833605453372 Scheduler time: 4.159302780870348 Scheduler overhead time: 0.10303457966074347 Adapter cache time: 0.030886576045304537 Engine time: 0.09803866175934672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.52350031491369,
    "estimated_duration": 3600.019495507271,
    "input_throughput": 3406.4307194180947,
    "output_throughput": 2981.8513520264682,
    "total_throughput": 6388.2820714445625,
    "itl": 39.05051604248242,
    "ttft": 18697.658089974182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.248878103024104,
    "arrivals": 49606,
    "finished_requests": 49416,
    "scheduler_time": 27.71418307036086
}
#Debug simulation 
Total elapsed time: 4.523603518959135. Arrivals time: 0.12859779689460993 Scheduler time: 4.11776602268219 Scheduler overhead time: 0.10248597199097276 Adapter cache time: 0.030806570313870907 Engine time: 0.09675062261521816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.6176885310560465,
    "estimated_duration": 3600.0077739589374,
    "input_throughput": 3406.6148658655325,
    "output_throughput": 2982.062171547536,
    "total_throughput": 6388.677037413068,
    "itl": 39.00201982916941,
    "ttft": 18798.577218326503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.712662180346227,
    "arrivals": 49606,
    "finished_requests": 49417,
    "scheduler_time": 27.722825488466974
}
#Debug simulation 
Total elapsed time: 4.617776497267187. Arrivals time: 0.13155401358380914 Scheduler time: 4.207198907621205 Scheduler overhead time: 0.1029076031409204 Adapter cache time: 0.030596876982599497 Engine time: 0.09797950927168131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.566160798072815,
    "estimated_duration": 3600.0182358517795,
    "input_throughput": 3406.431911336824,
    "output_throughput": 2981.8523953838026,
    "total_throughput": 6388.284306720627,
    "itl": 39.03936599330763,
    "ttft": 18670.425051324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.141169329862805,
    "arrivals": 49606,
    "finished_requests": 49416,
    "scheduler_time": 27.708994911300294
}
#Debug simulation 
Total elapsed time: 4.566239778883755. Arrivals time: 0.12992710201069713 Scheduler time: 4.157095789909363 Scheduler overhead time: 0.10300404578447342 Adapter cache time: 0.030708365608006716 Engine time: 0.09798551257699728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.181376148015261,
    "estimated_duration": 3599.9516724129508,
    "input_throughput": 3346.6246484150047,
    "output_throughput": 2930.2401698435788,
    "total_throughput": 6276.864818258584,
    "itl": 38.56248193098757,
    "ttft": 16465.604453150972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.201928364453329,
    "arrivals": 48532,
    "finished_requests": 48348,
    "scheduler_time": 26.448132842420808
}
#Debug simulation 
Total elapsed time: 4.181448315270245. Arrivals time: 0.12709404760971665 Scheduler time: 3.771509673446417 Scheduler overhead time: 0.10342094721272588 Adapter cache time: 0.031331940554082394 Engine time: 0.10052461829036474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.172759654931724,
    "estimated_duration": 3599.950180639659,
    "input_throughput": 3346.6260352134377,
    "output_throughput": 2930.2413840976114,
    "total_throughput": 6276.867419311049,
    "itl": 38.58737280901278,
    "ttft": 16469.53694488474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.707601526328077,
    "arrivals": 48532,
    "finished_requests": 48348,
    "scheduler_time": 26.45907354989973
}
#Debug simulation 
Total elapsed time: 4.172833637334406. Arrivals time: 0.12564040208235383 Scheduler time: 3.7658830657601357 Scheduler overhead time: 0.10330844204872847 Adapter cache time: 0.03120378963649273 Engine time: 0.0994768557138741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.19110822211951,
    "estimated_duration": 3599.9583863613243,
    "input_throughput": 3346.618406935881,
    "output_throughput": 2930.2347049245127,
    "total_throughput": 6276.853111860394,
    "itl": 38.58699251282226,
    "ttft": 16484.69971781082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.120469587035295,
    "arrivals": 48532,
    "finished_requests": 48348,
    "scheduler_time": 26.46421644964995
}
#Debug simulation 
Total elapsed time: 4.19118418218568. Arrivals time: 0.1260211318731308 Scheduler time: 3.7853667996823788 Scheduler overhead time: 0.10246142838150263 Adapter cache time: 0.03135616146028042 Engine time: 0.09850969910621643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.180815154220909,
    "estimated_duration": 3599.953425208183,
    "input_throughput": 3346.6230189639996,
    "output_throughput": 2930.238743127621,
    "total_throughput": 6276.86176209162,
    "itl": 38.565277068024045,
    "ttft": 16477.89353667868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.696419473830934,
    "arrivals": 48532,
    "finished_requests": 48348,
    "scheduler_time": 26.453285651663496
}
#Debug simulation 
Total elapsed time: 4.180888418108225. Arrivals time: 0.12668690644204617 Scheduler time: 3.7748528900556266 Scheduler overhead time: 0.1029904242604971 Adapter cache time: 0.031074522994458675 Engine time: 0.0976345962844789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.169661011081189,
    "estimated_duration": 3599.9402299011085,
    "input_throughput": 3346.6352857561064,
    "output_throughput": 2930.249483694838,
    "total_throughput": 6276.884769450945,
    "itl": 38.58706633935014,
    "ttft": 16478.370928937875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.972484543719133,
    "arrivals": 48532,
    "finished_requests": 48348,
    "scheduler_time": 26.462127479239765
}
#Debug simulation 
Total elapsed time: 4.169733752030879. Arrivals time: 0.12523078406229615 Scheduler time: 3.7630810756236315 Scheduler overhead time: 0.1031378023326397 Adapter cache time: 0.03120376355946064 Engine time: 0.09971482027322054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.148102920036763,
    "estimated_duration": 3599.944509887945,
    "input_throughput": 3346.631306929508,
    "output_throughput": 2930.245999910801,
    "total_throughput": 6276.877306840309,
    "itl": 38.54912201496329,
    "ttft": 16476.624716838713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.676634242372506,
    "arrivals": 48532,
    "finished_requests": 48348,
    "scheduler_time": 26.44603703240044
}
#Debug simulation 
Total elapsed time: 4.148178564850241. Arrivals time: 0.12525209598243237 Scheduler time: 3.7431299197487533 Scheduler overhead time: 0.10359677812084556 Adapter cache time: 0.031006036326289177 Engine time: 0.0975574548356235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.4-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.162750526331365,
    "estimated_duration": 3599.9661599527963,
    "input_throughput": 3346.6111804111993,
    "output_throughput": 2930.2283775185037,
    "total_throughput": 6276.839557929703,
    "itl": 38.58381607928794,
    "ttft": 16481.681607491133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.82444577615686,
    "arrivals": 48532,
    "finished_requests": 48348,
    "scheduler_time": 26.46184403467558
}
#Debug simulation 
Total elapsed time: 4.162826139014214. Arrivals time: 0.12600706471130252 Scheduler time: 3.7571316491812468 Scheduler overhead time: 0.10295019811019301 Adapter cache time: 0.03116583451628685 Engine time: 0.09814866352826357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.9305643439292908,
    "estimated_duration": 3600.0322333861927,
    "input_throughput": 3317.2886312651212,
    "output_throughput": 2864.4040751547873,
    "total_throughput": 6181.692706419908,
    "itl": 38.01924784380031,
    "ttft": 12498.874113864538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.646486005769516,
    "arrivals": 48139,
    "finished_requests": 47997,
    "scheduler_time": 25.09088802336967
}
#Debug simulation 
Total elapsed time: 3.9306424148380756. Arrivals time: 0.1243668245151639 Scheduler time: 3.5255765416659415 Scheduler overhead time: 0.10330206481739879 Adapter cache time: 0.031091956421732903 Engine time: 0.0987591277807951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.9401545198634267,
    "estimated_duration": 3600.0129266467798,
    "input_throughput": 3317.1944777218573,
    "output_throughput": 2864.312215013258,
    "total_throughput": 6181.506692735115,
    "itl": 38.03948262339361,
    "ttft": 12578.87318192774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.999222244312804,
    "arrivals": 48139,
    "finished_requests": 47996,
    "scheduler_time": 25.101020197032373
}
#Debug simulation 
Total elapsed time: 3.940228166989982. Arrivals time: 0.12473350344225764 Scheduler time: 3.5328841311857104 Scheduler overhead time: 0.10543033154681325 Adapter cache time: 0.030712978448718786 Engine time: 0.0980879096314311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.9511470426805317,
    "estimated_duration": 3600.0316778045144,
    "input_throughput": 3317.2891432119454,
    "output_throughput": 2864.4045172093483,
    "total_throughput": 6181.693660421293,
    "itl": 38.04744047514259,
    "ttft": 12497.303665645732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.423282553306578,
    "arrivals": 48139,
    "finished_requests": 47997,
    "scheduler_time": 25.10296425461397
}
#Debug simulation 
Total elapsed time: 3.9512252039276063. Arrivals time: 0.12532617151737213 Scheduler time: 3.5430217618122697 Scheduler overhead time: 0.10451900726184249 Adapter cache time: 0.031015214510262012 Engine time: 0.09939873544499278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.9401507270522416,
    "estimated_duration": 3600.01848431038,
    "input_throughput": 3316.6935258909534,
    "output_throughput": 2863.9961280574007,
    "total_throughput": 6180.689653948354,
    "itl": 38.025805912087634,
    "ttft": 13249.042975240047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.704664036971844,
    "arrivals": 48139,
    "finished_requests": 47988,
    "scheduler_time": 25.101073586433508
}
#Debug simulation 
Total elapsed time: 3.9402313977479935. Arrivals time: 0.12469987804070115 Scheduler time: 3.5347773008979857 Scheduler overhead time: 0.10383107885718346 Adapter cache time: 0.03059029346331954 Engine time: 0.09825424803420901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.91274446900934,
    "estimated_duration": 3600.031775705811,
    "input_throughput": 3317.2890529997117,
    "output_throughput": 2864.404439313115,
    "total_throughput": 6181.6934923128265,
    "itl": 38.044076483142184,
    "ttft": 12503.92300378252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.283633707286572,
    "arrivals": 48139,
    "finished_requests": 47997,
    "scheduler_time": 25.103090976526907
}
#Debug simulation 
Total elapsed time: 3.9128200570121408. Arrivals time: 0.1242035599425435 Scheduler time: 3.509211196564138 Scheduler overhead time: 0.10304851084947586 Adapter cache time: 0.030966013204306364 Engine time: 0.09806700004264712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.925560776144266,
    "estimated_duration": 3600.0018177828747,
    "input_throughput": 3316.70888081761,
    "output_throughput": 2864.0093871813287,
    "total_throughput": 6180.718267998938,
    "itl": 38.0107708363833,
    "ttft": 13089.723932953384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.134000962423938,
    "arrivals": 48139,
    "finished_requests": 47988,
    "scheduler_time": 25.083142542454983
}
#Debug simulation 
Total elapsed time: 3.9256373192183673. Arrivals time: 0.12439423706382513 Scheduler time: 3.52125745639205 Scheduler overhead time: 0.10299586458131671 Adapter cache time: 0.030903849750757217 Engine time: 0.09845625795423985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.9931219699792564,
    "estimated_duration": 3600.025766937308,
    "input_throughput": 3317.294589855075,
    "output_throughput": 2864.40922026311,
    "total_throughput": 6181.703810118185,
    "itl": 38.04304320789511,
    "ttft": 12503.752420162538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.116960738636305,
    "arrivals": 48139,
    "finished_requests": 47997,
    "scheduler_time": 25.101888031185883
}
#Debug simulation 
Total elapsed time: 3.9931965288706124. Arrivals time: 0.12533796299248934 Scheduler time: 3.5849055941216648 Scheduler overhead time: 0.10394548531621695 Adapter cache time: 0.031022853683680296 Engine time: 0.09996229037642479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.7435303358361125,
    "estimated_duration": 3600.035705176968,
    "input_throughput": 3264.5848437278964,
    "output_throughput": 2867.158507666136,
    "total_throughput": 6131.743351394032,
    "itl": 37.96660102202415,
    "ttft": 8846.709945214452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1797,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.882499030414353,
    "arrivals": 47462,
    "finished_requests": 47355,
    "scheduler_time": 24.943235875305316
}
#Debug simulation 
Total elapsed time: 3.7436015270650387. Arrivals time: 0.12200963543727994 Scheduler time: 3.3455955255776644 Scheduler overhead time: 0.10289280209690332 Adapter cache time: 0.02791033172979951 Engine time: 0.09772058948874474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.76572228083387,
    "estimated_duration": 3600.0247693954843,
    "input_throughput": 3264.5947605448,
    "output_throughput": 2867.1672172225767,
    "total_throughput": 6131.761977767377,
    "itl": 37.98355866524115,
    "ttft": 8852.118313452143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1794,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.018667425280318,
    "arrivals": 47462,
    "finished_requests": 47355,
    "scheduler_time": 24.952017714598828
}
#Debug simulation 
Total elapsed time: 3.7657986460253596. Arrivals time: 0.12330789770931005 Scheduler time: 3.3641738016158342 Scheduler overhead time: 0.10292658861726522 Adapter cache time: 0.028472403530031443 Engine time: 0.0992981600575149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.7525952509604394,
    "estimated_duration": 3600.0303918385816,
    "input_throughput": 3264.5896619772107,
    "output_throughput": 2867.1627393480107,
    "total_throughput": 6131.752401325221,
    "itl": 37.989833389005376,
    "ttft": 8852.079044481114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.335967233204105,
    "arrivals": 47462,
    "finished_requests": 47355,
    "scheduler_time": 24.954376410577417
}
#Debug simulation 
Total elapsed time: 3.7526693157851696. Arrivals time: 0.12240720121189952 Scheduler time: 3.3534263726323843 Scheduler overhead time: 0.10298499837517738 Adapter cache time: 0.028265833854675293 Engine time: 0.09816060028970242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.762676821090281,
    "estimated_duration": 3600.013986181892,
    "input_throughput": 3264.6045390686418,
    "output_throughput": 2867.175805321575,
    "total_throughput": 6131.780344390217,
    "itl": 37.97162025921582,
    "ttft": 8844.367056570212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1798,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.275128689827595,
    "arrivals": 47462,
    "finished_requests": 47355,
    "scheduler_time": 24.945495756404863
}
#Debug simulation 
Total elapsed time: 3.7627501720562577. Arrivals time: 0.12295870296657085 Scheduler time: 3.3617543559521437 Scheduler overhead time: 0.10370806278660893 Adapter cache time: 0.028059436939656734 Engine time: 0.09856779919937253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.785410156007856,
    "estimated_duration": 3600.0389640747167,
    "input_throughput": 3264.581888496494,
    "output_throughput": 2867.155912200781,
    "total_throughput": 6131.737800697275,
    "itl": 37.98730472754963,
    "ttft": 8845.314124935714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1799,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.275012438697564,
    "arrivals": 47462,
    "finished_requests": 47355,
    "scheduler_time": 24.952833721338695
}
#Debug simulation 
Total elapsed time: 3.7854843512177467. Arrivals time: 0.12346593337133527 Scheduler time: 3.385139390360564 Scheduler overhead time: 0.10364735964685678 Adapter cache time: 0.028128410689532757 Engine time: 0.09788097674027085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.7557508959434927,
    "estimated_duration": 3600.0271021455096,
    "input_throughput": 3264.5926451486393,
    "output_throughput": 2867.1653593520086,
    "total_throughput": 6131.758004500648,
    "itl": 37.95852522552377,
    "ttft": 8850.223261452353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.439986325502813,
    "arrivals": 47462,
    "finished_requests": 47355,
    "scheduler_time": 24.940597432210428
}
#Debug simulation 
Total elapsed time: 3.7558227092958987. Arrivals time: 0.12325902888551354 Scheduler time: 3.3564435499720275 Scheduler overhead time: 0.10252005653455853 Adapter cache time: 0.028002513106912374 Engine time: 0.09801704529672861 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.4-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.7591528398916125,
    "estimated_duration": 3600.0222347454937,
    "input_throughput": 3264.597059032015,
    "output_throughput": 2867.1692358949313,
    "total_throughput": 6131.766294926946,
    "itl": 37.98577476486885,
    "ttft": 8844.557953434238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1799,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.154625490102745,
    "arrivals": 47462,
    "finished_requests": 47355,
    "scheduler_time": 24.951794840481476
}
#Debug simulation 
Total elapsed time: 3.759227068629116. Arrivals time: 0.12316773692145944 Scheduler time: 3.358661959413439 Scheduler overhead time: 0.10397425526753068 Adapter cache time: 0.02809470472857356 Engine time: 0.0971298017539084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.8754626801237464,
    "estimated_duration": 3599.970984902404,
    "input_throughput": 1391.2314907548564,
    "output_throughput": 1210.723369237978,
    "total_throughput": 2601.9548599928344,
    "itl": 29.359219093437623,
    "ttft": 6535.135110191421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 73.62917457076139,
    "arrivals": 20310,
    "finished_requests": 20277,
    "scheduler_time": 0.03126000276882793
}
#Debug simulation 
Total elapsed time: 1.8755379179492593. Arrivals time: 0.06024954980239272 Scheduler time: 1.4449449847452343 Scheduler overhead time: 0.11910224193707108 Adapter cache time: 0.08509031357243657 Engine time: 0.10996644757688046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8910796530544758,
    "estimated_duration": 3599.973202732209,
    "input_throughput": 1391.2306336610693,
    "output_throughput": 1210.7226233495442,
    "total_throughput": 2601.9532570106135,
    "itl": 29.442020229688037,
    "ttft": 6553.733629447968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 80.65956711461034,
    "arrivals": 20310,
    "finished_requests": 20277,
    "scheduler_time": 0.033094481797049304
}
#Debug simulation 
Total elapsed time: 1.8911554338410497. Arrivals time: 0.060487819369882345 Scheduler time: 1.4638253152370453 Scheduler overhead time: 0.11819332093000412 Adapter cache time: 0.08533394243568182 Engine time: 0.1078798808157444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8949256502091885,
    "estimated_duration": 3599.981980973634,
    "input_throughput": 1391.2272412667614,
    "output_throughput": 1210.7196711082431,
    "total_throughput": 2601.9469123750046,
    "itl": 29.46035644763034,
    "ttft": 6558.065814516242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 82.7564996762745,
    "arrivals": 20310,
    "finished_requests": 20277,
    "scheduler_time": 0.033469410563398846
}
#Debug simulation 
Total elapsed time: 1.8950020871125162. Arrivals time: 0.060121049638837576 Scheduler time: 1.4649295308627188 Scheduler overhead time: 0.12056935438886285 Adapter cache time: 0.08548807073384523 Engine time: 0.10825746227055788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.8739929958246648,
    "estimated_duration": 3599.9811506305255,
    "input_throughput": 1391.2275621562062,
    "output_throughput": 1210.719950363243,
    "total_throughput": 2601.947512519449,
    "itl": 29.383654518635538,
    "ttft": 6541.047377383356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 76.06146748714006,
    "arrivals": 20310,
    "finished_requests": 20277,
    "scheduler_time": 0.03188890580999752
}
#Debug simulation 
Total elapsed time: 1.8740704818628728. Arrivals time: 0.06051091803237796 Scheduler time: 1.4442182029597461 Scheduler overhead time: 0.11726115737110376 Adapter cache time: 0.08501901803538203 Engine time: 0.11126462463289499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_96_slots_32_rate_0.1-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8814019062556326,
    "estimated_duration": 3599.967482955871,
    "input_throughput": 1391.2328441054958,
    "output_throughput": 1210.724546995423,
    "total_throughput": 2601.957391100919,
    "itl": 29.454772832439605,
    "ttft": 6556.839389659464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 82.06907291879705,
    "arrivals": 20310,
    "finished_requests": 20277,
    "scheduler_time": 0.03336903600293459
}
#Debug simulation 
Total elapsed time: 1.8815007042139769. Arrivals time: 0.060796530451625586 Scheduler time: 1.4544148184359074 Scheduler overhead time: 0.11716936156153679 Adapter cache time: 0.08489360474050045 Engine time: 0.10801395960152149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_96_slots_32_rate_0.1-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_96_slots_32_rate_0.1-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.9000991126522422,
    "estimated_duration": 3599.9762304741284,
    "input_throughput": 1391.2294635735354,
    "output_throughput": 1210.7216050773652,
    "total_throughput": 2601.9510686509007,
    "itl": 29.330724772428333,
    "ttft": 6528.661787754406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 71.15518280359959,
    "arrivals": 20310,
    "finished_requests": 20277,
    "scheduler_time": 0.030719824482723493
}
#Debug simulation 
Total elapsed time: 1.9001736156642437. Arrivals time: 0.05980393337085843 Scheduler time: 1.4739354052580893 Scheduler overhead time: 0.11769538279622793 Adapter cache time: 0.08486072439700365 Engine time: 0.10859940061345696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_96_slots_32_rate_0.1-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_96_slots_32_rate_0.1-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8858436471782625,
    "estimated_duration": 3599.966694444708,
    "input_throughput": 1391.2331488312673,
    "output_throughput": 1210.7248121839377,
    "total_throughput": 2601.957961015205,
    "itl": 29.444365500441048,
    "ttft": 6554.627617246641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 81.34436542827902,
    "arrivals": 20310,
    "finished_requests": 20277,
    "scheduler_time": 0.033352752482762565
}
#Debug simulation 
Total elapsed time: 1.8859211639501154. Arrivals time: 0.060239143669605255 Scheduler time: 1.456883059348911 Scheduler overhead time: 0.11875176150351763 Adapter cache time: 0.08459132816642523 Engine time: 0.10960305901244283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.78847069805488,
    "estimated_duration": 3599.697772119736,
    "input_throughput": 1275.1481625906117,
    "output_throughput": 1127.5682729358284,
    "total_throughput": 2402.7164355264404,
    "itl": 28.47254934872738,
    "ttft": 7119.258467137942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9945,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 65.76040782274706,
    "arrivals": 18897,
    "finished_requests": 18861,
    "scheduler_time": 0.018394317379464502
}
#Debug simulation 
Total elapsed time: 1.7885468029417098. Arrivals time: 0.05758822662755847 Scheduler time: 1.3604767322540283 Scheduler overhead time: 0.11972245527431369 Adapter cache time: 0.08086777804419398 Engine time: 0.11308078421279788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7593564367853105,
    "estimated_duration": 3599.7177794410704,
    "input_throughput": 1275.1410752852726,
    "output_throughput": 1127.5620058832023,
    "total_throughput": 2402.703081168475,
    "itl": 28.543177525258894,
    "ttft": 7124.4949797127165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9940,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 71.8701814685204,
    "arrivals": 18897,
    "finished_requests": 18861,
    "scheduler_time": 0.01908881885867529
}
#Debug simulation 
Total elapsed time: 1.7594328108243644. Arrivals time: 0.05696194199845195 Scheduler time: 1.3370729866437614 Scheduler overhead time: 0.11914745578542352 Adapter cache time: 0.08012820081785321 Engine time: 0.1096646748483181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7981000929139555,
    "estimated_duration": 3599.7155479287458,
    "input_throughput": 1275.1418657624608,
    "output_throughput": 1127.5627048741308,
    "total_throughput": 2402.7045706365916,
    "itl": 28.562307579910538,
    "ttft": 7125.789596836487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9938,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 73.62984837646569,
    "arrivals": 18897,
    "finished_requests": 18861,
    "scheduler_time": 0.019315154073740924
}
#Debug simulation 
Total elapsed time: 1.7981784571893513. Arrivals time: 0.057653225027024746 Scheduler time: 1.369630679488182 Scheduler overhead time: 0.11966842412948608 Adapter cache time: 0.08103416953235865 Engine time: 0.11360117560252547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.7626865417696536,
    "estimated_duration": 3599.688889343499,
    "input_throughput": 1275.1513092113742,
    "output_throughput": 1127.5710553809142,
    "total_throughput": 2402.7223645922886,
    "itl": 28.496017196343583,
    "ttft": 7120.9918328661715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9945,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 67.87348380867039,
    "arrivals": 18897,
    "finished_requests": 18861,
    "scheduler_time": 0.01867210070126365
}
#Debug simulation 
Total elapsed time: 1.7627604701556265. Arrivals time: 0.05640533519908786 Scheduler time: 1.3301045312546194 Scheduler overhead time: 0.12974402494728565 Adapter cache time: 0.08000324433669448 Engine time: 0.1094821859151125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7558814389631152,
    "estimated_duration": 3599.6894333188193,
    "input_throughput": 1275.151116513961,
    "output_throughput": 1127.5708849854295,
    "total_throughput": 2402.7220014993904,
    "itl": 28.554675229698947,
    "ttft": 7125.386344750397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9937,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 73.02543044138648,
    "arrivals": 18897,
    "finished_requests": 18861,
    "scheduler_time": 0.019208657099734835
}
#Debug simulation 
Total elapsed time: 1.7559584230184555. Arrivals time: 0.05713717686012387 Scheduler time: 1.333292766008526 Scheduler overhead time: 0.11872209189459682 Adapter cache time: 0.0793981240130961 Engine time: 0.11108508752658963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.7650863300077617,
    "estimated_duration": 3599.6950211152384,
    "input_throughput": 1275.1491371004827,
    "output_throughput": 1127.5691346603278,
    "total_throughput": 2402.7182717608107,
    "itl": 28.445155966323853,
    "ttft": 7117.227392766766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9945,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 63.48809375398132,
    "arrivals": 18897,
    "finished_requests": 18861,
    "scheduler_time": 0.01796955089886508
}
#Debug simulation 
Total elapsed time: 1.7651650803163648. Arrivals time: 0.05722692422568798 Scheduler time: 1.3386229765601456 Scheduler overhead time: 0.12082127714529634 Adapter cache time: 0.08004856342449784 Engine time: 0.11082869535312057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7735944790765643,
    "estimated_duration": 3599.7033028409073,
    "input_throughput": 1275.1462034044382,
    "output_throughput": 1127.5665404970148,
    "total_throughput": 2402.712743901453,
    "itl": 28.547872338971644,
    "ttft": 7125.096321465696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9941,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 72.39723769990648,
    "arrivals": 18897,
    "finished_requests": 18861,
    "scheduler_time": 0.019167177466182437
}
#Debug simulation 
Total elapsed time: 1.7736702067777514. Arrivals time: 0.057426020968705416 Scheduler time: 1.3474246426485479 Scheduler overhead time: 0.11940187448635697 Adapter cache time: 0.08018082939088345 Engine time: 0.11236091610044241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.6948513840325177,
    "estimated_duration": 3599.6637352994276,
    "input_throughput": 1245.6055147681818,
    "output_throughput": 1080.6664972211408,
    "total_throughput": 2326.272011989323,
    "itl": 27.971520548528417,
    "ttft": 4688.350654991923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 60.40435650686789,
    "arrivals": 18219,
    "finished_requests": 18196,
    "scheduler_time": 0.0035316035099917705
}
#Debug simulation 
Total elapsed time: 1.694928599987179. Arrivals time: 0.054992261342704296 Scheduler time: 1.2749381163157523 Scheduler overhead time: 0.12000612076371908 Adapter cache time: 0.07652608212083578 Engine time: 0.11134863877668977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6857861848548055,
    "estimated_duration": 3599.6751472392593,
    "input_throughput": 1245.633513190453,
    "output_throughput": 1080.7650248616774,
    "total_throughput": 2326.3985380521303,
    "itl": 27.849659802499453,
    "ttft": 4462.472359590934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 65.83265333313872,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.004628655942472153
}
#Debug simulation 
Total elapsed time: 1.6858623917214572. Arrivals time: 0.054632740560919046 Scheduler time: 1.2645960436202586 Scheduler overhead time: 0.12100155744701624 Adapter cache time: 0.07662060437723994 Engine time: 0.11164986807852983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6917712884023786,
    "estimated_duration": 3599.6560513695113,
    "input_throughput": 1245.6401211704886,
    "output_throughput": 1080.7707582283792,
    "total_throughput": 2326.4108793988676,
    "itl": 27.86071708900534,
    "ttft": 4463.047183550386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9141,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 67.3880218266343,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.004691605169689918
}
#Debug simulation 
Total elapsed time: 1.691872947383672. Arrivals time: 0.058770530857145786 Scheduler time: 1.2674403456039727 Scheduler overhead time: 0.12157118692994118 Adapter cache time: 0.07644952973350883 Engine time: 0.10977738723158836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.708497378975153,
    "estimated_duration": 3599.678814115817,
    "input_throughput": 1245.6322443038205,
    "output_throughput": 1080.7639239212492,
    "total_throughput": 2326.3961682250697,
    "itl": 27.80998616509879,
    "ttft": 4461.372433385995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 62.24355441088582,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.00451862166263838
}
#Debug simulation 
Total elapsed time: 1.70858504017815. Arrivals time: 0.059469619300216436 Scheduler time: 1.2806015526875854 Scheduler overhead time: 0.12066686106845737 Adapter cache time: 0.07717728987336159 Engine time: 0.11276305885985494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6927671590819955,
    "estimated_duration": 3599.666333725246,
    "input_throughput": 1245.6365630310233,
    "output_throughput": 1080.7676710340745,
    "total_throughput": 2326.4042340650976,
    "itl": 27.85858244425608,
    "ttft": 4462.8929871482405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 66.82021774889142,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.004639752478680039
}
#Debug simulation 
Total elapsed time: 1.692867607343942. Arrivals time: 0.05773557862266898 Scheduler time: 1.2657515131868422 Scheduler overhead time: 0.12138830963522196 Adapter cache time: 0.07687573181465268 Engine time: 0.11326960241422057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.70485624903813,
    "estimated_duration": 3599.6676626853123,
    "input_throughput": 1245.6041557611916,
    "output_throughput": 1080.6653181694212,
    "total_throughput": 2326.269473930613,
    "itl": 27.94979558887499,
    "ttft": 4687.608449339572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9140,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 58.34903739682135,
    "arrivals": 18219,
    "finished_requests": 18196,
    "scheduler_time": 0.0034787339116115323
}
#Debug simulation 
Total elapsed time: 1.7049545631743968. Arrivals time: 0.05863741831853986 Scheduler time: 1.2792408778332174 Scheduler overhead time: 0.12159364903345704 Adapter cache time: 0.0768139292486012 Engine time: 0.11096017062664032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_96_slots_32_rate_0.1-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7228982471860945,
    "estimated_duration": 3599.677910412813,
    "input_throughput": 1245.6325570211327,
    "output_throughput": 1080.7641952481927,
    "total_throughput": 2326.396752269325,
    "itl": 27.849127942273345,
    "ttft": 4462.678861584759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 66.25566520139256,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.004650645144005377
}
#Debug simulation 
Total elapsed time: 1.722997979260981. Arrivals time: 0.05789309972897172 Scheduler time: 1.2982308524660766 Scheduler overhead time: 0.12047274224460125 Adapter cache time: 0.07716460758820176 Engine time: 0.11149420030415058 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.6878041289746761,
    "estimated_duration": 3599.9923439023905,
    "input_throughput": 1210.638407990506,
    "output_throughput": 1063.2300389424888,
    "total_throughput": 2273.8684469329946,
    "itl": 27.550270877431274,
    "ttft": 7353.409865339558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 56.714632267038716,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.0011759718166529812
}
#Debug simulation 
Total elapsed time: 1.6879062582738698. Arrivals time: 0.05812692083418369 Scheduler time: 1.261974957305938 Scheduler overhead time: 0.12188025610521436 Adapter cache time: 0.07572409370914102 Engine time: 0.11232198355719447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6954689612612128,
    "estimated_duration": 3600.001803111599,
    "input_throughput": 1210.6352269693277,
    "output_throughput": 1063.2272452451728,
    "total_throughput": 2273.8624722145005,
    "itl": 27.60262975653869,
    "ttft": 7354.663638225804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 61.677655092695694,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.0012389547051939638
}
#Debug simulation 
Total elapsed time: 1.6955720530822873. Arrivals time: 0.057365215849131346 Scheduler time: 1.268756146542728 Scheduler overhead time: 0.12165630981326103 Adapter cache time: 0.07549203326925635 Engine time: 0.11417390173301101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.702392693143338,
    "estimated_duration": 3599.995935255159,
    "input_throughput": 1210.6372002586984,
    "output_throughput": 1063.22897826514,
    "total_throughput": 2273.8661785238382,
    "itl": 27.620384800843507,
    "ttft": 7354.849504202544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 63.11692020798197,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.0012438068210947617
}
#Debug simulation 
Total elapsed time: 1.7024925788864493. Arrivals time: 0.058496331330388784 Scheduler time: 1.2740064174868166 Scheduler overhead time: 0.12170522892847657 Adapter cache time: 0.0756572843529284 Engine time: 0.11454165494069457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.681609314866364,
    "estimated_duration": 3599.9947958877506,
    "input_throughput": 1210.637583414966,
    "output_throughput": 1063.2293147679725,
    "total_throughput": 2273.8668981829383,
    "itl": 27.570882446439896,
    "ttft": 7353.736055285397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 58.340664681343064,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.0011887884266323957
}
#Debug simulation 
Total elapsed time: 1.681707786861807. Arrivals time: 0.05662684142589569 Scheduler time: 1.2532019768841565 Scheduler overhead time: 0.12400883249938488 Adapter cache time: 0.07553910138085485 Engine time: 0.1131746475584805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.674896295182407,
    "estimated_duration": 3599.9916475678365,
    "input_throughput": 1210.6386421603147,
    "output_throughput": 1063.2302445995813,
    "total_throughput": 2273.8688867598958,
    "itl": 27.611434296609495,
    "ttft": 7354.734372142225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 62.609434249322945,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.001240064358814752
}
#Debug simulation 
Total elapsed time: 1.6750239450484514. Arrivals time: 0.05751409661024809 Scheduler time: 1.2502082451246679 Scheduler overhead time: 0.12200378067791462 Adapter cache time: 0.07494918303564191 Engine time: 0.11185549478977919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.6892196028493345,
    "estimated_duration": 3599.9804999764756,
    "input_throughput": 1210.6423909875289,
    "output_throughput": 1063.233536966384,
    "total_throughput": 2273.875927953913,
    "itl": 27.531787646039934,
    "ttft": 7352.92241500101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 54.75488990727966,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.0011641664164399501
}
#Debug simulation 
Total elapsed time: 1.689324156846851. Arrivals time: 0.056524887681007385 Scheduler time: 1.2632592730224133 Scheduler overhead time: 0.12221043463796377 Adapter cache time: 0.0752204367890954 Engine time: 0.11383545957505703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7004341408610344,
    "estimated_duration": 3599.9797108552716,
    "input_throughput": 1210.6426563622415,
    "output_throughput": 1063.2337700288444,
    "total_throughput": 2273.876426391086,
    "itl": 27.60679749648805,
    "ttft": 7354.706667651282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 62.104217600520826,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.0012436886822646187
}
#Debug simulation 
Total elapsed time: 1.7005368741229177. Arrivals time: 0.058103342074900866 Scheduler time: 1.2740439274348319 Scheduler overhead time: 0.12047689966857433 Adapter cache time: 0.07544316770508885 Engine time: 0.11462813289836049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.5700468802824616,
    "estimated_duration": 3600.008894274165,
    "input_throughput": 1097.992287265435,
    "output_throughput": 956.1004156057706,
    "total_throughput": 2054.092702871206,
    "itl": 26.47261651587482,
    "ttft": 6313.74625557503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7397,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.91198960933544,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 1.6679720180528082e-06
}
#Debug simulation 
Total elapsed time: 1.570161723997444. Arrivals time: 0.052539701107889414 Scheduler time: 1.1451156362891197 Scheduler overhead time: 0.1260705511085689 Adapter cache time: 0.07064783247187734 Engine time: 0.11511179758235812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5457168077118695,
    "estimated_duration": 3600.0167484024746,
    "input_throughput": 1097.989891784272,
    "output_throughput": 956.0983296889914,
    "total_throughput": 2054.0882214732633,
    "itl": 26.52178455166743,
    "ttft": 6314.195860368003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 53.60796300544273,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 1.6679720180528082e-06
}
#Debug simulation 
Total elapsed time: 1.5458304970525205. Arrivals time: 0.05279612308368087 Scheduler time: 1.1215657163411379 Scheduler overhead time: 0.12654501385986805 Adapter cache time: 0.07032479671761394 Engine time: 0.11470755329355597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.592549534048885,
    "estimated_duration": 3600.000065956229,
    "input_throughput": 1097.9949798834423,
    "output_throughput": 956.102760260852,
    "total_throughput": 2054.097740144294,
    "itl": 26.532409949150782,
    "ttft": 6090.995878370781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 54.96866320849583,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 1.6679720180528082e-06
}
#Debug simulation 
Total elapsed time: 1.5926989540457726. Arrivals time: 0.053657363168895245 Scheduler time: 1.1643937667831779 Scheduler overhead time: 0.12634322233498096 Adapter cache time: 0.07158542331308126 Engine time: 0.11637128982692957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.591074715834111,
    "estimated_duration": 3600.0269091031364,
    "input_throughput": 1097.9867928222639,
    "output_throughput": 956.0956312011255,
    "total_throughput": 2054.082424023389,
    "itl": 26.48603709982993,
    "ttft": 6313.94293302739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 50.433101639479496,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 1.6679720180528082e-06
}
#Debug simulation 
Total elapsed time: 1.591187693644315. Arrivals time: 0.05141080962494016 Scheduler time: 1.164612824562937 Scheduler overhead time: 0.1248471443541348 Adapter cache time: 0.07152708852663636 Engine time: 0.11859799083322287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5378639451228082,
    "estimated_duration": 3600.0203529088335,
    "input_throughput": 1097.9887924261689,
    "output_throughput": 956.0973723992621,
    "total_throughput": 2054.086164825431,
    "itl": 26.525610062494255,
    "ttft": 6314.231724645754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 54.47733238961277,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 1.6679720180528082e-06
}
#Debug simulation 
Total elapsed time: 1.5379730542190373. Arrivals time: 0.05069663701578975 Scheduler time: 1.1173953833058476 Scheduler overhead time: 0.1259257448837161 Adapter cache time: 0.07022260036319494 Engine time: 0.11357485176995397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.5791718289256096,
    "estimated_duration": 3600.0096712558384,
    "input_throughput": 1097.9920502883258,
    "output_throughput": 956.1002092528526,
    "total_throughput": 2054.0922595411785,
    "itl": 26.45458580223989,
    "ttft": 6313.500669757411,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 47.20909535552428,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 1.6679720180528082e-06
}
#Debug simulation 
Total elapsed time: 1.5792837529443204. Arrivals time: 0.05353558715432882 Scheduler time: 1.1485619070008397 Scheduler overhead time: 0.1289520561695099 Adapter cache time: 0.07090712105855346 Engine time: 0.11690759798511863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_96_slots_32_rate_0.1-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5943299499340355,
    "estimated_duration": 3600.010851357299,
    "input_throughput": 1097.9916903610713,
    "output_throughput": 956.099895838449,
    "total_throughput": 2054.0915861995204,
    "itl": 26.522611705077775,
    "ttft": 6314.229908882096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 54.0402024299066,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 1.6679720180528082e-06
}
#Debug simulation 
Total elapsed time: 1.5944402418099344. Arrivals time: 0.05120949586853385 Scheduler time: 1.1679450450465083 Scheduler overhead time: 0.1257784590125084 Adapter cache time: 0.07145167700946331 Engine time: 0.11800141027197242 
