INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:05 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.748489959165454,
    "estimated_duration": 3600.0446518173576,
    "input_throughput": 3942.784152089489,
    "output_throughput": 3439.269563989885,
    "total_throughput": 7382.053716079374,
    "itl": 99.76733481402783,
    "ttft": 2299859.0345388325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2807,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.124776979168164,
    "arrivals": 1486681,
    "finished_requests": 57569,
    "scheduler_time": 148.4494681228107
}
#Debug simulation 
Total elapsed time: 5.7485950901173055. Arrivals time: 0.22887622006237507 Scheduler time: 5.338196454104036 Scheduler overhead time: 0.05200005043298006 Adapter cache time: 0.05330285057425499 Engine time: 0.05234037199988961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 8.240662690252066,
    "estimated_duration": 3600.1255879501264,
    "input_throughput": 4489.56865674318,
    "output_throughput": 3912.7737785449567,
    "total_throughput": 8402.342435288137,
    "itl": 123.18351990574513,
    "ttft": 2240349.1969021917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1850,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.693162279082635,
    "arrivals": 1486681,
    "finished_requests": 65589,
    "scheduler_time": 136.6779960533668
}
#Debug simulation 
Total elapsed time: 8.240770158357918. Arrivals time: 0.26240725349634886 Scheduler time: 7.833350131288171 Scheduler overhead time: 0.04467147495597601 Adapter cache time: 0.0354998828843236 Engine time: 0.044413382187485695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.717869899235666,
    "estimated_duration": 3600.077634767705,
    "input_throughput": 3942.970246783562,
    "output_throughput": 3439.559436278377,
    "total_throughput": 7382.529683061939,
    "itl": 99.7621276137554,
    "ttft": 2299984.5493257693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.943340835939356,
    "arrivals": 1486681,
    "finished_requests": 57574,
    "scheduler_time": 148.45827397700995
}
#Debug simulation 
Total elapsed time: 5.71795984916389. Arrivals time: 0.231531020719558 Scheduler time: 5.306192694231868 Scheduler overhead time: 0.05154532194137573 Adapter cache time: 0.05347520252689719 Engine time: 0.051351582165807486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.278749790973961,
    "estimated_duration": 3600.076727128987,
    "input_throughput": 4470.644716740601,
    "output_throughput": 3901.7090647403666,
    "total_throughput": 8372.353781480968,
    "itl": 122.70376200889864,
    "ttft": 2240483.977361907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1782,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.3761471160971,
    "arrivals": 1486681,
    "finished_requests": 65357,
    "scheduler_time": 136.86843745141707
}
#Debug simulation 
Total elapsed time: 8.278839646372944. Arrivals time: 0.26924391090869904 Scheduler time: 7.864486784208566 Scheduler overhead time: 0.04523489251732826 Adapter cache time: 0.035037145018577576 Engine time: 0.04450920410454273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.752736558672041,
    "estimated_duration": 3600.0875309484227,
    "input_throughput": 3943.0152400378875,
    "output_throughput": 3439.620812980007,
    "total_throughput": 7382.6360530178945,
    "itl": 99.75597833889165,
    "ttft": 2299954.9307256322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.73850194826739,
    "arrivals": 1486681,
    "finished_requests": 57576,
    "scheduler_time": 148.46705828279227
}
#Debug simulation 
Total elapsed time: 5.752829348668456. Arrivals time: 0.23756510205566883 Scheduler time: 5.334198192227632 Scheduler overhead time: 0.05202083569020033 Adapter cache time: 0.053561875596642494 Engine time: 0.05148031981661916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 10.30223915586248,
    "estimated_duration": 3600.0702178905717,
    "input_throughput": 4574.382999020872,
    "output_throughput": 4016.4963805824073,
    "total_throughput": 8590.87937960328,
    "itl": 129.2053204745291,
    "ttft": 2223557.8434790615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.25228647187372,
    "arrivals": 1482883,
    "finished_requests": 67064,
    "scheduler_time": 134.76670348429877
}
#Debug simulation 
Total elapsed time: 10.302335910033435. Arrivals time: 0.43848494114354253 Scheduler time: 9.7305917465128 Scheduler overhead time: 0.043516258243471384 Adapter cache time: 0.02652219543233514 Engine time: 0.04335189564153552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.198096681386232,
    "estimated_duration": 3600.1036591491575,
    "input_throughput": 4472.157061112263,
    "output_throughput": 3926.583881571039,
    "total_throughput": 8398.740942683302,
    "itl": 123.64451233720104,
    "ttft": 2240545.5553318933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.123615035326027,
    "arrivals": 1482883,
    "finished_requests": 65644,
    "scheduler_time": 136.75394008649357
}
#Debug simulation 
Total elapsed time: 8.198273438028991. Arrivals time: 0.4189777746796608 Scheduler time: 7.635220185853541 Scheduler overhead time: 0.04464354692026973 Adapter cache time: 0.034929901361465454 Engine time: 0.04417816083878279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.93869558814913,
    "estimated_duration": 3600.007216249406,
    "input_throughput": 3926.6818511345623,
    "output_throughput": 3456.611126731114,
    "total_throughput": 7383.292977865676,
    "itl": 100.6007201942247,
    "ttft": 2298134.700228873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.9304832596008,
    "arrivals": 1482883,
    "finished_requests": 57676,
    "scheduler_time": 148.03445979628523
}
#Debug simulation 
Total elapsed time: 5.938786709215492. Arrivals time: 0.33188185933977365 Scheduler time: 5.428831798955798 Scheduler overhead time: 0.05131464125588536 Adapter cache time: 0.05215982999652624 Engine time: 0.050989660900086164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 8.200300063937902,
    "estimated_duration": 3600.0986234682678,
    "input_throughput": 4473.3288402208545,
    "output_throughput": 3927.6951769664856,
    "total_throughput": 8401.02401718734,
    "itl": 123.61572001053071,
    "ttft": 2240444.5718114316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.300399998151859,
    "arrivals": 1482883,
    "finished_requests": 65662,
    "scheduler_time": 136.78381792381714
}
#Debug simulation 
Total elapsed time: 8.2003957410343. Arrivals time: 0.4423859464004636 Scheduler time: 7.614837599452585 Scheduler overhead time: 0.04426105599850416 Adapter cache time: 0.03479840233922005 Engine time: 0.04388925200328231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.870285600889474,
    "estimated_duration": 3600.0391205834044,
    "input_throughput": 3927.0706585291023,
    "output_throughput": 3456.935767404441,
    "total_throughput": 7384.006425933543,
    "itl": 100.5964442459898,
    "ttft": 2298043.629256805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.741592394528173,
    "arrivals": 1482883,
    "finished_requests": 57681,
    "scheduler_time": 148.04339757622552
}
#Debug simulation 
Total elapsed time: 5.870395220816135. Arrivals time: 0.4030310735106468 Scheduler time: 5.28882575314492 Scheduler overhead time: 0.05116234626621008 Adapter cache time: 0.05247050756588578 Engine time: 0.05128262098878622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.20376396086067,
    "estimated_duration": 3600.0864107098023,
    "input_throughput": 4474.3301027666475,
    "output_throughput": 3928.48653796939,
    "total_throughput": 8402.816640736037,
    "itl": 123.58681543453851,
    "ttft": 2240470.284595697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1796,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.465522009265099,
    "arrivals": 1482883,
    "finished_requests": 65678,
    "scheduler_time": 136.81379383328718
}
#Debug simulation 
Total elapsed time: 8.203891938086599. Arrivals time: 0.4340684930793941 Scheduler time: 7.625943761318922 Scheduler overhead time: 0.044595081359148026 Adapter cache time: 0.034561578650027514 Engine time: 0.044363636057823896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.871418925002217,
    "estimated_duration": 3600.062876924937,
    "input_throughput": 3927.464736970706,
    "output_throughput": 3457.312670780754,
    "total_throughput": 7384.7774077514605,
    "itl": 100.59010974259807,
    "ttft": 2298165.394403092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.542138553448194,
    "arrivals": 1482883,
    "finished_requests": 57687,
    "scheduler_time": 148.0523876770975
}
#Debug simulation 
Total elapsed time: 5.871514837723225. Arrivals time: 0.396978285163641 Scheduler time: 5.2951109376735985 Scheduler overhead time: 0.05149267613887787 Adapter cache time: 0.052771049086004496 Engine time: 0.05136114126071334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 10.019020351115614,
    "estimated_duration": 3600.014594501948,
    "input_throughput": 4721.279193133783,
    "output_throughput": 4101.18976254945,
    "total_throughput": 8822.468955683233,
    "itl": 132.7101731077555,
    "ttft": 2218149.2258543815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.721767513142181,
    "arrivals": 1481051,
    "finished_requests": 68447,
    "scheduler_time": 133.8229627642164
}
#Debug simulation 
Total elapsed time: 10.019121688324958. Arrivals time: 0.2786910515278578 Scheduler time: 9.609696817118675 Scheduler overhead time: 0.0429638815112412 Adapter cache time: 0.0259811501018703 Engine time: 0.042489897925406694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.249632311053574,
    "estimated_duration": 3600.1294125034597,
    "input_throughput": 4514.91519820036,
    "output_throughput": 3924.4944781513245,
    "total_throughput": 8439.409676351685,
    "itl": 122.46429618140442,
    "ttft": 2240690.321238269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.319769747690131,
    "arrivals": 1481051,
    "finished_requests": 65380,
    "scheduler_time": 137.4457183783752
}
#Debug simulation 
Total elapsed time: 8.249727189075202. Arrivals time: 0.37778786569833755 Scheduler time: 7.727201452013105 Scheduler overhead time: 0.047547231893986464 Adapter cache time: 0.032377203926444054 Engine time: 0.0444216881878674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.642487321980298,
    "estimated_duration": 3600.040143670508,
    "input_throughput": 3963.545524648449,
    "output_throughput": 3439.4055360103944,
    "total_throughput": 7402.951060658843,
    "itl": 99.81944321590343,
    "ttft": 2297693.49424047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.133922775206283,
    "arrivals": 1481051,
    "finished_requests": 57250,
    "scheduler_time": 148.50404469111456
}
#Debug simulation 
Total elapsed time: 5.642593631986529. Arrivals time: 0.23124344134703279 Scheduler time: 5.2331471354700625 Scheduler overhead time: 0.05200658133253455 Adapter cache time: 0.050132596865296364 Engine time: 0.05188196199014783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 8.44960311613977,
    "estimated_duration": 3600.031495245378,
    "input_throughput": 4515.78354841363,
    "output_throughput": 3925.0920495174355,
    "total_throughput": 8440.875597931066,
    "itl": 122.4353686365339,
    "ttft": 2240605.5736597343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.546530665555169,
    "arrivals": 1481051,
    "finished_requests": 65392,
    "scheduler_time": 137.47039457664678
}
#Debug simulation 
Total elapsed time: 8.449771671090275. Arrivals time: 0.26569440215826035 Scheduler time: 8.040249185170978 Scheduler overhead time: 0.0457875756546855 Adapter cache time: 0.03218166250735521 Engine time: 0.04500230401754379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.745035146828741,
    "estimated_duration": 3600.075799953293,
    "input_throughput": 3963.6234882013123,
    "output_throughput": 3439.5959107751714,
    "total_throughput": 7403.219398976484,
    "itl": 99.8147766770796,
    "ttft": 2297640.0134091685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.95083119107887,
    "arrivals": 1481051,
    "finished_requests": 57252,
    "scheduler_time": 148.51296027827652
}
#Debug simulation 
Total elapsed time: 5.745156223885715. Arrivals time: 0.23001348692923784 Scheduler time: 5.3375087906606495 Scheduler overhead time: 0.05158487521111965 Adapter cache time: 0.050683712121099234 Engine time: 0.05156702967360616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.1244390970096,
    "estimated_duration": 3600.0076184702557,
    "input_throughput": 4522.663206729134,
    "output_throughput": 3931.6183464118253,
    "total_throughput": 8454.28155314096,
    "itl": 122.32349377627595,
    "ttft": 2240074.3012970644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.037799306246816,
    "arrivals": 1481051,
    "finished_requests": 65494,
    "scheduler_time": 137.6342407585686
}
#Debug simulation 
Total elapsed time: 8.124536043033004. Arrivals time: 0.26184531208127737 Scheduler time: 7.718629393260926 Scheduler overhead time: 0.04487790400162339 Adapter cache time: 0.03337353188544512 Engine time: 0.0453116768039763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.628843140788376,
    "estimated_duration": 3600.1030621296195,
    "input_throughput": 3964.0326273209853,
    "output_throughput": 3440.130681334948,
    "total_throughput": 7404.163308655933,
    "itl": 99.81048090137942,
    "ttft": 2297560.238795811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.758005099650557,
    "arrivals": 1481051,
    "finished_requests": 57257,
    "scheduler_time": 148.5218661733603
}
#Debug simulation 
Total elapsed time: 5.628933328669518. Arrivals time: 0.23232289357110858 Scheduler time: 5.2197674424387515 Scheduler overhead time: 0.05163314798846841 Adapter cache time: 0.05000007385388017 Engine time: 0.05152778886258602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 9.790298535022885,
    "estimated_duration": 3600.109329133778,
    "input_throughput": 4729.457759022213,
    "output_throughput": 4129.288485685206,
    "total_throughput": 8858.74624470742,
    "itl": 133.28348108477545,
    "ttft": 2217590.983626547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.536620060247577,
    "arrivals": 1480152,
    "finished_requests": 69109,
    "scheduler_time": 133.91997871734458
}
#Debug simulation 
Total elapsed time: 9.790398093871772. Arrivals time: 0.2955501712858677 Scheduler time: 9.363562121987343 Scheduler overhead time: 0.04268869012594223 Adapter cache time: 0.026714411564171314 Engine time: 0.042287605814635754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.100510669872165,
    "estimated_duration": 3600.105272924356,
    "input_throughput": 4500.056462748999,
    "output_throughput": 3934.9440991465294,
    "total_throughput": 8435.00056189553,
    "itl": 122.30320953703634,
    "ttft": 2236168.4298574496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.053854848523468,
    "arrivals": 1480152,
    "finished_requests": 65777,
    "scheduler_time": 137.68076046080202
}
#Debug simulation 
Total elapsed time: 8.10061851516366. Arrivals time: 0.2868413212709129 Scheduler time: 7.672691911458969 Scheduler overhead time: 0.04459601501002908 Adapter cache time: 0.03137658070772886 Engine time: 0.044603513553738594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.700141211040318,
    "estimated_duration": 3600.057934740017,
    "input_throughput": 3940.25742283628,
    "output_throughput": 3445.7901025128494,
    "total_throughput": 7386.0475253491295,
    "itl": 99.94965346796722,
    "ttft": 2295178.7239310797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.630093543846225,
    "arrivals": 1480152,
    "finished_requests": 57583,
    "scheduler_time": 148.39063212413578
}
#Debug simulation 
Total elapsed time: 5.700231896247715. Arrivals time: 0.23537273798137903 Scheduler time: 5.2874368922784925 Scheduler overhead time: 0.05169233679771423 Adapter cache time: 0.050015398766845465 Engine time: 0.05192967317998409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 8.148004176095128,
    "estimated_duration": 3600.0937305902557,
    "input_throughput": 4505.250477837073,
    "output_throughput": 3937.593035300171,
    "total_throughput": 8442.843513137244,
    "itl": 122.9157949535647,
    "ttft": 2233045.049317234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.45658911077785,
    "arrivals": 1480152,
    "finished_requests": 65875,
    "scheduler_time": 137.31074250709142
}
#Debug simulation 
Total elapsed time: 8.148171678651124. Arrivals time: 0.2635737443342805 Scheduler time: 7.74275416135788 Scheduler overhead time: 0.04476518556475639 Adapter cache time: 0.03224776033312082 Engine time: 0.04438922135159373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.659024509135634,
    "estimated_duration": 3600.098793284825,
    "input_throughput": 3940.301034643774,
    "output_throughput": 3445.9493231519514,
    "total_throughput": 7386.250357795726,
    "itl": 99.94527302031588,
    "ttft": 2295111.7177964905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.454458178076944,
    "arrivals": 1480152,
    "finished_requests": 57585,
    "scheduler_time": 148.39948765595582
}
#Debug simulation 
Total elapsed time: 5.659119987860322. Arrivals time: 0.2340216627344489 Scheduler time: 5.248941759578884 Scheduler overhead time: 0.051356769632548094 Adapter cache time: 0.04991518286988139 Engine time: 0.051166287157684565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.065009950194508,
    "estimated_duration": 3600.0293310484863,
    "input_throughput": 4506.36106214033,
    "output_throughput": 3938.3393012163892,
    "total_throughput": 8444.70036335672,
    "itl": 122.89106283915396,
    "ttft": 2233038.235379331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.693067575455961,
    "arrivals": 1480152,
    "finished_requests": 65888,
    "scheduler_time": 137.3359297719998
}
#Debug simulation 
Total elapsed time: 8.06511088134721. Arrivals time: 0.26627830835059285 Scheduler time: 7.657495274674147 Scheduler overhead time: 0.044566994067281485 Adapter cache time: 0.032227335032075644 Engine time: 0.04420685861259699 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.678791352082044,
    "estimated_duration": 3600.0222503235996,
    "input_throughput": 3940.385368097348,
    "output_throughput": 3446.077589905132,
    "total_throughput": 7386.46295800248,
    "itl": 99.9404853338037,
    "ttft": 2295089.226544737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.269709656536598,
    "arrivals": 1480152,
    "finished_requests": 57587,
    "scheduler_time": 148.4037896460849
}
#Debug simulation 
Total elapsed time: 5.678885621018708. Arrivals time: 0.2370716193690896 Scheduler time: 5.266323214862496 Scheduler overhead time: 0.051262355875223875 Adapter cache time: 0.04938702704384923 Engine time: 0.05127197224646807 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 11.662220590282232,
    "estimated_duration": 3600.0018798678652,
    "input_throughput": 4615.354256595513,
    "output_throughput": 4055.5323267041954,
    "total_throughput": 8670.886583299709,
    "itl": 136.3047551623412,
    "ttft": 2220165.53171608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1751,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.578328214944646,
    "arrivals": 1357053,
    "finished_requests": 67773,
    "scheduler_time": 131.21735735597346
}
#Debug simulation 
Total elapsed time: 11.6622908632271. Arrivals time: 0.30654332088306546 Scheduler time: 11.21541526215151 Scheduler overhead time: 0.04312546644359827 Adapter cache time: 0.03525649942457676 Engine time: 0.0426829750649631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.732185145840049,
    "estimated_duration": 3600.1096688081425,
    "input_throughput": 4422.800543537705,
    "output_throughput": 3893.4602802365325,
    "total_throughput": 8316.260823774237,
    "itl": 124.70701988013609,
    "ttft": 2239835.9610586273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.02045156314935,
    "arrivals": 1357053,
    "finished_requests": 64958,
    "scheduler_time": 135.54808605441082
}
#Debug simulation 
Total elapsed time: 8.7323181158863. Arrivals time: 0.27441485365852714 Scheduler time: 8.30320777464658 Scheduler overhead time: 0.04510544892400503 Adapter cache time: 0.04442713130265474 Engine time: 0.044775038957595825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.221223652828485,
    "estimated_duration": 3600.100726711917,
    "input_throughput": 3911.7080518083226,
    "output_throughput": 3446.308573519205,
    "total_throughput": 7358.016625327527,
    "itl": 100.83029481006467,
    "ttft": 2298010.0070357393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3770,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.34325594697719,
    "arrivals": 1357053,
    "finished_requests": 57395,
    "scheduler_time": 147.6544620009556
}
#Debug simulation 
Total elapsed time: 6.2213156200014055. Arrivals time: 0.23865284584462643 Scheduler time: 5.787887625861913 Scheduler overhead time: 0.05178881995379925 Adapter cache time: 0.06743585038930178 Engine time: 0.05167046934366226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 8.881363820284605,
    "estimated_duration": 3600.0480038124883,
    "input_throughput": 4426.093480732915,
    "output_throughput": 3896.0336043148363,
    "total_throughput": 8322.127085047752,
    "itl": 124.64884512743741,
    "ttft": 2239821.6919340556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.98531113828561,
    "arrivals": 1357053,
    "finished_requests": 65047,
    "scheduler_time": 135.59981541205218
}
#Debug simulation 
Total elapsed time: 8.881519816350192. Arrivals time: 0.276045935228467 Scheduler time: 8.451745205093175 Scheduler overhead time: 0.04512579273432493 Adapter cache time: 0.04184770677238703 Engine time: 0.044766259379684925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 6.6461081760935485,
    "estimated_duration": 3600.070732115546,
    "input_throughput": 3911.8834178361353,
    "output_throughput": 3446.4947839257543,
    "total_throughput": 7358.37820176189,
    "itl": 100.82259485845923,
    "ttft": 2298024.530153507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3770,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.083531007502337,
    "arrivals": 1357053,
    "finished_requests": 57397,
    "scheduler_time": 147.66355932095146
}
#Debug simulation 
Total elapsed time: 6.6461757477372885. Arrivals time: 0.24494032189249992 Scheduler time: 6.206396099645644 Scheduler overhead time: 0.05161938862875104 Adapter cache time: 0.06775660626590252 Engine time: 0.05164651060476899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.539144259877503,
    "estimated_duration": 3600.0899941009034,
    "input_throughput": 4431.84143344857,
    "output_throughput": 3898.499210574631,
    "total_throughput": 8330.340644023201,
    "itl": 124.74887717073288,
    "ttft": 2240539.527169333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.563999253111929,
    "arrivals": 1357053,
    "finished_requests": 65078,
    "scheduler_time": 135.54366720788204
}
#Debug simulation 
Total elapsed time: 8.53924091393128. Arrivals time: 0.2859797775745392 Scheduler time: 8.09787139762193 Scheduler overhead time: 0.044869475066661835 Adapter cache time: 0.04569855937734246 Engine time: 0.04443919938057661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.207349361851811,
    "estimated_duration": 3600.0340087646996,
    "input_throughput": 3912.049987781242,
    "output_throughput": 3446.7363279875667,
    "total_throughput": 7358.786315768809,
    "itl": 100.81565109450246,
    "ttft": 2297924.34671723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3770,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.81738543555242,
    "arrivals": 1357053,
    "finished_requests": 57400,
    "scheduler_time": 147.67268604217645
}
#Debug simulation 
Total elapsed time: 6.2074525081552565. Arrivals time: 0.24469595961272717 Scheduler time: 5.767907342873514 Scheduler overhead time: 0.051899323239922523 Adapter cache time: 0.06762406369671226 Engine time: 0.0515972888097167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 11.152436399832368,
    "estimated_duration": 3600.0186603676575,
    "input_throughput": 4680.444905882571,
    "output_throughput": 4058.637017872516,
    "total_throughput": 8739.081923755088,
    "itl": 135.87393503245218,
    "ttft": 2202331.265393515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.530007651215627,
    "arrivals": 1265213,
    "finished_requests": 68216,
    "scheduler_time": 131.41348784433777
}
#Debug simulation 
Total elapsed time: 11.152560277841985. Arrivals time: 0.3695611278526485 Scheduler time: 10.650129372254014 Scheduler overhead time: 0.042632380966097116 Adapter cache time: 0.0286850375123322 Engine time: 0.042396657168865204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.13485523313284,
    "estimated_duration": 3600.1136608492757,
    "input_throughput": 4492.815928534978,
    "output_throughput": 3894.257882039397,
    "total_throughput": 8387.073810574375,
    "itl": 124.37771999066712,
    "ttft": 2220690.9564236514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.660062352781868,
    "arrivals": 1265213,
    "finished_requests": 65400,
    "scheduler_time": 135.66250818432493
}
#Debug simulation 
Total elapsed time: 8.134950147010386. Arrivals time: 0.26773190405219793 Scheduler time: 7.716785828117281 Scheduler overhead time: 0.04455350339412689 Adapter cache time: 0.04143898515030742 Engine time: 0.044192640110850334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.786286763846874,
    "estimated_duration": 3600.073170960694,
    "input_throughput": 3977.753317768982,
    "output_throughput": 3449.7876599229817,
    "total_throughput": 7427.540977691963,
    "itl": 100.48050876411988,
    "ttft": 2278401.284417224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.559965218403896,
    "arrivals": 1265213,
    "finished_requests": 57941,
    "scheduler_time": 147.86408764603217
}
#Debug simulation 
Total elapsed time: 5.786377187818289. Arrivals time: 0.23476640554144979 Scheduler time: 5.366330620832741 Scheduler overhead time: 0.051857149694114923 Adapter cache time: 0.058491519652307034 Engine time: 0.05121217481791973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 8.323857497889549,
    "estimated_duration": 3600.078510985717,
    "input_throughput": 4490.257907062664,
    "output_throughput": 3891.025142161289,
    "total_throughput": 8381.283049223954,
    "itl": 123.98010227915117,
    "ttft": 2221750.2083257977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1888,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.932974832877717,
    "arrivals": 1265213,
    "finished_requests": 65360,
    "scheduler_time": 135.86127408850632
}
#Debug simulation 
Total elapsed time: 8.324008316732943. Arrivals time: 0.26838929671794176 Scheduler time: 7.909083575475961 Scheduler overhead time: 0.045049268286675215 Adapter cache time: 0.037026267033070326 Engine time: 0.044065152294933796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.798204511869699,
    "estimated_duration": 3600.0752578095157,
    "input_throughput": 3977.8732316594596,
    "output_throughput": 3449.787326822913,
    "total_throughput": 7427.660558482373,
    "itl": 100.47225914857769,
    "ttft": 2278480.1074090665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.345598940607665,
    "arrivals": 1265213,
    "finished_requests": 57944,
    "scheduler_time": 147.8729023938923
}
#Debug simulation 
Total elapsed time: 5.798315106891096. Arrivals time: 0.2360424199141562 Scheduler time: 5.377641957253218 Scheduler overhead time: 0.05172462249174714 Adapter cache time: 0.05776797141879797 Engine time: 0.05135802738368511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.660571910906583,
    "estimated_duration": 3600.0387528287374,
    "input_throughput": 4491.185820512519,
    "output_throughput": 3891.5325533626205,
    "total_throughput": 8382.71837387514,
    "itl": 123.94917948765602,
    "ttft": 2221719.6003912133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1888,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.052842735797666,
    "arrivals": 1265213,
    "finished_requests": 65375,
    "scheduler_time": 135.89174502292423
}
#Debug simulation 
Total elapsed time: 8.660678640939295. Arrivals time: 0.263903402723372 Scheduler time: 8.251154429279268 Scheduler overhead time: 0.04445887776091695 Adapter cache time: 0.036978280171751976 Engine time: 0.04388564033433795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.812772153876722,
    "estimated_duration": 3600.073702128944,
    "input_throughput": 3978.1613336223413,
    "output_throughput": 3450.0354791778477,
    "total_throughput": 7428.196812800189,
    "itl": 100.46662723693508,
    "ttft": 2278443.355024656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.126883202102512,
    "arrivals": 1265213,
    "finished_requests": 57948,
    "scheduler_time": 147.88174902660893
}
#Debug simulation 
Total elapsed time: 5.812861963175237. Arrivals time: 0.23172698775306344 Scheduler time: 5.395593206398189 Scheduler overhead time: 0.05180474370718002 Adapter cache time: 0.05844345549121499 Engine time: 0.05151049792766571 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 9.542596660088748,
    "estimated_duration": 3600.019961066236,
    "input_throughput": 4659.211388103576,
    "output_throughput": 4053.8222448293504,
    "total_throughput": 8713.033632932926,
    "itl": 135.6065995184848,
    "ttft": 2206485.1329056914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1522,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.064086546628062,
    "arrivals": 1250156,
    "finished_requests": 67969,
    "scheduler_time": 131.4177893989614
}
#Debug simulation 
Total elapsed time: 9.54269806202501. Arrivals time: 0.3025036179460585 Scheduler time: 9.105966912582517 Scheduler overhead time: 0.042109474539756775 Adapter cache time: 0.0314276609569788 Engine time: 0.04165190877392888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.98942717211321,
    "estimated_duration": 3600.0153132945043,
    "input_throughput": 4468.45238146464,
    "output_throughput": 3892.5037202622702,
    "total_throughput": 8360.95610172691,
    "itl": 123.93426062323735,
    "ttft": 2226709.049460163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1712,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.513232780806558,
    "arrivals": 1250156,
    "finished_requests": 65214,
    "scheduler_time": 135.85800970650058
}
#Debug simulation 
Total elapsed time: 7.989543334115297. Arrivals time: 0.2628552932292223 Scheduler time: 7.583625392522663 Scheduler overhead time: 0.04457896575331688 Adapter cache time: 0.03427932504564524 Engine time: 0.04387762350961566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.640153197105974,
    "estimated_duration": 3600.072115310448,
    "input_throughput": 3947.268983742171,
    "output_throughput": 3445.062099521439,
    "total_throughput": 7392.33108326361,
    "itl": 100.20904042091959,
    "ttft": 2284844.0039858795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3006,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.572996585830523,
    "arrivals": 1250156,
    "finished_requests": 57691,
    "scheduler_time": 148.0385892897863
}
#Debug simulation 
Total elapsed time: 5.640247019939125. Arrivals time: 0.31366406101733446 Scheduler time: 5.145046991761774 Scheduler overhead time: 0.051586448680609465 Adapter cache time: 0.05502736568450928 Engine time: 0.051205530762672424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.840611768886447,
    "estimated_duration": 3600.1032966566836,
    "input_throughput": 4473.706633628268,
    "output_throughput": 3893.291898878709,
    "total_throughput": 8366.998532506977,
    "itl": 123.9986886396426,
    "ttft": 2226335.876256586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1787,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.261822619387095,
    "arrivals": 1250156,
    "finished_requests": 65280,
    "scheduler_time": 135.8199056895161
}
#Debug simulation 
Total elapsed time: 7.840753194876015. Arrivals time: 0.2595000290311873 Scheduler time: 7.436258574016392 Scheduler overhead time: 0.04467348428443074 Adapter cache time: 0.03600846044719219 Engine time: 0.044110162649303675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.5843033022247255,
    "estimated_duration": 3600.098632923442,
    "input_throughput": 3947.509901543915,
    "output_throughput": 3445.1906085495734,
    "total_throughput": 7392.700510093488,
    "itl": 100.2040180072235,
    "ttft": 2284752.1431497913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3007,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.38218830067238,
    "arrivals": 1250156,
    "finished_requests": 57694,
    "scheduler_time": 148.04743300110667
}
#Debug simulation 
Total elapsed time: 5.584398967213929. Arrivals time: 0.23592854058369994 Scheduler time: 5.167213000357151 Scheduler overhead time: 0.05145243648439646 Adapter cache time: 0.05464677978307009 Engine time: 0.05141665507107973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.831945753190666,
    "estimated_duration": 3600.043451386715,
    "input_throughput": 4474.429605508023,
    "output_throughput": 3894.091610088763,
    "total_throughput": 8368.521215596786,
    "itl": 123.97153599932292,
    "ttft": 2226024.1076034163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1788,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.414450641740528,
    "arrivals": 1250156,
    "finished_requests": 65292,
    "scheduler_time": 135.8490895765596
}
#Debug simulation 
Total elapsed time: 7.832045834045857. Arrivals time: 0.2756446017883718 Scheduler time: 7.412098455708474 Scheduler overhead time: 0.04452255927026272 Adapter cache time: 0.03540309797972441 Engine time: 0.04413468111306429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.587094538845122,
    "estimated_duration": 3600.096528565912,
    "input_throughput": 3947.643872110637,
    "output_throughput": 3445.6256663043782,
    "total_throughput": 7393.269538415015,
    "itl": 100.19874899763272,
    "ttft": 2284729.93308454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3007,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.163265444990614,
    "arrivals": 1250156,
    "finished_requests": 57698,
    "scheduler_time": 148.05627559727156
}
#Debug simulation 
Total elapsed time: 5.587187516968697. Arrivals time: 0.23994077509269118 Scheduler time: 5.1659697643481195 Scheduler overhead time: 0.05180232599377632 Adapter cache time: 0.05437043262645602 Engine time: 0.051243589259684086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 9.455845145974308,
    "estimated_duration": 3600.0847915959325,
    "input_throughput": 4679.916161788837,
    "output_throughput": 4049.051576237455,
    "total_throughput": 8728.967738026291,
    "itl": 135.23078659479697,
    "ttft": 2208760.317839818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.510170424119776,
    "arrivals": 1242527,
    "finished_requests": 67903,
    "scheduler_time": 131.56460305088513
}
#Debug simulation 
Total elapsed time: 9.45595358684659. Arrivals time: 0.28449131920933723 Scheduler time: 9.040693613234907 Scheduler overhead time: 0.0420468389056623 Adapter cache time: 0.027664420194923878 Engine time: 0.042073634918779135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.629879612941295,
    "estimated_duration": 3600.0265283199096,
    "input_throughput": 4499.182956731997,
    "output_throughput": 3894.6082451629304,
    "total_throughput": 8393.791201894928,
    "itl": 124.07050482878535,
    "ttft": 2227150.826980391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.44763725692394,
    "arrivals": 1242527,
    "finished_requests": 65269,
    "scheduler_time": 135.83269693498025
}
#Debug simulation 
Total elapsed time: 7.629989210050553. Arrivals time: 0.25858723651617765 Scheduler time: 7.230760951992124 Scheduler overhead time: 0.04443316673859954 Adapter cache time: 0.03220997238531709 Engine time: 0.04374016309157014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.507115738932043,
    "estimated_duration": 3600.0625527796406,
    "input_throughput": 3973.8056742803674,
    "output_throughput": 3449.3436761014236,
    "total_throughput": 7423.149350381791,
    "itl": 100.21197776679729,
    "ttft": 2284896.9795437045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.977995685516028,
    "arrivals": 1242527,
    "finished_requests": 57684,
    "scheduler_time": 148.10100137878342
}
#Debug simulation 
Total elapsed time: 5.5072425422258675. Arrivals time: 0.23166412115097046 Scheduler time: 5.0983108202926815 Scheduler overhead time: 0.05172849725931883 Adapter cache time: 0.050216000992804766 Engine time: 0.05133911455050111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.653279653284699,
    "estimated_duration": 3600.0056006016193,
    "input_throughput": 4499.860499465,
    "output_throughput": 3895.359217679127,
    "total_throughput": 8395.219717144128,
    "itl": 124.04849160329452,
    "ttft": 2227010.001158992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.766851796074867,
    "arrivals": 1242527,
    "finished_requests": 65280,
    "scheduler_time": 135.85676007387517
}
#Debug simulation 
Total elapsed time: 7.653449282981455. Arrivals time: 0.2602494964376092 Scheduler time: 7.252222833223641 Scheduler overhead time: 0.044525842647999525 Adapter cache time: 0.03219978278502822 Engine time: 0.043964306358247995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.522280368953943,
    "estimated_duration": 3600.107097087985,
    "input_throughput": 3973.77706112457,
    "output_throughput": 3449.3021082746177,
    "total_throughput": 7423.079169399188,
    "itl": 100.20563094129086,
    "ttft": 2285036.264718766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.802981671276584,
    "arrivals": 1242527,
    "finished_requests": 57686,
    "scheduler_time": 148.10988361354137
}
#Debug simulation 
Total elapsed time: 5.522373450919986. Arrivals time: 0.2269959137775004 Scheduler time: 5.117764621041715 Scheduler overhead time: 0.051663688849657774 Adapter cache time: 0.05058333929628134 Engine time: 0.05153827229514718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.640555575024337,
    "estimated_duration": 3600.0258335970684,
    "input_throughput": 4501.35380939984,
    "output_throughput": 3896.582871457821,
    "total_throughput": 8397.936680857661,
    "itl": 124.02268112692695,
    "ttft": 2227079.665213849,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.990836271993109,
    "arrivals": 1242527,
    "finished_requests": 65298,
    "scheduler_time": 135.8861397995435
}
#Debug simulation 
Total elapsed time: 7.640647829975933. Arrivals time: 0.25953861605376005 Scheduler time: 7.239731670822948 Scheduler overhead time: 0.04460869263857603 Adapter cache time: 0.03250481467694044 Engine time: 0.044048655312508345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.614573767408729,
    "estimated_duration": 3600.0182797499942,
    "input_throughput": 3973.8750995990754,
    "output_throughput": 3449.3872072400604,
    "total_throughput": 7423.262306839136,
    "itl": 100.20019314579307,
    "ttft": 2284950.8228144343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.604770533256293,
    "arrivals": 1242527,
    "finished_requests": 57686,
    "scheduler_time": 148.11419251208068
}
#Debug simulation 
Total elapsed time: 5.614665890112519. Arrivals time: 0.3201249707490206 Scheduler time: 5.117004821076989 Scheduler overhead time: 0.05172125808894634 Adapter cache time: 0.050531826447695494 Engine time: 0.051387523766607046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 9.023327051196247,
    "estimated_duration": 3600.1531569954586,
    "input_throughput": 4646.656481126229,
    "output_throughput": 4046.6350637588657,
    "total_throughput": 8693.291544885094,
    "itl": 135.0926488852513,
    "ttft": 2215224.5943828477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.901828793180362,
    "arrivals": 1238735,
    "finished_requests": 67498,
    "scheduler_time": 131.6741867855499
}
#Debug simulation 
Total elapsed time: 9.023396282922477. Arrivals time: 0.6686920095235109 Scheduler time: 8.225965358316898 Scheduler overhead time: 0.04196660639718175 Adapter cache time: 0.026660616043955088 Engine time: 0.041121650487184525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.467174083925784,
    "estimated_duration": 3600.038072445621,
    "input_throughput": 4462.19308705454,
    "output_throughput": 3893.6427109721167,
    "total_throughput": 8355.835798026656,
    "itl": 124.00502330313117,
    "ttft": 2233205.0221307124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.62408302254034,
    "arrivals": 1238735,
    "finished_requests": 64851,
    "scheduler_time": 135.9449726230305
}
#Debug simulation 
Total elapsed time: 7.467298964969814. Arrivals time: 0.25769133446738124 Scheduler time: 7.07022489188239 Scheduler overhead time: 0.04437304101884365 Adapter cache time: 0.03094823658466339 Engine time: 0.04381644632667303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.397549960762262,
    "estimated_duration": 3600.0398963635234,
    "input_throughput": 3953.093968312777,
    "output_throughput": 3443.238507584668,
    "total_throughput": 7396.332475897445,
    "itl": 100.0470792905956,
    "ttft": 2292454.1545801107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.409415964572737,
    "arrivals": 1238735,
    "finished_requests": 57325,
    "scheduler_time": 148.22308504061158
}
#Debug simulation 
Total elapsed time: 5.39767692796886. Arrivals time: 0.22778463922441006 Scheduler time: 4.9937401954084635 Scheduler overhead time: 0.051426037680357695 Adapter cache time: 0.04968388890847564 Engine time: 0.05126662412658334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.455547451041639,
    "estimated_duration": 3600.0636395345223,
    "input_throughput": 4463.296654966234,
    "output_throughput": 3894.565042137098,
    "total_throughput": 8357.861697103332,
    "itl": 123.98411614489201,
    "ttft": 2233002.244055156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.968842723136818,
    "arrivals": 1238735,
    "finished_requests": 64867,
    "scheduler_time": 135.96939649064927
}
#Debug simulation 
Total elapsed time: 7.4556916621513665. Arrivals time: 0.2556091812439263 Scheduler time: 7.061089699622244 Scheduler overhead time: 0.044318224769085646 Adapter cache time: 0.030172129161655903 Engine time: 0.04404411371797323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.455615299753845,
    "estimated_duration": 3600.0888066972457,
    "input_throughput": 3953.2102579037437,
    "output_throughput": 3443.2750594761833,
    "total_throughput": 7396.485317379927,
    "itl": 100.04205927610516,
    "ttft": 2292432.9758527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.237922942335743,
    "arrivals": 1238735,
    "finished_requests": 57328,
    "scheduler_time": 148.23196850067063
}
#Debug simulation 
Total elapsed time: 5.455705089028925. Arrivals time: 0.22893360815942287 Scheduler time: 5.048475122079253 Scheduler overhead time: 0.05214454839006066 Adapter cache time: 0.05029479553923011 Engine time: 0.05195564078167081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.576852626167238,
    "estimated_duration": 3600.041390547953,
    "input_throughput": 4464.116174384838,
    "output_throughput": 3895.686320946812,
    "total_throughput": 8359.802495331649,
    "itl": 123.95723216223286,
    "ttft": 2233012.359767939,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.26306928476797,
    "arrivals": 1238735,
    "finished_requests": 64883,
    "scheduler_time": 135.99417475915138
}
#Debug simulation 
Total elapsed time: 7.576943790074438. Arrivals time: 0.35139433527365327 Scheduler time: 7.086101067718118 Scheduler overhead time: 0.04431495303288102 Adapter cache time: 0.0308640468865633 Engine time: 0.0439706277102232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.810686080250889,
    "estimated_duration": 3600.008634224866,
    "input_throughput": 3953.4699624587065,
    "output_throughput": 3443.4556301193816,
    "total_throughput": 7396.925592578088,
    "itl": 100.03716714910068,
    "ttft": 2292378.8971163156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.047996491380044,
    "arrivals": 1238735,
    "finished_requests": 57329,
    "scheduler_time": 148.236300054526
}
#Debug simulation 
Total elapsed time: 5.810784283094108. Arrivals time: 0.6143669420853257 Scheduler time: 5.018965116236359 Scheduler overhead time: 0.051696160808205605 Adapter cache time: 0.0500637786462903 Engine time: 0.05159415816888213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 9.030471466947347,
    "estimated_duration": 3600.0087006629574,
    "input_throughput": 4656.806522971107,
    "output_throughput": 4058.805746027554,
    "total_throughput": 8715.61226899866,
    "itl": 135.65791900217775,
    "ttft": 2208355.4095854894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1039,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.870292984196139,
    "arrivals": 1236839,
    "finished_requests": 67949,
    "scheduler_time": 131.49587269012002
}
#Debug simulation 
Total elapsed time: 9.030607804190367. Arrivals time: 0.4093643454834819 Scheduler time: 8.495464419480413 Scheduler overhead time: 0.04167730128392577 Adapter cache time: 0.023865888360887766 Engine time: 0.0414037867449224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.537168181035668,
    "estimated_duration": 3600.047576328614,
    "input_throughput": 4472.513115068418,
    "output_throughput": 3895.492407436975,
    "total_throughput": 8368.005522505393,
    "itl": 124.14750360112568,
    "ttft": 2226372.2584443083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1459,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.66626788029903,
    "arrivals": 1236839,
    "finished_requests": 65223,
    "scheduler_time": 135.82916142801176
}
#Debug simulation 
Total elapsed time: 7.537264101207256. Arrivals time: 0.2591810473240912 Scheduler time: 7.138302845414728 Scheduler overhead time: 0.04438958829268813 Adapter cache time: 0.031043097842484713 Engine time: 0.044058158062398434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.579524067230523,
    "estimated_duration": 3600.081635299883,
    "input_throughput": 3957.930525876279,
    "output_throughput": 3449.585108912545,
    "total_throughput": 7407.515634788823,
    "itl": 100.22022096308407,
    "ttft": 2284593.9982147953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.468430173778966,
    "arrivals": 1236839,
    "finished_requests": 57665,
    "scheduler_time": 148.08885476784513
}
#Debug simulation 
Total elapsed time: 5.579622852150351. Arrivals time: 0.3696142779663205 Scheduler time: 5.033096949569881 Scheduler overhead time: 0.05147792026400566 Adapter cache time: 0.04983532661572099 Engine time: 0.051592915784567595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.543755752965808,
    "estimated_duration": 3600.1277292703257,
    "input_throughput": 4470.521939859624,
    "output_throughput": 3897.248668686464,
    "total_throughput": 8367.770608546089,
    "itl": 124.10640311141692,
    "ttft": 2226178.58878784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.775379690020396,
    "arrivals": 1236839,
    "finished_requests": 65223,
    "scheduler_time": 135.88969947764915
}
#Debug simulation 
Total elapsed time: 7.543846318963915. Arrivals time: 0.4044372597709298 Scheduler time: 7.000753598287702 Scheduler overhead time: 0.04405712662264705 Adapter cache time: 0.030419350136071444 Engine time: 0.044027730356901884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.596119554713368,
    "estimated_duration": 3600.0236684288407,
    "input_throughput": 3957.9942556929464,
    "output_throughput": 3449.6406534515745,
    "total_throughput": 7407.634909144521,
    "itl": 100.21565844438196,
    "ttft": 2284534.744427643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.30087237789765,
    "arrivals": 1236839,
    "finished_requests": 57665,
    "scheduler_time": 148.09319202957855
}
#Debug simulation 
Total elapsed time: 5.596214904915541. Arrivals time: 0.3760901684872806 Scheduler time: 5.043095807544887 Scheduler overhead time: 0.05156945623457432 Adapter cache time: 0.04948546318337321 Engine time: 0.05193744832649827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.570379867684096,
    "estimated_duration": 3600.0930650233913,
    "input_throughput": 4471.195246697161,
    "output_throughput": 3898.192281845585,
    "total_throughput": 8369.387528542746,
    "itl": 124.08321628544209,
    "ttft": 2226126.603631115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.07155165655083,
    "arrivals": 1236839,
    "finished_requests": 65237,
    "scheduler_time": 135.9141242949273
}
#Debug simulation 
Total elapsed time: 7.570481159724295. Arrivals time: 0.275666322093457 Scheduler time: 7.155781285837293 Scheduler overhead time: 0.044346464797854424 Adapter cache time: 0.03059692680835724 Engine time: 0.04381656274199486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.566999301780015,
    "estimated_duration": 3600.04850346214,
    "input_throughput": 3958.1091716671235,
    "output_throughput": 3449.759354091049,
    "total_throughput": 7407.868525758173,
    "itl": 100.21071109130168,
    "ttft": 2284556.5400704592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.106803583409643,
    "arrivals": 1236839,
    "finished_requests": 57667,
    "scheduler_time": 148.10209481599992
}
#Debug simulation 
Total elapsed time: 5.567092130891979. Arrivals time: 0.22943482594564557 Scheduler time: 5.161050578113645 Scheduler overhead time: 0.05160412145778537 Adapter cache time: 0.049505711533129215 Engine time: 0.05172597197815776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.422732316888869,
    "estimated_duration": 3600.1025864956914,
    "input_throughput": 4686.490619263966,
    "output_throughput": 4062.825891369222,
    "total_throughput": 8749.316510633189,
    "itl": 135.93961074730458,
    "ttft": 2203500.9957549777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.326549207400699,
    "arrivals": 1235881,
    "finished_requests": 68442,
    "scheduler_time": 131.3505444146851
}
#Debug simulation 
Total elapsed time: 8.422839046921581. Arrivals time: 0.27382415160536766 Scheduler time: 8.023151699919254 Scheduler overhead time: 0.041447647381573915 Adapter cache time: 0.024553495924919844 Engine time: 0.041222119238227606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.185347264166921,
    "estimated_duration": 3600.038615294672,
    "input_throughput": 4501.715879142999,
    "output_throughput": 3901.424818147502,
    "total_throughput": 8403.1406972905,
    "itl": 124.34840917047585,
    "ttft": 2223036.0924870335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1480,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.811435987837509,
    "arrivals": 1235881,
    "finished_requests": 65726,
    "scheduler_time": 135.67427935184244
}
#Debug simulation 
Total elapsed time: 7.185471238102764. Arrivals time: 0.25766287650913 Scheduler time: 6.78811957873404 Scheduler overhead time: 0.04446820728480816 Adapter cache time: 0.030994959641247988 Engine time: 0.043920256197452545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.401739861816168,
    "estimated_duration": 3600.0489443943443,
    "input_throughput": 3971.7998896277627,
    "output_throughput": 3447.786458383324,
    "total_throughput": 7419.586348011087,
    "itl": 99.97173090279871,
    "ttft": 2282296.204058368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.519970073676618,
    "arrivals": 1235881,
    "finished_requests": 58018,
    "scheduler_time": 148.19825357805402
}
#Debug simulation 
Total elapsed time: 5.401831741910428. Arrivals time: 0.22808144986629486 Scheduler time: 4.999182525090873 Scheduler overhead time: 0.05166506627574563 Adapter cache time: 0.04743947787210345 Engine time: 0.05152540374547243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.428614416625351,
    "estimated_duration": 3600.04132687467,
    "input_throughput": 4501.727766016893,
    "output_throughput": 3902.841585487483,
    "total_throughput": 8404.569351504377,
    "itl": 124.20817280233436,
    "ttft": 2223379.8115722034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.813881783769476,
    "arrivals": 1235881,
    "finished_requests": 65749,
    "scheduler_time": 135.7780537165144
}
#Debug simulation 
Total elapsed time: 7.428708379622549. Arrivals time: 0.2570918737910688 Scheduler time: 7.035153935197741 Scheduler overhead time: 0.044306543190032244 Adapter cache time: 0.028036680538207293 Engine time: 0.04397132620215416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.412336601875722,
    "estimated_duration": 3600.1037485248085,
    "input_throughput": 3972.0433073239974,
    "output_throughput": 3447.9464668445685,
    "total_throughput": 7419.989774168565,
    "itl": 99.96626393921856,
    "ttft": 2282477.6757100583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.360075613330046,
    "arrivals": 1235881,
    "finished_requests": 58023,
    "scheduler_time": 148.20700140625726
}
#Debug simulation 
Total elapsed time: 5.412428348790854. Arrivals time: 0.22613482736051083 Scheduler time: 5.010970242321491 Scheduler overhead time: 0.051724434830248356 Adapter cache time: 0.047796761617064476 Engine time: 0.05181732401251793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.292302820831537,
    "estimated_duration": 3600.1102782890316,
    "input_throughput": 4496.821693943753,
    "output_throughput": 3900.1688600142174,
    "total_throughput": 8396.99055395797,
    "itl": 123.95974804382517,
    "ttft": 2223001.6683033197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.592757586007975,
    "arrivals": 1235881,
    "finished_requests": 65662,
    "scheduler_time": 135.89111206648
}
#Debug simulation 
Total elapsed time: 7.292394550982863. Arrivals time: 0.25476378155872226 Scheduler time: 6.899532209150493 Scheduler overhead time: 0.04458619328215718 Adapter cache time: 0.029337922111153603 Engine time: 0.04393542557954788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.387770814821124,
    "estimated_duration": 3600.0282788939207,
    "input_throughput": 3972.4176845615802,
    "output_throughput": 3448.1470806151347,
    "total_throughput": 7420.564765176715,
    "itl": 99.96097057214713,
    "ttft": 2282434.247401927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.177605380732476,
    "arrivals": 1235881,
    "finished_requests": 58027,
    "scheduler_time": 148.2112797690857
}
#Debug simulation 
Total elapsed time: 5.387861377559602. Arrivals time: 0.2247826959937811 Scheduler time: 4.988570116460323 Scheduler overhead time: 0.05173963634297252 Adapter cache time: 0.04753149673342705 Engine time: 0.051417458802461624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 9.266294573899359,
    "estimated_duration": 3600.0599969586183,
    "input_throughput": 4655.042697665528,
    "output_throughput": 4049.335570050381,
    "total_throughput": 8704.378267715909,
    "itl": 135.37066265075973,
    "ttft": 2203189.905180739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.09206241893139,
    "arrivals": 1142437,
    "finished_requests": 67604,
    "scheduler_time": 131.42713937523624
}
#Debug simulation 
Total elapsed time: 9.266391299664974. Arrivals time: 0.39087424986064434 Scheduler time: 8.743928209878504 Scheduler overhead time: 0.0420945449732244 Adapter cache time: 0.02909606834873557 Engine time: 0.04150168225169182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.753247840330005,
    "estimated_duration": 3600.126562827683,
    "input_throughput": 4466.489363465651,
    "output_throughput": 3891.211254805689,
    "total_throughput": 8357.70061827134,
    "itl": 124.07780781588721,
    "ttft": 2221938.4548597108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1774,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.972894043894305,
    "arrivals": 1142437,
    "finished_requests": 64868,
    "scheduler_time": 135.71176423931456
}
#Debug simulation 
Total elapsed time: 7.753341128118336. Arrivals time: 0.3858978101052344 Scheduler time: 7.223091296851635 Scheduler overhead time: 0.04437145171687007 Adapter cache time: 0.03557753609493375 Engine time: 0.04406128590926528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.905495012644678,
    "estimated_duration": 3600.0716292384404,
    "input_throughput": 3927.5157430662116,
    "output_throughput": 3435.2083162901167,
    "total_throughput": 7362.724059356328,
    "itl": 99.85769641212443,
    "ttft": 2281795.1294385255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.53231938549369,
    "arrivals": 1142437,
    "finished_requests": 57204,
    "scheduler_time": 148.12630359768164
}
#Debug simulation 
Total elapsed time: 5.90555765805766. Arrivals time: 0.7379781347699463 Scheduler time: 4.9826888288371265 Scheduler overhead time: 0.051730957347899675 Adapter cache time: 0.05806063348427415 Engine time: 0.05141776567324996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.506877246312797,
    "estimated_duration": 3600.104433073651,
    "input_throughput": 4466.34987926503,
    "output_throughput": 3891.252951248319,
    "total_throughput": 8357.60283051335,
    "itl": 123.98369538153167,
    "ttft": 2221939.131850843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1901,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.064553539171062,
    "arrivals": 1142437,
    "finished_requests": 64857,
    "scheduler_time": 135.75102248108598
}
#Debug simulation 
Total elapsed time: 7.5070336470380425. Arrivals time: 0.3739939611405134 Scheduler time: 6.986841611564159 Scheduler overhead time: 0.0445239064283669 Adapter cache time: 0.03754508541896939 Engine time: 0.04386913636699319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_256_slots_96_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.523343927692622,
    "estimated_duration": 3600.084463593491,
    "input_throughput": 3927.502019185005,
    "output_throughput": 3435.3030116563627,
    "total_throughput": 7362.805030841368,
    "itl": 99.85222034516339,
    "ttft": 2281751.5381932925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.3256164432322,
    "arrivals": 1142437,
    "finished_requests": 57205,
    "scheduler_time": 148.13516480716834
}
#Debug simulation 
Total elapsed time: 5.523435309063643. Arrivals time: 0.22077917587012053 Scheduler time: 5.117290661204606 Scheduler overhead time: 0.051626159343868494 Adapter cache time: 0.05815257504582405 Engine time: 0.05171157093718648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_256_slots_96_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_256_slots_96_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.471840772777796,
    "estimated_duration": 3600.006761893174,
    "input_throughput": 4467.060220615552,
    "output_throughput": 3892.0321340262444,
    "total_throughput": 8359.092354641796,
    "itl": 123.95204669975594,
    "ttft": 2221742.511480421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1902,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.142217628965666,
    "arrivals": 1142437,
    "finished_requests": 64866,
    "scheduler_time": 135.78113151120175
}
#Debug simulation 
Total elapsed time: 7.471931994892657. Arrivals time: 0.3727467521093786 Scheduler time: 6.953928254079074 Scheduler overhead time: 0.0443609906360507 Adapter cache time: 0.03692032163962722 Engine time: 0.04379461985081434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_256_slots_96_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_256_slots_96_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.539164307992905,
    "estimated_duration": 3600.07800442775,
    "input_throughput": 3927.7368386487638,
    "output_throughput": 3435.647778961403,
    "total_throughput": 7363.384617610167,
    "itl": 99.84683971256408,
    "ttft": 2281679.3113210695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.100272955075383,
    "arrivals": 1142437,
    "finished_requests": 57209,
    "scheduler_time": 148.14404808821757
}
#Debug simulation 
Total elapsed time: 5.539255557581782. Arrivals time: 0.34836661256849766 Scheduler time: 5.005731469485909 Scheduler overhead time: 0.051812035497277975 Adapter cache time: 0.058162638917565346 Engine time: 0.05139396293088794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.157880674116313,
    "estimated_duration": 3600.0188409330644,
    "input_throughput": 4624.596630079011,
    "output_throughput": 4039.3163598641218,
    "total_throughput": 8663.912989943134,
    "itl": 134.70057122393587,
    "ttft": 2201936.6345062032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.343333962145495,
    "arrivals": 1127186,
    "finished_requests": 67482,
    "scheduler_time": 131.6926713445831
}
#Debug simulation 
Total elapsed time: 8.157975437119603. Arrivals time: 0.26736182533204556 Scheduler time: 7.758363575208932 Scheduler overhead time: 0.04214385570958257 Adapter cache time: 0.029469128232449293 Engine time: 0.04164821282029152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.0384513372555375,
    "estimated_duration": 3600.1222728920457,
    "input_throughput": 4446.755356211771,
    "output_throughput": 3887.3651890599713,
    "total_throughput": 8334.120545271742,
    "itl": 123.85849056042689,
    "ttft": 2219626.599285961,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1710,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.51573536963186,
    "arrivals": 1127186,
    "finished_requests": 64852,
    "scheduler_time": 135.8312048982825
}
#Debug simulation 
Total elapsed time: 7.038541066925973. Arrivals time: 0.2585247722454369 Scheduler time: 6.63798814220354 Scheduler overhead time: 0.04424804961308837 Adapter cache time: 0.03368383552879095 Engine time: 0.04376077465713024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.668412759900093,
    "estimated_duration": 3600.016686604379,
    "input_throughput": 3926.280967695225,
    "output_throughput": 3439.488224061316,
    "total_throughput": 7365.769191756542,
    "itl": 99.98398372348119,
    "ttft": 2279184.075637368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2841,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.34639047564035,
    "arrivals": 1127186,
    "finished_requests": 57309,
    "scheduler_time": 148.09743784041225
}
#Debug simulation 
Total elapsed time: 5.668478562962264. Arrivals time: 0.604895340744406 Scheduler time: 4.883621311746538 Scheduler overhead time: 0.05152119742706418 Adapter cache time: 0.05286052357405424 Engine time: 0.05179622443392873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.060653233900666,
    "estimated_duration": 3600.0754021630505,
    "input_throughput": 4443.929977240909,
    "output_throughput": 3884.777522047741,
    "total_throughput": 8328.70749928865,
    "itl": 123.53790977259334,
    "ttft": 2220435.6252796818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.369726284118483,
    "arrivals": 1127186,
    "finished_requests": 64787,
    "scheduler_time": 135.99587610216113
}
#Debug simulation 
Total elapsed time: 7.060805619228631. Arrivals time: 0.24573301058262587 Scheduler time: 6.672788561321795 Scheduler overhead time: 0.044348426163196564 Adapter cache time: 0.03378423769026995 Engine time: 0.044002513866871595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_256_slots_96_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.218013694975525,
    "estimated_duration": 3600.0444395022896,
    "input_throughput": 3926.5229186877123,
    "output_throughput": 3439.676706244205,
    "total_throughput": 7366.199624931917,
    "itl": 99.97934261474748,
    "ttft": 2279128.1055137254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2841,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.1599850166871,
    "arrivals": 1127186,
    "finished_requests": 57311,
    "scheduler_time": 148.10617335799532
}
#Debug simulation 
Total elapsed time: 5.218120803125203. Arrivals time: 0.22092235134914517 Scheduler time: 4.8183158109895885 Scheduler overhead time: 0.05153399193659425 Adapter cache time: 0.05272969650104642 Engine time: 0.05097092408686876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_256_slots_96_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_256_slots_96_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.4678730559535325,
    "estimated_duration": 3600.0986201798023,
    "input_throughput": 4445.428775281912,
    "output_throughput": 3886.451588179328,
    "total_throughput": 8331.88036346124,
    "itl": 123.50714302737791,
    "ttft": 2220522.474191552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.559005235703962,
    "arrivals": 1127186,
    "finished_requests": 64812,
    "scheduler_time": 136.02615634140506
}
#Debug simulation 
Total elapsed time: 7.467977399006486. Arrivals time: 0.6140186358243227 Scheduler time: 6.709415323566645 Scheduler overhead time: 0.044700285885483027 Adapter cache time: 0.034004634246230125 Engine time: 0.04546624328941107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_256_slots_96_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_256_slots_96_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.280107546132058,
    "estimated_duration": 3600.0585197554574,
    "input_throughput": 3926.9994980415813,
    "output_throughput": 3440.269076748589,
    "total_throughput": 7367.2685747901705,
    "itl": 99.97512904675612,
    "ttft": 2279048.986599239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2842,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.959494093209738,
    "arrivals": 1127186,
    "finished_requests": 57320,
    "scheduler_time": 148.11497335288934
}
#Debug simulation 
Total elapsed time: 5.280228433199227. Arrivals time: 0.2194395288825035 Scheduler time: 4.880409855861217 Scheduler overhead time: 0.051814957056194544 Adapter cache time: 0.05290960427373648 Engine time: 0.05173490708693862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.100705679971725,
    "estimated_duration": 3600.143891824973,
    "input_throughput": 4627.864191159947,
    "output_throughput": 4055.4962353459796,
    "total_throughput": 8683.360426505926,
    "itl": 135.53092194969102,
    "ttft": 2201843.3729670323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 988,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.533060123566681,
    "arrivals": 1119422,
    "finished_requests": 67806,
    "scheduler_time": 131.3519665064388
}
#Debug simulation 
Total elapsed time: 8.100801958236843. Arrivals time: 0.2583118765614927 Scheduler time: 7.717657315079123 Scheduler overhead time: 0.041828375309705734 Adapter cache time: 0.022751500364392996 Engine time: 0.04137415625154972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.757610966917127,
    "estimated_duration": 3600.0138737111533,
    "input_throughput": 4439.270947454762,
    "output_throughput": 3897.18859209199,
    "total_throughput": 8336.459539546751,
    "itl": 123.94822785817077,
    "ttft": 2221741.1705372166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.539420498534948,
    "arrivals": 1119422,
    "finished_requests": 65071,
    "scheduler_time": 135.71165353826993
}
#Debug simulation 
Total elapsed time: 6.757700054906309. Arrivals time: 0.24483725661411881 Scheduler time: 6.375466681551188 Scheduler overhead time: 0.0439542792737484 Adapter cache time: 0.029812444932758808 Engine time: 0.04340825276449323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.0905746119096875,
    "estimated_duration": 3600.1174280975174,
    "input_throughput": 3922.705378936175,
    "output_throughput": 3451.21686949136,
    "total_throughput": 7373.922248427535,
    "itl": 100.2029797512822,
    "ttft": 2280799.799029725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2733,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.55448845183028,
    "arrivals": 1119422,
    "finished_requests": 57502,
    "scheduler_time": 147.86151847340875
}
#Debug simulation 
Total elapsed time: 5.090664312243462. Arrivals time: 0.22453557327389717 Scheduler time: 4.688576226122677 Scheduler overhead time: 0.051269319374114275 Adapter cache time: 0.05140261957421899 Engine time: 0.051148229744285345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.192032958846539,
    "estimated_duration": 3600.077563331162,
    "input_throughput": 4439.73934417416,
    "output_throughput": 3897.5788030012704,
    "total_throughput": 8337.31814717543,
    "itl": 123.92535865733919,
    "ttft": 2221681.298629217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.90500351373112,
    "arrivals": 1119422,
    "finished_requests": 65082,
    "scheduler_time": 135.7366692225166
}
#Debug simulation 
Total elapsed time: 7.192133630625904. Arrivals time: 0.6348825083114207 Scheduler time: 6.419199897442013 Scheduler overhead time: 0.044239426497370005 Adapter cache time: 0.030113790184259415 Engine time: 0.04353939043357968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.118672035168856,
    "estimated_duration": 3600.0533924843407,
    "input_throughput": 3922.8548747312584,
    "output_throughput": 3451.3954781724938,
    "total_throughput": 7374.250352903752,
    "itl": 100.19754339273587,
    "ttft": 2280826.4537483146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2733,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.3776103830013,
    "arrivals": 1119422,
    "finished_requests": 57504,
    "scheduler_time": 147.8659007952715
}
#Debug simulation 
Total elapsed time: 5.118764230050147. Arrivals time: 0.2893782523460686 Scheduler time: 4.6525067910552025 Scheduler overhead time: 0.05116176791489124 Adapter cache time: 0.051016774494200945 Engine time: 0.05102429073303938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 6.79099583812058,
    "estimated_duration": 3600.0594086217448,
    "input_throughput": 4440.232003315682,
    "output_throughput": 3898.2223366621456,
    "total_throughput": 8338.454339977829,
    "itl": 123.90307945952914,
    "ttft": 2221423.130680501,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.192846154421686,
    "arrivals": 1119422,
    "finished_requests": 65089,
    "scheduler_time": 135.76179999221313
}
#Debug simulation 
Total elapsed time: 6.791098155081272. Arrivals time: 0.2425162154249847 Scheduler time: 6.410940895788372 Scheduler overhead time: 0.04430526681244373 Adapter cache time: 0.029892565216869116 Engine time: 0.04325907537713647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.082991652190685,
    "estimated_duration": 3600.074475512849,
    "input_throughput": 3923.290780807512,
    "output_throughput": 3451.7760908903865,
    "total_throughput": 7375.066871697899,
    "itl": 100.19198549173309,
    "ttft": 2280832.189238405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2733,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.172357260976113,
    "arrivals": 1119422,
    "finished_requests": 57510,
    "scheduler_time": 147.874979154996
}
#Debug simulation 
Total elapsed time: 5.083082198165357. Arrivals time: 0.21855484787374735 Scheduler time: 4.68792832782492 Scheduler overhead time: 0.05109550151973963 Adapter cache time: 0.05093460436910391 Engine time: 0.05107989069074392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.549908638000488,
    "estimated_duration": 3600.0844669387325,
    "input_throughput": 4657.529331320924,
    "output_throughput": 4056.244272628761,
    "total_throughput": 8713.773603949685,
    "itl": 135.46868218813478,
    "ttft": 2197575.659298731,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.513222896470831,
    "arrivals": 1115519,
    "finished_requests": 68107,
    "scheduler_time": 131.36246299391578
}
#Debug simulation 
Total elapsed time: 7.549998035654426. Arrivals time: 0.253969166893512 Scheduler time: 7.173266201280057 Scheduler overhead time: 0.041206360794603825 Adapter cache time: 0.022621544543653727 Engine time: 0.04034811398014426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.860587990377098,
    "estimated_duration": 3600.0479605792893,
    "input_throughput": 4476.706470712321,
    "output_throughput": 3897.9805696093954,
    "total_throughput": 8374.687040321716,
    "itl": 123.88937293535338,
    "ttft": 2217157.3712869408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.851779768420359,
    "arrivals": 1115519,
    "finished_requests": 65458,
    "scheduler_time": 135.7640138346352
}
#Debug simulation 
Total elapsed time: 6.860681379213929. Arrivals time: 0.2572056818753481 Scheduler time: 6.469324884470552 Scheduler overhead time: 0.04408192588016391 Adapter cache time: 0.02634691772982478 Engine time: 0.043611353263258934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.029081752058119,
    "estimated_duration": 3600.0606724094378,
    "input_throughput": 3947.4334721389305,
    "output_throughput": 3450.2907395965467,
    "total_throughput": 7397.724211735477,
    "itl": 100.12235819683735,
    "ttft": 2276186.4265142907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.37678561507264,
    "arrivals": 1115519,
    "finished_requests": 57839,
    "scheduler_time": 147.9358042540808
}
#Debug simulation 
Total elapsed time: 5.029172771144658. Arrivals time: 0.21766421059146523 Scheduler time: 4.636297870893031 Scheduler overhead time: 0.05124170472845435 Adapter cache time: 0.049484943971037865 Engine time: 0.05094356928020716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 6.846669846680015,
    "estimated_duration": 3600.0725453727077,
    "input_throughput": 4477.803376690313,
    "output_throughput": 3899.0039292518786,
    "total_throughput": 8376.807305942191,
    "itl": 123.8727396864044,
    "ttft": 2217061.5015251026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.335361566348007,
    "arrivals": 1115519,
    "finished_requests": 65472,
    "scheduler_time": 135.78325149355467
}
#Debug simulation 
Total elapsed time: 6.846828127745539. Arrivals time: 0.2460131263360381 Scheduler time: 6.466638767160475 Scheduler overhead time: 0.04416733793914318 Adapter cache time: 0.026068035513162613 Engine time: 0.04378482000902295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.019180630333722,
    "estimated_duration": 3600.0136537816197,
    "input_throughput": 3947.549194729267,
    "output_throughput": 3450.379413686939,
    "total_throughput": 7397.928608416206,
    "itl": 100.11811448263275,
    "ttft": 2276126.3687110683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.222481821980367,
    "arrivals": 1115519,
    "finished_requests": 57840,
    "scheduler_time": 147.94002965724368
}
#Debug simulation 
Total elapsed time: 5.019270891323686. Arrivals time: 0.21537476032972336 Scheduler time: 4.628546049818397 Scheduler overhead time: 0.05101624596863985 Adapter cache time: 0.049422761891037226 Engine time: 0.05124192498624325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.195992003194988,
    "estimated_duration": 3600.0112779874808,
    "input_throughput": 4478.5443030648385,
    "output_throughput": 3899.8397271267722,
    "total_throughput": 8378.384030191612,
    "itl": 123.85138518584354,
    "ttft": 2216989.7753190696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.730928259030876,
    "arrivals": 1115519,
    "finished_requests": 65484,
    "scheduler_time": 135.80279570130392
}
#Debug simulation 
Total elapsed time: 7.196056867018342. Arrivals time: 0.24681992502883077 Scheduler time: 6.8146588946692646 Scheduler overhead time: 0.04415051732212305 Adapter cache time: 0.026314300950616598 Engine time: 0.04384398739784956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_256_slots_96_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.118487742263824,
    "estimated_duration": 3600.0358554694717,
    "input_throughput": 3947.8470689138726,
    "output_throughput": 3450.507300122455,
    "total_throughput": 7398.354369036328,
    "itl": 100.11338118410633,
    "ttft": 2276112.925084047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.030277082081902,
    "arrivals": 1115519,
    "finished_requests": 57844,
    "scheduler_time": 147.9488228456168
}
#Debug simulation 
Total elapsed time: 5.118580020964146. Arrivals time: 0.29202734492719173 Scheduler time: 4.649634633678943 Scheduler overhead time: 0.05153605015948415 Adapter cache time: 0.049453807063400745 Engine time: 0.05217448528856039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_256_slots_96_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.5764039172790945,
    "estimated_duration": 3600.0598824469694,
    "input_throughput": 4674.463911572973,
    "output_throughput": 4061.5846617699667,
    "total_throughput": 8736.04857334294,
    "itl": 135.91072717903032,
    "ttft": 2194494.192056996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 849,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.61393526812561,
    "arrivals": 1113643,
    "finished_requests": 68242,
    "scheduler_time": 131.2863937677286
}
#Debug simulation 
Total elapsed time: 7.576517891138792. Arrivals time: 0.2553689172491431 Scheduler time: 7.199460220523179 Scheduler overhead time: 0.041599702555686235 Adapter cache time: 0.02031257189810276 Engine time: 0.04102880833670497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_256_slots_96_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.568171933759004,
    "estimated_duration": 3600.057009292331,
    "input_throughput": 4485.096196622165,
    "output_throughput": 3898.043826466661,
    "total_throughput": 8383.140023088827,
    "itl": 124.05006977382712,
    "ttft": 2213409.3528034724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.590577238416293,
    "arrivals": 1113643,
    "finished_requests": 65436,
    "scheduler_time": 135.8064838854934
}
#Debug simulation 
Total elapsed time: 6.568255432881415. Arrivals time: 0.23761223396286368 Scheduler time: 6.197588383220136 Scheduler overhead time: 0.04408515337854624 Adapter cache time: 0.025411109440028667 Engine time: 0.04357946803793311 
