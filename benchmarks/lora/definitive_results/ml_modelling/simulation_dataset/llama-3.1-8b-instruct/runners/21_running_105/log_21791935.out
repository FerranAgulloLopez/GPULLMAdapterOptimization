INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.639151312992908,
    "estimated_duration": 3599.9492080603404,
    "input_throughput": 2375.0515648543806,
    "output_throughput": 2091.401729541792,
    "total_throughput": 4466.453294396172,
    "itl": 28.038949896068903,
    "ttft": 5431.324721084887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.537892939706306
}
#Debug simulation 
Total elapsed time: 2.639249171013944. Arrivals time: 0.08665555145125836 Scheduler time: 2.2327106315642595 Scheduler overhead time: 0.11786629701964557 Adapter cache time: 0.027979694423265755 Engine time: 0.11816357006318867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6161965230712667,
    "estimated_duration": 3599.9492871074135,
    "input_throughput": 2375.051512703403,
    "output_throughput": 2091.401683619149,
    "total_throughput": 4466.453196322552,
    "itl": 28.039081515351015,
    "ttft": 5431.335033500211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.537994059907336
}
#Debug simulation 
Total elapsed time: 2.6162984190741554. Arrivals time: 0.08489157655276358 Scheduler time: 2.219224880915135 Scheduler overhead time: 0.11661650671157986 Adapter cache time: 0.02769642206840217 Engine time: 0.1126815986353904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6286726030521095,
    "estimated_duration": 3599.953979788752,
    "input_throughput": 2375.048416730517,
    "output_throughput": 2091.398957394951,
    "total_throughput": 4466.447374125468,
    "itl": 28.03896506902587,
    "ttft": 5431.245800032638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.537951360112931
}
#Debug simulation 
Total elapsed time: 2.628765919012949. Arrivals time: 0.08575372688937932 Scheduler time: 2.22701416851487 Scheduler overhead time: 0.11667696037329733 Adapter cache time: 0.027692435076460242 Engine time: 0.1162237407406792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.632794711040333,
    "estimated_duration": 3599.9576459518803,
    "input_throughput": 2375.0459980034684,
    "output_throughput": 2091.396827533853,
    "total_throughput": 4466.442825537321,
    "itl": 28.038801801921707,
    "ttft": 5431.1877644076585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.537854159597892
}
#Debug simulation 
Total elapsed time: 2.6328908359864727. Arrivals time: 0.08647194760851562 Scheduler time: 2.229128379840404 Scheduler overhead time: 0.11725736199878156 Adapter cache time: 0.027855351916514337 Engine time: 0.11673553683795035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_32_slots_32_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6300158778904006,
    "estimated_duration": 3599.9538317191864,
    "input_throughput": 2375.0485144185445,
    "output_throughput": 2091.399043416203,
    "total_throughput": 4466.447557834747,
    "itl": 28.03908472935108,
    "ttft": 5431.261013011157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.537946606440876
}
#Debug simulation 
Total elapsed time: 2.6301039059180766. Arrivals time: 0.08589130500331521 Scheduler time: 2.2297381575917825 Scheduler overhead time: 0.11760508641600609 Adapter cache time: 0.02766756236087531 Engine time: 0.11363560589961708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_32_slots_32_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_32_slots_32_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.653693707077764,
    "estimated_duration": 3599.9615183951346,
    "input_throughput": 2375.0434431898107,
    "output_throughput": 2091.394577838823,
    "total_throughput": 4466.438021028634,
    "itl": 28.03865627409508,
    "ttft": 5431.275500065986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.5377791013391136
}
#Debug simulation 
Total elapsed time: 2.6537814560579136. Arrivals time: 0.08615075307898223 Scheduler time: 2.2519112528534606 Scheduler overhead time: 0.11694608919788152 Adapter cache time: 0.027977134683169425 Engine time: 0.11538347858004272 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_32_slots_32_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_32_slots_32_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.630100334994495,
    "estimated_duration": 3599.9500211973727,
    "input_throughput": 2375.051028390716,
    "output_throughput": 2091.4012571474013,
    "total_throughput": 4466.452285538117,
    "itl": 28.039076751117104,
    "ttft": 5431.302968974157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.538005610589462
}
#Debug simulation 
Total elapsed time: 2.6302371280035004. Arrivals time: 0.08642400533426553 Scheduler time: 2.2281408617272973 Scheduler overhead time: 0.1159655973315239 Adapter cache time: 0.02792009070981294 Engine time: 0.11563883814960718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5797983340453357,
    "estimated_duration": 3599.6543605049587,
    "input_throughput": 2370.837070813328,
    "output_throughput": 2041.1901433140051,
    "total_throughput": 4412.0272141273335,
    "itl": 27.64775395837364,
    "ttft": 7079.150595839268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.538977801075834
}
#Debug simulation 
Total elapsed time: 2.5798847950063646. Arrivals time: 0.0851852431660518 Scheduler time: 2.1754990867339075 Scheduler overhead time: 0.11868563888128847 Adapter cache time: 0.02729458874091506 Engine time: 0.11733253987040371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5740328199462965,
    "estimated_duration": 3599.6549831467414,
    "input_throughput": 2370.8366607234093,
    "output_throughput": 2041.1897902439816,
    "total_throughput": 4412.026450967391,
    "itl": 27.64776073992962,
    "ttft": 7079.113902937775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.539006239854127
}
#Debug simulation 
Total elapsed time: 2.5741258290363476. Arrivals time: 0.08405551861505955 Scheduler time: 2.1747769196517766 Scheduler overhead time: 0.11761381418909878 Adapter cache time: 0.026948512298986316 Engine time: 0.11426457914058119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.570408441941254,
    "estimated_duration": 3599.6551112431466,
    "input_throughput": 2370.8365763554225,
    "output_throughput": 2041.1897176067243,
    "total_throughput": 4412.026293962147,
    "itl": 27.647927170228186,
    "ttft": 7079.2585063986535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.538999859184968
}
#Debug simulation 
Total elapsed time: 2.5704939799616113. Arrivals time: 0.08482194202952087 Scheduler time: 2.1690598854329437 Scheduler overhead time: 0.11769904126413167 Adapter cache time: 0.027158420882187784 Engine time: 0.11565958964638412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.597049416974187,
    "estimated_duration": 3599.6580848673875,
    "input_throughput": 2370.834617842434,
    "output_throughput": 2041.188031410124,
    "total_throughput": 4412.022649252558,
    "itl": 27.64770736219382,
    "ttft": 7079.2608160162445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.538892067095883
}
#Debug simulation 
Total elapsed time: 2.597142242011614. Arrivals time: 0.08576603222172707 Scheduler time: 2.189867397886701 Scheduler overhead time: 0.11896835139486939 Adapter cache time: 0.027314357343129814 Engine time: 0.1184962869156152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5969602749682963,
    "estimated_duration": 3599.6550837193386,
    "input_throughput": 2370.836594483396,
    "output_throughput": 2041.1897332141402,
    "total_throughput": 4412.026327697536,
    "itl": 27.647939285769787,
    "ttft": 7079.26108708343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.538997566496044
}
#Debug simulation 
Total elapsed time: 2.5970479550305754. Arrivals time: 0.08498105267062783 Scheduler time: 2.1956506917485967 Scheduler overhead time: 0.11796914716251194 Adapter cache time: 0.02725663164164871 Engine time: 0.11494685872457922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.6405600120779127,
    "estimated_duration": 3599.6648486453487,
    "input_throughput": 2370.8301630391084,
    "output_throughput": 2041.1841960134407,
    "total_throughput": 4412.0143590525495,
    "itl": 27.647622294586174,
    "ttft": 7079.232790209907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.538844363385378
}
#Debug simulation 
Total elapsed time: 2.6406503720209002. Arrivals time: 0.08659011533018202 Scheduler time: 2.2340933246305212 Scheduler overhead time: 0.11966207576915622 Adapter cache time: 0.027256985777057707 Engine time: 0.11670484289061278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6466398170450702,
    "estimated_duration": 3599.6549101454966,
    "input_throughput": 2370.836708804137,
    "output_throughput": 2041.1898316394484,
    "total_throughput": 4412.026540443585,
    "itl": 27.647845667309983,
    "ttft": 7079.215866524384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.539050732742563
}
#Debug simulation 
Total elapsed time: 2.646778477006592. Arrivals time: 0.08618748176377267 Scheduler time: 2.2398722261423245 Scheduler overhead time: 0.11944516573566943 Adapter cache time: 0.02723033376969397 Engine time: 0.11749169928953052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.6335107129998505,
    "estimated_duration": 3599.8050854124103,
    "input_throughput": 2343.3377085286224,
    "output_throughput": 2036.4005344917632,
    "total_throughput": 4379.738243020386,
    "itl": 27.516809392566767,
    "ttft": 8177.632622470866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.3235384868844315
}
#Debug simulation 
Total elapsed time: 2.633606643998064. Arrivals time: 0.08684339665342122 Scheduler time: 2.2264683953253552 Scheduler overhead time: 0.11882630689069629 Adapter cache time: 0.026188141549937427 Engine time: 0.11871720582712442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5977431329665706,
    "estimated_duration": 3599.8052569323495,
    "input_throughput": 2343.337596875599,
    "output_throughput": 2036.4004374633769,
    "total_throughput": 4379.738034338976,
    "itl": 27.51691273165469,
    "ttft": 8177.64984727793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.323642234165493
}
#Debug simulation 
Total elapsed time: 2.5978372170357034. Arrivals time: 0.08545732183847576 Scheduler time: 2.194364224211313 Scheduler overhead time: 0.11916899681091309 Adapter cache time: 0.026373190223239362 Engine time: 0.11587000079452991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6485436080256477,
    "estimated_duration": 3599.785225859196,
    "input_throughput": 2343.3506364220943,
    "output_throughput": 2036.411769052228,
    "total_throughput": 4379.762405474323,
    "itl": 27.5168604537732,
    "ttft": 8177.680240288511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.3236977772480945
}
#Debug simulation 
Total elapsed time: 2.6486323829740286. Arrivals time: 0.08645188179798424 Scheduler time: 2.2381594504695386 Scheduler overhead time: 0.12122339371126145 Adapter cache time: 0.026631297660060227 Engine time: 0.11920189706142992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.6257498339982703,
    "estimated_duration": 3599.785196467748,
    "input_throughput": 2343.350655555033,
    "output_throughput": 2036.4117856790788,
    "total_throughput": 4379.762441234112,
    "itl": 27.516796965924307,
    "ttft": 8177.647946828404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.323624136717349
}
#Debug simulation 
Total elapsed time: 2.6258411060553044. Arrivals time: 0.08578309009317309 Scheduler time: 2.21896168240346 Scheduler overhead time: 0.11915915820281953 Adapter cache time: 0.026312301284633577 Engine time: 0.1188915972597897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.621495291008614,
    "estimated_duration": 3599.7827623188887,
    "input_throughput": 2343.3522401129635,
    "output_throughput": 2036.413162687013,
    "total_throughput": 4379.765402799977,
    "itl": 27.51690825479532,
    "ttft": 8177.632807173648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.323667086755788
}
#Debug simulation 
Total elapsed time: 2.621582444058731. Arrivals time: 0.08586113026831299 Scheduler time: 2.2163228074787185 Scheduler overhead time: 0.11970013566315174 Adapter cache time: 0.02643603936303407 Engine time: 0.11674272187519819 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.6333677490474656,
    "estimated_duration": 3599.7953511062174,
    "input_throughput": 2343.3440452129457,
    "output_throughput": 2036.4060411787832,
    "total_throughput": 4379.750086391729,
    "itl": 27.516766477636786,
    "ttft": 8177.546582940375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.32351780422425
}
#Debug simulation 
Total elapsed time: 2.6334682480664924. Arrivals time: 0.088612015475519 Scheduler time: 2.2261478760046884 Scheduler overhead time: 0.11961958976462483 Adapter cache time: 0.026224745786748827 Engine time: 0.11634610372129828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_32_slots_32_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.648450869950466,
    "estimated_duration": 3599.8125192579064,
    "input_throughput": 2343.3328693848125,
    "output_throughput": 2036.3963291930538,
    "total_throughput": 4379.729198577867,
    "itl": 27.517048012925766,
    "ttft": 8177.610159924846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.3237705837928395
}
#Debug simulation 
Total elapsed time: 2.6485872250050306. Arrivals time: 0.08594888017978519 Scheduler time: 2.2417103388579562 Scheduler overhead time: 0.12025589926633984 Adapter cache time: 0.026282496051862836 Engine time: 0.11725695722270757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.626386731979437,
    "estimated_duration": 3599.875375761946,
    "input_throughput": 2325.191882018507,
    "output_throughput": 2028.658283331342,
    "total_throughput": 4353.85016534985,
    "itl": 27.456812003495514,
    "ttft": 6933.127264424706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.158066723908896
}
#Debug simulation 
Total elapsed time: 2.626485034939833. Arrivals time: 0.08571585710160434 Scheduler time: 2.2204789821989834 Scheduler overhead time: 0.12021269323304296 Adapter cache time: 0.02589777100365609 Engine time: 0.1172876387136057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.636113796965219,
    "estimated_duration": 3599.8962893700364,
    "input_throughput": 2325.1783738094236,
    "output_throughput": 2028.6464978350734,
    "total_throughput": 4353.824871644497,
    "itl": 27.456476337127615,
    "ttft": 6933.282070077459,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113982,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.157793720300097
}
#Debug simulation 
Total elapsed time: 2.636199973989278. Arrivals time: 0.08584873413201421 Scheduler time: 2.2277428798843175 Scheduler overhead time: 0.1204738316591829 Adapter cache time: 0.025784930330701172 Engine time: 0.11921090807300061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6328422460937873,
    "estimated_duration": 3599.8784952956607,
    "input_throughput": 2325.18986708537,
    "output_throughput": 2028.6565253642557,
    "total_throughput": 4353.8463924496255,
    "itl": 27.456480980463468,
    "ttft": 6933.192917369596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137895,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.157734924527519
}
#Debug simulation 
Total elapsed time: 2.63293496100232. Arrivals time: 0.08454184699803591 Scheduler time: 2.2311284409370273 Scheduler overhead time: 0.11962502705864608 Adapter cache time: 0.02594291151035577 Engine time: 0.11502473650034517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.6443619749043137,
    "estimated_duration": 3599.881714276407,
    "input_throughput": 2325.187787922218,
    "output_throughput": 2028.6547113584593,
    "total_throughput": 4353.842499280677,
    "itl": 27.456375748229863,
    "ttft": 6933.124547364357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.157676629242927
}
#Debug simulation 
Total elapsed time: 2.6444561938988045. Arrivals time: 0.08556058187969029 Scheduler time: 2.237588157178834 Scheduler overhead time: 0.11975900118704885 Adapter cache time: 0.02610126754734665 Engine time: 0.11886910849716514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.653316232957877,
    "estimated_duration": 3599.872575335989,
    "input_throughput": 2325.1936908402267,
    "output_throughput": 2028.659861472567,
    "total_throughput": 4353.853552312793,
    "itl": 27.45649392186207,
    "ttft": 6933.121861095593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.157815236946345
}
#Debug simulation 
Total elapsed time: 2.6534088850021362. Arrivals time: 0.08579187816940248 Scheduler time: 2.247388600721024 Scheduler overhead time: 0.11972054583020508 Adapter cache time: 0.025795692461542785 Engine time: 0.11785202042665333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.637028897064738,
    "estimated_duration": 3599.8861392841663,
    "input_throughput": 2325.1849297834865,
    "output_throughput": 2028.6522177204686,
    "total_throughput": 4353.837147503955,
    "itl": 27.456378575568205,
    "ttft": 6933.18675949464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.157708278843203
}
#Debug simulation 
Total elapsed time: 2.637117682956159. Arrivals time: 0.08539291901979595 Scheduler time: 2.2293369128601626 Scheduler overhead time: 0.12112540705129504 Adapter cache time: 0.026021640980616212 Engine time: 0.11817656620405614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.633643256034702,
    "estimated_duration": 3599.900697661759,
    "input_throughput": 2325.175526490723,
    "output_throughput": 2028.644013637226,
    "total_throughput": 4353.819540127948,
    "itl": 27.456376621892986,
    "ttft": 6933.192309473949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.157828455600448
}
#Debug simulation 
Total elapsed time: 2.6337836449965835. Arrivals time: 0.08566807489842176 Scheduler time: 2.2266249309759587 Scheduler overhead time: 0.11984789255075157 Adapter cache time: 0.025979013415053487 Engine time: 0.11884936806745827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5840551101136953,
    "estimated_duration": 3599.938521351946,
    "input_throughput": 2280.060326396208,
    "output_throughput": 1992.5462497360054,
    "total_throughput": 4272.606576132213,
    "itl": 27.04862061761893,
    "ttft": 8804.319071251251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 33289,
    "finished_requests": 33208,
    "scheduler_time": 5.436076803339658
}
#Debug simulation 
Total elapsed time: 2.584147741086781. Arrivals time: 0.0833404480945319 Scheduler time: 2.1786808385513723 Scheduler overhead time: 0.12180432444438338 Adapter cache time: 0.025551118538714945 Engine time: 0.11718995089177042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.59939204598777,
    "estimated_duration": 3599.9386405383416,
    "input_throughput": 2280.060250908207,
    "output_throughput": 1992.5461837669905,
    "total_throughput": 4272.606434675197,
    "itl": 27.04885852225329,
    "ttft": 8804.33289025612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2348263315111399,
    "arrivals": 33289,
    "finished_requests": 33208,
    "scheduler_time": 5.4361512778564185
}
#Debug simulation 
Total elapsed time: 2.5994780220789835. Arrivals time: 0.08441995771136135 Scheduler time: 2.1929740716004744 Scheduler overhead time: 0.12080454535316676 Adapter cache time: 0.025511628482490778 Engine time: 0.11846606503240764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.582567890989594,
    "estimated_duration": 3599.9434325367483,
    "input_throughput": 2280.0572158479913,
    "output_throughput": 1992.543531425831,
    "total_throughput": 4272.600747273822,
    "itl": 27.048857725880524,
    "ttft": 8804.328633023806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2413462840113789,
    "arrivals": 33289,
    "finished_requests": 33208,
    "scheduler_time": 5.4362497709773985
}
#Debug simulation 
Total elapsed time: 2.582659200998023. Arrivals time: 0.08420584397390485 Scheduler time: 2.1742263291962445 Scheduler overhead time: 0.12037664675153792 Adapter cache time: 0.025666537112556398 Engine time: 0.12090764520689845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.569106592098251,
    "estimated_duration": 3599.9483751288526,
    "input_throughput": 2280.0540854162136,
    "output_throughput": 1992.5407957393988,
    "total_throughput": 4272.594881155613,
    "itl": 27.04861450185637,
    "ttft": 8804.368510798997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 33289,
    "finished_requests": 33208,
    "scheduler_time": 5.43614960988438
}
#Debug simulation 
Total elapsed time: 2.5691958150127903. Arrivals time: 0.08459067123476416 Scheduler time: 2.1600536559708416 Scheduler overhead time: 0.12168915732763708 Adapter cache time: 0.025503951124846935 Engine time: 0.11992572818417102 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.57303316204343,
    "estimated_duration": 3599.940239074291,
    "input_throughput": 2280.0592384585448,
    "output_throughput": 1992.5452989865512,
    "total_throughput": 4272.604537445096,
    "itl": 27.048845827284328,
    "ttft": 8804.253650674495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861717,
    "arrivals": 33289,
    "finished_requests": 33208,
    "scheduler_time": 5.436243933075339
}
#Debug simulation 
Total elapsed time: 2.573121627094224. Arrivals time: 0.0833948029903695 Scheduler time: 2.1696070578182116 Scheduler overhead time: 0.12049591820687056 Adapter cache time: 0.02550896815955639 Engine time: 0.11685424263123423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.597188521991484,
    "estimated_duration": 3599.959723595662,
    "input_throughput": 2280.0468978029903,
    "output_throughput": 1992.5345144793785,
    "total_throughput": 4272.581412282369,
    "itl": 27.048622558588736,
    "ttft": 8804.268279865166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 33289,
    "finished_requests": 33208,
    "scheduler_time": 5.436050282777355
}
#Debug simulation 
Total elapsed time: 2.5972824250347912. Arrivals time: 0.08425916510168463 Scheduler time: 2.189749262528494 Scheduler overhead time: 0.12104803521651775 Adapter cache time: 0.025508379098027945 Engine time: 0.11939277488272637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5695115320850164,
    "estimated_duration": 3599.9386964851706,
    "input_throughput": 2280.0602154736753,
    "output_throughput": 1992.5461528007295,
    "total_throughput": 4272.606368274405,
    "itl": 27.0488678185354,
    "ttft": 8804.284009482815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585545,
    "arrivals": 33289,
    "finished_requests": 33208,
    "scheduler_time": 5.436221582443127
}
#Debug simulation 
Total elapsed time: 2.5696481050690636. Arrivals time: 0.0842047716723755 Scheduler time: 2.1630135698942468 Scheduler overhead time: 0.12086543918121606 Adapter cache time: 0.02543030632659793 Engine time: 0.11885956721380353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5558859689626843,
    "estimated_duration": 3599.954747814412,
    "input_throughput": 2280.2253292166056,
    "output_throughput": 1984.5982798360214,
    "total_throughput": 4264.823609052627,
    "itl": 26.865945722649474,
    "ttft": 6251.660003347444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.152175111485005
}
#Debug simulation 
Total elapsed time: 2.555976073956117. Arrivals time: 0.08358664799015969 Scheduler time: 2.1500118790427223 Scheduler overhead time: 0.12149291334208101 Adapter cache time: 0.024753086850978434 Engine time: 0.11863664956763387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5745826720958576,
    "estimated_duration": 3599.956307544303,
    "input_throughput": 2280.2243412780585,
    "output_throughput": 1984.5974199819025,
    "total_throughput": 4264.821761259961,
    "itl": 26.865980193507866,
    "ttft": 6251.670278522763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.152253047067803
}
#Debug simulation 
Total elapsed time: 2.574677392025478. Arrivals time: 0.08391133416444063 Scheduler time: 2.1681710401317105 Scheduler overhead time: 0.12118532310705632 Adapter cache time: 0.02457571600098163 Engine time: 0.11924871162045747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.554596984060481,
    "estimated_duration": 3599.9621084435353,
    "input_throughput": 2280.220666975043,
    "output_throughput": 1984.5942220455622,
    "total_throughput": 4264.814889020605,
    "itl": 26.86610513211646,
    "ttft": 6251.772622433151,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.152061981981872
}
#Debug simulation 
Total elapsed time: 2.554697575047612. Arrivals time: 0.08691923273727298 Scheduler time: 2.143242464517243 Scheduler overhead time: 0.1223143368260935 Adapter cache time: 0.0245537773007527 Engine time: 0.11993564304430038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.5480821720557287,
    "estimated_duration": 3599.963475672434,
    "input_throughput": 2280.219800970815,
    "output_throughput": 1984.5934683172006,
    "total_throughput": 4264.813269288015,
    "itl": 26.865913557400614,
    "ttft": 6251.76140574269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2195559008046984,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.152010108341339
}
#Debug simulation 
Total elapsed time: 2.5481665710685775. Arrivals time: 0.08305734628811479 Scheduler time: 2.143318005139008 Scheduler overhead time: 0.12102773389779031 Adapter cache time: 0.02481483027804643 Engine time: 0.11847070476505905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.559233153006062,
    "estimated_duration": 3599.9608664445154,
    "input_throughput": 2280.2214536591036,
    "output_throughput": 1984.5949067374715,
    "total_throughput": 4264.816360396575,
    "itl": 26.865976748007967,
    "ttft": 6251.723599512922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.152074116405994
}
#Debug simulation 
Total elapsed time: 2.55932486790698. Arrivals time: 0.08306250674650073 Scheduler time: 2.1547444391762838 Scheduler overhead time: 0.12110001232940704 Adapter cache time: 0.024587107822299004 Engine time: 0.11822069250047207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5631298050284386,
    "estimated_duration": 3599.9501344185646,
    "input_throughput": 2280.228251363211,
    "output_throughput": 1984.6008231316562,
    "total_throughput": 4264.829074494867,
    "itl": 26.86590685095946,
    "ttft": 6251.7793748552385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.151933048612549
}
#Debug simulation 
Total elapsed time: 2.563230118015781. Arrivals time: 0.08245995943434536 Scheduler time: 2.1588343299226835 Scheduler overhead time: 0.12198524491395801 Adapter cache time: 0.024626936647109687 Engine time: 0.1177669974276796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_32_slots_32_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5490755579667166,
    "estimated_duration": 3599.95638694723,
    "input_throughput": 2280.2242909839806,
    "output_throughput": 1984.59737620836,
    "total_throughput": 4264.821667192341,
    "itl": 26.86597411571749,
    "ttft": 6251.65903257866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.152177154823028
}
#Debug simulation 
Total elapsed time: 2.549214167986065. Arrivals time: 0.08305905130691826 Scheduler time: 2.1434303432470188 Scheduler overhead time: 0.12154697778169066 Adapter cache time: 0.024749751552008092 Engine time: 0.11860499216709286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5268006769474596,
    "estimated_duration": 3600.0287075036417,
    "input_throughput": 2268.211357031724,
    "output_throughput": 1970.8009508957014,
    "total_throughput": 4239.012307927425,
    "itl": 26.72556885107858,
    "ttft": 6487.664686753018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.840010130215898
}
#Debug simulation 
Total elapsed time: 2.5268880679504946. Arrivals time: 0.08341419836506248 Scheduler time: 2.1200244686333463 Scheduler overhead time: 0.1219478021375835 Adapter cache time: 0.024057512870058417 Engine time: 0.11949463747441769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.541991221019998,
    "estimated_duration": 3600.006683596039,
    "input_throughput": 2268.225233360782,
    "output_throughput": 1970.8130077449966,
    "total_throughput": 4239.038241105779,
    "itl": 26.725456590549435,
    "ttft": 6487.634564937304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.839809432663851
}
#Debug simulation 
Total elapsed time: 2.54209201305639. Arrivals time: 0.08686557353939861 Scheduler time: 2.1294686894398183 Scheduler overhead time: 0.12080847530160099 Adapter cache time: 0.024002178222872317 Engine time: 0.12329883361235261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.530429345089942,
    "estimated_duration": 3600.0141493920833,
    "input_throughput": 2268.220529460666,
    "output_throughput": 1970.808920625517,
    "total_throughput": 4239.029450086183,
    "itl": 26.72551167875429,
    "ttft": 6487.644735913569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.839792294323699
}
#Debug simulation 
Total elapsed time: 2.5305279101012275. Arrivals time: 0.08358610235154629 Scheduler time: 2.1256533381529152 Scheduler overhead time: 0.1219233765732497 Adapter cache time: 0.024077312322333455 Engine time: 0.11731301958207041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.537893809028901,
    "estimated_duration": 3600.0272389651614,
    "input_throughput": 2268.2122822901847,
    "output_throughput": 1970.8017548332389,
    "total_throughput": 4239.014037123424,
    "itl": 26.723765848579852,
    "ttft": 6487.604648677197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.838307849772544
}
#Debug simulation 
Total elapsed time: 2.537994106998667. Arrivals time: 0.0854923581937328 Scheduler time: 2.129524171235971 Scheduler overhead time: 0.12193826434668154 Adapter cache time: 0.024128407472744584 Engine time: 0.11877594341058284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5132214199984446,
    "estimated_duration": 3600.0116660867434,
    "input_throughput": 2268.2220940900825,
    "output_throughput": 1970.810280098977,
    "total_throughput": 4239.03237418906,
    "itl": 26.72548404944466,
    "ttft": 6487.63301161288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.8397278276390345
}
#Debug simulation 
Total elapsed time: 2.513317240984179. Arrivals time: 0.08271957118995488 Scheduler time: 2.1077804362867028 Scheduler overhead time: 0.12195870047435164 Adapter cache time: 0.024032744811847806 Engine time: 0.11905662983190268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.524287214037031,
    "estimated_duration": 3600.0212283780306,
    "input_throughput": 2268.216069292174,
    "output_throughput": 1970.8050452792984,
    "total_throughput": 4239.021114571472,
    "itl": 26.725562256394323,
    "ttft": 6487.621584831396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.839995160335775
}
#Debug simulation 
Total elapsed time: 2.524371866020374. Arrivals time: 0.0828719837591052 Scheduler time: 2.1188581209862605 Scheduler overhead time: 0.12175891373772174 Adapter cache time: 0.02409143664408475 Engine time: 0.11903492826968431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5389960509492084,
    "estimated_duration": 3600.009503571022,
    "input_throughput": 2268.223456604802,
    "output_throughput": 1970.8114639592447,
    "total_throughput": 4239.034920564047,
    "itl": 26.725468147139896,
    "ttft": 6487.695087974191,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.839786456421634
}
#Debug simulation 
Total elapsed time: 2.5391358219785616. Arrivals time: 0.08337115263566375 Scheduler time: 2.1324952552095056 Scheduler overhead time: 0.12145141919609159 Adapter cache time: 0.024013000540435314 Engine time: 0.11958188319113106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.507537428988144,
    "estimated_duration": 3599.9814469468665,
    "input_throughput": 2227.9573153917922,
    "output_throughput": 1957.8884235577643,
    "total_throughput": 4185.845738949556,
    "itl": 26.54753656682747,
    "ttft": 6454.687289881136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.588297449004523
}
#Debug simulation 
Total elapsed time: 2.507626028964296. Arrivals time: 0.0824241746449843 Scheduler time: 2.1026911434018984 Scheduler overhead time: 0.12217148032505065 Adapter cache time: 0.022999344393610954 Engine time: 0.11901446187403053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5299483480630443,
    "estimated_duration": 3600.001532520845,
    "input_throughput": 2227.9448848966726,
    "output_throughput": 1957.8774998644221,
    "total_throughput": 4185.822384761095,
    "itl": 26.54924748575833,
    "ttft": 6454.652533917588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.589722430787044
}
#Debug simulation 
Total elapsed time: 2.5300425400491804. Arrivals time: 0.08273456711322069 Scheduler time: 2.1236779608298093 Scheduler overhead time: 0.12228844966739416 Adapter cache time: 0.023360553430393338 Engine time: 0.11971311853267252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.545285554951988,
    "estimated_duration": 3599.978172847215,
    "input_throughput": 2227.9593416691528,
    "output_throughput": 1957.8902042135066,
    "total_throughput": 4185.849545882659,
    "itl": 26.54928976275537,
    "ttft": 6454.61002204128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.589770384741503
}
#Debug simulation 
Total elapsed time: 2.5453851289348677. Arrivals time: 0.08314606652129441 Scheduler time: 2.1387963278684765 Scheduler overhead time: 0.12296549358870834 Adapter cache time: 0.023158513475209475 Engine time: 0.11897014384157956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.5207807150436565,
    "estimated_duration": 3599.978508963711,
    "input_throughput": 2227.959133652942,
    "output_throughput": 1957.8900214126388,
    "total_throughput": 4185.849155065581,
    "itl": 26.549270545075956,
    "ttft": 6454.588395360393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469847,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.589616682035945
}
#Debug simulation 
Total elapsed time: 2.5208703230600804. Arrivals time: 0.08286456170026213 Scheduler time: 2.11482456268277 Scheduler overhead time: 0.1226297514513135 Adapter cache time: 0.023005099617876112 Engine time: 0.11952380032744259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.515704242978245,
    "estimated_duration": 3599.976887663917,
    "input_throughput": 2227.9601370454075,
    "output_throughput": 1957.8909031757134,
    "total_throughput": 4185.851040221121,
    "itl": 26.549260614319838,
    "ttft": 6454.706784867646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.58973706716917
}
#Debug simulation 
Total elapsed time: 2.5157782080350444. Arrivals time: 0.08271238720044494 Scheduler time: 2.1075094459811226 Scheduler overhead time: 0.12255591317079961 Adapter cache time: 0.02330216020345688 Engine time: 0.12166968721430749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5136999459937215,
    "estimated_duration": 3600.0014275266835,
    "input_throughput": 2227.9449498747595,
    "output_throughput": 1957.8775569659845,
    "total_throughput": 4185.822506840745,
    "itl": 26.54749588548871,
    "ttft": 6454.733309013492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.588380471757333
}
#Debug simulation 
Total elapsed time: 2.5137742379447445. Arrivals time: 0.0824276803759858 Scheduler time: 2.1091254224302247 Scheduler overhead time: 0.12177673145197332 Adapter cache time: 0.02309543709270656 Engine time: 0.11937914823647588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_32_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.543292067013681,
    "estimated_duration": 3600.003765498246,
    "input_throughput": 2227.943502967402,
    "output_throughput": 1957.876285450078,
    "total_throughput": 4185.81978841748,
    "itl": 26.549245451139956,
    "ttft": 6454.730707956212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.589577318185567
}
#Debug simulation 
Total elapsed time: 2.5434080300619826. Arrivals time: 0.08244332286994904 Scheduler time: 2.138969238032587 Scheduler overhead time: 0.12214849481824785 Adapter cache time: 0.02305302955210209 Engine time: 0.11870753869879991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5139428360853344,
    "estimated_duration": 3599.970008481414,
    "input_throughput": 2222.987130766625,
    "output_throughput": 1938.013089987669,
    "total_throughput": 4161.000220754294,
    "itl": 26.417076910372817,
    "ttft": 6368.769637946319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.232730785690503
}
#Debug simulation 
Total elapsed time: 2.5140201470348984. Arrivals time: 0.08281126746442169 Scheduler time: 2.1044343154644594 Scheduler overhead time: 0.12303558795247227 Adapter cache time: 0.022962413378991187 Engine time: 0.1224546410376206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4857522830134258,
    "estimated_duration": 3599.9563380404043,
    "input_throughput": 2222.995572317461,
    "output_throughput": 1938.0204493807103,
    "total_throughput": 4161.016021698171,
    "itl": 26.416157118229297,
    "ttft": 6368.782607152817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.231848308673523
}
#Debug simulation 
Total elapsed time: 2.4858281599590555. Arrivals time: 0.081563989399001 Scheduler time: 2.08235061657615 Scheduler overhead time: 0.12170169083401561 Adapter cache time: 0.02283252391498536 Engine time: 0.11927478422876447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4887380229774863,
    "estimated_duration": 3599.962762026055,
    "input_throughput": 2222.9916054731902,
    "output_throughput": 1938.0169910628385,
    "total_throughput": 4161.008596536029,
    "itl": 26.418549609665625,
    "ttft": 6368.819855080826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.233392508105275
}
#Debug simulation 
Total elapsed time: 2.488824688945897. Arrivals time: 0.08336225606035441 Scheduler time: 2.083947879378684 Scheduler overhead time: 0.12232448987197131 Adapter cache time: 0.02281112875789404 Engine time: 0.11813522386364639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.5114059399347752,
    "estimated_duration": 3599.9637402394296,
    "input_throughput": 2222.9910014226284,
    "output_throughput": 1938.0164644480506,
    "total_throughput": 4161.007465870679,
    "itl": 26.418442555778313,
    "ttft": 6368.767452482547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2195559008046984,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.2333255394626015
}
#Debug simulation 
Total elapsed time: 2.5114895090227947. Arrivals time: 0.08326386683620512 Scheduler time: 2.0998041689163074 Scheduler overhead time: 0.12552673905156553 Adapter cache time: 0.02289990964345634 Engine time: 0.12115992850158364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4968687390210107,
    "estimated_duration": 3599.966720001435,
    "input_throughput": 2222.9891614100284,
    "output_throughput": 1938.0148603143805,
    "total_throughput": 4161.004021724409,
    "itl": 26.416325650872707,
    "ttft": 6368.8247649467285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.231932040290351
}
#Debug simulation 
Total elapsed time: 2.496944871963933. Arrivals time: 0.08190310280770063 Scheduler time: 2.0902144629508257 Scheduler overhead time: 0.12279179494362324 Adapter cache time: 0.022987183067016304 Engine time: 0.12063756142742932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.4992427329998463,
    "estimated_duration": 3599.9629010379695,
    "input_throughput": 2222.9915196327725,
    "output_throughput": 1938.0169162266639,
    "total_throughput": 4161.008435859437,
    "itl": 26.41695384549508,
    "ttft": 6368.808707874244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.232544265900624
}
#Debug simulation 
Total elapsed time: 2.499322924995795. Arrivals time: 0.08255348750390112 Scheduler time: 2.0918710430851206 Scheduler overhead time: 0.12318908981978893 Adapter cache time: 0.022919418406672776 Engine time: 0.12028563802596182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.479379663011059,
    "estimated_duration": 3599.9621418538914,
    "input_throughput": 2222.9919884320825,
    "output_throughput": 1938.0173249286245,
    "total_throughput": 4161.009313360707,
    "itl": 26.416166406728134,
    "ttft": 6368.7790696075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.231743852528458
}
#Debug simulation 
Total elapsed time: 2.4794982600724325. Arrivals time: 0.08170397696085274 Scheduler time: 2.0741949286311865 Scheduler overhead time: 0.12249541992787272 Adapter cache time: 0.022844549152068794 Engine time: 0.12001783633604646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.4849402400432155,
    "estimated_duration": 3599.9522015607517,
    "input_throughput": 2203.942040274922,
    "output_throughput": 1928.2553243319376,
    "total_throughput": 4132.197364606859,
    "itl": 26.264564357183414,
    "ttft": 8207.874530690668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 32188,
    "finished_requests": 32115,
    "scheduler_time": 3.9906709491431775
}
#Debug simulation 
Total elapsed time: 2.4850155799649656. Arrivals time: 0.08086585637647659 Scheduler time: 2.0780234488192946 Scheduler overhead time: 0.12382480618543923 Adapter cache time: 0.022208613576367497 Engine time: 0.121621512924321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.464966165018268,
    "estimated_duration": 3599.9525507748413,
    "input_throughput": 2203.9418264811006,
    "output_throughput": 1928.255137281159,
    "total_throughput": 4132.19696376226,
    "itl": 26.264810954796218,
    "ttft": 8207.84619314312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2348263315111399,
    "arrivals": 32188,
    "finished_requests": 32115,
    "scheduler_time": 3.9906412177588733
}
#Debug simulation 
Total elapsed time: 2.465042405994609. Arrivals time: 0.08022625790908933 Scheduler time: 2.0605833447771147 Scheduler overhead time: 0.12274463626090437 Adapter cache time: 0.02233798720408231 Engine time: 0.12062596820760518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4728026679949835,
    "estimated_duration": 3599.9302265872416,
    "input_throughput": 2203.9474380373035,
    "output_throughput": 1928.197093581025,
    "total_throughput": 4132.144531618328,
    "itl": 26.264758776857985,
    "ttft": 8319.794442877823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2413462840113789,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 3.9907121479556142
}
#Debug simulation 
Total elapsed time: 2.4728767540073022. Arrivals time: 0.08030673151370138 Scheduler time: 2.0693290451308712 Scheduler overhead time: 0.12307539058383554 Adapter cache time: 0.02228918031323701 Engine time: 0.11949663097038865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.475925142993219,
    "estimated_duration": 3599.9318102752636,
    "input_throughput": 2203.946468473061,
    "output_throughput": 1928.1962453253352,
    "total_throughput": 4132.142713798396,
    "itl": 26.264524768568105,
    "ttft": 8319.843518977868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 3.9906808737213035
}
#Debug simulation 
Total elapsed time: 2.4760053110076115. Arrivals time: 0.08086959458887577 Scheduler time: 2.068245282047428 Scheduler overhead time: 0.1239261575974524 Adapter cache time: 0.022163559682667255 Engine time: 0.12220883998088539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4877295929472893,
    "estimated_duration": 3599.926932474383,
    "input_throughput": 2203.9494547592344,
    "output_throughput": 1928.1988579776248,
    "total_throughput": 4132.148312736859,
    "itl": 26.26474957060466,
    "ttft": 8319.700860213086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861717,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 3.9906173242766276
}
#Debug simulation 
Total elapsed time: 2.487832491984591. Arrivals time: 0.0813547755824402 Scheduler time: 2.080296285334043 Scheduler overhead time: 0.12348804133944213 Adapter cache time: 0.022251854301430285 Engine time: 0.12180693948175758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.474302906077355,
    "estimated_duration": 3599.9398488136403,
    "input_throughput": 2203.9415471385355,
    "output_throughput": 1928.1919397313068,
    "total_throughput": 4132.133486869842,
    "itl": 26.26454969135986,
    "ttft": 8319.79422585079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 3.9908489625649874
}
#Debug simulation 
Total elapsed time: 2.4743782710283995. Arrivals time: 0.08071566850412637 Scheduler time: 2.0693350334186107 Scheduler overhead time: 0.12199799297377467 Adapter cache time: 0.022319134324789047 Engine time: 0.1218407719861716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.485882609966211,
    "estimated_duration": 3599.922892103534,
    "input_throughput": 2203.9519283603076,
    "output_throughput": 1928.2010220902157,
    "total_throughput": 4132.152950450523,
    "itl": 26.264716001434774,
    "ttft": 8319.679690906823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585545,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 3.9905547339399887
}
#Debug simulation 
Total elapsed time: 2.486027786973864. Arrivals time: 0.08062262961175293 Scheduler time: 2.0820331908762455 Scheduler overhead time: 0.12352715386077762 Adapter cache time: 0.02221480815205723 Engine time: 0.11897374736145139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.8407449779333547,
    "estimated_duration": 3599.9205091535027,
    "input_throughput": 1475.036181076291,
    "output_throughput": 1286.273401933763,
    "total_throughput": 2761.309583010054,
    "itl": 25.90552987753855,
    "ttft": 5200.203931123692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 21639,
    "finished_requests": 21608,
    "scheduler_time": 0.09875139740086045
}
#Debug simulation 
Total elapsed time: 1.840820220997557. Arrivals time: 0.060465440968982875 Scheduler time: 1.441741659422405 Scheduler overhead time: 0.1213412203360349 Adapter cache time: 0.040322992485016584 Engine time: 0.11844715243205428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.9092431210447103,
    "estimated_duration": 3599.9204627080844,
    "input_throughput": 1475.0362001068984,
    "output_throughput": 1286.2734185289924,
    "total_throughput": 2761.3096186358907,
    "itl": 25.90584104060839,
    "ttft": 5200.197312652389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 21639,
    "finished_requests": 21608,
    "scheduler_time": 0.09874176493476379
}
#Debug simulation 
Total elapsed time: 1.909319124999456. Arrivals time: 0.06152063503395766 Scheduler time: 1.5050256445538253 Scheduler overhead time: 0.12232708430383354 Adapter cache time: 0.04010871530044824 Engine time: 0.12134823948144913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.8534932599868625,
    "estimated_duration": 3599.9243732425593,
    "input_throughput": 1475.03459780104,
    "output_throughput": 1286.272021272821,
    "total_throughput": 2761.306619073861,
    "itl": 25.905822394034402,
    "ttft": 5200.223329569838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 21639,
    "finished_requests": 21608,
    "scheduler_time": 0.09879101149526373
}
#Debug simulation 
Total elapsed time: 1.8535950030200183. Arrivals time: 0.06078664481174201 Scheduler time: 1.4510055136634037 Scheduler overhead time: 0.1215109353652224 Adapter cache time: 0.03995151654817164 Engine time: 0.12154947977978736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.8452307620318606,
    "estimated_duration": 3599.9095879741635,
    "input_throughput": 1475.0406559483042,
    "output_throughput": 1286.2773041491266,
    "total_throughput": 2761.317960097431,
    "itl": 25.90575756771181,
    "ttft": 5200.19499197258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469847,
    "arrivals": 21639,
    "finished_requests": 21608,
    "scheduler_time": 0.09868809820021078
}
#Debug simulation 
Total elapsed time: 1.8453048239462078. Arrivals time: 0.060308072599582374 Scheduler time: 1.4457502336008474 Scheduler overhead time: 0.12108634342439473 Adapter cache time: 0.039928910555318 Engine time: 0.11918123613577336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.8712071729823947,
    "estimated_duration": 3599.924439055432,
    "input_throughput": 1475.0345708348452,
    "output_throughput": 1286.2719977575339,
    "total_throughput": 2761.3065685923793,
    "itl": 25.90578707549021,
    "ttft": 5200.204230151105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 21639,
    "finished_requests": 21608,
    "scheduler_time": 0.09879017750925473
}
#Debug simulation 
Total elapsed time: 1.8712835669284686. Arrivals time: 0.0615046612219885 Scheduler time: 1.4674848456634209 Scheduler overhead time: 0.1224310341058299 Adapter cache time: 0.040042160893790424 Engine time: 0.12055717885959893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.8764500020770356,
    "estimated_duration": 3599.9095976226727,
    "input_throughput": 1475.0406519948874,
    "output_throughput": 1286.2773007016349,
    "total_throughput": 2761.3179526965223,
    "itl": 25.905797609767102,
    "ttft": 5200.183543256796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 21639,
    "finished_requests": 21608,
    "scheduler_time": 0.0986728780760587
}
#Debug simulation 
Total elapsed time: 1.8765268890419975. Arrivals time: 0.06150596810039133 Scheduler time: 1.4719422564376146 Scheduler overhead time: 0.1218074430944398 Adapter cache time: 0.0402201161487028 Engine time: 0.12251396756619215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.8536331499926746,
    "estimated_duration": 3599.9205088091285,
    "input_throughput": 1475.0361812173953,
    "output_throughput": 1286.27340205681,
    "total_throughput": 2761.3095832742056,
    "itl": 25.905624326172287,
    "ttft": 5200.168005788398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 21639,
    "finished_requests": 21608,
    "scheduler_time": 0.09879505630330519
}
#Debug simulation 
Total elapsed time: 1.8537511349422857. Arrivals time: 0.06119161343667656 Scheduler time: 1.4431278395932168 Scheduler overhead time: 0.12178625073283911 Adapter cache time: 0.04079881007783115 Engine time: 0.12500784278381616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.7983580040745437,
    "estimated_duration": 3600.0071317280162,
    "input_throughput": 1412.1833135257498,
    "output_throughput": 1258.6286177230625,
    "total_throughput": 2670.811931248812,
    "itl": 25.688550492615096,
    "ttft": 7298.0435677155765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 20840,
    "finished_requests": 20797,
    "scheduler_time": 0.026840861099333427
}
#Debug simulation 
Total elapsed time: 1.7984333069762215. Arrivals time: 0.05817213177215308 Scheduler time: 1.405436929431744 Scheduler overhead time: 0.12148210522718728 Adapter cache time: 0.036638093180954456 Engine time: 0.11784508544951677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7997693929355592,
    "estimated_duration": 3600.0291139800775,
    "input_throughput": 1412.2071902855546,
    "output_throughput": 1258.7856532609915,
    "total_throughput": 2670.9928435465463,
    "itl": 25.549857644454757,
    "ttft": 7297.5778235027665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113982,
    "arrivals": 20840,
    "finished_requests": 20798,
    "scheduler_time": 0.023839737690629213
}
#Debug simulation 
Total elapsed time: 1.799847329966724. Arrivals time: 0.058442273759283125 Scheduler time: 1.3977430143859237 Scheduler overhead time: 0.12354083359241486 Adapter cache time: 0.03750259173102677 Engine time: 0.12281436624471098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7942133030155674,
    "estimated_duration": 3600.0099288337747,
    "input_throughput": 1412.1822162993096,
    "output_throughput": 1258.6276398042723,
    "total_throughput": 2670.8098561035818,
    "itl": 25.549774826516344,
    "ttft": 7297.846009393481,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137895,
    "arrivals": 20840,
    "finished_requests": 20797,
    "scheduler_time": 0.023784027618046485
}
#Debug simulation 
Total elapsed time: 1.7942971109878272. Arrivals time: 0.05884673842228949 Scheduler time: 1.3981937689241022 Scheduler overhead time: 0.1210793238133192 Adapter cache time: 0.03725866659078747 Engine time: 0.11993227631319314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.8063504620222375,
    "estimated_duration": 3600.009723940095,
    "input_throughput": 1412.1822966733178,
    "output_throughput": 1258.627711438759,
    "total_throughput": 2670.810008112077,
    "itl": 25.68862812433803,
    "ttft": 7297.973570949154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 20840,
    "finished_requests": 20797,
    "scheduler_time": 0.026768763418596065
}
#Debug simulation 
Total elapsed time: 1.806450971052982. Arrivals time: 0.0590678125154227 Scheduler time: 1.4098815388279036 Scheduler overhead time: 0.1212836311897263 Adapter cache time: 0.03659004345536232 Engine time: 0.12060883908998221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.799756052903831,
    "estimated_duration": 3600.005450655751,
    "input_throughput": 1412.1839729642518,
    "output_throughput": 1258.6292054570786,
    "total_throughput": 2670.8131784213306,
    "itl": 25.549736713024146,
    "ttft": 7297.750889585946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 20840,
    "finished_requests": 20797,
    "scheduler_time": 0.023764304065855783
}
#Debug simulation 
Total elapsed time: 1.7998332029674202. Arrivals time: 0.05839651438873261 Scheduler time: 1.404453910770826 Scheduler overhead time: 0.12159237405285239 Adapter cache time: 0.037489067181013525 Engine time: 0.11887490726076066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.804416872910224,
    "estimated_duration": 3600.001156616206,
    "input_throughput": 1412.185657400884,
    "output_throughput": 1258.6307067353687,
    "total_throughput": 2670.8163641362526,
    "itl": 25.688591340396552,
    "ttft": 7298.129651960981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 20840,
    "finished_requests": 20797,
    "scheduler_time": 0.026777103278686338
}
#Debug simulation 
Total elapsed time: 1.804492387920618. Arrivals time: 0.058535786578431726 Scheduler time: 1.4077141222078353 Scheduler overhead time: 0.12203288963064551 Adapter cache time: 0.037013221881352365 Engine time: 0.1201273986371234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.81666276010219,
    "estimated_duration": 3600.00311636935,
    "input_throughput": 1412.184888641749,
    "output_throughput": 1258.6300215677716,
    "total_throughput": 2670.8149102095203,
    "itl": 25.549736597631167,
    "ttft": 7297.850677112304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 20840,
    "finished_requests": 20797,
    "scheduler_time": 0.023817845678404927
}
#Debug simulation 
Total elapsed time: 1.816786181065254. Arrivals time: 0.05895273038186133 Scheduler time: 1.4102046438492835 Scheduler overhead time: 0.1267791063291952 Adapter cache time: 0.03773492854088545 Engine time: 0.12267134978901595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.7762099149404094,
    "estimated_duration": 3600.0217908809977,
    "input_throughput": 1386.459385507923,
    "output_throughput": 1229.7845005311933,
    "total_throughput": 2616.243886039116,
    "itl": 25.23008271935989,
    "ttft": 5340.821862549331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.018215905878671463
}
#Debug simulation 
Total elapsed time: 1.776286349981092. Arrivals time: 0.05760559032205492 Scheduler time: 1.3748730447841808 Scheduler overhead time: 0.12274859414901584 Adapter cache time: 0.035775354597717524 Engine time: 0.12566060898825526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7879906119778752,
    "estimated_duration": 3600.022555931242,
    "input_throughput": 1386.459090867799,
    "output_throughput": 1229.7842391864606,
    "total_throughput": 2616.24333005426,
    "itl": 25.230171940469027,
    "ttft": 5340.8441369897655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.018219950686712706
}
#Debug simulation 
Total elapsed time: 1.788067927933298. Arrivals time: 0.05778928229119629 Scheduler time: 1.3889854765729979 Scheduler overhead time: 0.12387743045110255 Adapter cache time: 0.03573984431568533 Engine time: 0.12127397453878075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7707097450038418,
    "estimated_duration": 3600.0086109396593,
    "input_throughput": 1386.4644614550507,
    "output_throughput": 1229.789002878084,
    "total_throughput": 2616.2534643331346,
    "itl": 25.230035888300037,
    "ttft": 5340.762540230149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.018228165424799082
}
#Debug simulation 
Total elapsed time: 1.7708081849850714. Arrivals time: 0.0570700247772038 Scheduler time: 1.3737612976692617 Scheduler overhead time: 0.12372616911306977 Adapter cache time: 0.035714375670067966 Engine time: 0.12085831945296377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.7790139460703358,
    "estimated_duration": 3600.009642903761,
    "input_throughput": 1386.4640640167952,
    "output_throughput": 1229.7886503517773,
    "total_throughput": 2616.2527143685725,
    "itl": 25.22989061519525,
    "ttft": 5340.762262116536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.018221618658730756
}
#Debug simulation 
Total elapsed time: 1.7790879800450057. Arrivals time: 0.05747814825735986 Scheduler time: 1.3795781694352627 Scheduler overhead time: 0.12580979755148292 Adapter cache time: 0.035862389486283064 Engine time: 0.11985093972180039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7799715480068699,
    "estimated_duration": 3600.026053594287,
    "input_throughput": 1386.4577438312351,
    "output_throughput": 1229.7830443698613,
    "total_throughput": 2616.240788201096,
    "itl": 25.23002092105398,
    "ttft": 5517.101485836552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.018209609356610902
}
#Debug simulation 
Total elapsed time: 1.7800587329547852. Arrivals time: 0.058791448129341006 Scheduler time: 1.3786765346303582 Scheduler overhead time: 0.12361777771729976 Adapter cache time: 0.03572223556693643 Engine time: 0.12365969212260097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.7822395480470732,
    "estimated_duration": 3600.009647819004,
    "input_throughput": 1386.4640621237982,
    "output_throughput": 1229.7886486726957,
    "total_throughput": 2616.252710796494,
    "itl": 25.22974742831121,
    "ttft": 5340.739237039345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.0182394658629138
}
#Debug simulation 
Total elapsed time: 1.782340059056878. Arrivals time: 0.057728254701942205 Scheduler time: 1.381941418047063 Scheduler overhead time: 0.12394829490222037 Adapter cache time: 0.03607031621504575 Engine time: 0.12239285744726658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7900638290448114,
    "estimated_duration": 3600.021788595298,
    "input_throughput": 1386.4593863882035,
    "output_throughput": 1229.7845013119993,
    "total_throughput": 2616.243887700203,
    "itl": 25.230114559790564,
    "ttft": 5340.798780610318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.018210318220616042
}
#Debug simulation 
Total elapsed time: 1.7901821460109204. Arrivals time: 0.05768227088265121 Scheduler time: 1.388822493609041 Scheduler overhead time: 0.12409589986782521 Adapter cache time: 0.03558337513823062 Engine time: 0.12404256383888423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.751517069991678,
    "estimated_duration": 3599.9236177036823,
    "input_throughput": 1375.528629454269,
    "output_throughput": 1211.6907088107682,
    "total_throughput": 2587.219338265037,
    "itl": 25.111897979656334,
    "ttft": 5394.210349124688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 20177,
    "finished_requests": 20147,
    "scheduler_time": 0.019923065598136877
}
#Debug simulation 
Total elapsed time: 1.7515935510164127. Arrivals time: 0.056636339984834194 Scheduler time: 1.3555326818022877 Scheduler overhead time: 0.12386445386800915 Adapter cache time: 0.03457225835882127 Engine time: 0.1211883801734075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7703628219896927,
    "estimated_duration": 3599.909719112879,
    "input_throughput": 1375.533940117883,
    "output_throughput": 1211.6953869262368,
    "total_throughput": 2587.2293270441196,
    "itl": 25.08726578172592,
    "ttft": 5394.116034335591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113982,
    "arrivals": 20177,
    "finished_requests": 20147,
    "scheduler_time": 0.019906636121964128
}
#Debug simulation 
Total elapsed time: 1.7704385239630938. Arrivals time: 0.05737775983288884 Scheduler time: 1.3659604154527187 Scheduler overhead time: 0.12748135766014457 Adapter cache time: 0.0351958837127313 Engine time: 0.1226756201358512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7739303009584546,
    "estimated_duration": 3599.9142841966873,
    "input_throughput": 1375.5321957908736,
    "output_throughput": 1211.6938503643757,
    "total_throughput": 2587.2260461552496,
    "itl": 25.0872337974394,
    "ttft": 5394.26770621528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137895,
    "arrivals": 20177,
    "finished_requests": 20147,
    "scheduler_time": 0.01990259131392288
}
#Debug simulation 
Total elapsed time: 1.7740047309780493. Arrivals time: 0.057024235487915576 Scheduler time: 1.3756396301323548 Scheduler overhead time: 0.12313048029318452 Adapter cache time: 0.035005120444111526 Engine time: 0.12349040957633406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.7696482439059764,
    "estimated_duration": 3599.9142507197857,
    "input_throughput": 1375.5322085824437,
    "output_throughput": 1211.6938616323541,
    "total_throughput": 2587.226070214798,
    "itl": 25.08709963405464,
    "ttft": 5394.254629260894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 20177,
    "finished_requests": 20147,
    "scheduler_time": 0.019900089355895798
}
#Debug simulation 
Total elapsed time: 1.769723731908016. Arrivals time: 0.05653515737503767 Scheduler time: 1.369106565369293 Scheduler overhead time: 0.12597407202702016 Adapter cache time: 0.03501184226479381 Engine time: 0.1222125057829544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.760920617962256,
    "estimated_duration": 3599.909758631644,
    "input_throughput": 1375.5339250176705,
    "output_throughput": 1211.6953736245964,
    "total_throughput": 2587.2292986422667,
    "itl": 25.087272226833672,
    "ttft": 5394.167877199424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 20177,
    "finished_requests": 20147,
    "scheduler_time": 0.01987356879362898
}
#Debug simulation 
Total elapsed time: 1.7609968590550125. Arrivals time: 0.05691706936340779 Scheduler time: 1.3614998144330457 Scheduler overhead time: 0.12471567536704242 Adapter cache time: 0.03504328930284828 Engine time: 0.12241637823171914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.7603095900267363,
    "estimated_duration": 3599.919459257014,
    "input_throughput": 1375.5302183960528,
    "output_throughput": 1211.6921084951914,
    "total_throughput": 2587.222326891244,
    "itl": 25.111937240842593,
    "ttft": 5394.23251723446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 20177,
    "finished_requests": 20147,
    "scheduler_time": 0.019960177734513262
}
#Debug simulation 
Total elapsed time: 1.7603892099577934. Arrivals time: 0.0570778779219836 Scheduler time: 1.3639608399244025 Scheduler overhead time: 0.12320208724122494 Adapter cache time: 0.03461886092554778 Engine time: 0.12184659927152097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7626485800137743,
    "estimated_duration": 3599.911106579318,
    "input_throughput": 1375.5334099639094,
    "output_throughput": 1211.694919918404,
    "total_throughput": 2587.2283298823136,
    "itl": 25.087254876246803,
    "ttft": 5394.183938603576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 20177,
    "finished_requests": 20147,
    "scheduler_time": 0.01985584671144981
}
#Debug simulation 
Total elapsed time: 1.7627645540051162. Arrivals time: 0.05703040410298854 Scheduler time: 1.3633729224093258 Scheduler overhead time: 0.12281079706735909 Adapter cache time: 0.0349392065545544 Engine time: 0.12466280930675566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.750707968021743,
    "estimated_duration": 3599.160705560673,
    "input_throughput": 1386.0942614461153,
    "output_throughput": 1196.4800552936595,
    "total_throughput": 2582.5743167397745,
    "itl": 24.96951345447724,
    "ttft": 6321.507804409126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 20063,
    "finished_requests": 20028,
    "scheduler_time": 0.019694553431663674
}
#Debug simulation 
Total elapsed time: 1.7507861510384828. Arrivals time: 0.05687789712101221 Scheduler time: 1.3513432517647743 Scheduler overhead time: 0.1256235868204385 Adapter cache time: 0.0348312760470435 Engine time: 0.1218535341322422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.737267327029258,
    "estimated_duration": 3599.1606677358236,
    "input_throughput": 1386.0942760130688,
    "output_throughput": 1196.4800678678905,
    "total_throughput": 2582.574343880959,
    "itl": 24.969903434721353,
    "ttft": 6321.5037877129635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 20063,
    "finished_requests": 20028,
    "scheduler_time": 0.019705853869778397
}
#Debug simulation 
Total elapsed time: 1.7373434259789065. Arrivals time: 0.056013156194239855 Scheduler time: 1.3390816529281437 Scheduler overhead time: 0.12570087623316795 Adapter cache time: 0.034911224036477506 Engine time: 0.12148654623888433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.725787910982035,
    "estimated_duration": 3599.1490912229146,
    "input_throughput": 1385.868957794466,
    "output_throughput": 1196.4833606092163,
    "total_throughput": 2582.352318403682,
    "itl": 24.97051359929461,
    "ttft": 6500.947047727403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 20063,
    "finished_requests": 20027,
    "scheduler_time": 0.01963292220303203
}
#Debug simulation 
Total elapsed time: 1.7258621390210465. Arrivals time: 0.05642047629225999 Scheduler time: 1.3299996728310362 Scheduler overhead time: 0.12375190621241927 Adapter cache time: 0.03463475126773119 Engine time: 0.12111079483292997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.7472914829850197,
    "estimated_duration": 3599.1492351780666,
    "input_throughput": 1385.8689023638728,
    "output_throughput": 1196.483312753478,
    "total_throughput": 2582.352215117351,
    "itl": 24.97040128591817,
    "ttft": 6500.951577200625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 20063,
    "finished_requests": 20027,
    "scheduler_time": 0.019627209422972726
}
#Debug simulation 
Total elapsed time: 1.7473627110011876. Arrivals time: 0.05716191267129034 Scheduler time: 1.3470356477191672 Scheduler overhead time: 0.12369388004299253 Adapter cache time: 0.03467448160517961 Engine time: 0.12462712277192622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7391248040366918,
    "estimated_duration": 3599.1492011581295,
    "input_throughput": 1385.8689154634058,
    "output_throughput": 1196.4833240628973,
    "total_throughput": 2582.352239526303,
    "itl": 24.97052978829396,
    "ttft": 6500.920618667561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 20063,
    "finished_requests": 20027,
    "scheduler_time": 0.019644931505151897
}
#Debug simulation 
Total elapsed time: 1.7392189550446346. Arrivals time: 0.05744411353953183 Scheduler time: 1.3386661077383906 Scheduler overhead time: 0.12546341842971742 Adapter cache time: 0.03527798340655863 Engine time: 0.12190665176603943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.7254527000477538,
    "estimated_duration": 3599.149120769825,
    "input_throughput": 1385.868946417292,
    "output_throughput": 1196.4833507867875,
    "total_throughput": 2582.352297204079,
    "itl": 24.97020988231022,
    "ttft": 6500.991161517381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 20063,
    "finished_requests": 20027,
    "scheduler_time": 0.01961507499884899
}
#Debug simulation 
Total elapsed time: 1.7255310619948432. Arrivals time: 0.05609552748501301 Scheduler time: 1.3300211319001392 Scheduler overhead time: 0.12389322009403259 Adapter cache time: 0.03443597920704633 Engine time: 0.12137509719468653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_32_slots_32_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7305087490240112,
    "estimated_duration": 3599.1634896593205,
    "input_throughput": 1386.0931892460972,
    "output_throughput": 1196.479129767905,
    "total_throughput": 2582.572319014002,
    "itl": 24.969915378456733,
    "ttft": 6321.4624918572335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 20063,
    "finished_requests": 20028,
    "scheduler_time": 0.01969038350161854
}
#Debug simulation 
Total elapsed time: 1.7306294069858268. Arrivals time: 0.056638930225744843 Scheduler time: 1.3352110895793885 Scheduler overhead time: 0.12229452049359679 Adapter cache time: 0.034537176601588726 Engine time: 0.12225564627442509 
