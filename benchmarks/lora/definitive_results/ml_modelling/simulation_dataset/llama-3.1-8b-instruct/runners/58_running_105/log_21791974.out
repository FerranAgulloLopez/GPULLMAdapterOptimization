INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 12.075156711041927,
    "estimated_duration": 3600.0807901220155,
    "input_throughput": 6948.44989830136,
    "output_throughput": 6038.828645082482,
    "total_throughput": 12987.278543383842,
    "itl": 99.91631680915174,
    "ttft": 1763295.4073581595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 373254,
    "finished_requests": 101054,
    "scheduler_time": 190.74791562081285
}
#Debug simulation 
Total elapsed time: 12.075349948834628. Arrivals time: 0.3833751706406474 Scheduler time: 11.540174624416977 Scheduler overhead time: 0.05759054888039827 Adapter cache time: 0.009630132466554642 Engine time: 0.05828635394573212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 11.651753806043416,
    "estimated_duration": 3600.081680475427,
    "input_throughput": 6867.603347470425,
    "output_throughput": 5974.059732211345,
    "total_throughput": 12841.663079681772,
    "itl": 97.27630420786446,
    "ttft": 1770521.7876663224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23066166859120127,
    "arrivals": 373254,
    "finished_requests": 99932,
    "scheduler_time": 192.6519439263015
}
#Debug simulation 
Total elapsed time: 11.651902093086392. Arrivals time: 0.3812212641350925 Scheduler time: 11.115189861506224 Scheduler overhead time: 0.05930502759292722 Adapter cache time: 0.009744215290993452 Engine time: 0.05971135152503848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 11.242049899883568,
    "estimated_duration": 3600.081962224211,
    "input_throughput": 6674.733312225476,
    "output_throughput": 5810.693539620356,
    "total_throughput": 12485.426851845832,
    "itl": 90.97435749991936,
    "ttft": 1791905.6462778847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23621986482758084,
    "arrivals": 373254,
    "finished_requests": 97135,
    "scheduler_time": 197.61996754645094
}
#Debug simulation 
Total elapsed time: 11.242195721250027. Arrivals time: 0.3692765668965876 Scheduler time: 10.708833889104426 Scheduler overhead time: 0.06218253169208765 Adapter cache time: 0.010292100720107555 Engine time: 0.06326825311407447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 11.614001803100109,
    "estimated_duration": 3600.001891634155,
    "input_throughput": 6867.471947015419,
    "output_throughput": 5974.119916430755,
    "total_throughput": 12841.591863446174,
    "itl": 97.27309444778098,
    "ttft": 1770495.4328033645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21816767983138557,
    "arrivals": 373254,
    "finished_requests": 99929,
    "scheduler_time": 192.6500030407555
}
#Debug simulation 
Total elapsed time: 11.61411698302254. Arrivals time: 0.3786247638054192 Scheduler time: 11.079691919032484 Scheduler overhead time: 0.059032092336565256 Adapter cache time: 0.009835985489189625 Engine time: 0.060204749926924706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 11.268534811213613,
    "estimated_duration": 3600.0962818839416,
    "input_throughput": 6675.012865884983,
    "output_throughput": 5810.799590352287,
    "total_throughput": 12485.81245623727,
    "itl": 90.97083636585687,
    "ttft": 1791924.839436315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23435581023804852,
    "arrivals": 373254,
    "finished_requests": 97140,
    "scheduler_time": 197.62233007045415
}
#Debug simulation 
Total elapsed time: 11.268658712971956. Arrivals time: 0.38401055661961436 Scheduler time: 10.719857711810619 Scheduler overhead time: 0.06285242643207312 Adapter cache time: 0.01059429906308651 Engine time: 0.0629938580095768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 11.656521108932793,
    "estimated_duration": 3600.0486705811063,
    "input_throughput": 6867.3524338795505,
    "output_throughput": 5974.067288520417,
    "total_throughput": 12841.419722399967,
    "itl": 97.27581677673683,
    "ttft": 1770550.393481245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 373254,
    "finished_requests": 99929,
    "scheduler_time": 192.65127988725408
}
#Debug simulation 
Total elapsed time: 11.656639392953366. Arrivals time: 0.4119899496436119 Scheduler time: 11.089361260645092 Scheduler overhead time: 0.05890169832855463 Adapter cache time: 0.009730911813676357 Engine time: 0.059930386021733284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 11.23393978876993,
    "estimated_duration": 3600.061902400795,
    "input_throughput": 6674.928279420658,
    "output_throughput": 5810.816749025737,
    "total_throughput": 12485.745028446396,
    "itl": 90.97417164240241,
    "ttft": 1791822.5328019108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2322846384719015,
    "arrivals": 373254,
    "finished_requests": 97137,
    "scheduler_time": 197.6185252369982
}
#Debug simulation 
Total elapsed time: 11.234103907831013. Arrivals time: 0.370058246422559 Scheduler time: 10.70022273529321 Scheduler overhead time: 0.06229814514517784 Adapter cache time: 0.010361923836171627 Engine time: 0.06286090146750212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 11.082427683286369,
    "estimated_duration": 3600.1084308408767,
    "input_throughput": 6948.825148068358,
    "output_throughput": 6039.317264375195,
    "total_throughput": 12988.142412443553,
    "itl": 99.93334825062749,
    "ttft": 1760244.9704979837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22482190708629776,
    "arrivals": 372568,
    "finished_requests": 101072,
    "scheduler_time": 190.71303074345133
}
#Debug simulation 
Total elapsed time: 11.082542562391609. Arrivals time: 0.37513532722368836 Scheduler time: 10.5557416905649 Scheduler overhead time: 0.05746440403163433 Adapter cache time: 0.009670630097389221 Engine time: 0.0583900292403996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.933278313837945,
    "estimated_duration": 3600.045314899792,
    "input_throughput": 6870.1363001283335,
    "output_throughput": 5974.978679011084,
    "total_throughput": 12845.114979139416,
    "itl": 97.2515611182023,
    "ttft": 1768277.5329707542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24481773144565525,
    "arrivals": 372568,
    "finished_requests": 99961,
    "scheduler_time": 192.63383377664968
}
#Debug simulation 
Total elapsed time: 10.933433334808797. Arrivals time: 0.39723855489864945 Scheduler time: 10.381458024028689 Scheduler overhead time: 0.05856752721592784 Adapter cache time: 0.009712520521134138 Engine time: 0.059599990490823984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.529178576078266,
    "estimated_duration": 3600.0029239525697,
    "input_throughput": 6680.172907636461,
    "output_throughput": 5810.2319475437125,
    "total_throughput": 12490.404855180173,
    "itl": 90.95120547862685,
    "ttft": 1789861.265051457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2506632051803172,
    "arrivals": 372568,
    "finished_requests": 97218,
    "scheduler_time": 197.58067332107296
}
#Debug simulation 
Total elapsed time: 10.529346778988838. Arrivals time: 0.36809377558529377 Scheduler time: 9.994814422912896 Scheduler overhead time: 0.06500733131542802 Adapter cache time: 0.010212980210781097 Engine time: 0.06284154206514359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 10.900570528116077,
    "estimated_duration": 3600.014515172113,
    "input_throughput": 6870.2456325561625,
    "output_throughput": 5975.176185913682,
    "total_throughput": 12845.421818469844,
    "itl": 97.25248087060028,
    "ttft": 1768253.7436349483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2323237426858395,
    "arrivals": 372568,
    "finished_requests": 99962,
    "scheduler_time": 192.6319369916097
}
#Debug simulation 
Total elapsed time: 10.90069781197235. Arrivals time: 0.3948661503382027 Scheduler time: 10.35065284697339 Scheduler overhead time: 0.058882043696939945 Adapter cache time: 0.00976791325956583 Engine time: 0.059660411439836025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 10.547245267312974,
    "estimated_duration": 3600.079594485747,
    "input_throughput": 6680.4028546583895,
    "output_throughput": 5810.309036511169,
    "total_throughput": 12490.711891169558,
    "itl": 90.95046624926655,
    "ttft": 1789849.476912241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2487991505907849,
    "arrivals": 372568,
    "finished_requests": 97222,
    "scheduler_time": 197.58506104111942
}
#Debug simulation 
Total elapsed time: 10.54737493628636. Arrivals time: 0.36492173140868545 Scheduler time: 10.017885017208755 Scheduler overhead time: 0.062276801094412804 Adapter cache time: 0.010344617068767548 Engine time: 0.06365372287109494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 11.232529364991933,
    "estimated_duration": 3600.0961435543777,
    "input_throughput": 6870.08985698396,
    "output_throughput": 5975.040705096962,
    "total_throughput": 12845.130562080922,
    "itl": 97.25300866818381,
    "ttft": 1768266.1136986997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2170533119793981,
    "arrivals": 372568,
    "finished_requests": 99962,
    "scheduler_time": 192.63536272680054
}
#Debug simulation 
Total elapsed time: 11.232640080619603. Arrivals time: 0.3925166744738817 Scheduler time: 10.685737813357264 Scheduler overhead time: 0.058394734747707844 Adapter cache time: 0.009952693246304989 Engine time: 0.05934653710573912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.543788152746856,
    "estimated_duration": 3600.0336641379636,
    "input_throughput": 6680.126422028477,
    "output_throughput": 5810.132890801327,
    "total_throughput": 12490.259312829803,
    "itl": 90.95200852402749,
    "ttft": 1789793.9916863744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24652086164802317,
    "arrivals": 372568,
    "finished_requests": 97218,
    "scheduler_time": 197.58080877033953
}
#Debug simulation 
Total elapsed time: 10.543921777047217. Arrivals time: 0.37764089833945036 Scheduler time: 10.001075064763427 Scheduler overhead time: 0.06246844073757529 Adapter cache time: 0.010278667788952589 Engine time: 0.06334408139809966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 10.57376599824056,
    "estimated_duration": 3600.0143522814724,
    "input_throughput": 6909.581064374363,
    "output_throughput": 6040.409807316177,
    "total_throughput": 12949.990871690541,
    "itl": 100.26832492304422,
    "ttft": 1762289.665897788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21820949805434783,
    "arrivals": 372217,
    "finished_requests": 100763,
    "scheduler_time": 190.68767365654014
}
#Debug simulation 
Total elapsed time: 10.57385436212644. Arrivals time: 0.28081597993150353 Scheduler time: 10.142438247334212 Scheduler overhead time: 0.05712595069780946 Adapter cache time: 0.009441971778869629 Engine time: 0.05772784259170294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.607717847917229,
    "estimated_duration": 3600.0587344767355,
    "input_throughput": 6839.491746119766,
    "output_throughput": 5977.967468669107,
    "total_throughput": 12817.459214788872,
    "itl": 97.56556903225355,
    "ttft": 1771143.0216728656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23843381050508472,
    "arrivals": 372217,
    "finished_requests": 99724,
    "scheduler_time": 192.6110495475679
}
#Debug simulation 
Total elapsed time: 10.607826123945415. Arrivals time: 0.4053740007802844 Scheduler time: 10.047556704375893 Scheduler overhead time: 0.0586924753151834 Adapter cache time: 0.00986640714108944 Engine time: 0.059606149327009916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.172768843825907,
    "estimated_duration": 3600.081544397736,
    "input_throughput": 6646.526948045274,
    "output_throughput": 5812.2094574667435,
    "total_throughput": 12458.736405512018,
    "itl": 91.23565610733115,
    "ttft": 1792366.7257404344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24429593820124867,
    "arrivals": 372217,
    "finished_requests": 96947,
    "scheduler_time": 197.58409518200347
}
#Debug simulation 
Total elapsed time: 10.172919271048158. Arrivals time: 0.34984885016456246 Scheduler time: 9.661265706177801 Scheduler overhead time: 0.061185195576399565 Adapter cache time: 0.010090782772749662 Engine time: 0.062400361988693476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 10.594076107256114,
    "estimated_duration": 3600.0809360347816,
    "input_throughput": 6839.478177710089,
    "output_throughput": 5977.978379481654,
    "total_throughput": 12817.456557191743,
    "itl": 97.56424131677792,
    "ttft": 1771167.984596159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22593982174526894,
    "arrivals": 372217,
    "finished_requests": 99726,
    "scheduler_time": 192.61374857860878
}
#Debug simulation 
Total elapsed time: 10.594195654150099. Arrivals time: 0.3759102779440582 Scheduler time: 10.063630452845246 Scheduler overhead time: 0.058499164413660765 Adapter cache time: 0.009641129989176989 Engine time: 0.05980164557695389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 10.169249484781176,
    "estimated_duration": 3600.0098677336528,
    "input_throughput": 6646.515670542679,
    "output_throughput": 5812.111846563427,
    "total_throughput": 12458.627517106106,
    "itl": 91.23611734678278,
    "ttft": 1792339.4902421392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24243188361171636,
    "arrivals": 372217,
    "finished_requests": 96945,
    "scheduler_time": 197.57935460590858
}
#Debug simulation 
Total elapsed time: 10.169401643797755. Arrivals time: 0.35932774981483817 Scheduler time: 9.647888252977282 Scheduler overhead time: 0.06135561550036073 Adapter cache time: 0.010196995455771685 Engine time: 0.06250868644565344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 10.553847704082727,
    "estimated_duration": 3600.040781878584,
    "input_throughput": 6839.666684873251,
    "output_throughput": 5978.177832965962,
    "total_throughput": 12817.844517839212,
    "itl": 97.56457530197308,
    "ttft": 1771063.3683667846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21066939103882756,
    "arrivals": 372217,
    "finished_requests": 99725,
    "scheduler_time": 192.6110517609686
}
#Debug simulation 
Total elapsed time: 10.553958526812494. Arrivals time: 0.3714518779888749 Scheduler time: 10.02823683898896 Scheduler overhead time: 0.05835497239604592 Adapter cache time: 0.009742348920553923 Engine time: 0.05946506420150399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.244397633709013,
    "estimated_duration": 3600.074060850637,
    "input_throughput": 6646.744093466242,
    "output_throughput": 5812.514588951445,
    "total_throughput": 12459.258682417687,
    "itl": 91.23611517231114,
    "ttft": 1792365.002198179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24015359466895464,
    "arrivals": 372217,
    "finished_requests": 96949,
    "scheduler_time": 197.5838108380964
}
#Debug simulation 
Total elapsed time: 10.244540869724005. Arrivals time: 0.3743157973513007 Scheduler time: 9.70764534547925 Scheduler overhead time: 0.0610434589907527 Adapter cache time: 0.01023293798789382 Engine time: 0.06309215584769845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.685640922747552,
    "estimated_duration": 3600.0564912461246,
    "input_throughput": 6894.277648240145,
    "output_throughput": 6047.904540648905,
    "total_throughput": 12942.18218888905,
    "itl": 100.4128543328221,
    "ttft": 1762722.518790513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 371082,
    "finished_requests": 100981,
    "scheduler_time": 190.37697797995963
}
#Debug simulation 
Total elapsed time: 9.685774544719607. Arrivals time: 0.3708510575816035 Scheduler time: 9.165342101827264 Scheduler overhead time: 0.056352823972702026 Adapter cache time: 0.009324663318693638 Engine time: 0.0579208773560822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.58999970369041,
    "estimated_duration": 3600.0551812651793,
    "input_throughput": 6820.621285968926,
    "output_throughput": 5983.169122543899,
    "total_throughput": 12803.790408512825,
    "itl": 97.7228912248554,
    "ttft": 1770540.674171412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2306616685912013,
    "arrivals": 371082,
    "finished_requests": 99895,
    "scheduler_time": 192.2890112502443
}
#Debug simulation 
Total elapsed time: 9.590125762857497. Arrivals time: 0.44104317110031843 Scheduler time: 8.996315888594836 Scheduler overhead time: 0.057817259803414345 Adapter cache time: 0.009632549714297056 Engine time: 0.05904250405728817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.329981550108641,
    "estimated_duration": 3600.0566411683053,
    "input_throughput": 6626.974900110924,
    "output_throughput": 5821.2942430813955,
    "total_throughput": 12448.26914319232,
    "itl": 91.39617859203796,
    "ttft": 1792268.8193595423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23621986482758078,
    "arrivals": 371082,
    "finished_requests": 97091,
    "scheduler_time": 197.23424597080762
}
#Debug simulation 
Total elapsed time: 9.330094811040908. Arrivals time: 0.36085005896165967 Scheduler time: 8.808191169053316 Scheduler overhead time: 0.06087804585695267 Adapter cache time: 0.010204626247286797 Engine time: 0.0620396388694644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 9.643920884933323,
    "estimated_duration": 3600.068748716291,
    "input_throughput": 6820.634747254677,
    "output_throughput": 5983.147129532073,
    "total_throughput": 12803.78187678675,
    "itl": 97.72073100316541,
    "ttft": 1770560.3195045567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21816767983138563,
    "arrivals": 371082,
    "finished_requests": 99896,
    "scheduler_time": 192.29087099036465
}
#Debug simulation 
Total elapsed time: 9.64403943484649. Arrivals time: 0.4177580079995096 Scheduler time: 9.074352159164846 Scheduler overhead time: 0.057300633285194635 Adapter cache time: 0.009778417646884918 Engine time: 0.05861900467425585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 9.296200778800994,
    "estimated_duration": 3600.0121354292464,
    "input_throughput": 6627.003494018884,
    "output_throughput": 5821.305098878847,
    "total_throughput": 12448.30859289773,
    "itl": 91.39696536644546,
    "ttft": 1792280.1440092179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23435581023804844,
    "arrivals": 371082,
    "finished_requests": 97090,
    "scheduler_time": 197.23020147329697
}
#Debug simulation 
Total elapsed time: 9.296316080726683. Arrivals time: 0.4681853810325265 Scheduler time: 8.66768978536129 Scheduler overhead time: 0.06084473943337798 Adapter cache time: 0.010003408882766962 Engine time: 0.06176080275326967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.895581128075719,
    "estimated_duration": 3600.07013004486,
    "input_throughput": 6820.593242080545,
    "output_throughput": 5983.191499586591,
    "total_throughput": 12803.784741667136,
    "itl": 97.72320925842071,
    "ttft": 1770521.1741208297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 371082,
    "finished_requests": 99896,
    "scheduler_time": 192.29023897989194
}
#Debug simulation 
Total elapsed time: 9.895706351380795. Arrivals time: 0.7606927007436752 Scheduler time: 8.98189745657146 Scheduler overhead time: 0.05779167404398322 Adapter cache time: 0.009629720356315374 Engine time: 0.05921865673735738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.261080350261182,
    "estimated_duration": 3600.0752824939104,
    "input_throughput": 6626.9016973092575,
    "output_throughput": 5821.203545910973,
    "total_throughput": 12448.10524322023,
    "itl": 91.39308564917778,
    "ttft": 1792326.9670236658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23228463847190142,
    "arrivals": 371082,
    "finished_requests": 97091,
    "scheduler_time": 197.23680609649105
}
#Debug simulation 
Total elapsed time: 9.26127921929583. Arrivals time: 0.4381302590481937 Scheduler time: 8.662027445621789 Scheduler overhead time: 0.06079199071973562 Adapter cache time: 0.010079918894916773 Engine time: 0.06225264398381114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.240451181307435,
    "estimated_duration": 3600.009753202921,
    "input_throughput": 6893.412990873412,
    "output_throughput": 6044.850567596052,
    "total_throughput": 12938.263558469464,
    "itl": 100.32243116483075,
    "ttft": 1761880.4924499365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21820949805434783,
    "arrivals": 370740,
    "finished_requests": 100910,
    "scheduler_time": 190.39085905129582
}
#Debug simulation 
Total elapsed time: 9.240592918358743. Arrivals time: 0.37587590515613556 Scheduler time: 8.715611045248806 Scheduler overhead time: 0.05634460784494877 Adapter cache time: 0.009485045447945595 Engine time: 0.057475556153804064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.144193606916815,
    "estimated_duration": 3600.016606972506,
    "input_throughput": 6827.055728687004,
    "output_throughput": 5981.712683850534,
    "total_throughput": 12808.768412537538,
    "itl": 97.59653160326687,
    "ttft": 1770562.379114411,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2384338105050847,
    "arrivals": 370740,
    "finished_requests": 99876,
    "scheduler_time": 192.32449362697872
}
#Debug simulation 
Total elapsed time: 9.144303158856928. Arrivals time: 0.3702879948541522 Scheduler time: 8.621118943206966 Scheduler overhead time: 0.057632952462881804 Adapter cache time: 0.009672929532825947 Engine time: 0.059216614346951246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.805797315202653,
    "estimated_duration": 3600.0439063598706,
    "input_throughput": 6639.310970006461,
    "output_throughput": 5818.872087363358,
    "total_throughput": 12458.183057369819,
    "itl": 91.29826822308138,
    "ttft": 1791967.156699068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24429593820124867,
    "arrivals": 370740,
    "finished_requests": 97158,
    "scheduler_time": 197.24754567547114
}
#Debug simulation 
Total elapsed time: 8.805972813162953. Arrivals time: 0.35510433325544 Scheduler time: 8.290623080451041 Scheduler overhead time: 0.06032929476350546 Adapter cache time: 0.010033194441348314 Engine time: 0.061971218790858984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 9.100647607818246,
    "estimated_duration": 3600.036437152348,
    "input_throughput": 6826.936457187845,
    "output_throughput": 5981.5877911039925,
    "total_throughput": 12808.524248291837,
    "itl": 97.59553417590571,
    "ttft": 1770709.4794870147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22593982174526897,
    "arrivals": 370740,
    "finished_requests": 99876,
    "scheduler_time": 192.3256898336523
}
#Debug simulation 
Total elapsed time: 9.100810751784593. Arrivals time: 0.36201545549556613 Scheduler time: 8.586198722943664 Scheduler overhead time: 0.05753150023519993 Adapter cache time: 0.00961361639201641 Engine time: 0.059097117744386196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 8.855444606859237,
    "estimated_duration": 3600.075296180367,
    "input_throughput": 6639.2530804451535,
    "output_throughput": 5818.821351382779,
    "total_throughput": 12458.074431827932,
    "itl": 91.2994821892498,
    "ttft": 1791938.8759320527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24243188361171636,
    "arrivals": 370740,
    "finished_requests": 97158,
    "scheduler_time": 197.24880001918024
}
#Debug simulation 
Total elapsed time: 8.855559435673058. Arrivals time: 0.35178591310977936 Scheduler time: 8.34310300881043 Scheduler overhead time: 0.060536669101566076 Adapter cache time: 0.010202806442975998 Engine time: 0.0619844482280314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.129735061898828,
    "estimated_duration": 3600.095612073683,
    "input_throughput": 6826.859241619769,
    "output_throughput": 5981.411695784504,
    "total_throughput": 12808.270937404273,
    "itl": 97.59472710008949,
    "ttft": 1770733.821723324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21066939103882756,
    "arrivals": 370740,
    "finished_requests": 99876,
    "scheduler_time": 192.32959160151165
}
#Debug simulation 
Total elapsed time: 9.129853615071625. Arrivals time: 0.3702412946149707 Scheduler time: 8.606838769745082 Scheduler overhead time: 0.05766727402806282 Adapter cache time: 0.009531439282000065 Engine time: 0.059166941326111555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.817475559655577,
    "estimated_duration": 3600.0323787635393,
    "input_throughput": 6639.019732430542,
    "output_throughput": 5818.615722338167,
    "total_throughput": 12457.635454768708,
    "itl": 91.29561523030581,
    "ttft": 1792011.4007983548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24015359466895464,
    "arrivals": 370740,
    "finished_requests": 97155,
    "scheduler_time": 197.24901630735636
}
#Debug simulation 
Total elapsed time: 8.817579083610326. Arrivals time: 0.3538915575481951 Scheduler time: 8.30250834254548 Scheduler overhead time: 0.060793285723775625 Adapter cache time: 0.010119413491338491 Engine time: 0.0623203688301146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.66260743001476,
    "estimated_duration": 3600.0541748300725,
    "input_throughput": 6948.099607745673,
    "output_throughput": 6044.897644082591,
    "total_throughput": 12992.997251828263,
    "itl": 100.2626984757083,
    "ttft": 1758586.7178862751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 369999,
    "finished_requests": 101355,
    "scheduler_time": 190.4530972755077
}
#Debug simulation 
Total elapsed time: 8.66273509990424. Arrivals time: 0.361137384083122 Scheduler time: 8.152765958104283 Scheduler overhead time: 0.0562753607518971 Adapter cache time: 0.009228404145687819 Engine time: 0.05729337548837066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.49369238410145,
    "estimated_duration": 3600.0589760102102,
    "input_throughput": 6870.197450879164,
    "output_throughput": 5982.96337463637,
    "total_throughput": 12853.160825515533,
    "itl": 97.59495531746664,
    "ttft": 1766079.690430898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2306616685912013,
    "arrivals": 369999,
    "finished_requests": 100277,
    "scheduler_time": 192.36556778273356
}
#Debug simulation 
Total elapsed time: 8.49383979709819. Arrivals time: 0.35094754630699754 Scheduler time: 7.991409516427666 Scheduler overhead time: 0.05726642347872257 Adapter cache time: 0.009469572454690933 Engine time: 0.058332523331046104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.234462330583483,
    "estimated_duration": 3600.095583021161,
    "input_throughput": 6673.683641432022,
    "output_throughput": 5817.2886016612465,
    "total_throughput": 12490.972243093269,
    "itl": 91.27009527939822,
    "ttft": 1787281.4408969246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23621986482758078,
    "arrivals": 369999,
    "finished_requests": 97413,
    "scheduler_time": 197.30727932305737
}
#Debug simulation 
Total elapsed time: 8.234567399602383. Arrivals time: 0.3534115757793188 Scheduler time: 7.720970554742962 Scheduler overhead time: 0.060622591990977526 Adapter cache time: 0.010040126275271177 Engine time: 0.06156820571050048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 8.536638255696744,
    "estimated_duration": 3600.0183457774883,
    "input_throughput": 6869.964990319704,
    "output_throughput": 5982.769789287967,
    "total_throughput": 12852.734779607672,
    "itl": 97.59477493179507,
    "ttft": 1766156.7656213804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2181676798313856,
    "arrivals": 369999,
    "finished_requests": 100274,
    "scheduler_time": 192.3632096593529
}
#Debug simulation 
Total elapsed time: 8.536815806757659. Arrivals time: 0.36067225178703666 Scheduler time: 8.023836095817387 Scheduler overhead time: 0.05767289409413934 Adapter cache time: 0.009510105941444635 Engine time: 0.058565810322761536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 8.2456318247132,
    "estimated_duration": 3600.050868235405,
    "input_throughput": 6673.512647274356,
    "output_throughput": 5817.3047455479955,
    "total_throughput": 12490.817392822351,
    "itl": 91.27226055326828,
    "ttft": 1787304.8249609254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23435581023804847,
    "arrivals": 369999,
    "finished_requests": 97412,
    "scheduler_time": 197.30382571907342
}
#Debug simulation 
Total elapsed time: 8.245802805759013. Arrivals time: 0.37012859201058745 Scheduler time: 7.716785523574799 Scheduler overhead time: 0.06025744415819645 Adapter cache time: 0.009960980154573917 Engine time: 0.0607483540661633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.412401583977044,
    "estimated_duration": 3600.014473767301,
    "input_throughput": 6869.922379539474,
    "output_throughput": 5982.931223457137,
    "total_throughput": 12852.85360299661,
    "itl": 97.59665087168612,
    "ttft": 1766090.6471501633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 369999,
    "finished_requests": 100274,
    "scheduler_time": 192.36149798371625
}
#Debug simulation 
Total elapsed time: 8.412539531011134. Arrivals time: 0.33268077950924635 Scheduler time: 7.93043525563553 Scheduler overhead time: 0.05644478090107441 Adapter cache time: 0.009396135341376066 Engine time: 0.05736449267715216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.19833024777472,
    "estimated_duration": 3600.0786462046326,
    "input_throughput": 6673.715038233734,
    "output_throughput": 5817.315969493848,
    "total_throughput": 12491.03100772758,
    "itl": 91.27244572189828,
    "ttft": 1787366.112647929,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23228463847190145,
    "arrivals": 369999,
    "finished_requests": 97413,
    "scheduler_time": 197.3038410119276
}
#Debug simulation 
Total elapsed time: 8.198464557994157. Arrivals time: 0.40254339389503 Scheduler time: 7.637446701060981 Scheduler overhead time: 0.05985470535233617 Adapter cache time: 0.010019681882113218 Engine time: 0.060728294774889946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 111.09899450698867,
    "estimated_duration": 3600.1059279513966,
    "input_throughput": 6857.49572209068,
    "output_throughput": 5988.413516562243,
    "total_throughput": 12845.909238652923,
    "itl": 98.31223866048303,
    "ttft": 1708718.2860199846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0645978541439387,
    "arrivals": 322758,
    "finished_requests": 99810,
    "scheduler_time": 191.26657393796344
}
#Debug simulation 
Total elapsed time: 111.09919555019587. Arrivals time: 0.6673801355063915 Scheduler time: 110.16825061058626 Scheduler overhead time: 0.10532213095575571 Adapter cache time: 0.01819992996752262 Engine time: 0.105411637108773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 104.06249732896686,
    "estimated_duration": 3600.0493863127376,
    "input_throughput": 6799.648108458891,
    "output_throughput": 5926.61008516135,
    "total_throughput": 12726.258193620242,
    "itl": 96.22016372012918,
    "ttft": 1714487.2549987258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.262675513676369,
    "arrivals": 322758,
    "finished_requests": 98785,
    "scheduler_time": 192.7166665489175
}
#Debug simulation 
Total elapsed time: 104.06270040664822. Arrivals time: 0.6473851292394102 Scheduler time: 103.15857573319227 Scheduler overhead time: 0.0998608348891139 Adapter cache time: 0.018897640984505415 Engine time: 0.10370193142443895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 68.12808772409335,
    "estimated_duration": 3600.036569819957,
    "input_throughput": 6634.786213072982,
    "output_throughput": 5786.4106644455005,
    "total_throughput": 12421.196877518483,
    "itl": 90.37035373429723,
    "ttft": 1747535.7137443065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2916826557787153,
    "arrivals": 322758,
    "finished_requests": 96444,
    "scheduler_time": 197.47427993019332
}
#Debug simulation 
Total elapsed time: 68.12830208521336. Arrivals time: 0.5156613010913134 Scheduler time: 67.39053358044475 Scheduler overhead time: 0.08685186505317688 Adapter cache time: 0.015781020279973745 Engine time: 0.08712427830323577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 110.1773367873393,
    "estimated_duration": 3600.0381342685528,
    "input_throughput": 6823.464942265397,
    "output_throughput": 5941.749837698893,
    "total_throughput": 12765.214779964292,
    "itl": 96.42853926312672,
    "ttft": 1711680.970001306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2274033277342085,
    "arrivals": 322758,
    "finished_requests": 99133,
    "scheduler_time": 192.5155765651654
}
#Debug simulation 
Total elapsed time: 110.17751519428566. Arrivals time: 0.7232395238243043 Scheduler time: 109.17525352491066 Scheduler overhead time: 0.11016673408448696 Adapter cache time: 0.020633303094655275 Engine time: 0.11183743458241224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 78.79959976812825,
    "estimated_duration": 3600.0710832854056,
    "input_throughput": 6636.981728203773,
    "output_throughput": 5788.790698260734,
    "total_throughput": 12425.772426464508,
    "itl": 90.51290729353414,
    "ttft": 1746254.883225886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3057602789811804,
    "arrivals": 322758,
    "finished_requests": 96516,
    "scheduler_time": 197.0733740597806
}
#Debug simulation 
Total elapsed time: 78.79976978106424. Arrivals time: 0.6175228017382324 Scheduler time: 77.93421816639602 Scheduler overhead time: 0.09781472804024816 Adapter cache time: 0.01810113014653325 Engine time: 0.09758773166686296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 99.64353533321992,
    "estimated_duration": 3600.005340799244,
    "input_throughput": 6850.29900386896,
    "output_throughput": 5968.880589279739,
    "total_throughput": 12819.1795931487,
    "itl": 96.56341944993689,
    "ttft": 1711336.6511083983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1235700855404112,
    "arrivals": 322758,
    "finished_requests": 99550,
    "scheduler_time": 192.6007942708101
}
#Debug simulation 
Total elapsed time: 99.64396588690579. Arrivals time: 0.6515152212232351 Scheduler time: 98.74364709341899 Scheduler overhead time: 0.09767851745709777 Adapter cache time: 0.017813123762607574 Engine time: 0.0994110694155097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 99.13256630999967,
    "estimated_duration": 3600.083422529181,
    "input_throughput": 6623.410682869175,
    "output_throughput": 5775.834212583849,
    "total_throughput": 12399.244895453025,
    "itl": 90.42308243547068,
    "ttft": 1730406.339149665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2742314474470926,
    "arrivals": 322758,
    "finished_requests": 96315,
    "scheduler_time": 197.5354495839387
}
#Debug simulation 
Total elapsed time: 99.13273062417284. Arrivals time: 0.6240886161103845 Scheduler time: 98.25251978635788 Scheduler overhead time: 0.10254353517666459 Adapter cache time: 0.01818680763244629 Engine time: 0.10072897793725133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 82.56124417716637,
    "estimated_duration": 3600.0040328013492,
    "input_throughput": 6906.100041408454,
    "output_throughput": 5999.541612511247,
    "total_throughput": 12905.641653919702,
    "itl": 99.15564741612697,
    "ttft": 1648356.449742291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9984737638244394,
    "arrivals": 287932,
    "finished_requests": 100601,
    "scheduler_time": 189.28451468820796
}
#Debug simulation 
Total elapsed time: 82.56139649590477. Arrivals time: 0.6290635908953846 Scheduler time: 81.70214134920388 Scheduler overhead time: 0.09132243460044265 Adapter cache time: 0.017078346107155085 Engine time: 0.09011725336313248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 89.4546259441413,
    "estimated_duration": 3600.093163030357,
    "input_throughput": 6844.570927508583,
    "output_throughput": 5944.258115250211,
    "total_throughput": 12788.829042758794,
    "itl": 96.47193086871542,
    "ttft": 1654825.5426036832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2654519556229948,
    "arrivals": 287932,
    "finished_requests": 99708,
    "scheduler_time": 191.15317619376447
}
#Debug simulation 
Total elapsed time: 89.4548273300752. Arrivals time: 0.6296041402965784 Scheduler time: 88.58266253583133 Scheduler overhead time: 0.09561278391629457 Adapter cache time: 0.01793594751507044 Engine time: 0.09548488399013877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 81.73410161305219,
    "estimated_duration": 3600.036832603706,
    "input_throughput": 6695.551773720813,
    "output_throughput": 5806.962254016823,
    "total_throughput": 12502.514027737636,
    "itl": 90.41044955942921,
    "ttft": 1684276.181376407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0374680323433196,
    "arrivals": 287932,
    "finished_requests": 97420,
    "scheduler_time": 195.8890025893498
}
#Debug simulation 
Total elapsed time: 81.73424281505868. Arrivals time: 0.6110482891090214 Scheduler time: 80.88312219688669 Scheduler overhead time: 0.09372759470716119 Adapter cache time: 0.019004682544618845 Engine time: 0.0936347977258265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 91.94656881969422,
    "estimated_duration": 3600.0179562874396,
    "input_throughput": 6856.64774446173,
    "output_throughput": 5949.7950454916545,
    "total_throughput": 12806.442789953384,
    "itl": 96.58583757976497,
    "ttft": 1647651.1197913461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0566995364334428,
    "arrivals": 287932,
    "finished_requests": 99793,
    "scheduler_time": 190.78252487332287
}
#Debug simulation 
Total elapsed time: 91.94672047672793. Arrivals time: 0.6283369967713952 Scheduler time: 91.07173978211358 Scheduler overhead time: 0.09811755083501339 Adapter cache time: 0.017552948091179132 Engine time: 0.09702857071533799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 80.28799443598837,
    "estimated_duration": 3600.0924712236515,
    "input_throughput": 6676.788219228696,
    "output_throughput": 5797.084426808178,
    "total_throughput": 12473.872646036874,
    "itl": 90.55722903442752,
    "ttft": 1678033.442043715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8683336018025924,
    "arrivals": 287932,
    "finished_requests": 97174,
    "scheduler_time": 195.69152105901543
}
#Debug simulation 
Total elapsed time: 80.28818590426818. Arrivals time: 0.6400119420140982 Scheduler time: 79.40370449889451 Scheduler overhead time: 0.0951002687215805 Adapter cache time: 0.019374750554561615 Engine time: 0.09582984307780862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 87.55432874476537,
    "estimated_duration": 3600.06223405295,
    "input_throughput": 6854.216509535227,
    "output_throughput": 5956.135090437057,
    "total_throughput": 12810.351599972284,
    "itl": 96.69752249442736,
    "ttft": 1654534.1040553288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1171861645998407,
    "arrivals": 287932,
    "finished_requests": 99803,
    "scheduler_time": 190.76860691669177
}
#Debug simulation 
Total elapsed time: 87.55447265179828. Arrivals time: 0.6342638684436679 Scheduler time: 86.67880343506113 Scheduler overhead time: 0.09528538398444653 Adapter cache time: 0.01835459377616644 Engine time: 0.09419936453923583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 98.92192410398275,
    "estimated_duration": 3600.0275023707954,
    "input_throughput": 6668.840441965942,
    "output_throughput": 5791.47631129751,
    "total_throughput": 12460.31675326345,
    "itl": 90.3710784620056,
    "ttft": 1661520.2806238264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.542857488468291,
    "arrivals": 287932,
    "finished_requests": 96931,
    "scheduler_time": 195.83823929163637
}
#Debug simulation 
Total elapsed time: 98.92205633269623. Arrivals time: 0.6172447497956455 Scheduler time: 98.04905384825543 Scheduler overhead time: 0.10029029520228505 Adapter cache time: 0.019297264516353607 Engine time: 0.10095186438411474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 104.0757133802399,
    "estimated_duration": 3600.101239373491,
    "input_throughput": 6872.93144131301,
    "output_throughput": 6015.434167003794,
    "total_throughput": 12888.365608316803,
    "itl": 99.27399710052305,
    "ttft": 1617802.1004011526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0910474902717384,
    "arrivals": 282072,
    "finished_requests": 99876,
    "scheduler_time": 188.66323844656674
}
#Debug simulation 
Total elapsed time: 104.07594653591514. Arrivals time: 0.6400127457454801 Scheduler time: 103.18405034579337 Scheduler overhead time: 0.10105994064360857 Adapter cache time: 0.01889910828322172 Engine time: 0.09855780843645334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 91.21390307787806,
    "estimated_duration": 3600.0617748600844,
    "input_throughput": 6764.027264767254,
    "output_throughput": 5946.812676799706,
    "total_throughput": 12710.83994156696,
    "itl": 96.88338121839635,
    "ttft": 1659028.4832094961,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6551639414858097,
    "arrivals": 282072,
    "finished_requests": 98522,
    "scheduler_time": 190.38245744546
}
#Debug simulation 
Total elapsed time: 91.21405110321939. Arrivals time: 0.6165788727812469 Scheduler time: 90.35892554465681 Scheduler overhead time: 0.09415895491838455 Adapter cache time: 0.018263536971062422 Engine time: 0.09342720359563828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 85.96840536221862,
    "estimated_duration": 3600.075445947744,
    "input_throughput": 6595.621218640611,
    "output_throughput": 5787.7267609636765,
    "total_throughput": 12383.347979604288,
    "itl": 90.55179875380175,
    "ttft": 1673944.0551645274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8177052448596844,
    "arrivals": 282072,
    "finished_requests": 96006,
    "scheduler_time": 195.36083327173313
}
#Debug simulation 
Total elapsed time: 85.96855980483815. Arrivals time: 0.5984418853186071 Scheduler time: 85.12949307914823 Scheduler overhead time: 0.09447495685890317 Adapter cache time: 0.01860013324767351 Engine time: 0.09365286631509662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 92.6334709469229,
    "estimated_duration": 3600.0430224248034,
    "input_throughput": 6761.805858532199,
    "output_throughput": 5944.710901145077,
    "total_throughput": 12706.516759677275,
    "itl": 96.88277025367627,
    "ttft": 1656389.861234467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6778947502514332,
    "arrivals": 282072,
    "finished_requests": 98520,
    "scheduler_time": 190.4054366478255
}
#Debug simulation 
Total elapsed time: 92.63370465300977. Arrivals time: 0.6257481542415917 Scheduler time: 91.76475949212909 Scheduler overhead time: 0.09638792043551803 Adapter cache time: 0.019006215035915375 Engine time: 0.09447426162660122 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 80.67024385230616,
    "estimated_duration": 3600.037712446373,
    "input_throughput": 6570.633112597529,
    "output_throughput": 5768.7420685066945,
    "total_throughput": 12339.375181104224,
    "itl": 90.28121316613017,
    "ttft": 1686874.1459065904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4276264690794076,
    "arrivals": 282072,
    "finished_requests": 95609,
    "scheduler_time": 195.74695214192607
}
#Debug simulation 
Total elapsed time: 80.67039548000321. Arrivals time: 0.592033542227 Scheduler time: 79.83693739399314 Scheduler overhead time: 0.0952231464907527 Adapter cache time: 0.017896268516778946 Engine time: 0.09412486292421818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 94.04297070391476,
    "estimated_duration": 3600.0482572980154,
    "input_throughput": 6751.329499744536,
    "output_throughput": 5946.841117087767,
    "total_throughput": 12698.170616832303,
    "itl": 96.86641351032063,
    "ttft": 1655025.466809047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.442766132568936,
    "arrivals": 282072,
    "finished_requests": 98441,
    "scheduler_time": 190.47750396006643
}
#Debug simulation 
Total elapsed time: 94.04313387116417. Arrivals time: 0.61963095003739 Scheduler time: 93.18206126103178 Scheduler overhead time: 0.0956222964450717 Adapter cache time: 0.01842373190447688 Engine time: 0.09463283140212297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 85.0093610282056,
    "estimated_duration": 3600.080386041217,
    "input_throughput": 6597.4074057045445,
    "output_throughput": 5786.591066347787,
    "total_throughput": 12383.998472052332,
    "itl": 90.53455452284976,
    "ttft": 1674669.6264560844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7826505730301176,
    "arrivals": 282072,
    "finished_requests": 95967,
    "scheduler_time": 195.38863845294438
}
#Debug simulation 
Total elapsed time: 85.00957102980465. Arrivals time: 0.60396917629987 Scheduler time: 84.16110687050968 Scheduler overhead time: 0.09633242199197412 Adapter cache time: 0.018396799452602863 Engine time: 0.09530052030459046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 90.02441780921072,
    "estimated_duration": 3600.1002415230473,
    "input_throughput": 6890.509523564107,
    "output_throughput": 6024.79890693931,
    "total_throughput": 12915.308430503417,
    "itl": 99.51993644843148,
    "ttft": 1614919.1081905297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1571715805912377,
    "arrivals": 279133,
    "finished_requests": 100265,
    "scheduler_time": 187.92613112259542
}
#Debug simulation 
Total elapsed time: 90.02458793716505. Arrivals time: 0.6896144566126168 Scheduler time: 89.09273602673784 Scheduler overhead time: 0.09578778175637126 Adapter cache time: 0.01823132997378707 Engine time: 0.09534238092601299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 82.60513542685658,
    "estimated_duration": 3600.0326498012673,
    "input_throughput": 6798.358620817246,
    "output_throughput": 5951.916575307405,
    "total_throughput": 12750.27519612465,
    "itl": 96.84392237123032,
    "ttft": 1646259.650473962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.172186252656394,
    "arrivals": 279133,
    "finished_requests": 98856,
    "scheduler_time": 190.11811552842704
}
#Debug simulation 
Total elapsed time: 82.60531189898029. Arrivals time: 0.6016482226550579 Scheduler time: 81.77427113708109 Scheduler overhead time: 0.09028455801308155 Adapter cache time: 0.017099666874855757 Engine time: 0.08958739880472422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 79.50888398196548,
    "estimated_duration": 3600.0830019712516,
    "input_throughput": 6606.632399024318,
    "output_throughput": 5783.068887189577,
    "total_throughput": 12389.701286213896,
    "itl": 90.48009583250737,
    "ttft": 1672167.5900276585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2674544356577109,
    "arrivals": 279133,
    "finished_requests": 96035,
    "scheduler_time": 195.13234829685032
}
#Debug simulation 
Total elapsed time: 79.50909617170691. Arrivals time: 0.5865305168554187 Scheduler time: 78.6854686611332 Scheduler overhead time: 0.09309030557051301 Adapter cache time: 0.01760376151651144 Engine time: 0.09238585690036416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 98.58360219281167,
    "estimated_duration": 3600.019901207715,
    "input_throughput": 6815.547045106267,
    "output_throughput": 5958.426505587905,
    "total_throughput": 12773.973550694172,
    "itl": 96.86109585349068,
    "ttft": 1619328.6867942493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.575194831276311,
    "arrivals": 279133,
    "finished_requests": 99134,
    "scheduler_time": 189.95653238671366
}
#Debug simulation 
Total elapsed time: 98.58374307211488. Arrivals time: 0.6342835687100887 Scheduler time: 97.70071571832523 Scheduler overhead time: 0.09860982466489077 Adapter cache time: 0.018863141536712646 Engine time: 0.09781752061098814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 93.14683145424351,
    "estimated_duration": 3600.056666703496,
    "input_throughput": 6610.369836703462,
    "output_throughput": 5789.288872245562,
    "total_throughput": 12399.658708949024,
    "itl": 90.40476144732085,
    "ttft": 1652785.625637171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6339240673184428,
    "arrivals": 279133,
    "finished_requests": 96154,
    "scheduler_time": 194.9011277839924
}
#Debug simulation 
Total elapsed time: 93.14699473232031. Arrivals time: 0.6148106409236789 Scheduler time: 92.27989207115024 Scheduler overhead time: 0.09986156830564141 Adapter cache time: 0.018911621067672968 Engine time: 0.09846933977678418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 81.69549426203594,
    "estimated_duration": 3600.0421328168864,
    "input_throughput": 6773.546280948576,
    "output_throughput": 5935.407756819955,
    "total_throughput": 12708.95403776853,
    "itl": 96.55863589135967,
    "ttft": 1646962.8730081313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9575881410855781,
    "arrivals": 279133,
    "finished_requests": 98494,
    "scheduler_time": 190.63868380327102
}
#Debug simulation 
Total elapsed time: 81.69577123317868. Arrivals time: 0.5935058556497097 Scheduler time: 80.87214616406709 Scheduler overhead time: 0.09029304562136531 Adapter cache time: 0.016714424826204777 Engine time: 0.09057463752105832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 83.34530476806685,
    "estimated_duration": 3600.033055846227,
    "input_throughput": 6607.862936527467,
    "output_throughput": 5783.245786087945,
    "total_throughput": 12391.108722615412,
    "itl": 90.54877628703295,
    "ttft": 1672472.496115319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5500054244510866,
    "arrivals": 279133,
    "finished_requests": 96068,
    "scheduler_time": 195.0770128246335
}
#Debug simulation 
Total elapsed time: 83.34553643781692. Arrivals time: 0.5873642079532146 Scheduler time: 82.51216733083129 Scheduler overhead time: 0.09663845133036375 Adapter cache time: 0.01908954046666622 Engine time: 0.09593137400224805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 90.64065041486174,
    "estimated_duration": 3600.081982334167,
    "input_throughput": 6873.900961542872,
    "output_throughput": 6000.779733908497,
    "total_throughput": 12874.680695451369,
    "itl": 98.91265213666671,
    "ttft": 1632162.2237889327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8860628102812906,
    "arrivals": 277639,
    "finished_requests": 100020,
    "scheduler_time": 188.7671320266247
}
#Debug simulation 
Total elapsed time: 90.64079905766994. Arrivals time: 0.6175055126659572 Scheduler time: 89.78369445865974 Scheduler overhead time: 0.09484834875911474 Adapter cache time: 0.017251869663596153 Engine time: 0.09513709787279367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 83.96303779492155,
    "estimated_duration": 3600.0242764557684,
    "input_throughput": 6538.999237853954,
    "output_throughput": 5711.796482729258,
    "total_throughput": 12250.795720583212,
    "itl": 89.2122500101805,
    "ttft": 1656904.0805991676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.972055967794732,
    "arrivals": 277639,
    "finished_requests": 95084,
    "scheduler_time": 197.796895372453
}
#Debug simulation 
Total elapsed time: 83.96325672278181. Arrivals time: 0.5935344956815243 Scheduler time: 83.12822171999142 Scheduler overhead time: 0.09517919644713402 Adapter cache time: 0.017313949298113585 Engine time: 0.09426219994202256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 86.82261822419241,
    "estimated_duration": 3600.03309748822,
    "input_throughput": 6627.855454064496,
    "output_throughput": 5780.00797118174,
    "total_throughput": 12407.863425246236,
    "itl": 90.38502299467505,
    "ttft": 1669099.467312973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5275975900096848,
    "arrivals": 277639,
    "finished_requests": 96404,
    "scheduler_time": 195.15799680724564
}
#Debug simulation 
Total elapsed time: 86.82275581313297. Arrivals time: 0.5986876222305 Scheduler time: 85.98265423253179 Scheduler overhead time: 0.09463670430704951 Adapter cache time: 0.018456834368407726 Engine time: 0.09391884272918105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 86.01982644200325,
    "estimated_duration": 3600.040166519925,
    "input_throughput": 6769.28419483653,
    "output_throughput": 5914.603453045154,
    "total_throughput": 12683.887647881684,
    "itl": 96.28495784423208,
    "ttft": 1643806.1504023643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9123624659422777,
    "arrivals": 277639,
    "finished_requests": 98511,
    "scheduler_time": 191.39108364413258
}
#Debug simulation 
Total elapsed time: 86.01997830485925. Arrivals time: 0.6614230172708631 Scheduler time: 85.12162687815726 Scheduler overhead time: 0.09332339465618134 Adapter cache time: 0.017417943105101585 Engine time: 0.09283548081293702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 88.41166330175474,
    "estimated_duration": 3600.0488482192045,
    "input_throughput": 6610.511691187738,
    "output_throughput": 5771.028359872678,
    "total_throughput": 12381.540051060416,
    "itl": 89.98914844473738,
    "ttft": 1667372.696639185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2253141109133143,
    "arrivals": 277639,
    "finished_requests": 96192,
    "scheduler_time": 195.49879169846162
}
#Debug simulation 
Total elapsed time: 88.41197887202725. Arrivals time: 0.5979456398636103 Scheduler time: 87.57011620607227 Scheduler overhead time: 0.09662302536889911 Adapter cache time: 0.018227061722427607 Engine time: 0.09451591316610575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 86.50792053202167,
    "estimated_duration": 3600.0099998040605,
    "input_throughput": 6777.851173004533,
    "output_throughput": 5913.56857374249,
    "total_throughput": 12691.419746747022,
    "itl": 96.27797754432459,
    "ttft": 1643371.6576790612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8618293269770206,
    "arrivals": 277639,
    "finished_requests": 98514,
    "scheduler_time": 191.3571431381641
}
#Debug simulation 
Total elapsed time: 86.50820579519495. Arrivals time: 0.6390620414167643 Scheduler time: 85.6328387549147 Scheduler overhead time: 0.09365683700889349 Adapter cache time: 0.01667563384398818 Engine time: 0.09224918438121676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 98.55109381023794,
    "estimated_duration": 3600.083917203436,
    "input_throughput": 6597.458988803299,
    "output_throughput": 5751.009830927237,
    "total_throughput": 12348.468819730537,
    "itl": 89.65593898619653,
    "ttft": 1668774.652927878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.120996881052852,
    "arrivals": 277639,
    "finished_requests": 95904,
    "scheduler_time": 196.18028557025494
}
#Debug simulation 
Total elapsed time: 98.5512431440875. Arrivals time: 0.612120290286839 Scheduler time: 97.67639114707708 Scheduler overhead time: 0.1050386717543006 Adapter cache time: 0.019199739210307598 Engine time: 0.10240590199828148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 86.61751221166924,
    "estimated_duration": 3600.0712824688876,
    "input_throughput": 6815.317274266233,
    "output_throughput": 5986.579517175506,
    "total_throughput": 12801.896791441739,
    "itl": 99.50438620081586,
    "ttft": 1632330.6911881594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.031535808984189,
    "arrivals": 276898,
    "finished_requests": 99783,
    "scheduler_time": 189.11131059252222
}
#Debug simulation 
Total elapsed time: 86.61771842185408. Arrivals time: 0.5797208463773131 Scheduler time: 85.80187488719821 Scheduler overhead time: 0.09370628092437983 Adapter cache time: 0.01722148945555091 Engine time: 0.09187243925407529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 87.3122551641427,
    "estimated_duration": 3600.004127904573,
    "input_throughput": 6751.319480888178,
    "output_throughput": 5928.295146823153,
    "total_throughput": 12679.614627711331,
    "itl": 96.85212592987307,
    "ttft": 1643056.1033537062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1644141107425108,
    "arrivals": 276898,
    "finished_requests": 98858,
    "scheduler_time": 190.91235824351386
}
#Debug simulation 
Total elapsed time: 87.31247027916834. Arrivals time: 0.5878953505307436 Scheduler time: 86.48649774119258 Scheduler overhead time: 0.09397369436919689 Adapter cache time: 0.0172877567820251 Engine time: 0.0937373605556786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 83.89543475303799,
    "estimated_duration": 3600.073105402958,
    "input_throughput": 6565.7702796438825,
    "output_throughput": 5763.7207335758985,
    "total_throughput": 12329.491013219782,
    "itl": 90.4904670271793,
    "ttft": 1664576.2696194935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2899738493841153,
    "arrivals": 276898,
    "finished_requests": 96105,
    "scheduler_time": 195.66680904821715
}
#Debug simulation 
Total elapsed time: 83.89558959100395. Arrivals time: 0.5774763249792159 Scheduler time: 83.07399554830045 Scheduler overhead time: 0.09549279464408755 Adapter cache time: 0.01833687210455537 Engine time: 0.09494005749002099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 87.95940779009834,
    "estimated_duration": 3600.0149982276384,
    "input_throughput": 6721.528108053155,
    "output_throughput": 5912.659255719593,
    "total_throughput": 12634.187363772748,
    "itl": 96.66432109933699,
    "ttft": 1637153.4719315886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0483702105935655,
    "arrivals": 276898,
    "finished_requests": 98469,
    "scheduler_time": 191.28499729059288
}
#Debug simulation 
Total elapsed time: 87.95963235618547. Arrivals time: 0.6016731834970415 Scheduler time: 87.11671236623079 Scheduler overhead time: 0.09558175317943096 Adapter cache time: 0.01782134175300598 Engine time: 0.09478836646303535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 86.67055200459436,
    "estimated_duration": 3600.0685017276273,
    "input_throughput": 6574.820725950398,
    "output_throughput": 5769.456050636962,
    "total_throughput": 12344.27677658736,
    "itl": 90.05046770561113,
    "ttft": 1664328.4694697454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2452425252925614,
    "arrivals": 276898,
    "finished_requests": 96199,
    "scheduler_time": 195.54436489887613
}
#Debug simulation 
Total elapsed time: 86.67070567561314. Arrivals time: 0.5935536841861904 Scheduler time: 85.82460919395089 Scheduler overhead time: 0.09950221329927444 Adapter cache time: 0.018312253523617983 Engine time: 0.09902885323390365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 84.76333031291142,
    "estimated_duration": 3600.0930080185726,
    "input_throughput": 6719.4644544236735,
    "output_throughput": 5910.617851429194,
    "total_throughput": 12630.082305852868,
    "itl": 96.5085472207684,
    "ttft": 1639146.923473979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9895077457884306,
    "arrivals": 276898,
    "finished_requests": 98444,
    "scheduler_time": 191.40995400602552
}
#Debug simulation 
Total elapsed time: 84.76346529088914. Arrivals time: 0.5952080488204956 Scheduler time: 83.92896592151374 Scheduler overhead time: 0.09516149526461959 Adapter cache time: 0.017922495491802692 Engine time: 0.09284770535305142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 83.2221584902145,
    "estimated_duration": 3600.0849182095226,
    "input_throughput": 6567.784798742878,
    "output_throughput": 5761.9565847120775,
    "total_throughput": 12329.741383454955,
    "itl": 90.48539678173253,
    "ttft": 1664243.8528208637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2727297582291075,
    "arrivals": 276898,
    "finished_requests": 96116,
    "scheduler_time": 195.76739283998998
}
#Debug simulation 
Total elapsed time: 83.2223718999885. Arrivals time: 0.5690439669415355 Scheduler time: 82.41070661973208 Scheduler overhead time: 0.09598360722884536 Adapter cache time: 0.01839090557768941 Engine time: 0.09366062795743346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 83.81110839592293,
    "estimated_duration": 3600.039307043668,
    "input_throughput": 6896.683308824448,
    "output_throughput": 5998.088120246759,
    "total_throughput": 12894.771429071208,
    "itl": 99.64481278436983,
    "ttft": 1633903.3297440838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 131,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8662255831854409,
    "arrivals": 276581,
    "finished_requests": 100247,
    "scheduler_time": 188.78621552384118
}
#Debug simulation 
Total elapsed time: 83.81125555280596. Arrivals time: 0.5956097664311528 Scheduler time: 82.98596485285088 Scheduler overhead time: 0.09076602756977081 Adapter cache time: 0.017366761341691017 Engine time: 0.08953197347000241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 84.46131471917033,
    "estimated_duration": 3600.0162386298443,
    "input_throughput": 6822.662558141157,
    "output_throughput": 5933.693790261925,
    "total_throughput": 12756.356348403082,
    "itl": 96.91734796419823,
    "ttft": 1641874.5094142482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9387481421185667,
    "arrivals": 276581,
    "finished_requests": 99162,
    "scheduler_time": 190.63210654077952
}
#Debug simulation 
Total elapsed time: 84.46146686701104. Arrivals time: 0.6096236710436642 Scheduler time: 83.61592738423496 Scheduler overhead time: 0.09323553182184696 Adapter cache time: 0.01747513934969902 Engine time: 0.09219013061374426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 81.68642758298665,
    "estimated_duration": 3600.0010033992785,
    "input_throughput": 6633.430651116798,
    "output_throughput": 5771.876724584183,
    "total_throughput": 12405.30737570098,
    "itl": 90.6415974175579,
    "ttft": 1665212.7514031336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9372713445359844,
    "arrivals": 276581,
    "finished_requests": 96445,
    "scheduler_time": 195.43682909764013
}
#Debug simulation 
Total elapsed time: 81.68663197895512. Arrivals time: 0.5798386144451797 Scheduler time: 80.86778036458418 Scheduler overhead time: 0.09462942834943533 Adapter cache time: 0.0172971417196095 Engine time: 0.09273238386958838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 83.6036237468943,
    "estimated_duration": 3600.0647865348146,
    "input_throughput": 6819.194224454436,
    "output_throughput": 5927.190554961877,
    "total_throughput": 12746.384779416314,
    "itl": 96.721733598579,
    "ttft": 1639614.1176996122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8535189565038309,
    "arrivals": 276581,
    "finished_requests": 99110,
    "scheduler_time": 190.87125314144814
}
#Debug simulation 
Total elapsed time: 83.60375552903861. Arrivals time: 0.606535576749593 Scheduler time: 82.76646857103333 Scheduler overhead time: 0.09113476751372218 Adapter cache time: 0.016619050409644842 Engine time: 0.09004511218518019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 80.70886876108125,
    "estimated_duration": 3600.0928118045263,
    "input_throughput": 6635.383655019237,
    "output_throughput": 5772.477568316749,
    "total_throughput": 12407.861223335985,
    "itl": 90.63294746617436,
    "ttft": 1664870.5254646174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0057084268890324,
    "arrivals": 276581,
    "finished_requests": 96480,
    "scheduler_time": 195.3938822074008
}
#Debug simulation 
Total elapsed time: 80.70901815313846. Arrivals time: 0.5792765151709318 Scheduler time: 79.89006285043433 Scheduler overhead time: 0.09501246362924576 Adapter cache time: 0.01729276403784752 Engine time: 0.09363235533237457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 82.88245474128053,
    "estimated_duration": 3600.0911368903367,
    "input_throughput": 6825.57748280374,
    "output_throughput": 5936.043613178942,
    "total_throughput": 12761.621095982682,
    "itl": 97.01790451298551,
    "ttft": 1642648.223391883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 131,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8362936432147386,
    "arrivals": 276581,
    "finished_requests": 99194,
    "scheduler_time": 190.56970184626041
}
#Debug simulation 
Total elapsed time: 82.88265296537429. Arrivals time: 0.6216031080111861 Scheduler time: 82.02914595976472 Scheduler overhead time: 0.09122717147693038 Adapter cache time: 0.017005116678774357 Engine time: 0.09058633539825678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_96_slots_32_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 80.50609001005068,
    "estimated_duration": 3600.021338064386,
    "input_throughput": 6633.533459232206,
    "output_throughput": 5773.210780793513,
    "total_throughput": 12406.744240025719,
    "itl": 90.68899030568748,
    "ttft": 1665787.2197346925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9224107768014076,
    "arrivals": 276581,
    "finished_requests": 96452,
    "scheduler_time": 195.37586548509864
}
#Debug simulation 
Total elapsed time: 80.50625083688647. Arrivals time: 0.5820121271535754 Scheduler time: 79.68577147368342 Scheduler overhead time: 0.09300028998404741 Adapter cache time: 0.017177562694996595 Engine time: 0.09394699474796653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 72.41516608325765,
    "estimated_duration": 3600.020541245125,
    "input_throughput": 6856.523932905886,
    "output_throughput": 6008.0043300306515,
    "total_throughput": 12864.528262936537,
    "itl": 98.96617792369241,
    "ttft": 1542954.0475191996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9918613547924895,
    "arrivals": 241359,
    "finished_requests": 100239,
    "scheduler_time": 186.30752599163065
}
#Debug simulation 
Total elapsed time: 72.41530901519582. Arrivals time: 0.6431908686645329 Scheduler time: 71.53903658408672 Scheduler overhead time: 0.09226959245279431 Adapter cache time: 0.017287205904722214 Engine time: 0.09173408569768071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 84.5970497042872,
    "estimated_duration": 3600.0775677503566,
    "input_throughput": 6797.817974597882,
    "output_throughput": 5951.3837123760895,
    "total_throughput": 12749.201686973973,
    "itl": 96.37477074621832,
    "ttft": 1541682.1633670833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1444313108734798,
    "arrivals": 241359,
    "finished_requests": 99314,
    "scheduler_time": 187.934533317899
}
#Debug simulation 
Total elapsed time: 84.59725765418261. Arrivals time: 0.6600191714242101 Scheduler time: 83.6824520691298 Scheduler overhead time: 0.10175715247169137 Adapter cache time: 0.018809134606271982 Engine time: 0.09944870183244348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 60.88650469481945,
    "estimated_duration": 3600.029547604611,
    "input_throughput": 6623.6895238446905,
    "output_throughput": 5803.179313875597,
    "total_throughput": 12426.868837720287,
    "itl": 90.71430351438566,
    "ttft": 1578560.525952964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.275530509031379,
    "arrivals": 241359,
    "finished_requests": 96835,
    "scheduler_time": 192.16578103510264
}
#Debug simulation 
Total elapsed time: 60.886657727882266. Arrivals time: 0.5853937640786171 Scheduler time: 60.069104083348066 Scheduler overhead time: 0.09115356858819723 Adapter cache time: 0.016945243813097477 Engine time: 0.09069745941087604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 64.9032923891209,
    "estimated_duration": 3600.094798735932,
    "input_throughput": 6785.940472616969,
    "output_throughput": 5944.190416184002,
    "total_throughput": 12730.130888800972,
    "itl": 96.11645508628615,
    "ttft": 1567979.2993611488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5299502007663235,
    "arrivals": 241359,
    "finished_requests": 99219,
    "scheduler_time": 188.21049783417928
}
#Debug simulation 
Total elapsed time: 64.90345472516492. Arrivals time: 0.6137077584862709 Scheduler time: 64.065230387263 Scheduler overhead time: 0.08883972885087132 Adapter cache time: 0.016846688464283943 Engine time: 0.08649174496531487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_96_slots_32_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 54.58306145342067,
    "estimated_duration": 3600.0054876718823,
    "input_throughput": 6627.627674931656,
    "output_throughput": 5795.002277480262,
    "total_throughput": 12422.629952411919,
    "itl": 90.45644517173245,
    "ttft": 1588574.9596439463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6602734729228577,
    "arrivals": 241359,
    "finished_requests": 96676,
    "scheduler_time": 192.46607719857502
}
#Debug simulation 
Total elapsed time: 54.583277616184205. Arrivals time: 0.5794767579063773 Scheduler time: 53.78489387175068 Scheduler overhead time: 0.08519543102011085 Adapter cache time: 0.016683862078934908 Engine time: 0.08447078941389918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_96_slots_32_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_96_slots_32_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 66.51812546607107,
    "estimated_duration": 3600.0846995177944,
    "input_throughput": 6782.760417628698,
    "output_throughput": 5947.735619350298,
    "total_throughput": 12730.496036978995,
    "itl": 96.28162998147434,
    "ttft": 1562186.5782366293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.340623397519808,
    "arrivals": 241359,
    "finished_requests": 99086,
    "scheduler_time": 188.08348294363034
}
#Debug simulation 
Total elapsed time: 66.5182940182276. Arrivals time: 0.6237642699852586 Scheduler time: 65.66622796142474 Scheduler overhead time: 0.08950673695653677 Adapter cache time: 0.017439159099012613 Engine time: 0.0891783619299531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_96_slots_32_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_96_slots_32_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 59.49335408397019,
    "estimated_duration": 3600.029194595307,
    "input_throughput": 6623.470452905722,
    "output_throughput": 5800.861290611719,
    "total_throughput": 12424.331743517441,
    "itl": 90.62470413665392,
    "ttft": 1578061.1615487442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2142831763066357,
    "arrivals": 241359,
    "finished_requests": 96777,
    "scheduler_time": 192.24952272515108
}
#Debug simulation 
Total elapsed time: 59.49350898619741. Arrivals time: 0.6045473939739168 Scheduler time: 58.661034520715475 Scheduler overhead time: 0.0893010119907558 Adapter cache time: 0.016915149055421352 Engine time: 0.08834619633853436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 83.77087466232479,
    "estimated_duration": 3600.0609936654264,
    "input_throughput": 6909.34849264159,
    "output_throughput": 6044.122596335712,
    "total_throughput": 12953.471088977303,
    "itl": 99.14091154140492,
    "ttft": 1509600.9087320636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3621562605816855,
    "arrivals": 235577,
    "finished_requests": 100655,
    "scheduler_time": 186.1772579742695
}
#Debug simulation 
Total elapsed time: 83.7710725623183. Arrivals time: 0.6466581202112138 Scheduler time: 82.87891671992838 Scheduler overhead time: 0.09805050538852811 Adapter cache time: 0.01963776722550392 Engine time: 0.09467908553779125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 99.81992483092472,
    "estimated_duration": 3600.0623851608643,
    "input_throughput": 6816.669372495733,
    "output_throughput": 5959.31395201124,
    "total_throughput": 12775.983324506973,
    "itl": 96.6676693333695,
    "ttft": 1492864.4781607967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0589377498207626,
    "arrivals": 235577,
    "finished_requests": 99235,
    "scheduler_time": 187.31918586510315
}
#Debug simulation 
Total elapsed time: 99.8200918706134. Arrivals time: 0.6661854912526906 Scheduler time: 98.88737006159499 Scheduler overhead time: 0.10801177332177758 Adapter cache time: 0.01929680909961462 Engine time: 0.10413087112829089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 56.72656579921022,
    "estimated_duration": 3600.0147332797483,
    "input_throughput": 6603.127976185642,
    "output_throughput": 5784.439382287887,
    "total_throughput": 12387.56735847353,
    "itl": 90.27629227492875,
    "ttft": 1572218.0563158193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9136822081450426,
    "arrivals": 235577,
    "finished_requests": 96323,
    "scheduler_time": 192.57408064629894
}
#Debug simulation 
Total elapsed time: 56.72673010500148. Arrivals time: 0.5436622747220099 Scheduler time: 55.96335598221049 Scheduler overhead time: 0.08560972474515438 Adapter cache time: 0.017300705425441265 Engine time: 0.08427435019984841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 79.62214026786387,
    "estimated_duration": 3600.0872172159393,
    "input_throughput": 6816.493745664732,
    "output_throughput": 5955.3962741436535,
    "total_throughput": 12771.890019808387,
    "itl": 96.4970111620756,
    "ttft": 1511907.410561495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9795353012206031,
    "arrivals": 235577,
    "finished_requests": 99180,
    "scheduler_time": 187.47434397179794
}
#Debug simulation 
Total elapsed time: 79.6223619687371. Arrivals time: 0.6639566118828952 Scheduler time: 78.70518052391708 Scheduler overhead time: 0.10148339672014117 Adapter cache time: 0.01870707981288433 Engine time: 0.09888794971629977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_96_slots_32_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 59.097012985963374,
    "estimated_duration": 3600.0974836352934,
    "input_throughput": 6610.11545053227,
    "output_throughput": 5788.660194544655,
    "total_throughput": 12398.775645076925,
    "itl": 90.24289671497634,
    "ttft": 1568112.314381032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4638085541548236,
    "arrivals": 235577,
    "finished_requests": 96419,
    "scheduler_time": 192.2736929330863
}
#Debug simulation 
Total elapsed time: 59.09715561661869. Arrivals time: 0.5612575281411409 Scheduler time: 58.308243182487786 Scheduler overhead time: 0.08901331666857004 Adapter cache time: 0.01760432031005621 Engine time: 0.08756817691028118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_96_slots_32_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_96_slots_32_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 76.74573234654963,
    "estimated_duration": 3600.064499744834,
    "input_throughput": 6825.680762592359,
    "output_throughput": 5962.125401231376,
    "total_throughput": 12787.806163823736,
    "itl": 96.48243250949866,
    "ttft": 1514068.654241177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9384363782638666,
    "arrivals": 235577,
    "finished_requests": 99321,
    "scheduler_time": 187.44545040135182
}
#Debug simulation 
Total elapsed time: 76.7458973606117. Arrivals time: 0.6523379562422633 Scheduler time: 75.85106399049982 Scheduler overhead time: 0.09561460604891181 Adapter cache time: 0.01771702477708459 Engine time: 0.09604754624888301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_96_slots_32_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_96_slots_32_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 51.4695060076192,
    "estimated_duration": 3600.041990704449,
    "input_throughput": 6636.005097075346,
    "output_throughput": 5795.616566104909,
    "total_throughput": 12431.621663180256,
    "itl": 90.40052419327064,
    "ttft": 1559388.2950457598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.868428422193979,
    "arrivals": 235577,
    "finished_requests": 96504,
    "scheduler_time": 192.19319940973986
}
#Debug simulation 
Total elapsed time: 51.469716522842646. Arrivals time: 0.5521198860369623 Scheduler time: 50.701474187429994 Scheduler overhead time: 0.08396883774548769 Adapter cache time: 0.017209540121257305 Engine time: 0.0828863624483347 
