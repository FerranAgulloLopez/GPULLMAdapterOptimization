INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.519070761278272,
    "estimated_duration": 3600.021877486439,
    "input_throughput": 4768.4149108520705,
    "output_throughput": 4135.036259944528,
    "total_throughput": 8903.451170796598,
    "itl": 49.360369937173274,
    "ttft": 16239.41440359949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.036110674324734,
    "arrivals": 69367,
    "finished_requests": 69091,
    "scheduler_time": 47.56453392127973
}
#Debug simulation 
Total elapsed time: 5.5192135772667825. Arrivals time: 0.19895140500739217 Scheduler time: 5.079629581421614 Scheduler overhead time: 0.08747955551370978 Adapter cache time: 0.025090443436056376 Engine time: 0.08707840368151665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.515032724943012,
    "estimated_duration": 3600.018688011494,
    "input_throughput": 4768.419135480108,
    "output_throughput": 4135.039923423996,
    "total_throughput": 8903.459058904104,
    "itl": 49.39300795920847,
    "ttft": 16241.131420454409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.213462349721064,
    "arrivals": 69367,
    "finished_requests": 69091,
    "scheduler_time": 47.57276416696197
}
#Debug simulation 
Total elapsed time: 5.515143529046327. Arrivals time: 0.1959994207136333 Scheduler time: 5.078755545429885 Scheduler overhead time: 0.08801571046933532 Adapter cache time: 0.025004392955452204 Engine time: 0.0863087666220963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.497999656014144,
    "estimated_duration": 3600.0331044870336,
    "input_throughput": 4768.400040156305,
    "output_throughput": 4135.023364492402,
    "total_throughput": 8903.423404648707,
    "itl": 49.4011360930858,
    "ttft": 16246.445901017101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.54860289279352,
    "arrivals": 69367,
    "finished_requests": 69091,
    "scheduler_time": 47.575867932207316
}
#Debug simulation 
Total elapsed time: 5.498096094001085. Arrivals time: 0.1909717172384262 Scheduler time: 5.06851819017902 Scheduler overhead time: 0.08761953515931964 Adapter cache time: 0.024727728683501482 Engine time: 0.08572927676141262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.497935856226832,
    "estimated_duration": 3600.033530682834,
    "input_throughput": 4768.399475641543,
    "output_throughput": 4135.022874960963,
    "total_throughput": 8903.422350602506,
    "itl": 49.36812802545882,
    "ttft": 16240.17710379145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.34193343160224,
    "arrivals": 69367,
    "finished_requests": 69091,
    "scheduler_time": 47.56696478784604
}
#Debug simulation 
Total elapsed time: 5.49802787322551. Arrivals time: 0.19040161557495594 Scheduler time: 5.067739190068096 Scheduler overhead time: 0.08774796780198812 Adapter cache time: 0.02490973938256502 Engine time: 0.08633173396810889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_64_slots_32_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.496161948889494,
    "estimated_duration": 3600.03886461051,
    "input_throughput": 4768.392410635056,
    "output_throughput": 4135.016748384617,
    "total_throughput": 8903.409159019673,
    "itl": 49.39777837417781,
    "ttft": 16241.650068946798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.41666925128996,
    "arrivals": 69367,
    "finished_requests": 69091,
    "scheduler_time": 47.574394887238604
}
#Debug simulation 
Total elapsed time: 5.496282153762877. Arrivals time: 0.18656121753156185 Scheduler time: 5.071078521665186 Scheduler overhead time: 0.08744795201346278 Adapter cache time: 0.024757543578743935 Engine time: 0.08583579631522298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_64_slots_32_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_64_slots_32_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.485630589071661,
    "estimated_duration": 3600.024274051413,
    "input_throughput": 4768.411736480097,
    "output_throughput": 4135.033507217792,
    "total_throughput": 8903.445243697888,
    "itl": 49.350103921204834,
    "ttft": 16238.778780947445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.654764049812533,
    "arrivals": 69367,
    "finished_requests": 69091,
    "scheduler_time": 47.56185007156482
}
#Debug simulation 
Total elapsed time: 5.485790926031768. Arrivals time: 0.1920701339840889 Scheduler time: 5.054580495227128 Scheduler overhead time: 0.08754063630476594 Adapter cache time: 0.024737545289099216 Engine time: 0.08620752114802599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_64_slots_32_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_64_slots_32_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46129286 . Total output tokens: 40647886
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.554877920076251,
    "estimated_duration": 3600.0033533683936,
    "input_throughput": 4768.439447129409,
    "output_throughput": 4135.057537119097,
    "total_throughput": 8903.496984248506,
    "itl": 49.393606197690694,
    "ttft": 16241.606157296377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.314146248865688,
    "arrivals": 69367,
    "finished_requests": 69091,
    "scheduler_time": 47.57329766706723
}
#Debug simulation 
Total elapsed time: 5.554977784398943. Arrivals time: 0.19865079689770937 Scheduler time: 5.115886745043099 Scheduler overhead time: 0.08769537787884474 Adapter cache time: 0.024977797642350197 Engine time: 0.0867752879858017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.220752662979066,
    "estimated_duration": 3600.015489619893,
    "input_throughput": 4727.347715327996,
    "output_throughput": 4100.037914436433,
    "total_throughput": 8827.38562976443,
    "itl": 48.95351539428202,
    "ttft": 13230.930691613512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1929,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.755337022631773,
    "arrivals": 68405,
    "finished_requests": 68170,
    "scheduler_time": 46.801602459596516
}
#Debug simulation 
Total elapsed time: 5.22084750700742. Arrivals time: 0.196494709700346 Scheduler time: 4.7834328156895936 Scheduler overhead time: 0.08727277116850019 Adapter cache time: 0.02671824675053358 Engine time: 0.0863128025084734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.221286986023188,
    "estimated_duration": 3600.0093403510646,
    "input_throughput": 4727.3557902325865,
    "output_throughput": 4100.044917816968,
    "total_throughput": 8827.400708049556,
    "itl": 48.985083156197334,
    "ttft": 13235.210369286353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1918,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.040718303480904,
    "arrivals": 68405,
    "finished_requests": 68170,
    "scheduler_time": 46.8109853650489
}
#Debug simulation 
Total elapsed time: 5.221389433834702. Arrivals time: 0.19841656787320971 Scheduler time: 4.782035449054092 Scheduler overhead time: 0.08735304465517402 Adapter cache time: 0.02650264510884881 Engine time: 0.0861267913132906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.231532949022949,
    "estimated_duration": 3600.04723071247,
    "input_throughput": 4727.306034991084,
    "output_throughput": 4100.001764998753,
    "total_throughput": 8827.307799989838,
    "itl": 48.99421754164652,
    "ttft": 13287.925691309096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1917,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.417246273485686,
    "arrivals": 68405,
    "finished_requests": 68170,
    "scheduler_time": 46.814083981837015
}
#Debug simulation 
Total elapsed time: 5.2318140338175. Arrivals time: 0.20167816570028663 Scheduler time: 4.789624197408557 Scheduler overhead time: 0.08681064145639539 Adapter cache time: 0.026360630989074707 Engine time: 0.0860754307359457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.242899377830327,
    "estimated_duration": 3600.052820424326,
    "input_throughput": 4727.298695021393,
    "output_throughput": 4099.995399028691,
    "total_throughput": 8827.294094050083,
    "itl": 48.95794073768435,
    "ttft": 13285.886964088331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1920,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.937356482819037,
    "arrivals": 68405,
    "finished_requests": 68170,
    "scheduler_time": 46.80365409168771
}
#Debug simulation 
Total elapsed time: 5.243007990065962. Arrivals time: 0.20136917661875486 Scheduler time: 4.799039086792618 Scheduler overhead time: 0.08801192091777921 Adapter cache time: 0.026446502655744553 Engine time: 0.08702876511961222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.23995623877272,
    "estimated_duration": 3600.0438980262434,
    "input_throughput": 4727.310411223197,
    "output_throughput": 4100.0055605134185,
    "total_throughput": 8827.315971736614,
    "itl": 48.99153524652637,
    "ttft": 13287.559114609328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1917,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.250516946310855,
    "arrivals": 68405,
    "finished_requests": 68170,
    "scheduler_time": 46.81284340009423
}
#Debug simulation 
Total elapsed time: 5.240059419069439. Arrivals time: 0.201659734826535 Scheduler time: 4.79877099301666 Scheduler overhead time: 0.08716726722195745 Adapter cache time: 0.026338082272559404 Engine time: 0.08557758759707212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.230905605014414,
    "estimated_duration": 3600.038589794861,
    "input_throughput": 4727.317381608889,
    "output_throughput": 4100.011605942555,
    "total_throughput": 8827.328987551446,
    "itl": 48.93998535057775,
    "ttft": 13234.373636104207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.269896047777094,
    "arrivals": 68405,
    "finished_requests": 68170,
    "scheduler_time": 46.79898957654119
}
#Debug simulation 
Total elapsed time: 5.230997593142092. Arrivals time: 0.19697547378018498 Scheduler time: 4.795294240582734 Scheduler overhead time: 0.0866196621209383 Adapter cache time: 0.02636415185406804 Engine time: 0.08529321383684874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45485399 . Total output tokens: 40094334
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.229406281374395,
    "estimated_duration": 3600.0133277159953,
    "input_throughput": 4727.350554226223,
    "output_throughput": 4100.0403766184145,
    "total_throughput": 8827.390930844638,
    "itl": 48.989721871954885,
    "ttft": 13235.439679982788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1918,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.155603913925338,
    "arrivals": 68405,
    "finished_requests": 68170,
    "scheduler_time": 46.81173852701083
}
#Debug simulation 
Total elapsed time: 5.229553858283907. Arrivals time: 0.19251628406345844 Scheduler time: 4.795426254160702 Scheduler overhead time: 0.08722146041691303 Adapter cache time: 0.026572054252028465 Engine time: 0.08700481941923499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.027719448786229,
    "estimated_duration": 3600.005862798241,
    "input_throughput": 4652.157979260106,
    "output_throughput": 4087.4097323170586,
    "total_throughput": 8739.567711577165,
    "itl": 48.70008853396929,
    "ttft": 11725.436359921947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2093,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.839772103871598,
    "arrivals": 67954,
    "finished_requests": 67739,
    "scheduler_time": 46.44358750488805
}
#Debug simulation 
Total elapsed time: 5.027811829000711. Arrivals time: 0.1880893367342651 Scheduler time: 4.6004651840776205 Scheduler overhead time: 0.08596426481381059 Adapter cache time: 0.027205917052924633 Engine time: 0.08574560470879078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.057175804395229,
    "estimated_duration": 3600.0339229203205,
    "input_throughput": 4651.9981085107875,
    "output_throughput": 4087.285929812521,
    "total_throughput": 8739.284038323309,
    "itl": 48.81590123763866,
    "ttft": 11942.845760106222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1899,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.973564423569211,
    "arrivals": 67954,
    "finished_requests": 67738,
    "scheduler_time": 46.489052454001694
}
#Debug simulation 
Total elapsed time: 5.057294572237879. Arrivals time: 0.18476197076961398 Scheduler time: 4.634178522974253 Scheduler overhead time: 0.0862996899522841 Adapter cache time: 0.025941350497305393 Engine time: 0.08551347209140658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.030631859321147,
    "estimated_duration": 3600.043786915884,
    "input_throughput": 4652.10897180438,
    "output_throughput": 4087.3666741164593,
    "total_throughput": 8739.475645920838,
    "itl": 48.752754894948794,
    "ttft": 11779.796166849548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2092,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.750245213331315,
    "arrivals": 67954,
    "finished_requests": 67739,
    "scheduler_time": 46.45831726379466
}
#Debug simulation 
Total elapsed time: 5.030788441188633. Arrivals time: 0.19024265836924314 Scheduler time: 4.6034766915254295 Scheduler overhead time: 0.08510903036221862 Adapter cache time: 0.02719575585797429 Engine time: 0.08475915668532252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.075700641144067,
    "estimated_duration": 3600.002008744681,
    "input_throughput": 4652.039348677973,
    "output_throughput": 4087.3221637814845,
    "total_throughput": 8739.361512459458,
    "itl": 48.783152125720434,
    "ttft": 11888.526289459329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1899,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.729718431481464,
    "arrivals": 67954,
    "finished_requests": 67738,
    "scheduler_time": 46.479459405405734
}
#Debug simulation 
Total elapsed time: 5.075800360180438. Arrivals time: 0.19320363318547606 Scheduler time: 4.644302157685161 Scheduler overhead time: 0.08612412447109818 Adapter cache time: 0.025980327278375626 Engine time: 0.08575232746079564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.094549556728452,
    "estimated_duration": 3600.0172532928295,
    "input_throughput": 4652.0196492618725,
    "output_throughput": 4087.304855703456,
    "total_throughput": 8739.324504965329,
    "itl": 48.82132459447828,
    "ttft": 11890.213104250393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1899,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.183701927005021,
    "arrivals": 67954,
    "finished_requests": 67738,
    "scheduler_time": 46.49027069255128
}
#Debug simulation 
Total elapsed time: 5.094667382072657. Arrivals time: 0.19773241691291332 Scheduler time: 4.657750109676272 Scheduler overhead time: 0.08647682005539536 Adapter cache time: 0.02609456656500697 Engine time: 0.08587588416412473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.043350506108254,
    "estimated_duration": 3600.0100304691205,
    "input_throughput": 4652.152593535297,
    "output_throughput": 4087.405000391767,
    "total_throughput": 8739.557593927064,
    "itl": 48.68816163190793,
    "ttft": 11725.462379897435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2094,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.367930449555372,
    "arrivals": 67954,
    "finished_requests": 67739,
    "scheduler_time": 46.44018624127767
}
#Debug simulation 
Total elapsed time: 5.043453050311655. Arrivals time: 0.20076236315071583 Scheduler time: 4.60269142780453 Scheduler overhead time: 0.08651465177536011 Adapter cache time: 0.027422272600233555 Engine time: 0.08588183764368296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45162460 . Total output tokens: 39809499
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.044826666358858,
    "estimated_duration": 3600.0450202284464,
    "input_throughput": 4652.1073780730785,
    "output_throughput": 4087.3652738560077,
    "total_throughput": 8739.472651929085,
    "itl": 48.745993456638395,
    "ttft": 11779.668501080867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2093,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.463593544382265,
    "arrivals": 67954,
    "finished_requests": 67739,
    "scheduler_time": 46.45618889069191
}
#Debug simulation 
Total elapsed time: 5.044919402338564. Arrivals time: 0.18019857490435243 Scheduler time: 4.6292400932870805 Scheduler overhead time: 0.0849322504363954 Adapter cache time: 0.027028704527765512 Engine time: 0.08386074798181653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.943377643823624,
    "estimated_duration": 3600.0564741753496,
    "input_throughput": 4677.595510180818,
    "output_throughput": 4033.154508590297,
    "total_throughput": 8710.750018771114,
    "itl": 47.868999036561995,
    "ttft": 13974.631061019154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2015,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.324004199379486,
    "arrivals": 67703,
    "finished_requests": 67445,
    "scheduler_time": 45.46874191920778
}
#Debug simulation 
Total elapsed time: 4.943483628798276. Arrivals time: 0.2019959418103099 Scheduler time: 4.501621071249247 Scheduler overhead time: 0.08597224904224277 Adapter cache time: 0.027130814734846354 Engine time: 0.0857914094813168 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.938748545013368,
    "estimated_duration": 3600.0157721147943,
    "input_throughput": 4677.514784916628,
    "output_throughput": 4033.0301084961557,
    "total_throughput": 8710.544893412783,
    "itl": 47.905319421503904,
    "ttft": 14028.727840529942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2011,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.70244577864626,
    "arrivals": 67703,
    "finished_requests": 67443,
    "scheduler_time": 45.478536324042004
}
#Debug simulation 
Total elapsed time: 4.938929859083146. Arrivals time: 0.19964159186929464 Scheduler time: 4.499866236932576 Scheduler overhead time: 0.08572704438120127 Adapter cache time: 0.026975546032190323 Engine time: 0.08639738243073225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.935735278762877,
    "estimated_duration": 3600.02425955264,
    "input_throughput": 4677.503757180938,
    "output_throughput": 4033.0206002012364,
    "total_throughput": 8710.524357382175,
    "itl": 47.914534034079985,
    "ttft": 14029.807508358015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2009,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.081639929711525,
    "arrivals": 67703,
    "finished_requests": 67443,
    "scheduler_time": 45.48158505677248
}
#Debug simulation 
Total elapsed time: 4.935821813996881. Arrivals time: 0.19630844239145517 Scheduler time: 4.501744447275996 Scheduler overhead time: 0.08559256605803967 Adapter cache time: 0.026885768864303827 Engine time: 0.08486307552084327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.937019089702517,
    "estimated_duration": 3600.037217765145,
    "input_throughput": 4677.5321979736655,
    "output_throughput": 4033.104693571308,
    "total_throughput": 8710.636891544973,
    "itl": 47.87122394051085,
    "ttft": 14027.712952236812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2015,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.361972024669372,
    "arrivals": 67703,
    "finished_requests": 67444,
    "scheduler_time": 45.468777998498986
}
#Debug simulation 
Total elapsed time: 4.937147573102266. Arrivals time: 0.19367341231554747 Scheduler time: 4.504957964643836 Scheduler overhead time: 0.08559489483013749 Adapter cache time: 0.026980575174093246 Engine time: 0.0855699353851378 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.9303189483471215,
    "estimated_duration": 3600.040859977774,
    "input_throughput": 4677.5274656476995,
    "output_throughput": 4033.100613221829,
    "total_throughput": 8710.628078869528,
    "itl": 47.90942637826212,
    "ttft": 14029.561734744719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2013,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.911228376282088,
    "arrivals": 67703,
    "finished_requests": 67444,
    "scheduler_time": 45.48055447966453
}
#Debug simulation 
Total elapsed time: 4.930418136995286. Arrivals time: 0.18817213783040643 Scheduler time: 4.504111182875931 Scheduler overhead time: 0.0856009665876627 Adapter cache time: 0.02686867630109191 Engine time: 0.08560494985431433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.930491630919278,
    "estimated_duration": 3600.0332243171156,
    "input_throughput": 4677.537386670707,
    "output_throughput": 4033.109167417239,
    "total_throughput": 8710.646554087945,
    "itl": 47.85646415111724,
    "ttft": 14028.133895392373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.876368537131375,
    "arrivals": 67703,
    "finished_requests": 67444,
    "scheduler_time": 45.46485608366323
}
#Debug simulation 
Total elapsed time: 4.930586754810065. Arrivals time: 0.1876325886696577 Scheduler time: 4.505085799377412 Scheduler overhead time: 0.08544558752328157 Adapter cache time: 0.02684174431487918 Engine time: 0.085511633194983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45006848 . Total output tokens: 39671291
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.933200247120112,
    "estimated_duration": 3600.009624356671,
    "input_throughput": 4677.522772736806,
    "output_throughput": 4033.0369957259686,
    "total_throughput": 8710.559768462774,
    "itl": 47.908122970340145,
    "ttft": 13923.784111893478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2011,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.821342514659921,
    "arrivals": 67703,
    "finished_requests": 67443,
    "scheduler_time": 45.479365333652275
}
#Debug simulation 
Total elapsed time: 4.93333948822692. Arrivals time: 0.1843184377066791 Scheduler time: 4.5122351329773664 Scheduler overhead time: 0.08561104349792004 Adapter cache time: 0.026783322915434837 Engine time: 0.08437784528359771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.804209333844483,
    "estimated_duration": 3600.000685593231,
    "input_throughput": 4571.28079610023,
    "output_throughput": 3992.7134062840873,
    "total_throughput": 8563.994202384318,
    "itl": 46.5875803372436,
    "ttft": 11445.402049650618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2090,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.819934876775747,
    "arrivals": 66533,
    "finished_requests": 66323,
    "scheduler_time": 44.4341692709503
}
#Debug simulation 
Total elapsed time: 4.804308793973178. Arrivals time: 0.1828723857179284 Scheduler time: 4.3844051053747535 Scheduler overhead time: 0.08494881307706237 Adapter cache time: 0.02748738881200552 Engine time: 0.0844734818674624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.8409887799061835,
    "estimated_duration": 3600.0357951029478,
    "input_throughput": 4571.236214480306,
    "output_throughput": 3992.6744671684473,
    "total_throughput": 8563.910681648755,
    "itl": 46.625957892603694,
    "ttft": 11445.646289813078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2088,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.271748065575684,
    "arrivals": 66533,
    "finished_requests": 66323,
    "scheduler_time": 44.44595081674762
}
#Debug simulation 
Total elapsed time: 4.841091627720743. Arrivals time: 0.18814571667462587 Scheduler time: 4.4121924000792205 Scheduler overhead time: 0.08570125931873918 Adapter cache time: 0.02752022724598646 Engine time: 0.08682272536680102 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.82183985831216,
    "estimated_duration": 3600.025911669214,
    "input_throughput": 4571.248764253924,
    "output_throughput": 3992.685428571083,
    "total_throughput": 8563.934192825007,
    "itl": 46.63691906898192,
    "ttft": 11445.878664162094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2088,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.685473598339266,
    "arrivals": 66533,
    "finished_requests": 66323,
    "scheduler_time": 44.449109643957726
}
#Debug simulation 
Total elapsed time: 4.821948366239667. Arrivals time: 0.19453089823946357 Scheduler time: 4.387060324661434 Scheduler overhead time: 0.08638601191341877 Adapter cache time: 0.027491008397191763 Engine time: 0.08608319004997611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.8224625387229025,
    "estimated_duration": 3600.023199274613,
    "input_throughput": 4571.252208406859,
    "output_throughput": 3992.6884368123638,
    "total_throughput": 8563.940645219223,
    "itl": 46.59475154533646,
    "ttft": 11445.68285126853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.087038391414469,
    "arrivals": 66533,
    "finished_requests": 66323,
    "scheduler_time": 44.43668015698727
}
#Debug simulation 
Total elapsed time: 4.822565488051623. Arrivals time: 0.18985238811001182 Scheduler time: 4.393516762647778 Scheduler overhead time: 0.08538607228547335 Adapter cache time: 0.027393409051001072 Engine time: 0.08625090727582574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.849824330769479,
    "estimated_duration": 3600.030984609616,
    "input_throughput": 4571.2423227336585,
    "output_throughput": 3992.6798023263896,
    "total_throughput": 8563.922125060048,
    "itl": 46.633528362215785,
    "ttft": 11445.919002832574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.51563601700091,
    "arrivals": 66533,
    "finished_requests": 66323,
    "scheduler_time": 44.44776204156792
}
#Debug simulation 
Total elapsed time: 4.849920324981213. Arrivals time: 0.1916387532837689 Scheduler time: 4.4178432282060385 Scheduler overhead time: 0.08598089125007391 Adapter cache time: 0.027534896042197943 Engine time: 0.08642415562644601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.81740170577541,
    "estimated_duration": 3600.00041447617,
    "input_throughput": 4571.281140364695,
    "output_throughput": 3992.7137069764763,
    "total_throughput": 8563.994847341171,
    "itl": 46.57490386520516,
    "ttft": 11445.329924728972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.336010844852515,
    "arrivals": 66533,
    "finished_requests": 66323,
    "scheduler_time": 44.43052250623286
}
#Debug simulation 
Total elapsed time: 4.817516318056732. Arrivals time: 0.19452805491164327 Scheduler time: 4.383864936418831 Scheduler overhead time: 0.08535694377496839 Adapter cache time: 0.027365966234356165 Engine time: 0.086221176199615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44258747 . Total output tokens: 38968819
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.768190630711615,
    "estimated_duration": 3600.0389785994553,
    "input_throughput": 4571.232172158929,
    "output_throughput": 3992.670936466336,
    "total_throughput": 8563.903108625265,
    "itl": 46.62925232664354,
    "ttft": 11445.847076087375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2088,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.395716668255307,
    "arrivals": 66533,
    "finished_requests": 66323,
    "scheduler_time": 44.44694004621008
}
#Debug simulation 
Total elapsed time: 4.768288106657565. Arrivals time: 0.19145969208329916 Scheduler time: 4.338431961834431 Scheduler overhead time: 0.0859228759072721 Adapter cache time: 0.027450188994407654 Engine time: 0.08487666258588433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.763158497400582,
    "estimated_duration": 3600.0434497832775,
    "input_throughput": 4564.826294246338,
    "output_throughput": 3960.4152557820685,
    "total_throughput": 8525.241550028406,
    "itl": 44.98743049506221,
    "ttft": 10373.416435383655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.639366132407725,
    "arrivals": 66085,
    "finished_requests": 65896,
    "scheduler_time": 43.44815989004484
}
#Debug simulation 
Total elapsed time: 4.763351762201637. Arrivals time: 0.19118187064304948 Scheduler time: 4.334505552425981 Scheduler overhead time: 0.08551710564643145 Adapter cache time: 0.02518780529499054 Engine time: 0.08602006500586867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.757671676110476,
    "estimated_duration": 3600.035594159812,
    "input_throughput": 4564.836255135783,
    "output_throughput": 3960.423897788572,
    "total_throughput": 8525.260152924355,
    "itl": 45.01833667108401,
    "ttft": 10373.54555309841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.812096889829101,
    "arrivals": 66085,
    "finished_requests": 65896,
    "scheduler_time": 43.45737605168523
}
#Debug simulation 
Total elapsed time: 4.757762399036437. Arrivals time: 0.1867840108461678 Scheduler time: 4.334554838947952 Scheduler overhead time: 0.0850717555731535 Adapter cache time: 0.025052870623767376 Engine time: 0.08612093795090914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.762428469024599,
    "estimated_duration": 3600.0078337026966,
    "input_throughput": 4564.843400103874,
    "output_throughput": 3960.4374930861472,
    "total_throughput": 8525.280893190022,
    "itl": 45.02564582221956,
    "ttft": 10373.804753957897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.139530902332437,
    "arrivals": 66085,
    "finished_requests": 65895,
    "scheduler_time": 43.45968382039028
}
#Debug simulation 
Total elapsed time: 4.762562884017825. Arrivals time: 0.18596533732488751 Scheduler time: 4.338844768702984 Scheduler overhead time: 0.08597135078161955 Adapter cache time: 0.02519428636878729 Engine time: 0.08612005319446325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.742030187975615,
    "estimated_duration": 3600.0461274264267,
    "input_throughput": 4564.822899018771,
    "output_throughput": 3960.412310103485,
    "total_throughput": 8525.235209122256,
    "itl": 44.99332973637544,
    "ttft": 10373.39163665622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.834232140691169,
    "arrivals": 66085,
    "finished_requests": 65896,
    "scheduler_time": 43.44969879628813
}
#Debug simulation 
Total elapsed time: 4.7421276438981295. Arrivals time: 0.17769660195335746 Scheduler time: 4.3235609834082425 Scheduler overhead time: 0.08630192279815674 Adapter cache time: 0.025214030407369137 Engine time: 0.08878965629264712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.789162785280496,
    "estimated_duration": 3600.0354095622793,
    "input_throughput": 4564.836489205011,
    "output_throughput": 3960.4241008655968,
    "total_throughput": 8525.260590070608,
    "itl": 45.022434679664045,
    "ttft": 10373.558100396547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.984815867915568,
    "arrivals": 66085,
    "finished_requests": 65896,
    "scheduler_time": 43.45873838334878
}
#Debug simulation 
Total elapsed time: 4.789260277990252. Arrivals time: 0.1793889799155295 Scheduler time: 4.370893873739988 Scheduler overhead time: 0.08608394302427769 Adapter cache time: 0.02505050878971815 Engine time: 0.08690668689087033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.752266637980938,
    "estimated_duration": 3600.044863850743,
    "input_throughput": 4564.824501220808,
    "output_throughput": 3960.4137001641325,
    "total_throughput": 8525.23820138494,
    "itl": 44.97898719688108,
    "ttft": 10373.466590180404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.27172879337825,
    "arrivals": 66085,
    "finished_requests": 65896,
    "scheduler_time": 43.44524246870694
}
#Debug simulation 
Total elapsed time: 4.75237694196403. Arrivals time: 0.17426732927560806 Scheduler time: 4.340204978827387 Scheduler overhead time: 0.08558131149038672 Adapter cache time: 0.02504949690774083 Engine time: 0.08647808199748397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 43922378 . Total output tokens: 38702961
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.769700031727552,
    "estimated_duration": 3600.027362803089,
    "input_throughput": 4564.846692499673,
    "output_throughput": 3960.4329531813764,
    "total_throughput": 8525.279645681048,
    "itl": 45.02005949383563,
    "ttft": 10373.593053182407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.902797465976152,
    "arrivals": 66085,
    "finished_requests": 65896,
    "scheduler_time": 43.458012167938584
}
#Debug simulation 
Total elapsed time: 4.769807583652437. Arrivals time: 0.1818402255885303 Scheduler time: 4.348091986961663 Scheduler overhead time: 0.08650361280888319 Adapter cache time: 0.025209831073880196 Engine time: 0.08695601671934128 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.704264308791608,
    "estimated_duration": 3600.033130522307,
    "input_throughput": 4562.326068820668,
    "output_throughput": 3948.34255259694,
    "total_throughput": 8510.668621417608,
    "itl": 44.2712478750467,
    "ttft": 8767.84290893643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.768054376365832,
    "arrivals": 65845,
    "finished_requests": 65686,
    "scheduler_time": 43.00635333124492
}
#Debug simulation 
Total elapsed time: 4.704365201760083. Arrivals time: 0.1795318117365241 Scheduler time: 4.2853780020959675 Scheduler overhead time: 0.08693085610866547 Adapter cache time: 0.02359272399917245 Engine time: 0.08762316266074777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.736454878933728,
    "estimated_duration": 3600.039620087072,
    "input_throughput": 4562.317844602708,
    "output_throughput": 3948.3354351684084,
    "total_throughput": 8510.653279771115,
    "itl": 44.29472003105364,
    "ttft": 8767.83385592438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1327,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.712532638278741,
    "arrivals": 65845,
    "finished_requests": 65686,
    "scheduler_time": 43.01382211085963
}
#Debug simulation 
Total elapsed time: 4.736625620163977. Arrivals time: 0.182569844648242 Scheduler time: 4.314608479384333 Scheduler overhead time: 0.08702023746445775 Adapter cache time: 0.023803655989468098 Engine time: 0.08658266719430685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.7569843381643295,
    "estimated_duration": 3600.024791365495,
    "input_throughput": 4562.336637068033,
    "output_throughput": 3948.3516986027607,
    "total_throughput": 8510.688335670793,
    "itl": 44.299444935814506,
    "ttft": 8767.82677876231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.97066893101663,
    "arrivals": 65845,
    "finished_requests": 65686,
    "scheduler_time": 43.015786337678925
}
#Debug simulation 
Total elapsed time: 4.757098664995283. Arrivals time: 0.18110873783007264 Scheduler time: 4.332417123019695 Scheduler overhead time: 0.08706128504127264 Adapter cache time: 0.023761951830238104 Engine time: 0.09120096685364842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.724149732384831,
    "estimated_duration": 3600.0176761921166,
    "input_throughput": 4562.345654194921,
    "output_throughput": 3948.359502233026,
    "total_throughput": 8510.705156427946,
    "itl": 44.27298486747804,
    "ttft": 8767.80735622746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.855169260697476,
    "arrivals": 65845,
    "finished_requests": 65686,
    "scheduler_time": 43.00690055485232
}
#Debug simulation 
Total elapsed time: 4.724263531155884. Arrivals time: 0.1805134122259915 Scheduler time: 4.303797450847924 Scheduler overhead time: 0.08739841170608997 Adapter cache time: 0.023743195924907923 Engine time: 0.08777459664270282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.716814678162336,
    "estimated_duration": 3600.01218909942,
    "input_throughput": 4562.3526080640195,
    "output_throughput": 3948.3655202722575,
    "total_throughput": 8510.718128336277,
    "itl": 44.29643076339521,
    "ttft": 8767.827946369447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.837338834772758,
    "arrivals": 65845,
    "finished_requests": 65686,
    "scheduler_time": 43.014674606247816
}
#Debug simulation 
Total elapsed time: 4.716920416336507. Arrivals time: 0.18230990879237652 Scheduler time: 4.295759363099933 Scheduler overhead time: 0.08730884315446019 Adapter cache time: 0.023621670436114073 Engine time: 0.08682377310469747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.726771300658584,
    "estimated_duration": 3600.0209304513346,
    "input_throughput": 4562.341530036842,
    "output_throughput": 3948.3559330911917,
    "total_throughput": 8510.697463128035,
    "itl": 44.26312464585508,
    "ttft": 8767.766842650184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.465079167196548,
    "arrivals": 65845,
    "finished_requests": 65686,
    "scheduler_time": 43.00388285149858
}
#Debug simulation 
Total elapsed time: 4.72688446380198. Arrivals time: 0.18444429710507393 Scheduler time: 4.3007540898397565 Scheduler overhead time: 0.08822486875578761 Adapter cache time: 0.023747379891574383 Engine time: 0.08804869232699275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43777438 . Total output tokens: 38558919
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.713168846908957,
    "estimated_duration": 3600.003326755507,
    "input_throughput": 4562.363839480826,
    "output_throughput": 3948.3752402002574,
    "total_throughput": 8510.739079681083,
    "itl": 44.294808532487586,
    "ttft": 8713.36763099337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.7855061751231,
    "arrivals": 65845,
    "finished_requests": 65686,
    "scheduler_time": 43.01413426994248
}
#Debug simulation 
Total elapsed time: 4.713362306822091. Arrivals time: 0.183321263641119 Scheduler time: 4.291086546611041 Scheduler overhead time: 0.08680400997400284 Adapter cache time: 0.023724395781755447 Engine time: 0.08675496373325586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.692020402755588,
    "estimated_duration": 3599.9748594803305,
    "input_throughput": 4474.7712494651705,
    "output_throughput": 3925.1115775957837,
    "total_throughput": 8399.882827060954,
    "itl": 43.23193468426503,
    "ttft": 9241.425109683367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.200913435793646,
    "arrivals": 65172,
    "finished_requests": 65006,
    "scheduler_time": 42.3119084951809
}
#Debug simulation 
Total elapsed time: 4.69212593883276. Arrivals time: 0.1862292429432273 Scheduler time: 4.263209941331297 Scheduler overhead time: 0.08918906608596444 Adapter cache time: 0.022627814672887325 Engine time: 0.08914728695526719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.674027726985514,
    "estimated_duration": 3599.967473284015,
    "input_throughput": 4474.780430531156,
    "output_throughput": 3925.1196309031784,
    "total_throughput": 8399.900061434333,
    "itl": 43.24889387940275,
    "ttft": 9241.414181091757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.962714772853081,
    "arrivals": 65172,
    "finished_requests": 65006,
    "scheduler_time": 42.317723251294325
}
#Debug simulation 
Total elapsed time: 4.674274298828095. Arrivals time: 0.18695309152826667 Scheduler time: 4.2425095634534955 Scheduler overhead time: 0.08851483231410384 Adapter cache time: 0.02214637165889144 Engine time: 0.09223552793264389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.702123707160354,
    "estimated_duration": 3599.955989826039,
    "input_throughput": 4474.794704581496,
    "output_throughput": 3925.132151596892,
    "total_throughput": 8399.926856178388,
    "itl": 43.25387596309626,
    "ttft": 9241.46982489384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.177964795473969,
    "arrivals": 65172,
    "finished_requests": 65006,
    "scheduler_time": 42.319350774986546
}
#Debug simulation 
Total elapsed time: 4.702325428370386. Arrivals time: 0.18598122149705887 Scheduler time: 4.273556643631309 Scheduler overhead time: 0.08903028117492795 Adapter cache time: 0.02204003557562828 Engine time: 0.08911761688068509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.693830932024866,
    "estimated_duration": 3599.9442413885226,
    "input_throughput": 4474.809308098235,
    "output_throughput": 3925.1449612869137,
    "total_throughput": 8399.954269385149,
    "itl": 43.2351072434766,
    "ttft": 9241.390588984284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.358838649461961,
    "arrivals": 65172,
    "finished_requests": 65006,
    "scheduler_time": 42.31271254600444
}
#Debug simulation 
Total elapsed time: 4.6939322226680815. Arrivals time: 0.1882998440414667 Scheduler time: 4.264280730858445 Scheduler overhead time: 0.08865648228675127 Adapter cache time: 0.02214350225403905 Engine time: 0.08847820712253451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.671051662880927,
    "estimated_duration": 3599.967765848179,
    "input_throughput": 4474.780066872234,
    "output_throughput": 3925.119311914393,
    "total_throughput": 8399.899378786626,
    "itl": 43.25128599009454,
    "ttft": 9241.549560946889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.087868823646577,
    "arrivals": 65172,
    "finished_requests": 65006,
    "scheduler_time": 42.31870759356495
}
#Debug simulation 
Total elapsed time: 4.67114955605939. Arrivals time: 0.17413253337144852 Scheduler time: 4.255772227887064 Scheduler overhead time: 0.08850380126386881 Adapter cache time: 0.02194472448900342 Engine time: 0.08894152380526066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.69959747698158,
    "estimated_duration": 3599.9698720376573,
    "input_throughput": 4474.7774488684645,
    "output_throughput": 3925.117015493787,
    "total_throughput": 8399.89446436225,
    "itl": 43.22546961489795,
    "ttft": 9241.45644664824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.952089904281276,
    "arrivals": 65172,
    "finished_requests": 65006,
    "scheduler_time": 42.30988560054947
}
#Debug simulation 
Total elapsed time: 4.699714895803481. Arrivals time: 0.1759300804696977 Scheduler time: 4.282409172039479 Scheduler overhead time: 0.0883307782933116 Adapter cache time: 0.022093905601650476 Engine time: 0.08891160320490599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43289625 . Total output tokens: 38134959
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.680294780060649,
    "estimated_duration": 3599.9495183479,
    "input_throughput": 4474.802748732105,
    "output_throughput": 3925.139207642201,
    "total_throughput": 8399.941956374307,
    "itl": 43.25030246395759,
    "ttft": 9241.4572705529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.027183490898475,
    "arrivals": 65172,
    "finished_requests": 65006,
    "scheduler_time": 42.318057395521166
}
#Debug simulation 
Total elapsed time: 4.680400458164513. Arrivals time: 0.16982420394197106 Scheduler time: 4.270297464448959 Scheduler overhead time: 0.08855980494990945 Adapter cache time: 0.021952216047793627 Engine time: 0.08797705313190818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.625000529922545,
    "estimated_duration": 3600.0120708320874,
    "input_throughput": 4458.553383764098,
    "output_throughput": 3901.757472927972,
    "total_throughput": 8360.31085669207,
    "itl": 42.43334823555324,
    "ttft": 10000.884458623543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.468462269422707,
    "arrivals": 64890,
    "finished_requests": 64711,
    "scheduler_time": 41.712085385299716
}
#Debug simulation 
Total elapsed time: 4.6251118732616305. Arrivals time: 0.17112789303064346 Scheduler time: 4.211333030834794 Scheduler overhead time: 0.08999795466661453 Adapter cache time: 0.02037243777886033 Engine time: 0.08988627977669239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.633163668215275,
    "estimated_duration": 3600.0189823474593,
    "input_throughput": 4458.545101763254,
    "output_throughput": 3901.7505376709983,
    "total_throughput": 8360.295639434253,
    "itl": 42.445801256021554,
    "ttft": 9945.4548861959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.043024153173909,
    "arrivals": 64890,
    "finished_requests": 64712,
    "scheduler_time": 41.71666184641024
}
#Debug simulation 
Total elapsed time: 4.633263459894806. Arrivals time: 0.17533359676599503 Scheduler time: 4.216074604075402 Scheduler overhead time: 0.0892203226685524 Adapter cache time: 0.02037652349099517 Engine time: 0.08970611169934273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.642961954232305,
    "estimated_duration": 3599.998761100263,
    "input_throughput": 4458.569867700288,
    "output_throughput": 3901.771898306716,
    "total_throughput": 8360.341766007004,
    "itl": 42.45032218948782,
    "ttft": 10000.96038722293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.205573308719346,
    "arrivals": 64890,
    "finished_requests": 64711,
    "scheduler_time": 41.71766260692113
}
#Debug simulation 
Total elapsed time: 4.643112091347575. Arrivals time: 0.17595192603766918 Scheduler time: 4.224410763941705 Scheduler overhead time: 0.08927384903654456 Adapter cache time: 0.02044716803357005 Engine time: 0.0903335134498775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.641509104985744,
    "estimated_duration": 3599.9956084194514,
    "input_throughput": 4458.573772273848,
    "output_throughput": 3901.775315266828,
    "total_throughput": 8360.349087540675,
    "itl": 42.43500275812151,
    "ttft": 10000.76647670453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.536323497914694,
    "arrivals": 64890,
    "finished_requests": 64711,
    "scheduler_time": 41.712341378529146
}
#Debug simulation 
Total elapsed time: 4.6416305638849735. Arrivals time: 0.17716183187440038 Scheduler time: 4.221427037846297 Scheduler overhead time: 0.08958656899631023 Adapter cache time: 0.020577250979840755 Engine time: 0.08988012094050646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.619120566640049,
    "estimated_duration": 3599.991626483675,
    "input_throughput": 4458.578703883767,
    "output_throughput": 3901.7796310042877,
    "total_throughput": 8360.358334888055,
    "itl": 42.44865631410722,
    "ttft": 10000.77318443836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.129975539254981,
    "arrivals": 64890,
    "finished_requests": 64711,
    "scheduler_time": 41.71701031563439
}
#Debug simulation 
Total elapsed time: 4.619226330891252. Arrivals time: 0.17836515977978706 Scheduler time: 4.19811089662835 Scheduler overhead time: 0.08909917483106256 Adapter cache time: 0.020466595888137817 Engine time: 0.0902128885500133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.665753746870905,
    "estimated_duration": 3599.983044920937,
    "input_throughput": 4458.58933214851,
    "output_throughput": 3901.7889319832857,
    "total_throughput": 8360.378264131796,
    "itl": 42.4302039535862,
    "ttft": 10000.92015251667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.279502617851805,
    "arrivals": 64890,
    "finished_requests": 64711,
    "scheduler_time": 41.71031094887382
}
#Debug simulation 
Total elapsed time: 4.666059651877731. Arrivals time: 0.18035374861210585 Scheduler time: 4.2426772667095065 Scheduler overhead time: 0.0900240526534617 Adapter cache time: 0.020550108049064875 Engine time: 0.08956824289634824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43139668 . Total output tokens: 38003095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.652815676294267,
    "estimated_duration": 3600.0167241942154,
    "input_throughput": 4458.547898438619,
    "output_throughput": 3901.75298509036,
    "total_throughput": 8360.300883528978,
    "itl": 42.448224935053766,
    "ttft": 9945.362657608614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.091658861581261,
    "arrivals": 64890,
    "finished_requests": 64712,
    "scheduler_time": 41.71702383863569
}
#Debug simulation 
Total elapsed time: 4.652925636153668. Arrivals time: 0.1800390318967402 Scheduler time: 4.230046411510557 Scheduler overhead time: 0.08985331607982516 Adapter cache time: 0.020669784396886826 Engine time: 0.08956118393689394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.628855617251247,
    "estimated_duration": 3600.0437627239667,
    "input_throughput": 4402.62425810274,
    "output_throughput": 3870.1985082141637,
    "total_throughput": 8272.822766316904,
    "itl": 41.52472423720342,
    "ttft": 10463.323484928378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9343833740102725,
    "arrivals": 64419,
    "finished_requests": 64233,
    "scheduler_time": 40.97292919014628
}
#Debug simulation 
Total elapsed time: 4.628971825353801. Arrivals time: 0.17871700366958976 Scheduler time: 4.204713029786944 Scheduler overhead time: 0.09141652472317219 Adapter cache time: 0.019132150802761316 Engine time: 0.09196168696507812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.624330193735659,
    "estimated_duration": 3600.0407767231113,
    "input_throughput": 4402.627909794655,
    "output_throughput": 3870.2017182933746,
    "total_throughput": 8272.82962808803,
    "itl": 41.53359498838429,
    "ttft": 10463.320521367219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.359831416783861,
    "arrivals": 64419,
    "finished_requests": 64233,
    "scheduler_time": 40.97607502771343
}
#Debug simulation 
Total elapsed time: 4.624442401807755. Arrivals time: 0.17919847182929516 Scheduler time: 4.201554622501135 Scheduler overhead time: 0.09100949298590422 Adapter cache time: 0.018926361110061407 Engine time: 0.09048921521753073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.65662630693987,
    "estimated_duration": 3600.0335588151506,
    "input_throughput": 4402.63673686877,
    "output_throughput": 3870.209477876538,
    "total_throughput": 8272.846214745308,
    "itl": 41.53564626631875,
    "ttft": 10463.274521373207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.481058400957878,
    "arrivals": 64419,
    "finished_requests": 64233,
    "scheduler_time": 40.976936726099204
}
#Debug simulation 
Total elapsed time: 4.656740210019052. Arrivals time: 0.18056636350229383 Scheduler time: 4.2309943423606455 Scheduler overhead time: 0.09106702217832208 Adapter cache time: 0.019135055132210255 Engine time: 0.09147398499771953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.639172398019582,
    "estimated_duration": 3600.0093524337967,
    "input_throughput": 4402.666340098479,
    "output_throughput": 3870.2355010774163,
    "total_throughput": 8272.901841175895,
    "itl": 41.52600656042301,
    "ttft": 10463.237971941619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.014164394428946,
    "arrivals": 64419,
    "finished_requests": 64233,
    "scheduler_time": 40.97314087675098
}
#Debug simulation 
Total elapsed time: 4.639307976234704. Arrivals time: 0.1800739369355142 Scheduler time: 4.214278139639646 Scheduler overhead time: 0.09119777148589492 Adapter cache time: 0.019085523672401905 Engine time: 0.09110811352729797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.616647210903466,
    "estimated_duration": 3600.037154177806,
    "input_throughput": 4402.632339948675,
    "output_throughput": 3870.2056126923667,
    "total_throughput": 8272.83795264104,
    "itl": 41.53442614860707,
    "ttft": 10463.391173620012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.429486223980819,
    "arrivals": 64419,
    "finished_requests": 64233,
    "scheduler_time": 40.976598071406016
}
#Debug simulation 
Total elapsed time: 4.616767725907266. Arrivals time: 0.1792263137176633 Scheduler time: 4.192967243958265 Scheduler overhead time: 0.09137158002704382 Adapter cache time: 0.018978368490934372 Engine time: 0.09105214243754745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.641866095829755,
    "estimated_duration": 3600.0126422090243,
    "input_throughput": 4402.662316839646,
    "output_throughput": 3870.2319643662595,
    "total_throughput": 8272.894281205907,
    "itl": 41.52141420870565,
    "ttft": 10463.276039847946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.792049038698879,
    "arrivals": 64419,
    "finished_requests": 64233,
    "scheduler_time": 40.971497732616264
}
#Debug simulation 
Total elapsed time: 4.641989508643746. Arrivals time: 0.1816833126358688 Scheduler time: 4.216306603047997 Scheduler overhead time: 0.09142485307529569 Adapter cache time: 0.019053224474191666 Engine time: 0.09037059405818582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 42825638 . Total output tokens: 37707665
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.625379031989723,
    "estimated_duration": 3600.019051941466,
    "input_throughput": 4402.654478023497,
    "output_throughput": 3870.225073527344,
    "total_throughput": 8272.879551550841,
    "itl": 41.53369265644954,
    "ttft": 10463.332541352891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.396347475722467,
    "arrivals": 64419,
    "finished_requests": 64233,
    "scheduler_time": 40.976174293679264
}
#Debug simulation 
Total elapsed time: 4.625491850078106. Arrivals time: 0.18343285797163844 Scheduler time: 4.198732598684728 Scheduler overhead time: 0.0905790994875133 Adapter cache time: 0.019061995670199394 Engine time: 0.09074343321844935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.8441964359954,
    "estimated_duration": 3600.0242380751147,
    "input_throughput": 2988.2382141271405,
    "output_throughput": 2569.4618669972956,
    "total_throughput": 5557.700081124436,
    "itl": 36.18310871593467,
    "ttft": 17031.81039770333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.0737475848237,
    "arrivals": 43251,
    "finished_requests": 43101,
    "scheduler_time": 20.11196917180067
}
#Debug simulation 
Total elapsed time: 3.844306014943868. Arrivals time: 0.1412146077491343 Scheduler time: 3.4138198038563132 Scheduler overhead time: 0.10270750243216753 Adapter cache time: 0.03697296464815736 Engine time: 0.10099806962534785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.819061368703842,
    "estimated_duration": 3600.0319194172894,
    "input_throughput": 2987.755453496367,
    "output_throughput": 2569.582216786934,
    "total_throughput": 5557.337670283301,
    "itl": 36.2129804929983,
    "ttft": 16552.671810016825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.677852498706663,
    "arrivals": 43251,
    "finished_requests": 43102,
    "scheduler_time": 20.096929438275815
}
#Debug simulation 
Total elapsed time: 3.8191968519240618. Arrivals time: 0.1370586585253477 Scheduler time: 3.3925931812264025 Scheduler overhead time: 0.10267946729436517 Adapter cache time: 0.038306611124426126 Engine time: 0.09991693077608943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.8392536607570946,
    "estimated_duration": 3600.0230343751614,
    "input_throughput": 2987.763105206595,
    "output_throughput": 2569.691057992256,
    "total_throughput": 5557.45416319885,
    "itl": 36.21801000028239,
    "ttft": 16499.19574866117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.27270263481489,
    "arrivals": 43251,
    "finished_requests": 43103,
    "scheduler_time": 20.10389032210318
}
#Debug simulation 
Total elapsed time: 3.8393808607943356. Arrivals time: 0.13687008153647184 Scheduler time: 3.4122536717914045 Scheduler overhead time: 0.10252909222617745 Adapter cache time: 0.03823927044868469 Engine time: 0.10104676196351647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.824159520678222,
    "estimated_duration": 3600.023544024091,
    "input_throughput": 2987.8068486121047,
    "output_throughput": 2569.7259717527263,
    "total_throughput": 5557.532820364831,
    "itl": 36.18840901786351,
    "ttft": 16558.391852913694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3370,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.018645104794473,
    "arrivals": 43251,
    "finished_requests": 43102,
    "scheduler_time": 20.086355969635374
}
#Debug simulation 
Total elapsed time: 3.8242635875940323. Arrivals time: 0.1348499576561153 Scheduler time: 3.397732206620276 Scheduler overhead time: 0.10308366222307086 Adapter cache time: 0.03829955914989114 Engine time: 0.10147327650338411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.8152227690443397,
    "estimated_duration": 3600.003588426725,
    "input_throughput": 2987.7545218505074,
    "output_throughput": 2569.4938276554662,
    "total_throughput": 5557.248349505974,
    "itl": 36.218420799486495,
    "ttft": 16560.045971297346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3366,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.09710566939854,
    "arrivals": 43251,
    "finished_requests": 43101,
    "scheduler_time": 20.100340971854333
}
#Debug simulation 
Total elapsed time: 3.8154000621289015. Arrivals time: 0.13654261734336615 Scheduler time: 3.3874921509996057 Scheduler overhead time: 0.10290873982012272 Adapter cache time: 0.038467408157885075 Engine time: 0.10137644177302718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.872745501808822,
    "estimated_duration": 3600.009632140838,
    "input_throughput": 2988.132282746947,
    "output_throughput": 2569.216181374416,
    "total_throughput": 5557.348464121363,
    "itl": 36.16730089465071,
    "ttft": 17207.525148385503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.37109172136218,
    "arrivals": 43251,
    "finished_requests": 43098,
    "scheduler_time": 20.109294462479017
}
#Debug simulation 
Total elapsed time: 3.872841170988977. Arrivals time: 0.13958533806726336 Scheduler time: 3.4442631318233907 Scheduler overhead time: 0.10272728186100721 Adapter cache time: 0.03714193496853113 Engine time: 0.1008294215425849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28806885 . Total output tokens: 25260878
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.8614660599268973,
    "estimated_duration": 3600.0099926025377,
    "input_throughput": 2987.9103174999386,
    "output_throughput": 2569.8117558034796,
    "total_throughput": 5557.722073303418,
    "itl": 36.21945950228293,
    "ttft": 16414.078049326392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.778163175583536,
    "arrivals": 43251,
    "finished_requests": 43103,
    "scheduler_time": 20.101103538578606
}
#Debug simulation 
Total elapsed time: 3.8615858708508313. Arrivals time: 0.13552688900381327 Scheduler time: 3.432622782420367 Scheduler overhead time: 0.10488721122965217 Adapter cache time: 0.03807682590559125 Engine time: 0.10135969426482916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.498932609334588,
    "estimated_duration": 3599.973365389608,
    "input_throughput": 2850.8194251290797,
    "output_throughput": 2460.1173678521145,
    "total_throughput": 5310.936792981194,
    "itl": 35.45590334466086,
    "ttft": 11612.050657697071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.943533104689294,
    "arrivals": 41341,
    "finished_requests": 41235,
    "scheduler_time": 17.966319583381384
}
#Debug simulation 
Total elapsed time: 3.499055936001241. Arrivals time: 0.1286089438945055 Scheduler time: 3.073628736194223 Scheduler overhead time: 0.1039770687930286 Adapter cache time: 0.039809325709939 Engine time: 0.1037217229604721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.465800710953772,
    "estimated_duration": 3599.98544398578,
    "input_throughput": 2850.8098601191286,
    "output_throughput": 2460.109113717567,
    "total_throughput": 5310.918973836696,
    "itl": 35.48899033312298,
    "ttft": 11607.327213355164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.415724081615174,
    "arrivals": 41341,
    "finished_requests": 41235,
    "scheduler_time": 17.983333928457288
}
#Debug simulation 
Total elapsed time: 3.4659142098389566. Arrivals time: 0.12540724547579885 Scheduler time: 3.0466667963191867 Scheduler overhead time: 0.10304317064583302 Adapter cache time: 0.0392863224260509 Engine time: 0.10234274808317423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.4947700048796833,
    "estimated_duration": 3599.983684295373,
    "input_throughput": 2850.8112536095446,
    "output_throughput": 2460.1103162314635,
    "total_throughput": 5310.921569841008,
    "itl": 35.49835267981972,
    "ttft": 11625.187544764392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.09785296644643,
    "arrivals": 41341,
    "finished_requests": 41235,
    "scheduler_time": 17.989830298080328
}
#Debug simulation 
Total elapsed time: 3.494898835197091. Arrivals time: 0.12672845367342234 Scheduler time: 3.0729435002431273 Scheduler overhead time: 0.10435741627588868 Adapter cache time: 0.039523620158433914 Engine time: 0.10163860209286213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.473192274104804,
    "estimated_duration": 3599.9901618373833,
    "input_throughput": 2850.8061240817324,
    "output_throughput": 2460.105889700499,
    "total_throughput": 5310.912013782232,
    "itl": 35.4629896535539,
    "ttft": 11618.52397176387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.61046067701738,
    "arrivals": 41341,
    "finished_requests": 41235,
    "scheduler_time": 17.97175724625553
}
#Debug simulation 
Total elapsed time: 3.4732826626859605. Arrivals time: 0.12730740197002888 Scheduler time: 3.0535548413172364 Scheduler overhead time: 0.1034808442927897 Adapter cache time: 0.039625284262001514 Engine time: 0.10044559929519892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.482952364720404,
    "estimated_duration": 3599.991132841732,
    "input_throughput": 2850.805355150632,
    "output_throughput": 2460.1052261506657,
    "total_throughput": 5310.9105813012975,
    "itl": 35.4954032096068,
    "ttft": 11617.512910202677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3606,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.82911711264958,
    "arrivals": 41341,
    "finished_requests": 41235,
    "scheduler_time": 17.98726471829045
}
#Debug simulation 
Total elapsed time: 3.4830517596565187. Arrivals time: 0.12502677785232663 Scheduler time: 3.064484112430364 Scheduler overhead time: 0.10425423085689545 Adapter cache time: 0.039654039312154055 Engine time: 0.10065331915393472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.493828141130507,
    "estimated_duration": 3599.977098166101,
    "input_throughput": 2850.8164691459037,
    "output_throughput": 2460.1148169835865,
    "total_throughput": 5310.93128612949,
    "itl": 35.44650028040532,
    "ttft": 11619.648278969762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.122561646748448,
    "arrivals": 41341,
    "finished_requests": 41235,
    "scheduler_time": 17.961311394770743
}
#Debug simulation 
Total elapsed time: 3.4939208761788905. Arrivals time: 0.12727566435933113 Scheduler time: 3.0693313432857394 Scheduler overhead time: 0.10548888519406319 Adapter cache time: 0.03957022028043866 Engine time: 0.10302981780841947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27525434 . Total output tokens: 24151462
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.496007981710136,
    "estimated_duration": 3599.9943245486766,
    "input_throughput": 2850.8028276646337,
    "output_throughput": 2460.10304505419,
    "total_throughput": 5310.905872718823,
    "itl": 35.49158100018788,
    "ttft": 11617.554744747835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.612833414767827,
    "arrivals": 41341,
    "finished_requests": 41235,
    "scheduler_time": 17.98575992133655
}
#Debug simulation 
Total elapsed time: 3.496117735747248. Arrivals time: 0.13893934851512313 Scheduler time: 3.0598557936027646 Scheduler overhead time: 0.10411068378016353 Adapter cache time: 0.04159946646541357 Engine time: 0.10277257207781076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.2914360696449876,
    "estimated_duration": 3600.027816303417,
    "input_throughput": 2776.110494130049,
    "output_throughput": 2409.474715922285,
    "total_throughput": 5185.585210052334,
    "itl": 35.182433011708,
    "ttft": 9855.699699761057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3799,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.120541912376105,
    "arrivals": 40385,
    "finished_requests": 40289,
    "scheduler_time": 16.97033680545207
}
#Debug simulation 
Total elapsed time: 3.2915665018372238. Arrivals time: 0.1227148063480854 Scheduler time: 2.875085100531578 Scheduler overhead time: 0.10305503103882074 Adapter cache time: 0.040645633824169636 Engine time: 0.10087518440559506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.296752931084484,
    "estimated_duration": 3600.0129530079444,
    "input_throughput": 2776.0486227278157,
    "output_throughput": 2409.3235533369093,
    "total_throughput": 5185.372176064725,
    "itl": 35.22192357292099,
    "ttft": 9948.407545715378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3788,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.718091341908234,
    "arrivals": 40385,
    "finished_requests": 40288,
    "scheduler_time": 16.989125018554155
}
#Debug simulation 
Total elapsed time: 3.296845979988575. Arrivals time: 0.1254368145018816 Scheduler time: 2.8761336631141603 Scheduler overhead time: 0.10302532278001308 Adapter cache time: 0.04047664999961853 Engine time: 0.1030778493732214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.28910710895434,
    "estimated_duration": 3600.007798563052,
    "input_throughput": 2776.052597438551,
    "output_throughput": 2409.327002975404,
    "total_throughput": 5185.379600413955,
    "itl": 35.23044853780349,
    "ttft": 9953.133105800476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3780,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.412055035951685,
    "arrivals": 40385,
    "finished_requests": 40288,
    "scheduler_time": 16.994344335824163
}
#Debug simulation 
Total elapsed time: 3.2891962518915534. Arrivals time: 0.12323003821074963 Scheduler time: 2.874216081108898 Scheduler overhead time: 0.10294314660131931 Adapter cache time: 0.04030360048636794 Engine time: 0.09975140215829015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.3233242738060653,
    "estimated_duration": 3600.0263301425207,
    "input_throughput": 2776.1116401624613,
    "output_throughput": 2409.4757106003167,
    "total_throughput": 5185.587350762778,
    "itl": 35.19037390665848,
    "ttft": 9853.65447598816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3798,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.63296448462745,
    "arrivals": 40385,
    "finished_requests": 40289,
    "scheduler_time": 16.97379730856228
}
#Debug simulation 
Total elapsed time: 3.323477887082845. Arrivals time: 0.12473515188321471 Scheduler time: 2.9038976808078587 Scheduler overhead time: 0.10343218594789505 Adapter cache time: 0.040572354570031166 Engine time: 0.10185303771868348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.3029749132692814,
    "estimated_duration": 3600.0342702725397,
    "input_throughput": 2776.0321846167926,
    "output_throughput": 2409.3092867539194,
    "total_throughput": 5185.3414713707125,
    "itl": 35.22470882162373,
    "ttft": 9949.216920575793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.12436143631221,
    "arrivals": 40385,
    "finished_requests": 40288,
    "scheduler_time": 16.992073110305114
}
#Debug simulation 
Total elapsed time: 3.303067060187459. Arrivals time: 0.1246219053864479 Scheduler time: 2.8828046219423413 Scheduler overhead time: 0.10350170405581594 Adapter cache time: 0.04051610268652439 Engine time: 0.10279192356392741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.2778566419146955,
    "estimated_duration": 3600.0315282162774,
    "input_throughput": 2776.107631743938,
    "output_throughput": 2409.4722315662134,
    "total_throughput": 5185.579863310151,
    "itl": 35.17275113203442,
    "ttft": 9855.898396951588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3799,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.252515653229583,
    "arrivals": 40385,
    "finished_requests": 40289,
    "scheduler_time": 16.964183545872704
}
#Debug simulation 
Total elapsed time: 3.2779640168882906. Arrivals time: 0.12203661212697625 Scheduler time: 2.862473186571151 Scheduler overhead time: 0.10360345104709268 Adapter cache time: 0.04028481291607022 Engine time: 0.10067499848082662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 4320, 1080, 4320, 135, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 135, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 4320, 1080]
Prompts retrieved: 120555 . Total input tokens: 26870685 . Total output tokens: 23614535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.301449445076287,
    "estimated_duration": 3600.0235467403027,
    "input_throughput": 2776.040453693435,
    "output_throughput": 2409.316463458591,
    "total_throughput": 5185.356917152027,
    "itl": 35.22401549391143,
    "ttft": 9949.419938910323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3788,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.941006376483585,
    "arrivals": 40385,
    "finished_requests": 40288,
    "scheduler_time": 16.99063589844435
}
#Debug simulation 
Total elapsed time: 3.301547348033637. Arrivals time: 0.1222080159932375 Scheduler time: 2.884856550488621 Scheduler overhead time: 0.1032991181127727 Adapter cache time: 0.040365738328546286 Engine time: 0.10180211439728737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.2293706350028515,
    "estimated_duration": 3600.0341051390765,
    "input_throughput": 2723.4086438248446,
    "output_throughput": 2403.9230594082583,
    "total_throughput": 5127.331703233102,
    "itl": 35.179505308201016,
    "ttft": 10796.140156309231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.260928738222816,
    "arrivals": 39938,
    "finished_requests": 39827,
    "scheduler_time": 16.80748999900188
}
#Debug simulation 
Total elapsed time: 3.2294800309464335. Arrivals time: 0.11864387569949031 Scheduler time: 2.8198817200027406 Scheduler overhead time: 0.1029025362804532 Adapter cache time: 0.03966584336012602 Engine time: 0.09953143168240786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.237521482165903,
    "estimated_duration": 3600.0128109459883,
    "input_throughput": 2723.4247528757187,
    "output_throughput": 2403.9372786914896,
    "total_throughput": 5127.362031567209,
    "itl": 35.21485925972698,
    "ttft": 10803.264192263176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3660,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.745468712504255,
    "arrivals": 39938,
    "finished_requests": 39827,
    "scheduler_time": 16.826193533618063
}
#Debug simulation 
Total elapsed time: 3.2376650222577155. Arrivals time: 0.11966006457805634 Scheduler time: 2.8232669550925493 Scheduler overhead time: 0.10285703279078007 Adapter cache time: 0.039770109578967094 Engine time: 0.10327148763462901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.2330530136823654,
    "estimated_duration": 3600.015934336933,
    "input_throughput": 2723.422390019452,
    "output_throughput": 2403.935193024075,
    "total_throughput": 5127.357583043527,
    "itl": 35.22217511166797,
    "ttft": 10801.465508935113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3664,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.49744500773588,
    "arrivals": 39938,
    "finished_requests": 39827,
    "scheduler_time": 16.831487878056294
}
#Debug simulation 
Total elapsed time: 3.233148390892893. Arrivals time: 0.12481908593326807 Scheduler time: 2.817006742581725 Scheduler overhead time: 0.10297153750434518 Adapter cache time: 0.03956067934632301 Engine time: 0.10031966818496585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.2627396089956164,
    "estimated_duration": 3600.006442518113,
    "input_throughput": 2723.4295706265725,
    "output_throughput": 2403.9415312675396,
    "total_throughput": 5127.371101894112,
    "itl": 35.18355573111984,
    "ttft": 10796.5793452658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3665,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.618704703707397,
    "arrivals": 39938,
    "finished_requests": 39827,
    "scheduler_time": 16.80991939833672
}
#Debug simulation 
Total elapsed time: 3.2628320758230984. Arrivals time: 0.12897389847785234 Scheduler time: 2.8409054381772876 Scheduler overhead time: 0.10396565217524767 Adapter cache time: 0.039776706136763096 Engine time: 0.1001429008319974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.254997374024242,
    "estimated_duration": 3600.0338346383105,
    "input_throughput": 2723.4088484574004,
    "output_throughput": 2403.92324003518,
    "total_throughput": 5127.33208849258,
    "itl": 35.21920946910929,
    "ttft": 10801.311395197461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3661,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.1567421004977,
    "arrivals": 39938,
    "finished_requests": 39827,
    "scheduler_time": 16.8291905758038
}
#Debug simulation 
Total elapsed time: 3.255102255847305. Arrivals time: 0.13205318246036768 Scheduler time: 2.824841709341854 Scheduler overhead time: 0.1032377677038312 Adapter cache time: 0.040017115883529186 Engine time: 0.10626218421384692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.224122431129217,
    "estimated_duration": 3600.0268962949517,
    "input_throughput": 2723.41409729199,
    "output_throughput": 2403.927873124134,
    "total_throughput": 5127.341970416123,
    "itl": 35.167082030686615,
    "ttft": 10794.655287971544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3671,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.435373772836446,
    "arrivals": 39938,
    "finished_requests": 39827,
    "scheduler_time": 16.80147164359265
}
#Debug simulation 
Total elapsed time: 3.224219917319715. Arrivals time: 0.1304781991057098 Scheduler time: 2.7973041320219636 Scheduler overhead time: 0.10356470895931125 Adapter cache time: 0.039657758083194494 Engine time: 0.10376330418512225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 4320, 1080, 4320, 66, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 66, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 4320, 1080]
Prompts retrieved: 119106 . Total input tokens: 26551989 . Total output tokens: 23338899
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.2615802087821066,
    "estimated_duration": 3600.0146532564395,
    "input_throughput": 2723.423359160868,
    "output_throughput": 2403.936048474616,
    "total_throughput": 5127.359407635484,
    "itl": 35.21541038139597,
    "ttft": 10801.459469224239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3663,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.985918946993518,
    "arrivals": 39938,
    "finished_requests": 39827,
    "scheduler_time": 16.827786202543148
}
#Debug simulation 
Total elapsed time: 3.2617279831320047. Arrivals time: 0.12870060559362173 Scheduler time: 2.838305313140154 Scheduler overhead time: 0.1041203597560525 Adapter cache time: 0.039782618172466755 Engine time: 0.10182645171880722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.1175342611968517,
    "estimated_duration": 3599.7845751120603,
    "input_throughput": 2740.586775166369,
    "output_throughput": 2351.584330497466,
    "total_throughput": 5092.171105663835,
    "itl": 34.771346696677035,
    "ttft": 10385.860618886183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3717,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.578324371756338,
    "arrivals": 39719,
    "finished_requests": 39611,
    "scheduler_time": 15.849993409139486
}
#Debug simulation 
Total elapsed time: 3.117629485204816. Arrivals time: 0.12027176236733794 Scheduler time: 2.7066026381216943 Scheduler overhead time: 0.10274341935291886 Adapter cache time: 0.040277366526424885 Engine time: 0.09911786811426282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.1490562921389937,
    "estimated_duration": 3599.79841460822,
    "input_throughput": 2740.576238926341,
    "output_throughput": 2351.5752897850257,
    "total_throughput": 5092.151528711366,
    "itl": 34.806020776165994,
    "ttft": 10388.809308481614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3708,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.081049558091195,
    "arrivals": 39719,
    "finished_requests": 39611,
    "scheduler_time": 15.868759899698091
}
#Debug simulation 
Total elapsed time: 3.1491553392261267. Arrivals time: 0.1209781477227807 Scheduler time: 2.732384226284921 Scheduler overhead time: 0.10330327600240707 Adapter cache time: 0.040380607824772596 Engine time: 0.10296199284493923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.146548612974584,
    "estimated_duration": 3599.7874469051726,
    "input_throughput": 2740.5845888155527,
    "output_throughput": 2351.5824544801226,
    "total_throughput": 5092.167043295675,
    "itl": 34.813037559453726,
    "ttft": 10390.522988641615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3704,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.77093255723925,
    "arrivals": 39719,
    "finished_requests": 39611,
    "scheduler_time": 15.874018411696827
}
#Debug simulation 
Total elapsed time: 3.1466940878890455. Arrivals time: 0.12363011436536908 Scheduler time: 2.729433906264603 Scheduler overhead time: 0.10196480760350823 Adapter cache time: 0.04024657653644681 Engine time: 0.1022384911775589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.166024318896234,
    "estimated_duration": 3599.806151835119,
    "input_throughput": 2740.5703484813275,
    "output_throughput": 2351.5702354374243,
    "total_throughput": 5092.140583918752,
    "itl": 34.77465632020408,
    "ttft": 10386.339019039995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.81768270998384,
    "arrivals": 39719,
    "finished_requests": 39611,
    "scheduler_time": 15.851981317301968
}
#Debug simulation 
Total elapsed time: 3.166112489067018. Arrivals time: 0.12204167572781444 Scheduler time: 2.748246666043997 Scheduler overhead time: 0.10357050644233823 Adapter cache time: 0.04062316054478288 Engine time: 0.10219752648845315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.1271122191101313,
    "estimated_duration": 3599.800722263699,
    "input_throughput": 2740.574482077487,
    "output_throughput": 2351.573782305579,
    "total_throughput": 5092.148264383066,
    "itl": 34.81214967764004,
    "ttft": 10388.985848066823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3708,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.459422337553498,
    "arrivals": 39719,
    "finished_requests": 39611,
    "scheduler_time": 15.87147680578784
}
#Debug simulation 
Total elapsed time: 3.127269670832902. Arrivals time: 0.1190712652169168 Scheduler time: 2.7161632953211665 Scheduler overhead time: 0.1028717290610075 Adapter cache time: 0.04031143989413977 Engine time: 0.10010183835402131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.161880270112306,
    "estimated_duration": 3599.813135956693,
    "input_throughput": 2740.5650314063096,
    "output_throughput": 2351.565673074937,
    "total_throughput": 5092.1307044812465,
    "itl": 34.7597851310575,
    "ttft": 10385.359804318467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.703498452340444,
    "arrivals": 39719,
    "finished_requests": 39611,
    "scheduler_time": 15.84379132456056
}
#Debug simulation 
Total elapsed time: 3.161974861752242. Arrivals time: 0.11876488896086812 Scheduler time: 2.7471966929733753 Scheduler overhead time: 0.10387192945927382 Adapter cache time: 0.04041722463443875 Engine time: 0.10281065804883838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 4320, 1080, 4320, 33, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 33, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 4320, 1080]
Prompts retrieved: 118413 . Total input tokens: 26390488 . Total output tokens: 23203918
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.1377815133892,
    "estimated_duration": 3599.7774792091677,
    "input_throughput": 2740.5921774274084,
    "output_throughput": 2351.58896595456,
    "total_throughput": 5092.1811433819685,
    "itl": 34.80888450852487,
    "ttft": 10389.87276402669,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3704,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.26499716375117,
    "arrivals": 39719,
    "finished_requests": 39611,
    "scheduler_time": 15.87013959326393
}
#Debug simulation 
Total elapsed time: 3.1378814321942627. Arrivals time: 0.12055632658302784 Scheduler time: 2.7226334726437926 Scheduler overhead time: 0.1032453035004437 Adapter cache time: 0.04029189236462116 Engine time: 0.10201524291187525 
