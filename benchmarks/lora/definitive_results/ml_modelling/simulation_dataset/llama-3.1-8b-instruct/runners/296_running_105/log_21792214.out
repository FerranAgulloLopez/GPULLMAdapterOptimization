INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_384_slots_128_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_384_slots_128_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.472287792246789,
    "estimated_duration": 3600.035837682389,
    "input_throughput": 3979.7995481132843,
    "output_throughput": 3442.63933994021,
    "total_throughput": 7422.438888053494,
    "itl": 150.44916962612876,
    "ttft": 2286543.978575845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.901828793180362,
    "arrivals": 1490652,
    "finished_requests": 57888,
    "scheduler_time": 121.60228494067061
}
#Debug simulation 
Total elapsed time: 4.472395613323897. Arrivals time: 0.21792174316942692 Scheduler time: 4.135201227385551 Scheduler overhead time: 0.03637457452714443 Adapter cache time: 0.028586825355887413 Engine time: 0.03737558610737324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_384_slots_128_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_384_slots_128_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.1774516510777175,
    "estimated_duration": 3600.112342003626,
    "input_throughput": 3757.7355134620334,
    "output_throughput": 3248.2997443050963,
    "total_throughput": 7006.03525776713,
    "itl": 133.29616152517585,
    "ttft": 2314798.615146862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1721,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.597064267764637,
    "arrivals": 1490652,
    "finished_requests": 54624,
    "scheduler_time": 127.90122722881267
}
#Debug simulation 
Total elapsed time: 4.177593706175685. Arrivals time: 0.20424498384818435 Scheduler time: 3.8334467723034322 Scheduler overhead time: 0.039676494896411896 Adapter cache time: 0.04084501974284649 Engine time: 0.040713032241910696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_384_slots_128_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_384_slots_128_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.184731942135841,
    "estimated_duration": 3600.0668479655587,
    "input_throughput": 3758.2393803731497,
    "output_throughput": 3249.001336352935,
    "total_throughput": 7007.240716726085,
    "itl": 133.2704513005897,
    "ttft": 2314548.6248293505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1721,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.823825185629673,
    "arrivals": 1490652,
    "finished_requests": 54632,
    "scheduler_time": 127.92592007367266
}
#Debug simulation 
Total elapsed time: 4.184823433868587. Arrivals time: 0.20632479432970285 Scheduler time: 3.838392595294863 Scheduler overhead time: 0.039849957916885614 Adapter cache time: 0.040710232220590115 Engine time: 0.04091711714863777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_384_slots_128_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_384_slots_128_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.169426358770579,
    "estimated_duration": 3600.133306553182,
    "input_throughput": 3758.430271281989,
    "output_throughput": 3249.3552332371646,
    "total_throughput": 7007.785504519154,
    "itl": 133.24983503818274,
    "ttft": 2314133.9136092267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1726,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.018647543425102,
    "arrivals": 1490652,
    "finished_requests": 54639,
    "scheduler_time": 127.95582775857123
}
#Debug simulation 
Total elapsed time: 4.169513901695609. Arrivals time: 0.20576813304796815 Scheduler time: 3.8233240377157927 Scheduler overhead time: 0.03991402918472886 Adapter cache time: 0.0409855917096138 Engine time: 0.04096984304487705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_384_slots_128_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_384_slots_128_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.407317431643605,
    "estimated_duration": 3600.017099439924,
    "input_throughput": 3970.797250441935,
    "output_throughput": 3443.5022550111285,
    "total_throughput": 7414.299505453063,
    "itl": 150.43806427065923,
    "ttft": 2286561.318342544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.412510524816051,
    "arrivals": 1487737,
    "finished_requests": 57698,
    "scheduler_time": 121.63747050240801
}
#Debug simulation 
Total elapsed time: 4.407412096858025. Arrivals time: 0.2137692146934569 Scheduler time: 4.076538917142898 Scheduler overhead time: 0.036083159036934376 Adapter cache time: 0.027013613376766443 Engine time: 0.03716347087174654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_384_slots_128_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_384_slots_128_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.187193328049034,
    "estimated_duration": 3600.0556581907067,
    "input_throughput": 3744.935434352614,
    "output_throughput": 3246.8611904390737,
    "total_throughput": 6991.7966247916875,
    "itl": 132.97489388982478,
    "ttft": 2314696.8179827337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1660,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.213197974283066,
    "arrivals": 1487737,
    "finished_requests": 54372,
    "scheduler_time": 128.08396778773601
}
#Debug simulation 
Total elapsed time: 4.1872836840339005. Arrivals time: 0.2203816450200975 Scheduler time: 3.827623423654586 Scheduler overhead time: 0.039640363305807114 Adapter cache time: 0.03995158663019538 Engine time: 0.04103095317259431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_384_slots_128_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_384_slots_128_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.147926496807486,
    "estimated_duration": 3600.059532695288,
    "input_throughput": 3745.462506256084,
    "output_throughput": 3247.0938032650106,
    "total_throughput": 6992.556309521095,
    "itl": 132.94857956778733,
    "ttft": 2314449.4242754932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1660,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.469111532587656,
    "arrivals": 1487737,
    "finished_requests": 54377,
    "scheduler_time": 128.1090993438116
}
#Debug simulation 
Total elapsed time: 4.148012072779238. Arrivals time: 0.20669338293373585 Scheduler time: 3.8015720737166703 Scheduler overhead time: 0.03997596073895693 Adapter cache time: 0.03998538991436362 Engine time: 0.04102834686636925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_384_slots_128_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_384_slots_128_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.163114487193525,
    "estimated_duration": 3600.0793571837344,
    "input_throughput": 3746.0577009474296,
    "output_throughput": 3247.960625275527,
    "total_throughput": 6994.018326222957,
    "itl": 132.91884352503217,
    "ttft": 2314145.5974441576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1660,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.59730876134739,
    "arrivals": 1487737,
    "finished_requests": 54387,
    "scheduler_time": 128.1391274229165
}
#Debug simulation 
Total elapsed time: 4.163228880148381. Arrivals time: 0.20906648132950068 Scheduler time: 3.814121449831873 Scheduler overhead time: 0.04004622623324394 Adapter cache time: 0.040165172424167395 Engine time: 0.04112875601276755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_384_slots_128_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_384_slots_128_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.3979086321778595,
    "estimated_duration": 3600.1329200631017,
    "input_throughput": 3948.383383508754,
    "output_throughput": 3443.89034385505,
    "total_throughput": 7392.273727363804,
    "itl": 150.50559871325862,
    "ttft": 2288864.935418995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1080,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.141401754506095,
    "arrivals": 1486263,
    "finished_requests": 57624,
    "scheduler_time": 121.62544678824652
}
#Debug simulation 
Total elapsed time: 4.398001078981906. Arrivals time: 0.2151846601627767 Scheduler time: 4.065291980281472 Scheduler overhead time: 0.03635303769260645 Adapter cache time: 0.027035189792513847 Engine time: 0.03717966889962554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_384_slots_128_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_384_slots_128_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.162593467161059,
    "estimated_duration": 3600.0599245380126,
    "input_throughput": 3727.5396191417776,
    "output_throughput": 3248.9378635831927,
    "total_throughput": 6976.47748272497,
    "itl": 132.73563345633292,
    "ttft": 2317453.7766677137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1528,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.206710335276894,
    "arrivals": 1486263,
    "finished_requests": 54349,
    "scheduler_time": 128.28020962605365
}
#Debug simulation 
Total elapsed time: 4.162689505144954. Arrivals time: 0.21345188235864043 Scheduler time: 3.8098735618405044 Scheduler overhead time: 0.03984236251562834 Adapter cache time: 0.039919018279761076 Engine time: 0.04101914446800947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_384_slots_128_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_384_slots_128_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.139911830890924,
    "estimated_duration": 3600.0678204999876,
    "input_throughput": 3727.9047698984996,
    "output_throughput": 3249.1898995323495,
    "total_throughput": 6977.0946694308495,
    "itl": 132.71039016181115,
    "ttft": 2317205.3304509087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1528,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.491776534020953,
    "arrivals": 1486263,
    "finished_requests": 54353,
    "scheduler_time": 128.30468672061374
}
#Debug simulation 
Total elapsed time: 4.140008883085102. Arrivals time: 0.21013550460338593 Scheduler time: 3.7907482916489244 Scheduler overhead time: 0.039776220452040434 Adapter cache time: 0.03984843287616968 Engine time: 0.04089911049231887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_384_slots_128_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_384_slots_128_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.15237159607932,
    "estimated_duration": 3600.050898770046,
    "input_throughput": 3729.06727640617,
    "output_throughput": 3250.068215423686,
    "total_throughput": 6979.135491829856,
    "itl": 132.68681796864684,
    "ttft": 2317024.5056260456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1528,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.754631197191967,
    "arrivals": 1486263,
    "finished_requests": 54368,
    "scheduler_time": 128.32890742409813
}
#Debug simulation 
Total elapsed time: 4.1524578193202615. Arrivals time: 0.20717026852071285 Scheduler time: 3.8061512568965554 Scheduler overhead time: 0.03984583402052522 Adapter cache time: 0.03924974612891674 Engine time: 0.04125360073521733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_384_slots_128_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_384_slots_128_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.400980270002037,
    "estimated_duration": 3600.0745925837145,
    "input_throughput": 3963.127327803619,
    "output_throughput": 3444.303633470245,
    "total_throughput": 7407.430961273864,
    "itl": 150.2359665158694,
    "ttft": 2288837.2669539177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 933,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.169377626809423,
    "arrivals": 1482016,
    "finished_requests": 57793,
    "scheduler_time": 121.68351623517643
}
#Debug simulation 
Total elapsed time: 4.401074747089297. Arrivals time: 0.21672666119411588 Scheduler time: 4.0699551571160555 Scheduler overhead time: 0.03624675469473004 Adapter cache time: 0.02417542180046439 Engine time: 0.0371055081486702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_384_slots_128_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_384_slots_128_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.155426251236349,
    "estimated_duration": 3600.0101882524536,
    "input_throughput": 3744.4316252181407,
    "output_throughput": 3255.349953798013,
    "total_throughput": 6999.781579016154,
    "itl": 132.92959354890175,
    "ttft": 2315179.9118414004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.335144506120026,
    "arrivals": 1482016,
    "finished_requests": 54600,
    "scheduler_time": 128.22931653492284
}
#Debug simulation 
Total elapsed time: 4.155516775324941. Arrivals time: 0.20792305143550038 Scheduler time: 3.8103899443522096 Scheduler overhead time: 0.039653042796999216 Adapter cache time: 0.038043621461838484 Engine time: 0.040747618302702904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_384_slots_128_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_384_slots_128_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.152670674957335,
    "estimated_duration": 3600.117375992706,
    "input_throughput": 3745.2126116532813,
    "output_throughput": 3256.1796674108637,
    "total_throughput": 7001.3922790641445,
    "itl": 132.91114016791605,
    "ttft": 2315048.143783375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.714052547123298,
    "arrivals": 1482016,
    "finished_requests": 54614,
    "scheduler_time": 128.2541421925912
}
#Debug simulation 
Total elapsed time: 4.152764744590968. Arrivals time: 0.20840111235156655 Scheduler time: 3.8069805740378797 Scheduler overhead time: 0.03969356697052717 Adapter cache time: 0.03806075267493725 Engine time: 0.04096156219020486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_384_slots_128_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_384_slots_128_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.156290133949369,
    "estimated_duration": 3600.0788962193033,
    "input_throughput": 3749.873374768851,
    "output_throughput": 3260.1979951955554,
    "total_throughput": 7010.071369964407,
    "itl": 133.1690429053868,
    "ttft": 2314856.9405087098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.931105395858259,
    "arrivals": 1482016,
    "finished_requests": 54687,
    "scheduler_time": 128.15203876820803
}
#Debug simulation 
Total elapsed time: 4.156392478849739. Arrivals time: 0.20986973401159048 Scheduler time: 3.8094761967658997 Scheduler overhead time: 0.039576319977641106 Adapter cache time: 0.038026916328817606 Engine time: 0.04078901745378971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_384_slots_128_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_384_slots_128_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.356354178860784,
    "estimated_duration": 3600.095009885299,
    "input_throughput": 3986.1672985283844,
    "output_throughput": 3449.9189509989374,
    "total_throughput": 7436.086249527321,
    "itl": 150.64930288414604,
    "ttft": 2293153.314662944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 863,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.706508994572912,
    "arrivals": 1480643,
    "finished_requests": 57924,
    "scheduler_time": 121.67955279315703
}
#Debug simulation 
Total elapsed time: 4.356431114021689. Arrivals time: 0.2167026698589325 Scheduler time: 4.026388463564217 Scheduler overhead time: 0.03607348399236798 Adapter cache time: 0.023595668841153383 Engine time: 0.03680904535576701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_384_slots_128_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_384_slots_128_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.165110807865858,
    "estimated_duration": 3600.110799684925,
    "input_throughput": 3770.674780672874,
    "output_throughput": 3266.0900328481735,
    "total_throughput": 7036.7648135210475,
    "itl": 133.22399570797128,
    "ttft": 2318685.504237826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.939228212055726,
    "arrivals": 1480643,
    "finished_requests": 54737,
    "scheduler_time": 128.32308005359064
}
#Debug simulation 
Total elapsed time: 4.1651992201805115. Arrivals time: 0.20970266219228506 Scheduler time: 3.8190892077982426 Scheduler overhead time: 0.03962674643844366 Adapter cache time: 0.03723394451662898 Engine time: 0.040914484299719334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_384_slots_128_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_384_slots_128_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.148071888834238,
    "estimated_duration": 3600.020850500897,
    "input_throughput": 3771.628433238325,
    "output_throughput": 3267.0069114637395,
    "total_throughput": 7038.635344702065,
    "itl": 133.20307661683333,
    "ttft": 2318734.992500752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.401986695383682,
    "arrivals": 1480643,
    "finished_requests": 54749,
    "scheduler_time": 128.3375333040206
}
#Debug simulation 
Total elapsed time: 4.148171144071966. Arrivals time: 0.20924090081825852 Scheduler time: 3.802417007740587 Scheduler overhead time: 0.03943411121144891 Adapter cache time: 0.037360801827162504 Engine time: 0.041126458905637264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_384_slots_128_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_384_slots_128_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.175228809006512,
    "estimated_duration": 3600.014242385512,
    "input_throughput": 3771.4081350420606,
    "output_throughput": 3266.9509641180334,
    "total_throughput": 7038.359099160094,
    "itl": 133.11225428365825,
    "ttft": 2318813.7166049425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.699008654328024,
    "arrivals": 1480643,
    "finished_requests": 54749,
    "scheduler_time": 128.38643532483027
}
#Debug simulation 
Total elapsed time: 4.175323782023042. Arrivals time: 0.21227238420397043 Scheduler time: 3.826670629903674 Scheduler overhead time: 0.03960160445421934 Adapter cache time: 0.03704266855493188 Engine time: 0.041024419479072094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_384_slots_128_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_384_slots_128_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.424160287249833,
    "estimated_duration": 3600.1267485149156,
    "input_throughput": 3951.1356109525227,
    "output_throughput": 3452.992871744859,
    "total_throughput": 7404.1284826973815,
    "itl": 150.3540518631886,
    "ttft": 2289608.5127583463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 691,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5691746410774865,
    "arrivals": 1477777,
    "finished_requests": 57651,
    "scheduler_time": 121.89420932981244
}
#Debug simulation 
Total elapsed time: 4.424280587118119. Arrivals time: 0.31538001354783773 Scheduler time: 3.9975495631806552 Scheduler overhead time: 0.03572163172066212 Adapter cache time: 0.021936326287686825 Engine time: 0.03688264824450016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_384_slots_128_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_384_slots_128_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.255174839869142,
    "estimated_duration": 3600.059937063357,
    "input_throughput": 3755.576639379173,
    "output_throughput": 3282.4845159771103,
    "total_throughput": 7038.061155356283,
    "itl": 132.85696230433524,
    "ttft": 2315216.0448808805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.0968909180117885,
    "arrivals": 1477777,
    "finished_requests": 54789,
    "scheduler_time": 128.77307329581728
}
#Debug simulation 
Total elapsed time: 4.255272799637169. Arrivals time: 0.209655508864671 Scheduler time: 3.9114248394034803 Scheduler overhead time: 0.03900144901126623 Adapter cache time: 0.03618905646726489 Engine time: 0.04042910085991025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_384_slots_128_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_384_slots_128_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.1709034140221775,
    "estimated_duration": 3600.145489957719,
    "input_throughput": 3755.7737701758265,
    "output_throughput": 3282.884825895968,
    "total_throughput": 7038.658596071795,
    "itl": 132.84551363231404,
    "ttft": 2315068.759711442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.745671011763621,
    "arrivals": 1477777,
    "finished_requests": 54794,
    "scheduler_time": 128.78742521028954
}
#Debug simulation 
Total elapsed time: 4.1709956638515. Arrivals time: 0.20865287445485592 Scheduler time: 3.8269824711605906 Scheduler overhead time: 0.03921141289174557 Adapter cache time: 0.03642707783728838 Engine time: 0.0411266996525228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_384_slots_128_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_384_slots_128_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.1530914208851755,
    "estimated_duration": 3600.10223006555,
    "input_throughput": 3756.1858346877507,
    "output_throughput": 3283.4900912757603,
    "total_throughput": 7039.675925963511,
    "itl": 132.8289752094106,
    "ttft": 2314959.2836956847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.266734775970664,
    "arrivals": 1477777,
    "finished_requests": 54801,
    "scheduler_time": 128.80145715982144
}
#Debug simulation 
Total elapsed time: 4.153183801099658. Arrivals time: 0.20922227762639523 Scheduler time: 3.809710271190852 Scheduler overhead time: 0.03901921771466732 Adapter cache time: 0.03613407909870148 Engine time: 0.040496323723345995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 9.283630200196058,
    "estimated_duration": 3600.00009472805,
    "input_throughput": 3929.8051743714495,
    "output_throughput": 3434.8785207279598,
    "total_throughput": 7364.683695099409,
    "itl": 150.88659394864067,
    "ttft": 2282198.185061498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.304674436613126,
    "arrivals": 1289312,
    "finished_requests": 57501,
    "scheduler_time": 121.18387588635223
}
#Debug simulation 
Total elapsed time: 9.283718560822308. Arrivals time: 0.34467908879742026 Scheduler time: 8.788978252094239 Scheduler overhead time: 0.03811040800064802 Adapter cache time: 0.056174518540501595 Engine time: 0.038634506054222584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.552159449085593,
    "estimated_duration": 3600.100753160965,
    "input_throughput": 3708.3062156741516,
    "output_throughput": 3236.8083003700835,
    "total_throughput": 6945.114516044235,
    "itl": 134.23562995405788,
    "ttft": 2310736.9445857173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3787,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.674225454688276,
    "arrivals": 1289312,
    "finished_requests": 54251,
    "scheduler_time": 127.08210378747934
}
#Debug simulation 
Total elapsed time: 7.552225146908313. Arrivals time: 0.6455229404382408 Scheduler time: 6.728484029881656 Scheduler overhead time: 0.04082660423591733 Adapter cache time: 0.07709231972694397 Engine time: 0.041523175314068794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 7.139019697904587,
    "estimated_duration": 3600.0832994863194,
    "input_throughput": 3709.983322304166,
    "output_throughput": 3238.2809035733817,
    "total_throughput": 6948.264225877548,
    "itl": 134.16611748671602,
    "ttft": 2310471.4190122266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.905905787810532,
    "arrivals": 1289312,
    "finished_requests": 54275,
    "scheduler_time": 127.14286394605394
}
#Debug simulation 
Total elapsed time: 7.139116734731942. Arrivals time: 0.25077233184129 Scheduler time: 6.710743823554367 Scheduler overhead time: 0.04071989934891462 Adapter cache time: 0.0769815156236291 Engine time: 0.04115921398624778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.053972824942321,
    "estimated_duration": 3600.028037422725,
    "input_throughput": 3708.678060618184,
    "output_throughput": 3237.364509067429,
    "total_throughput": 6946.042569685614,
    "itl": 134.10724718569855,
    "ttft": 2309296.260950441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3857,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.622783067782724,
    "arrivals": 1289312,
    "finished_requests": 54222,
    "scheduler_time": 127.20663002195738
}
#Debug simulation 
Total elapsed time: 7.054076365660876. Arrivals time: 0.23483031149953604 Scheduler time: 6.639858434908092 Scheduler overhead time: 0.040588962379843 Adapter cache time: 0.07876766985282302 Engine time: 0.041299260687083006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.73430310189724,
    "estimated_duration": 3600.067868067955,
    "input_throughput": 3959.4078562884856,
    "output_throughput": 3429.4778466567927,
    "total_throughput": 7388.885702945278,
    "itl": 150.7473141302169,
    "ttft": 2272291.1958633587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3079,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.359607409373275,
    "arrivals": 1150725,
    "finished_requests": 57826,
    "scheduler_time": 121.08325289554463
}
#Debug simulation 
Total elapsed time: 7.734413870144635. Arrivals time: 0.24905289383605123 Scheduler time: 7.329331224784255 Scheduler overhead time: 0.03760433942079544 Adapter cache time: 0.0634144376963377 Engine time: 0.037883385084569454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.5105598429217935,
    "estimated_duration": 3600.0535715270285,
    "input_throughput": 3744.5556662319214,
    "output_throughput": 3233.83437737618,
    "total_throughput": 6978.390043608101,
    "itl": 134.14893575742826,
    "ttft": 2302736.0949658775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4062,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.718553675794123,
    "arrivals": 1150725,
    "finished_requests": 54618,
    "scheduler_time": 126.96559331528898
}
#Debug simulation 
Total elapsed time: 6.510663455817848. Arrivals time: 0.31722698360681534 Scheduler time: 6.012803033459932 Scheduler overhead time: 0.04034348763525486 Adapter cache time: 0.08060248102992773 Engine time: 0.04096802417188883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.381295720115304,
    "estimated_duration": 3600.060942560607,
    "input_throughput": 3746.5840759924663,
    "output_throughput": 3241.18485385989,
    "total_throughput": 6987.768929852356,
    "itl": 134.5732862339144,
    "ttft": 2301532.839757055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.220558991074448,
    "arrivals": 1150725,
    "finished_requests": 54710,
    "scheduler_time": 126.84381695886731
}
#Debug simulation 
Total elapsed time: 6.381402960047126. Arrivals time: 0.2355331713333726 Scheduler time: 5.963616338092834 Scheduler overhead time: 0.04021915514022112 Adapter cache time: 0.08222031034529209 Engine time: 0.04118627076968551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.361465875059366,
    "estimated_duration": 3600.0694923947067,
    "input_throughput": 3748.4679194410546,
    "output_throughput": 3243.4304461808974,
    "total_throughput": 6991.898365621952,
    "itl": 134.5034820135561,
    "ttft": 2301338.6058211406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.288986433271855,
    "arrivals": 1150725,
    "finished_requests": 54739,
    "scheduler_time": 126.91080981420211
}
#Debug simulation 
Total elapsed time: 6.361583863850683. Arrivals time: 0.23230533068999648 Scheduler time: 5.947531178127974 Scheduler overhead time: 0.0401931363157928 Adapter cache time: 0.08172744978219271 Engine time: 0.041145758237689734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.372165658976883,
    "estimated_duration": 3600.080011506508,
    "input_throughput": 3955.478476724477,
    "output_throughput": 3432.2051066941026,
    "total_throughput": 7387.6835834185795,
    "itl": 150.9533813695069,
    "ttft": 2276690.9276083214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3081,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.372832227437172,
    "arrivals": 1127715,
    "finished_requests": 57626,
    "scheduler_time": 121.02571380417194
}
#Debug simulation 
Total elapsed time: 7.372261513955891. Arrivals time: 0.2433891468681395 Scheduler time: 6.973109335172921 Scheduler overhead time: 0.03702704468742013 Adapter cache time: 0.06399568449705839 Engine time: 0.03771547554060817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.487739412114024,
    "estimated_duration": 3600.1416047303724,
    "input_throughput": 3729.343863129943,
    "output_throughput": 3237.8609732138625,
    "total_throughput": 6967.204836343805,
    "itl": 134.39060303845616,
    "ttft": 2306120.190100055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3959,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.974940118569982,
    "arrivals": 1127715,
    "finished_requests": 54360,
    "scheduler_time": 126.91267857925344
}
#Debug simulation 
Total elapsed time: 6.487805371172726. Arrivals time: 0.6031680041924119 Scheduler time: 5.705420744605362 Scheduler overhead time: 0.040201259311288595 Adapter cache time: 0.07899077422916889 Engine time: 0.041384294629096985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.174528317991644,
    "estimated_duration": 3600.0972340563344,
    "input_throughput": 3730.827565695084,
    "output_throughput": 3239.054181562011,
    "total_throughput": 6969.881747257095,
    "itl": 134.3231593124824,
    "ttft": 2305624.0962153906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3960,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.155813459605724,
    "arrivals": 1127715,
    "finished_requests": 54379,
    "scheduler_time": 126.97422270539498
}
#Debug simulation 
Total elapsed time: 6.174629396758974. Arrivals time: 0.22881040396168828 Scheduler time: 5.766919428948313 Scheduler overhead time: 0.04019149160012603 Adapter cache time: 0.07911355560645461 Engine time: 0.04098133370280266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.145497368182987,
    "estimated_duration": 3600.0249907914363,
    "input_throughput": 3725.169695850296,
    "output_throughput": 3234.3750473354903,
    "total_throughput": 6959.5447431857865,
    "itl": 133.86089980495652,
    "ttft": 2305613.146053606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3937,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.133496743028434,
    "arrivals": 1127715,
    "finished_requests": 54294,
    "scheduler_time": 127.21438066948721
}
#Debug simulation 
Total elapsed time: 6.145584183745086. Arrivals time: 0.23504277365282178 Scheduler time: 5.733128339983523 Scheduler overhead time: 0.04030503239482641 Adapter cache time: 0.0772337494418025 Engine time: 0.04117395868524909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.210736022330821,
    "estimated_duration": 3600.123435667124,
    "input_throughput": 3954.9927257873396,
    "output_throughput": 3430.2737727412346,
    "total_throughput": 7385.266498528574,
    "itl": 150.65633422332297,
    "ttft": 2278881.5924384524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2820,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.646993470098646,
    "arrivals": 1116080,
    "finished_requests": 57674,
    "scheduler_time": 121.0857678716183
}
#Debug simulation 
Total elapsed time: 7.210835290141404. Arrivals time: 0.24115289514884353 Scheduler time: 6.821639969013631 Scheduler overhead time: 0.03691188432276249 Adapter cache time: 0.05659141903743148 Engine time: 0.03756753075867891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.05743731604889,
    "estimated_duration": 3600.1236177857436,
    "input_throughput": 3722.84383063583,
    "output_throughput": 3238.714065927365,
    "total_throughput": 6961.557896563195,
    "itl": 133.99327161301676,
    "ttft": 2307387.7934620767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.7212364870163,
    "arrivals": 1116080,
    "finished_requests": 54337,
    "scheduler_time": 127.04263182802
}
#Debug simulation 
Total elapsed time: 6.05752617912367. Arrivals time: 0.3054865375161171 Scheduler time: 5.580978666432202 Scheduler overhead time: 0.0400728820823133 Adapter cache time: 0.07155077392235398 Engine time: 0.040855430997908115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.065982691012323,
    "estimated_duration": 3600.141157658291,
    "input_throughput": 3725.0244956292004,
    "output_throughput": 3240.6926531705103,
    "total_throughput": 6965.717148799711,
    "itl": 133.93654902251362,
    "ttft": 2307252.755061382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.133942730595017,
    "arrivals": 1116080,
    "finished_requests": 54371,
    "scheduler_time": 127.09847917897939
}
#Debug simulation 
Total elapsed time: 6.066075274720788. Arrivals time: 0.30696217296645045 Scheduler time: 5.5878335875459015 Scheduler overhead time: 0.040082672610878944 Adapter cache time: 0.07172757480293512 Engine time: 0.04074138915166259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.37341586779803,
    "estimated_duration": 3600.0573626090227,
    "input_throughput": 3726.83533861155,
    "output_throughput": 3241.9152875780205,
    "total_throughput": 6968.7506261895705,
    "itl": 133.87777616039844,
    "ttft": 2306869.4125599917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.458633868929024,
    "arrivals": 1116080,
    "finished_requests": 54391,
    "scheduler_time": 127.15393042233197
}
#Debug simulation 
Total elapsed time: 6.373483346775174. Arrivals time: 0.6033182442188263 Scheduler time: 5.5983325582928956 Scheduler overhead time: 0.04020277410745621 Adapter cache time: 0.07055836822837591 Engine time: 0.042344263289123774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.218063983134925,
    "estimated_duration": 3600.156073624596,
    "input_throughput": 3959.904434267192,
    "output_throughput": 3432.258143064184,
    "total_throughput": 7392.162577331375,
    "itl": 150.5092341732561,
    "ttft": 2273274.091682147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2653,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.542721161763268,
    "arrivals": 1110230,
    "finished_requests": 57529,
    "scheduler_time": 121.16599834474239
}
#Debug simulation 
Total elapsed time: 7.218163664918393. Arrivals time: 0.2502845171838999 Scheduler time: 6.820720709394664 Scheduler overhead time: 0.03711829148232937 Adapter cache time: 0.055317232850939035 Engine time: 0.037632714956998825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.060153305064887,
    "estimated_duration": 3600.1189705788966,
    "input_throughput": 3604.9969753952546,
    "output_throughput": 3132.103714391093,
    "total_throughput": 6737.100689786348,
    "itl": 125.64190659968943,
    "ttft": 2318482.8227171996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.50666802781123,
    "arrivals": 1110230,
    "finished_requests": 52465,
    "scheduler_time": 130.88667382282284
}
#Debug simulation 
Total elapsed time: 6.06024456769228. Arrivals time: 0.22572658956050873 Scheduler time: 5.671657318249345 Scheduler overhead time: 0.042530117090791464 Adapter cache time: 0.05758066521957517 Engine time: 0.043095933739095926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.432692538946867,
    "estimated_duration": 3600.036633254699,
    "input_throughput": 3606.1235822100934,
    "output_throughput": 3133.267560614281,
    "total_throughput": 6739.391142824375,
    "itl": 125.6020769054256,
    "ttft": 2318183.2135423394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.326954053617463,
    "arrivals": 1110230,
    "finished_requests": 52478,
    "scheduler_time": 130.92519535808603
}
#Debug simulation 
Total elapsed time: 6.432777057867497. Arrivals time: 0.5892553539015353 Scheduler time: 5.6802986627444625 Scheduler overhead time: 0.04214091785252094 Adapter cache time: 0.05810296256095171 Engine time: 0.0433030198328197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.075216652825475,
    "estimated_duration": 3600.122059770453,
    "input_throughput": 3607.3604684470224,
    "output_throughput": 3134.2881748624563,
    "total_throughput": 6741.648643309479,
    "itl": 125.56042876636647,
    "ttft": 2317955.1623938167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.04506891132449,
    "arrivals": 1110230,
    "finished_requests": 52496,
    "scheduler_time": 130.9739313739162
}
#Debug simulation 
Total elapsed time: 6.075306203681976. Arrivals time: 0.22158080991357565 Scheduler time: 5.691007959190756 Scheduler overhead time: 0.042098027653992176 Adapter cache time: 0.05781098734587431 Engine time: 0.04310391750186682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.940368863288313,
    "estimated_duration": 3600.1501522775084,
    "input_throughput": 3943.444689666283,
    "output_throughput": 3438.719074583122,
    "total_throughput": 7382.163764249405,
    "itl": 151.1844115319157,
    "ttft": 2269542.8748198757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.714643796593926,
    "arrivals": 1107373,
    "finished_requests": 57608,
    "scheduler_time": 121.11436739450639
}
#Debug simulation 
Total elapsed time: 6.940492964349687. Arrivals time: 0.23890276439487934 Scheduler time: 6.5541662480682135 Scheduler overhead time: 0.03685172274708748 Adapter cache time: 0.055648157838732004 Engine time: 0.037910462357103825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.90050330478698,
    "estimated_duration": 3600.05514438657,
    "input_throughput": 3711.049821231688,
    "output_throughput": 3235.8568224056944,
    "total_throughput": 6946.906643637382,
    "itl": 133.95751263404858,
    "ttft": 2299592.0782342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.644440954964917,
    "arrivals": 1107373,
    "finished_requests": 54187,
    "scheduler_time": 127.22311262021624
}
#Debug simulation 
Total elapsed time: 5.900595637969673. Arrivals time: 0.2253745556809008 Scheduler time: 5.508484746795148 Scheduler overhead time: 0.04025353956967592 Adapter cache time: 0.066866893786937 Engine time: 0.04098433954641223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.911912836134434,
    "estimated_duration": 3600.026610151295,
    "input_throughput": 3718.453625385124,
    "output_throughput": 3242.7504749775694,
    "total_throughput": 6961.204100362693,
    "itl": 134.43074886840787,
    "ttft": 2298311.219550774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.175230743974073,
    "arrivals": 1107373,
    "finished_requests": 54295,
    "scheduler_time": 127.05606455096111
}
#Debug simulation 
Total elapsed time: 5.912010096944869. Arrivals time: 0.22622619848698378 Scheduler time: 5.516626595053822 Scheduler overhead time: 0.040347684640437365 Adapter cache time: 0.06897893268615007 Engine time: 0.041056170128285885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.91384237492457,
    "estimated_duration": 3600.1444490227755,
    "input_throughput": 3720.1290641644673,
    "output_throughput": 3243.8817845683943,
    "total_throughput": 6964.010848732862,
    "itl": 134.36586088615962,
    "ttft": 2298244.524797309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.532965332546173,
    "arrivals": 1107373,
    "finished_requests": 54324,
    "scheduler_time": 127.1170555666925
}
#Debug simulation 
Total elapsed time: 5.913932173047215. Arrivals time: 0.22757193446159363 Scheduler time: 5.517482228111476 Scheduler overhead time: 0.040083580650389194 Adapter cache time: 0.06872337032109499 Engine time: 0.04115061229094863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.1109122219495475,
    "estimated_duration": 3600.151444419452,
    "input_throughput": 3949.5171854618698,
    "output_throughput": 3443.451530133919,
    "total_throughput": 7392.968715595788,
    "itl": 151.23251037231378,
    "ttft": 2272872.483160779,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2522,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.67649557857803,
    "arrivals": 1105887,
    "finished_requests": 57616,
    "scheduler_time": 121.16969917514851
}
#Debug simulation 
Total elapsed time: 7.1110156159847975. Arrivals time: 0.2512577739544213 Scheduler time: 6.7144939308054745 Scheduler overhead time: 0.03708674479275942 Adapter cache time: 0.053597568068653345 Engine time: 0.03753613214939833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.281114622950554,
    "estimated_duration": 3600.0316315152554,
    "input_throughput": 3723.9061686681157,
    "output_throughput": 3244.540380630959,
    "total_throughput": 6968.446549299075,
    "itl": 134.6851470329788,
    "ttft": 2302327.0251937835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.872335659292816,
    "arrivals": 1105887,
    "finished_requests": 54270,
    "scheduler_time": 126.94619358005485
}
#Debug simulation 
Total elapsed time: 6.281176598742604. Arrivals time: 0.5925874523818493 Scheduler time: 5.520296244416386 Scheduler overhead time: 0.040185260120779276 Adapter cache time: 0.06879935273900628 Engine time: 0.040687937289476395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.896713885013014,
    "estimated_duration": 3600.0065112202296,
    "input_throughput": 3725.03048486282,
    "output_throughput": 3244.0305215007897,
    "total_throughput": 6969.0610063636095,
    "itl": 134.41807963495566,
    "ttft": 2303254.5517129404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.52101103528455,
    "arrivals": 1105887,
    "finished_requests": 54289,
    "scheduler_time": 127.06287261197137
}
#Debug simulation 
Total elapsed time: 5.896805585827678. Arrivals time: 0.23321694368496537 Scheduler time: 5.49403202207759 Scheduler overhead time: 0.04025810584425926 Adapter cache time: 0.06935412483289838 Engine time: 0.04123018775135279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.9242482371628284,
    "estimated_duration": 3600.0724762201817,
    "input_throughput": 3719.138736356127,
    "output_throughput": 3241.7366808829292,
    "total_throughput": 6960.875417239056,
    "itl": 134.0409428705662,
    "ttft": 2302217.03846704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.262565065372467,
    "arrivals": 1105887,
    "finished_requests": 54220,
    "scheduler_time": 127.28282690291387
}
#Debug simulation 
Total elapsed time: 5.924343527760357. Arrivals time: 0.2325513926334679 Scheduler time: 5.5249728369526565 Scheduler overhead time: 0.04019693937152624 Adapter cache time: 0.06682788766920567 Engine time: 0.04107990814372897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 8.547451887279749,
    "estimated_duration": 3600.031296767186,
    "input_throughput": 3816.9723725289723,
    "output_throughput": 3358.6946899206223,
    "total_throughput": 7175.667062449595,
    "itl": 143.08851594500032,
    "ttft": 2281035.8490609066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2088,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.806710058711847,
    "arrivals": 966726,
    "finished_requests": 55861,
    "scheduler_time": 123.73108685030832
}
#Debug simulation 
Total elapsed time: 8.547559882048517. Arrivals time: 0.3093769415281713 Scheduler time: 8.093874817714095 Scheduler overhead time: 0.03944431012496352 Adapter cache time: 0.04709569178521633 Engine time: 0.039808379486203194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.2218104880303144,
    "estimated_duration": 3600.078825620181,
    "input_throughput": 3674.3359356086453,
    "output_throughput": 3231.882290242811,
    "total_throughput": 6906.218225851457,
    "itl": 134.04865561556642,
    "ttft": 2301215.5041225427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3966,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.96826338914145,
    "arrivals": 966726,
    "finished_requests": 53735,
    "scheduler_time": 126.87120814742816
}
#Debug simulation 
Total elapsed time: 6.221928499173373. Arrivals time: 0.22347794193774462 Scheduler time: 5.818713664542884 Scheduler overhead time: 0.04027566080912948 Adapter cache time: 0.07937559438869357 Engine time: 0.041348622646182775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.147792605217546,
    "estimated_duration": 3600.1128579404867,
    "input_throughput": 3669.7799544978598,
    "output_throughput": 3230.9339898455714,
    "total_throughput": 6900.713944343432,
    "itl": 133.9233700861971,
    "ttft": 2300348.7640960077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.528092086390508,
    "arrivals": 966726,
    "finished_requests": 53696,
    "scheduler_time": 126.94298259406322
}
#Debug simulation 
Total elapsed time: 6.147909029386938. Arrivals time: 0.22394482186064124 Scheduler time: 5.741206512320787 Scheduler overhead time: 0.040481921285390854 Adapter cache time: 0.08206867473199964 Engine time: 0.041345748119056225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.1592006287537515,
    "estimated_duration": 3600.0228078072682,
    "input_throughput": 3670.812854668859,
    "output_throughput": 3232.3100772485354,
    "total_throughput": 6903.122931917394,
    "itl": 133.8519702107888,
    "ttft": 2299841.4051277526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.614566401240996,
    "arrivals": 966726,
    "finished_requests": 53717,
    "scheduler_time": 127.00768385241058
}
#Debug simulation 
Total elapsed time: 6.159315922763199. Arrivals time: 0.22517989855259657 Scheduler time: 5.752393544185907 Scheduler overhead time: 0.04035118967294693 Adapter cache time: 0.08141877362504601 Engine time: 0.04115205444395542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.129434274043888,
    "estimated_duration": 3600.0258086062395,
    "input_throughput": 3907.3272659247623,
    "output_throughput": 3425.5654419251005,
    "total_throughput": 7332.892707849863,
    "itl": 150.33336524348502,
    "ttft": 2264812.970609718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2845,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.812303695897356,
    "arrivals": 943309,
    "finished_requests": 56851,
    "scheduler_time": 121.04742159393429
}
#Debug simulation 
Total elapsed time: 7.129529311787337. Arrivals time: 0.2380701289512217 Scheduler time: 6.740478334482759 Scheduler overhead time: 0.037161883898079395 Adapter cache time: 0.05880057578906417 Engine time: 0.037906314712017775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.8119253176264465,
    "estimated_duration": 3600.0590884109024,
    "input_throughput": 3673.7877560331954,
    "output_throughput": 3213.115039482853,
    "total_throughput": 6886.902795516048,
    "itl": 132.7538933380362,
    "ttft": 2296595.914696568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4031,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.458182182837408,
    "arrivals": 943309,
    "finished_requests": 53361,
    "scheduler_time": 127.35048907471673
}
#Debug simulation 
Total elapsed time: 5.812021948862821. Arrivals time: 0.2222966654226184 Scheduler time: 5.408937202300876 Scheduler overhead time: 0.040435625705868006 Adapter cache time: 0.0800625397823751 Engine time: 0.04140346543863416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.7996638379991055,
    "estimated_duration": 3600.132295151028,
    "input_throughput": 3675.700478513906,
    "output_throughput": 3214.3507658277717,
    "total_throughput": 6890.051244341678,
    "itl": 132.68554023800507,
    "ttft": 2296473.3745379113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.60795747853432,
    "arrivals": 943309,
    "finished_requests": 53385,
    "scheduler_time": 127.41743515321973
}
#Debug simulation 
Total elapsed time: 5.799757976084948. Arrivals time: 0.22153713088482618 Scheduler time: 5.397178984712809 Scheduler overhead time: 0.040643824730068445 Adapter cache time: 0.07984487758949399 Engine time: 0.04160707397386432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.800365273840725,
    "estimated_duration": 3600.0463922341655,
    "input_throughput": 3677.730939401406,
    "output_throughput": 3216.1038327105257,
    "total_throughput": 6893.834772111932,
    "itl": 132.61518343442916,
    "ttft": 2296090.2387678223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4034,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.75273707426386,
    "arrivals": 943309,
    "finished_requests": 53412,
    "scheduler_time": 127.47843612614476
}
#Debug simulation 
Total elapsed time: 5.8004577942192554. Arrivals time: 0.22274988470599055 Scheduler time: 5.397253734059632 Scheduler overhead time: 0.04057605192065239 Adapter cache time: 0.07951192604377866 Engine time: 0.04137593042105436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.228376537095755,
    "estimated_duration": 3600.1032313419446,
    "input_throughput": 3988.835063112122,
    "output_throughput": 3433.397657153451,
    "total_throughput": 7422.232720265572,
    "itl": 151.15225044442548,
    "ttft": 2260237.0887180953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2919,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.301621964261535,
    "arrivals": 931969,
    "finished_requests": 57933,
    "scheduler_time": 120.75387110588541
}
#Debug simulation 
Total elapsed time: 7.22844106098637. Arrivals time: 0.5790365324355662 Scheduler time: 6.497416233178228 Scheduler overhead time: 0.03703095996752381 Adapter cache time: 0.06017123814672232 Engine time: 0.0376890292391181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.867369908839464,
    "estimated_duration": 3600.061896265851,
    "input_throughput": 3758.1106630509175,
    "output_throughput": 3239.4695802582346,
    "total_throughput": 6997.580243309152,
    "itl": 134.10175831549955,
    "ttft": 2290272.7692208937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.450883071808374,
    "arrivals": 931969,
    "finished_requests": 54577,
    "scheduler_time": 126.84131730853193
}
#Debug simulation 
Total elapsed time: 5.867470129858702. Arrivals time: 0.227310910820961 Scheduler time: 5.467420588247478 Scheduler overhead time: 0.040592425502836704 Adapter cache time: 0.072003030218184 Engine time: 0.04146232036873698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.838906943332404,
    "estimated_duration": 3600.061723050408,
    "input_throughput": 3761.721059750391,
    "output_throughput": 3241.2452612375987,
    "total_throughput": 7002.96632098799,
    "itl": 134.12473711106998,
    "ttft": 2289673.175623372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.79641648010877,
    "arrivals": 931969,
    "finished_requests": 54618,
    "scheduler_time": 126.86573991193724
}
#Debug simulation 
Total elapsed time: 5.838998241350055. Arrivals time: 0.22675462858751416 Scheduler time: 5.440974947065115 Scheduler overhead time: 0.040340298786759377 Adapter cache time: 0.0710854446515441 Engine time: 0.04111367044970393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.186888059135526,
    "estimated_duration": 3600.047425405928,
    "input_throughput": 3763.751806261882,
    "output_throughput": 3242.451173732468,
    "total_throughput": 7006.202979994349,
    "itl": 134.06301209256037,
    "ttft": 2289396.2978941877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.16497350566274,
    "arrivals": 931969,
    "finished_requests": 54641,
    "scheduler_time": 126.92136688246823
}
#Debug simulation 
Total elapsed time: 6.186950311996043. Arrivals time: 0.22374375443905592 Scheduler time: 5.792120029684156 Scheduler overhead time: 0.04037695098668337 Adapter cache time: 0.07086637057363987 Engine time: 0.04111584788188338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.112637771759182,
    "estimated_duration": 3600.0879220829747,
    "input_throughput": 3935.273056276616,
    "output_throughput": 3431.2434216475485,
    "total_throughput": 7366.516477924165,
    "itl": 150.51443977405,
    "ttft": 2263344.316671912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.43844885342789,
    "arrivals": 926257,
    "finished_requests": 57370,
    "scheduler_time": 121.04949968299219
}
#Debug simulation 
Total elapsed time: 7.112701416946948. Arrivals time: 0.5786225888878107 Scheduler time: 6.390700335148722 Scheduler overhead time: 0.036784234922379255 Adapter cache time: 0.052008280996233225 Engine time: 0.037579245399683714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.988107841927558,
    "estimated_duration": 3600.0834852055873,
    "input_throughput": 3707.449578558258,
    "output_throughput": 3237.351035856448,
    "total_throughput": 6944.800614414707,
    "itl": 133.9406590874215,
    "ttft": 2291512.6077474705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.528689863304006,
    "arrivals": 926257,
    "finished_requests": 54054,
    "scheduler_time": 126.99511098480767
}
#Debug simulation 
Total elapsed time: 5.988169541116804. Arrivals time: 0.5642191525548697 Scheduler time: 5.258376648649573 Scheduler overhead time: 0.04025009926408529 Adapter cache time: 0.06557800248265266 Engine time: 0.04099636571481824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 6.218857069965452,
    "estimated_duration": 3600.071969617913,
    "input_throughput": 3708.978907279221,
    "output_throughput": 3239.125244831599,
    "total_throughput": 6948.10415211082,
    "itl": 133.88808097224447,
    "ttft": 2291309.070943345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.081606425187136,
    "arrivals": 926257,
    "finished_requests": 54080,
    "scheduler_time": 127.04516987678839
}
#Debug simulation 
Total elapsed time: 6.218923659063876. Arrivals time: 0.56386244809255 Scheduler time: 5.486978000495583 Scheduler overhead time: 0.04118645191192627 Adapter cache time: 0.06637706235051155 Engine time: 0.0416106516495347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.98779662232846,
    "estimated_duration": 3600.13361027742,
    "input_throughput": 3711.3169805301773,
    "output_throughput": 3240.4114021482233,
    "total_throughput": 6951.7283826784005,
    "itl": 133.82513064057022,
    "ttft": 2291371.3208755143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.54984150769818,
    "arrivals": 926257,
    "finished_requests": 54110,
    "scheduler_time": 127.10055474702182
}
#Debug simulation 
Total elapsed time: 5.987864593043923. Arrivals time: 0.5621637743897736 Scheduler time: 5.259750599972904 Scheduler overhead time: 0.04023679159581661 Adapter cache time: 0.06577863590791821 Engine time: 0.0411082012578845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.597297182772309,
    "estimated_duration": 3600.1317513952517,
    "input_throughput": 3970.135813629782,
    "output_throughput": 3436.6367273100564,
    "total_throughput": 7406.772540939839,
    "itl": 150.76812413936884,
    "ttft": 2263068.8720627204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2400,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.869781676680294,
    "arrivals": 923309,
    "finished_requests": 57645,
    "scheduler_time": 120.93775591469453
}
#Debug simulation 
Total elapsed time: 6.59739456186071. Arrivals time: 0.23362743947654963 Scheduler time: 6.22075659269467 Scheduler overhead time: 0.0368850352242589 Adapter cache time: 0.05126096634194255 Engine time: 0.037850646767765284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.597457523923367,
    "estimated_duration": 3600.0178750065647,
    "input_throughput": 3741.3350343365832,
    "output_throughput": 3241.824736767097,
    "total_throughput": 6983.15977110368,
    "itl": 134.14084043193358,
    "ttft": 2293117.757606895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.495665368432473,
    "arrivals": 923309,
    "finished_requests": 54371,
    "scheduler_time": 126.85275820929225
}
#Debug simulation 
Total elapsed time: 5.597548300866038. Arrivals time: 0.22062710346654058 Scheduler time: 5.210278019309044 Scheduler overhead time: 0.04007863439619541 Adapter cache time: 0.0668793092481792 Engine time: 0.04096515150740743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.634872497059405,
    "estimated_duration": 3600.039944903886,
    "input_throughput": 3742.927913640066,
    "output_throughput": 3243.0520712768653,
    "total_throughput": 6985.979984916931,
    "itl": 134.08318146703922,
    "ttft": 2292933.554631454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.0555230351822,
    "arrivals": 923309,
    "finished_requests": 54399,
    "scheduler_time": 126.90338325583137
}
#Debug simulation 
Total elapsed time: 5.634969188831747. Arrivals time: 0.2208318510092795 Scheduler time: 5.246781598776579 Scheduler overhead time: 0.04047224950045347 Adapter cache time: 0.066993304528296 Engine time: 0.0411320379935205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.94042722415179,
    "estimated_duration": 3600.0760266791253,
    "input_throughput": 3744.3848130159163,
    "output_throughput": 3244.201487259586,
    "total_throughput": 6988.586300275502,
    "itl": 134.02797686225082,
    "ttft": 2292640.577886843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.498770140173608,
    "arrivals": 923309,
    "finished_requests": 54419,
    "scheduler_time": 126.95858884686183
}
#Debug simulation 
Total elapsed time: 5.940491430927068. Arrivals time: 0.564006638713181 Scheduler time: 5.210434873122722 Scheduler overhead time: 0.040007052943110466 Adapter cache time: 0.06632889062166214 Engine time: 0.04108818108215928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.694528329651803,
    "estimated_duration": 3600.0148655223006,
    "input_throughput": 3955.1725567483763,
    "output_throughput": 3434.4649846900497,
    "total_throughput": 7389.6375414384265,
    "itl": 150.67108554179472,
    "ttft": 2263903.120183954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.67802181475374,
    "arrivals": 921853,
    "finished_requests": 57335,
    "scheduler_time": 120.97486264802315
}
#Debug simulation 
Total elapsed time: 6.694618106819689. Arrivals time: 0.29092422779649496 Scheduler time: 6.261104566510767 Scheduler overhead time: 0.03687583701685071 Adapter cache time: 0.05132193397730589 Engine time: 0.03740233136340976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.673170975875109,
    "estimated_duration": 3600.0319401277156,
    "input_throughput": 3724.5622325018358,
    "output_throughput": 3238.7007098571376,
    "total_throughput": 6963.262942358973,
    "itl": 133.88100619424952,
    "ttft": 2293565.713611003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.968831270066428,
    "arrivals": 921853,
    "finished_requests": 54022,
    "scheduler_time": 126.96843476882052
}
#Debug simulation 
Total elapsed time: 5.673259160947055. Arrivals time: 0.2850798247382045 Scheduler time: 5.222377800848335 Scheduler overhead time: 0.040301448199898005 Adapter cache time: 0.06572787184268236 Engine time: 0.04094109497964382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.618349558208138,
    "estimated_duration": 3600.087514206187,
    "input_throughput": 3725.7135964255913,
    "output_throughput": 3239.507915843419,
    "total_throughput": 6965.22151226901,
    "itl": 133.83199863258008,
    "ttft": 2293141.5654081968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.555622319235045,
    "arrivals": 921853,
    "finished_requests": 54037,
    "scheduler_time": 127.01916875960754
}
#Debug simulation 
Total elapsed time: 5.61844732100144. Arrivals time: 0.23907399270683527 Scheduler time: 5.213303327560425 Scheduler overhead time: 0.04017036501318216 Adapter cache time: 0.06601241696625948 Engine time: 0.04115411313250661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.594643867108971,
    "estimated_duration": 3600.028475453252,
    "input_throughput": 3726.4566354050785,
    "output_throughput": 3240.5529788292056,
    "total_throughput": 6967.009614234284,
    "itl": 133.77982528988804,
    "ttft": 2292703.409037848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.03912783245247,
    "arrivals": 921853,
    "finished_requests": 54052,
    "scheduler_time": 127.06954102618067
}
#Debug simulation 
Total elapsed time: 5.594732625875622. Arrivals time: 0.22419868921861053 Scheduler time: 5.2046649334952235 Scheduler overhead time: 0.04032159177586436 Adapter cache time: 0.06539214961230755 Engine time: 0.04135291324928403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.676240966189653,
    "estimated_duration": 3600.0073084098926,
    "input_throughput": 3951.9780881456236,
    "output_throughput": 3419.696946514724,
    "total_throughput": 7371.675034660348,
    "itl": 149.10329921737178,
    "ttft": 2252694.7194369296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2411,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.942518176031745,
    "arrivals": 805539,
    "finished_requests": 57271,
    "scheduler_time": 121.3756491553175
}
#Debug simulation 
Total elapsed time: 6.6763262008316815. Arrivals time: 0.5627903868444264 Scheduler time: 5.970519280061126 Scheduler overhead time: 0.037023307755589485 Adapter cache time: 0.050909973215311766 Engine time: 0.037861259654164314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.215532276779413,
    "estimated_duration": 3600.0794373959475,
    "input_throughput": 3727.420806499314,
    "output_throughput": 3231.5231378380518,
    "total_throughput": 6958.943944337366,
    "itl": 133.59032569197532,
    "ttft": 2286592.378209364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.520171447041786,
    "arrivals": 805539,
    "finished_requests": 54109,
    "scheduler_time": 126.90133762413956
}
#Debug simulation 
Total elapsed time: 5.215649244841188. Arrivals time: 0.21259933291003108 Scheduler time: 4.835010877810419 Scheduler overhead time: 0.04026485467329621 Adapter cache time: 0.06780637009069324 Engine time: 0.04112211521714926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.204664283897728,
    "estimated_duration": 3600.1055230171532,
    "input_throughput": 3732.5022597500056,
    "output_throughput": 3233.594383712328,
    "total_throughput": 6966.096643462333,
    "itl": 133.56566877057756,
    "ttft": 2287095.509418972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.264860534896798,
    "arrivals": 805539,
    "finished_requests": 54165,
    "scheduler_time": 126.92189013231042
}
#Debug simulation 
Total elapsed time: 5.204757442232221. Arrivals time: 0.21861877758055925 Scheduler time: 4.816232790704817 Scheduler overhead time: 0.04039664100855589 Adapter cache time: 0.0692280507646501 Engine time: 0.0413169520907104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.503897276706994,
    "estimated_duration": 3600.1362937795434,
    "input_throughput": 3737.26850931936,
    "output_throughput": 3237.7932524778444,
    "total_throughput": 6975.061761797205,
    "itl": 133.74385269390436,
    "ttft": 2286228.0220921813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.692563356060457,
    "arrivals": 805539,
    "finished_requests": 54234,
    "scheduler_time": 126.8797948972814
}
#Debug simulation 
Total elapsed time: 5.503985866904259. Arrivals time: 0.5424016728065908 Scheduler time: 4.793294460978359 Scheduler overhead time: 0.04017612896859646 Adapter cache time: 0.068342091049999 Engine time: 0.041010644286870956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 6.008994487114251,
    "estimated_duration": 3600.0014580719585,
    "input_throughput": 3953.4620098799733,
    "output_throughput": 3440.533050959788,
    "total_throughput": 7393.995060839761,
    "itl": 151.2174413808177,
    "ttft": 2250078.836985542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.089517410910176,
    "arrivals": 794133,
    "finished_requests": 57624,
    "scheduler_time": 120.64256639274198
}
#Debug simulation 
Total elapsed time: 6.009087099228054. Arrivals time: 0.2237567682750523 Scheduler time: 5.646950775757432 Scheduler overhead time: 0.03686873987317085 Adapter cache time: 0.04705182649195194 Engine time: 0.03730661887675524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.005219421815127,
    "estimated_duration": 3600.1425866815885,
    "input_throughput": 3721.572042608931,
    "output_throughput": 3245.078970821683,
    "total_throughput": 6966.651013430614,
    "itl": 134.40086754507297,
    "ttft": 2280614.709656858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.80310422336628,
    "arrivals": 794133,
    "finished_requests": 54233,
    "scheduler_time": 126.61637301512133
}
#Debug simulation 
Total elapsed time: 5.005311698652804. Arrivals time: 0.20995709486305714 Scheduler time: 4.631512575317174 Scheduler overhead time: 0.0402429043315351 Adapter cache time: 0.0635810294188559 Engine time: 0.04125265311449766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 5.00127139268443,
    "estimated_duration": 3600.017003010645,
    "input_throughput": 3723.4424139636108,
    "output_throughput": 3246.712443364934,
    "total_throughput": 6970.154857328545,
    "itl": 134.345799012411,
    "ttft": 2280383.5181565206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3086,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.123706520246877,
    "arrivals": 794133,
    "finished_requests": 54263,
    "scheduler_time": 126.665858213282
}
#Debug simulation 
Total elapsed time: 5.001362133771181. Arrivals time: 0.2110764686949551 Scheduler time: 4.62715381802991 Scheduler overhead time: 0.040091538336127996 Adapter cache time: 0.06329878931865096 Engine time: 0.040997416246682405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.980716079007834,
    "estimated_duration": 3600.1007773644756,
    "input_throughput": 3725.685981996075,
    "output_throughput": 3247.9104678175004,
    "total_throughput": 6973.596449813575,
    "itl": 134.29393557811127,
    "ttft": 2280162.5906848097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.7199317854239,
    "arrivals": 794133,
    "finished_requests": 54291,
    "scheduler_time": 126.71659209502519
}
#Debug simulation 
Total elapsed time: 4.980811051093042. Arrivals time: 0.21011769259348512 Scheduler time: 4.607965810690075 Scheduler overhead time: 0.04004545556381345 Adapter cache time: 0.06324811605736613 Engine time: 0.040783525444567204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.709311083890498,
    "estimated_duration": 3600.058562937135,
    "input_throughput": 3939.6273010705645,
    "output_throughput": 3435.2443949995704,
    "total_throughput": 7374.871696070135,
    "itl": 150.27423865615413,
    "ttft": 2252082.001947766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.886058967095249,
    "arrivals": 788332,
    "finished_requests": 57464,
    "scheduler_time": 120.86770571668718
}
#Debug simulation 
Total elapsed time: 5.709403967950493. Arrivals time: 0.22322763642296195 Scheduler time: 5.349803972057998 Scheduler overhead time: 0.03669507522135973 Adapter cache time: 0.04521683417260647 Engine time: 0.03739018505439162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.924818791914731,
    "estimated_duration": 3600.1229184160857,
    "input_throughput": 3708.436990221947,
    "output_throughput": 3241.479878452102,
    "total_throughput": 6949.916868674049,
    "itl": 133.8097675608861,
    "ttft": 2283395.2210974446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2837,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.732144906012795,
    "arrivals": 788332,
    "finished_requests": 54167,
    "scheduler_time": 126.76286824072965
}
#Debug simulation 
Total elapsed time: 4.924908497836441. Arrivals time: 0.2577813952229917 Scheduler time: 4.5065880049951375 Scheduler overhead time: 0.040210568346083164 Adapter cache time: 0.06069073686376214 Engine time: 0.040973370894789696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.878722944762558,
    "estimated_duration": 3600.0359818949623,
    "input_throughput": 3709.6823662774314,
    "output_throughput": 3242.404258930703,
    "total_throughput": 6952.086625208134,
    "itl": 133.76257776121804,
    "ttft": 2283158.6711652796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2838,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.458589089559847,
    "arrivals": 788332,
    "finished_requests": 54183,
    "scheduler_time": 126.80329947890839
}
#Debug simulation 
Total elapsed time: 4.878814629744738. Arrivals time: 0.21136476891115308 Scheduler time: 4.507503342349082 Scheduler overhead time: 0.040039801970124245 Adapter cache time: 0.06034534564241767 Engine time: 0.040877444203943014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.892983895260841,
    "estimated_duration": 3600.0130625266115,
    "input_throughput": 3711.099034352807,
    "output_throughput": 3244.481838574901,
    "total_throughput": 6955.580872927708,
    "itl": 133.7178226234541,
    "ttft": 2282753.0403488697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2838,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.11756762934048,
    "arrivals": 788332,
    "finished_requests": 54211,
    "scheduler_time": 126.8483883633528
}
#Debug simulation 
Total elapsed time: 4.89309238223359. Arrivals time: 0.2092454731464386 Scheduler time: 4.5233537489548326 Scheduler overhead time: 0.04019719362258911 Adapter cache time: 0.060495870653539896 Engine time: 0.04102562880143523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.58165426319465,
    "estimated_duration": 3600.0574599942997,
    "input_throughput": 3957.6332206715833,
    "output_throughput": 3438.7867798142315,
    "total_throughput": 7396.420000485815,
    "itl": 150.56901943936575,
    "ttft": 2246678.311604179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2051,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.562050924529691,
    "arrivals": 785377,
    "finished_requests": 57599,
    "scheduler_time": 120.78074685326676
}
#Debug simulation 
Total elapsed time: 5.581784800160676. Arrivals time: 0.26872087409719825 Scheduler time: 5.1777999135665596 Scheduler overhead time: 0.0366933299228549 Adapter cache time: 0.044276997447013855 Engine time: 0.03734622150659561 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.735695971176028,
    "estimated_duration": 3600.063052993529,
    "input_throughput": 3718.0251576066103,
    "output_throughput": 3243.007088526398,
    "total_throughput": 6961.032246133009,
    "itl": 133.82779146196134,
    "ttft": 2275696.299072919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2804,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.521475514973964,
    "arrivals": 785377,
    "finished_requests": 54162,
    "scheduler_time": 126.76384951092305
}
#Debug simulation 
Total elapsed time: 4.735817002132535. Arrivals time: 0.2102906103245914 Scheduler time: 4.367045592982322 Scheduler overhead time: 0.03992938622832298 Adapter cache time: 0.05917956819757819 Engine time: 0.04077416146174073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.8358832327649,
    "estimated_duration": 3600.0648428002332,
    "input_throughput": 3719.2030101283854,
    "output_throughput": 3244.492671669398,
    "total_throughput": 6963.695681797783,
    "itl": 133.781136474938,
    "ttft": 2275527.525421598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2806,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.22376275804869,
    "arrivals": 785377,
    "finished_requests": 54181,
    "scheduler_time": 126.80874536171997
}
#Debug simulation 
Total elapsed time: 4.836002046708018. Arrivals time: 0.25785880256444216 Scheduler time: 4.418378136586398 Scheduler overhead time: 0.04024237301200628 Adapter cache time: 0.05976516334339976 Engine time: 0.04100857907906175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.757924376986921,
    "estimated_duration": 3600.0678649759275,
    "input_throughput": 3720.8193018576803,
    "output_throughput": 3245.6304820459627,
    "total_throughput": 6966.449783903643,
    "itl": 133.73373179104053,
    "ttft": 2275261.710122602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.92605000112334,
    "arrivals": 785377,
    "finished_requests": 54203,
    "scheduler_time": 126.85350288510188
}
#Debug simulation 
Total elapsed time: 4.758015614002943. Arrivals time: 0.20814062468707561 Scheduler time: 4.39124287571758 Scheduler overhead time: 0.04006006708368659 Adapter cache time: 0.05906564416363835 Engine time: 0.040884381625801325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.483421127777547,
    "estimated_duration": 3600.0601833383075,
    "input_throughput": 3958.9121498474324,
    "output_throughput": 3436.581715288891,
    "total_throughput": 7395.493865136323,
    "itl": 149.95642913116066,
    "ttft": 2247583.423326737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.66937570521642,
    "arrivals": 784050,
    "finished_requests": 57800,
    "scheduler_time": 120.88001016707055
}
#Debug simulation 
Total elapsed time: 5.483534345868975. Arrivals time: 0.22065932303667068 Scheduler time: 5.127791949547827 Scheduler overhead time: 0.036711812019348145 Adapter cache time: 0.04383949050679803 Engine time: 0.03741775220260024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.730909384787083,
    "estimated_duration": 3600.1323485909993,
    "input_throughput": 3724.1200327669308,
    "output_throughput": 3238.746765674711,
    "total_throughput": 6962.866798441642,
    "itl": 133.30801610715778,
    "ttft": 2278057.541943102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2764,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.234189594964967,
    "arrivals": 784050,
    "finished_requests": 54376,
    "scheduler_time": 126.88141430888624
}
#Debug simulation 
Total elapsed time: 4.730998986866325. Arrivals time: 0.2084212126210332 Scheduler time: 4.364685204811394 Scheduler overhead time: 0.04019372118636966 Adapter cache time: 0.05796669330447912 Engine time: 0.04093929473310709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.732352846302092,
    "estimated_duration": 3600.0035948666655,
    "input_throughput": 3723.8943369712174,
    "output_throughput": 3237.938155567794,
    "total_throughput": 6961.832492539012,
    "itl": 133.10528265845912,
    "ttft": 2278480.877518694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2777,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.09971077359787,
    "arrivals": 784050,
    "finished_requests": 54372,
    "scheduler_time": 126.97915675552711
}
#Debug simulation 
Total elapsed time: 4.732443816028535. Arrivals time: 0.2582804700359702 Scheduler time: 4.3161375764757395 Scheduler overhead time: 0.03989967470988631 Adapter cache time: 0.058426034171134233 Engine time: 0.041000072844326496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.021215538028628,
    "estimated_duration": 3600.056614022771,
    "input_throughput": 3724.845033760678,
    "output_throughput": 3239.096006040266,
    "total_throughput": 6963.941039800943,
    "itl": 133.05698038449412,
    "ttft": 2278159.938458208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2777,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.728148451965627,
    "arrivals": 784050,
    "finished_requests": 54386,
    "scheduler_time": 127.02857241087027
}
#Debug simulation 
Total elapsed time: 5.0212799669243395. Arrivals time: 0.5370737561024725 Scheduler time: 4.325724831782281 Scheduler overhead time: 0.04014812130481005 Adapter cache time: 0.05828612530604005 Engine time: 0.04122538212686777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_384_slots_128_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_384_slots_128_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.434332604985684,
    "estimated_duration": 3600.1107615649003,
    "input_throughput": 4006.7889449406202,
    "output_throughput": 3439.194741502343,
    "total_throughput": 7445.9836864429635,
    "itl": 150.5051554168673,
    "ttft": 2236504.768086653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.786872831615996,
    "arrivals": 771112,
    "finished_requests": 58235,
    "scheduler_time": 120.74101248304213
}
#Debug simulation 
Total elapsed time: 5.4344576257281005. Arrivals time: 0.21930275252088904 Scheduler time: 5.0798866539262235 Scheduler overhead time: 0.036714919842779636 Adapter cache time: 0.044208282604813576 Engine time: 0.03736176248639822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_384_slots_128_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_384_slots_128_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.621908142231405,
    "estimated_duration": 3600.0369037291453,
    "input_throughput": 3777.231001691721,
    "output_throughput": 3242.789535825915,
    "total_throughput": 7020.020537517636,
    "itl": 133.62862063479292,
    "ttft": 2266996.499849029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2693,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.70180261270562,
    "arrivals": 771112,
    "finished_requests": 54887,
    "scheduler_time": 126.76925314013614
}
#Debug simulation 
Total elapsed time: 4.622001509182155. Arrivals time: 0.2082117423415184 Scheduler time: 4.258225690573454 Scheduler overhead time: 0.039910960011184216 Adapter cache time: 0.05615028413012624 Engine time: 0.04077264480292797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_384_slots_128_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_384_slots_128_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.6554965460672975,
    "estimated_duration": 3600.1173655339453,
    "input_throughput": 3779.4228961149424,
    "output_throughput": 3244.3708951882136,
    "total_throughput": 7023.7937913031565,
    "itl": 133.58987361773245,
    "ttft": 2266847.068114265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2712,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.554263140969436,
    "arrivals": 771112,
    "finished_requests": 54915,
    "scheduler_time": 126.81709172238682
}
#Debug simulation 
Total elapsed time: 4.655587994959205. Arrivals time: 0.21037179930135608 Scheduler time: 4.288642386440188 Scheduler overhead time: 0.0403046365827322 Adapter cache time: 0.05640172166749835 Engine time: 0.04108752170577645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_384_slots_128_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_384_slots_128_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.685636525973678,
    "estimated_duration": 3600.036505690168,
    "input_throughput": 3780.531941408966,
    "output_throughput": 3245.5359776303258,
    "total_throughput": 7026.067919039291,
    "itl": 133.544767795354,
    "ttft": 2266687.182879478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2712,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.313193590828487,
    "arrivals": 771112,
    "finished_requests": 54934,
    "scheduler_time": 126.85676497576343
}
#Debug simulation 
Total elapsed time: 4.685737545136362. Arrivals time: 0.22290933365002275 Scheduler time: 4.306514489930123 Scheduler overhead time: 0.040379632730036974 Adapter cache time: 0.05625274404883385 Engine time: 0.04092795215547085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_384_slots_128_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 5.055587565060705,
    "estimated_duration": 3600.089271142823,
    "input_throughput": 3904.1890190512427,
    "output_throughput": 3432.8782063998174,
    "total_throughput": 7337.06722545106,
    "itl": 150.42502184321992,
    "ttft": 2248675.4164462094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1929,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.755337022631773,
    "arrivals": 765327,
    "finished_requests": 56982,
    "scheduler_time": 120.99694229394834
}
#Debug simulation 
Total elapsed time: 5.055702582933009. Arrivals time: 0.2118541467934847 Scheduler time: 4.711573958396912 Scheduler overhead time: 0.03645114880055189 Adapter cache time: 0.04154514288529754 Engine time: 0.03730313898995519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_384_slots_128_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 4.4320692219771445,
    "estimated_duration": 3600.081581750768,
    "input_throughput": 3685.5703679768894,
    "output_throughput": 3240.915444567762,
    "total_throughput": 6926.485812544651,
    "itl": 133.90082639765515,
    "ttft": 2280435.450407344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.79386918512131,
    "arrivals": 765327,
    "finished_requests": 53808,
    "scheduler_time": 126.8926984910303
}
#Debug simulation 
Total elapsed time: 4.432161597069353. Arrivals time: 0.2034702911041677 Scheduler time: 4.07592108938843 Scheduler overhead time: 0.03996392246335745 Adapter cache time: 0.053407936822623014 Engine time: 0.040796803310513496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_384_slots_128_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 4.527568748686463,
    "estimated_duration": 3600.0841815937606,
    "input_throughput": 3687.575437228495,
    "output_throughput": 3243.030276817412,
    "total_throughput": 6930.605714045907,
    "itl": 133.9008328199869,
    "ttft": 2280069.3340471354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2544,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.45816466640718,
    "arrivals": 765327,
    "finished_requests": 53836,
    "scheduler_time": 126.92167325130308
}
#Debug simulation 
Total elapsed time: 4.527658191043884. Arrivals time: 0.255192460026592 Scheduler time: 4.118140673264861 Scheduler overhead time: 0.040185637306421995 Adapter cache time: 0.05428787227720022 Engine time: 0.04109773878008127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_384_slots_128_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.490344090387225,
    "estimated_duration": 3600.0268129183387,
    "input_throughput": 3688.588360605973,
    "output_throughput": 3244.060838128239,
    "total_throughput": 6932.649198734212,
    "itl": 133.85897582723402,
    "ttft": 2279867.339706307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.247078793753065,
    "arrivals": 765327,
    "finished_requests": 53851,
    "scheduler_time": 126.96131585003347
}
#Debug simulation 
Total elapsed time: 4.490470413118601. Arrivals time: 0.21030989801511168 Scheduler time: 4.1256949519738555 Scheduler overhead time: 0.04010794684290886 Adapter cache time: 0.05446173110976815 Engine time: 0.04106092173606157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_384_slots_128_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 4.967017256189138,
    "estimated_duration": 3600.1001850219673,
    "input_throughput": 3980.581168163396,
    "output_throughput": 3443.0130726832444,
    "total_throughput": 7423.59424084664,
    "itl": 151.01324496151753,
    "ttft": 2238617.248146823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1794,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.862661803318503,
    "arrivals": 762451,
    "finished_requests": 58083,
    "scheduler_time": 120.71506814673405
}
#Debug simulation 
Total elapsed time: 4.967113114427775. Arrivals time: 0.2154147387482226 Scheduler time: 4.621841586660594 Scheduler overhead time: 0.03639929834753275 Adapter cache time: 0.03952463390305638 Engine time: 0.037034683395177126 
