INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 53.214425111189485,
    "estimated_duration": 3600.075732061304,
    "input_throughput": 5385.11393728366,
    "output_throughput": 4677.91298111315,
    "total_throughput": 10063.02691839681,
    "itl": 111.97746199514242,
    "ttft": 2117230.5477137286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 634,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.642952673872939,
    "arrivals": 923309,
    "finished_requests": 78586,
    "scheduler_time": 196.25615047648753
}
#Debug simulation 
Total elapsed time: 53.21464193193242. Arrivals time: 0.45596108632162213 Scheduler time: 52.58674244116992 Scheduler overhead time: 0.06314372131600976 Adapter cache time: 0.019760045688599348 Engine time: 0.06307680951431394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.295291792135686,
    "estimated_duration": 3600.0599935426358,
    "input_throughput": 5102.506634041862,
    "output_throughput": 4424.255437011891,
    "total_throughput": 9526.762071053752,
    "itl": 99.94808880544186,
    "ttft": 2146872.9924048553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 914,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.879141983520263,
    "arrivals": 923309,
    "finished_requests": 74348,
    "scheduler_time": 207.7457509560593
}
#Debug simulation 
Total elapsed time: 31.29554056096822. Arrivals time: 0.3801968810148537 Scheduler time: 30.73611604515463 Scheduler overhead time: 0.0649215648882091 Adapter cache time: 0.023449573200196028 Engine time: 0.06354074832051992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 52.46150522259995,
    "estimated_duration": 3600.1051197350107,
    "input_throughput": 5391.228132090937,
    "output_throughput": 4681.662462467371,
    "total_throughput": 10072.890594558308,
    "itl": 112.21692311970723,
    "ttft": 2116825.4095197027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 691,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.7361330776894395,
    "arrivals": 923309,
    "finished_requests": 78689,
    "scheduler_time": 196.0646526986485
}
#Debug simulation 
Total elapsed time: 52.461675620637834. Arrivals time: 0.4016214581206441 Scheduler time: 51.88779211649671 Scheduler overhead time: 0.06398807326331735 Adapter cache time: 0.019991641864180565 Engine time: 0.06234839744865894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 32.872701103333384,
    "estimated_duration": 3600.021792129789,
    "input_throughput": 5084.287000710499,
    "output_throughput": 4414.330500649111,
    "total_throughput": 9498.61750135961,
    "itl": 99.66903805561539,
    "ttft": 2148929.0937852855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1066,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.933860650556176,
    "arrivals": 923309,
    "finished_requests": 74151,
    "scheduler_time": 208.00413086018753
}
#Debug simulation 
Total elapsed time: 32.87281992100179. Arrivals time: 0.35758164897561073 Scheduler time: 32.33591047255322 Scheduler overhead time: 0.06450700433924794 Adapter cache time: 0.024873156566172838 Engine time: 0.06290525011718273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 51.24223244097084,
    "estimated_duration": 3600.0842352358486,
    "input_throughput": 5401.147231413642,
    "output_throughput": 4678.649414684195,
    "total_throughput": 10079.796646097837,
    "itl": 111.8859414142629,
    "ttft": 2114217.4035427584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 847,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.407181036663215,
    "arrivals": 923309,
    "finished_requests": 78554,
    "scheduler_time": 196.17763165744688
}
#Debug simulation 
Total elapsed time: 51.242401835974306. Arrivals time: 0.48524514911696315 Scheduler time: 50.58361484343186 Scheduler overhead time: 0.06349461106583476 Adapter cache time: 0.02174075273796916 Engine time: 0.06283947778865695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 38.823685716837645,
    "estimated_duration": 3600.014857603336,
    "input_throughput": 5094.6536960156245,
    "output_throughput": 4417.649545643159,
    "total_throughput": 9512.303241658783,
    "itl": 99.69729939946964,
    "ttft": 2144393.317745238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 934,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.890088187344393,
    "arrivals": 923309,
    "finished_requests": 74199,
    "scheduler_time": 207.947868272602
}
#Debug simulation 
Total elapsed time: 38.82382181100547. Arrivals time: 0.3765180101618171 Scheduler time: 38.26579821575433 Scheduler overhead time: 0.06602851022034883 Adapter cache time: 0.023656100500375032 Engine time: 0.06478370213881135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 53.79537240508944,
    "estimated_duration": 3600.0604471982015,
    "input_throughput": 5535.483165431099,
    "output_throughput": 4815.890248035461,
    "total_throughput": 10351.373413466561,
    "itl": 119.53587799291937,
    "ttft": 2100833.669819735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1724300991604775,
    "arrivals": 921853,
    "finished_requests": 80233,
    "scheduler_time": 190.22815162719866
}
#Debug simulation 
Total elapsed time: 53.79554351605475. Arrivals time: 0.41503379633650184 Scheduler time: 53.214807216543704 Scheduler overhead time: 0.062072173692286015 Adapter cache time: 0.01899027358740568 Engine time: 0.060349574778229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 51.760463437996805,
    "estimated_duration": 3600.045258321493,
    "input_throughput": 5373.906051675623,
    "output_throughput": 4675.02261564541,
    "total_throughput": 10048.928667321032,
    "itl": 111.61156520771664,
    "ttft": 2111978.7041189484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 537,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9223722115857576,
    "arrivals": 921853,
    "finished_requests": 77972,
    "scheduler_time": 196.56459064543927
}
#Debug simulation 
Total elapsed time: 51.76063594361767. Arrivals time: 0.4048585630953312 Scheduler time: 51.18270667642355 Scheduler overhead time: 0.0649099750444293 Adapter cache time: 0.018195252865552902 Engine time: 0.06409139418974519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 24.674085198901594,
    "estimated_duration": 3600.0686791397593,
    "input_throughput": 5059.429867419176,
    "output_throughput": 4417.529335523715,
    "total_throughput": 9476.959202942891,
    "itl": 99.72278117549796,
    "ttft": 2145035.3368170653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1039,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.801502029104172,
    "arrivals": 921853,
    "finished_requests": 73479,
    "scheduler_time": 207.97266490318813
}
#Debug simulation 
Total elapsed time: 24.674238589126617. Arrivals time: 0.3432604242116213 Scheduler time: 24.158019623719156 Scheduler overhead time: 0.061813898384571075 Adapter cache time: 0.02400296786800027 Engine time: 0.06061137747019529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 46.24393661227077,
    "estimated_duration": 3600.0550758873655,
    "input_throughput": 5384.608177201812,
    "output_throughput": 4679.825904009895,
    "total_throughput": 10064.434081211706,
    "itl": 111.9763379356348,
    "ttft": 2114623.866330486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.721703161713661,
    "arrivals": 921853,
    "finished_requests": 78110,
    "scheduler_time": 196.1915961602517
}
#Debug simulation 
Total elapsed time: 46.24410025821999. Arrivals time: 0.41007136879488826 Scheduler time: 45.664970812853426 Scheduler overhead time: 0.0626493995077908 Adapter cache time: 0.02006481122225523 Engine time: 0.061198013368994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 25.092328320723027,
    "estimated_duration": 3600.095424664317,
    "input_throughput": 5082.472224109476,
    "output_throughput": 4417.4654069017815,
    "total_throughput": 9499.937631011258,
    "itl": 99.7482944111586,
    "ttft": 2146182.1609367165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.564346800120541,
    "arrivals": 921853,
    "finished_requests": 73693,
    "scheduler_time": 207.86776829917358
}
#Debug simulation 
Total elapsed time: 25.09243136877194. Arrivals time: 0.35084258671849966 Scheduler time: 24.56629378022626 Scheduler overhead time: 0.062085953541100025 Adapter cache time: 0.025056852493435144 Engine time: 0.06124796299263835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 46.99607731541619,
    "estimated_duration": 3600.04267687409,
    "input_throughput": 5409.998088386877,
    "output_throughput": 4680.71840043616,
    "total_throughput": 10090.716488823036,
    "itl": 111.91691636243777,
    "ttft": 2112643.153133445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 755,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.819860310130729,
    "arrivals": 921853,
    "finished_requests": 78345,
    "scheduler_time": 196.28699536245523
}
#Debug simulation 
Total elapsed time: 46.99623784935102. Arrivals time: 0.4108828827738762 Scheduler time: 46.41451426409185 Scheduler overhead time: 0.06298914877697825 Adapter cache time: 0.020558477845042944 Engine time: 0.06178852031007409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 4320, 33, 17280, 17280, 33, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 33, 33, 4320, 33, 17280, 33, 17280, 4320, 17280, 4320, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 17280, 17280, 33, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 33, 17280, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 17280, 4320, 4320, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 33, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 4320, 33, 17280, 4320, 4320, 33, 17280, 17280, 17280, 33, 4320, 17280, 33, 17280, 4320, 4320, 17280, 17280, 33, 4320, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 17280, 17280, 4320, 33, 17280, 33, 17280, 4320, 4320, 4320, 4320, 4320, 33, 33, 17280, 33, 17280, 33, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 17280, 33, 4320, 4320, 33, 33, 4320, 33, 17280, 17280, 33, 17280, 17280, 33, 4320, 17280, 33, 33, 4320, 4320, 33, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 4320, 33, 17280, 17280, 17280, 4320, 33, 17280, 17280, 4320, 4320, 17280, 17280, 33, 17280, 17280, 4320, 33, 17280, 4320, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 4320, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 17280, 33, 33, 33, 4320, 33, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 33, 4320, 4320, 4320, 4320, 33, 4320, 17280, 17280, 33, 17280, 17280, 33, 17280, 4320, 17280, 4320, 4320, 4320, 33, 4320, 33, 33, 17280, 33, 4320, 33, 33, 4320, 17280, 4320, 17280, 33, 17280, 17280, 4320, 17280, 4320, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2769024 . Total input tokens: 617927716 . Total output tokens: 543860307
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 33.17155798105523,
    "estimated_duration": 3600.01284329308,
    "input_throughput": 5070.600243554161,
    "output_throughput": 4423.016998304553,
    "total_throughput": 9493.617241858714,
    "itl": 99.79918386529963,
    "ttft": 2146105.4184912466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.373079085350061,
    "arrivals": 921853,
    "finished_requests": 73666,
    "scheduler_time": 207.89258688599955
}
#Debug simulation 
Total elapsed time: 33.17170448321849. Arrivals time: 0.3783704345114529 Scheduler time: 32.61657118797302 Scheduler overhead time: 0.06461181491613388 Adapter cache time: 0.022197456564754248 Engine time: 0.0630472032353282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 56.446781154721975,
    "estimated_duration": 3600.0539323944713,
    "input_throughput": 5528.298846001634,
    "output_throughput": 4799.243379253585,
    "total_throughput": 10327.54222525522,
    "itl": 119.19501720804303,
    "ttft": 2086189.6329455867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 677,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.476600914630184,
    "arrivals": 805539,
    "finished_requests": 80215,
    "scheduler_time": 190.40581887412185
}
#Debug simulation 
Total elapsed time: 56.446924393996596. Arrivals time: 0.47737815603613853 Scheduler time: 55.80101228971034 Scheduler overhead time: 0.062427937518805265 Adapter cache time: 0.019983164500445127 Engine time: 0.061298842541873455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 49.43797744624317,
    "estimated_duration": 3600.0012068961246,
    "input_throughput": 5394.3823582057885,
    "output_throughput": 4675.008432708456,
    "total_throughput": 10069.390790914244,
    "itl": 111.88749689924026,
    "ttft": 2100758.9201434324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.022947112922563,
    "arrivals": 805539,
    "finished_requests": 78180,
    "scheduler_time": 196.0158478635209
}
#Debug simulation 
Total elapsed time: 49.43814315414056. Arrivals time: 0.3973776758648455 Scheduler time: 48.86725617526099 Scheduler overhead time: 0.0640971465036273 Adapter cache time: 0.020866175182163715 Engine time: 0.06267246278002858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 27.649676420725882,
    "estimated_duration": 3600.050709610677,
    "input_throughput": 5071.312454366615,
    "output_throughput": 4402.312711232315,
    "total_throughput": 9473.62516559893,
    "itl": 99.22014111145808,
    "ttft": 2131144.9177435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.150490087694461,
    "arrivals": 805539,
    "finished_requests": 73542,
    "scheduler_time": 208.19466969980834
}
#Debug simulation 
Total elapsed time: 27.649823620915413. Arrivals time: 0.3454168504104018 Scheduler time: 27.122130301315337 Scheduler overhead time: 0.06444707605987787 Adapter cache time: 0.026517036836594343 Engine time: 0.06400497537106276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 48.24120103660971,
    "estimated_duration": 3600.060662556647,
    "input_throughput": 5378.930194539548,
    "output_throughput": 4669.3771510122415,
    "total_throughput": 10048.30734555179,
    "itl": 111.56724831130585,
    "ttft": 2099738.1945379325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.72558449382893,
    "arrivals": 805539,
    "finished_requests": 78032,
    "scheduler_time": 196.338772606252
}
#Debug simulation 
Total elapsed time: 48.24135436490178. Arrivals time: 0.45783446403220296 Scheduler time: 47.609869077801704 Scheduler overhead time: 0.0641783494502306 Adapter cache time: 0.020977319218218327 Engine time: 0.06256145052611828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 27.619289287831634,
    "estimated_duration": 3600.0738172999168,
    "input_throughput": 5071.381290090643,
    "output_throughput": 4402.332786577877,
    "total_throughput": 9473.71407666852,
    "itl": 99.21804445674273,
    "ttft": 2131116.625576669,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.064950693752593,
    "arrivals": 805539,
    "finished_requests": 73543,
    "scheduler_time": 208.2007641307474
}
#Debug simulation 
Total elapsed time: 27.619381483178586. Arrivals time: 0.3532614903524518 Scheduler time: 27.085057362448424 Scheduler overhead time: 0.0642765131779015 Adapter cache time: 0.026545205153524876 Engine time: 0.06322966702282429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 47.521942887920886,
    "estimated_duration": 3600.1030798848374,
    "input_throughput": 5379.522355404201,
    "output_throughput": 4675.317241343662,
    "total_throughput": 10054.839596747863,
    "itl": 111.84329578651587,
    "ttft": 2099916.8449804387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.366601923350224,
    "arrivals": 805539,
    "finished_requests": 78082,
    "scheduler_time": 196.1060375054348
}
#Debug simulation 
Total elapsed time: 47.52210787497461. Arrivals time: 0.40162209514528513 Scheduler time: 46.94909762125462 Scheduler overhead time: 0.06345984013751149 Adapter cache time: 0.020347938407212496 Engine time: 0.062233513686805964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 1080, 540, 17280, 17280, 540, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 540, 540, 1080, 540, 17280, 540, 17280, 1080, 17280, 1080, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 17280, 17280, 540, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 540, 17280, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 17280, 1080, 1080, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 540, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 1080, 540, 17280, 1080, 1080, 540, 17280, 17280, 17280, 540, 1080, 17280, 540, 17280, 1080, 1080, 17280, 17280, 540, 1080, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 17280, 17280, 1080, 540, 17280, 540, 17280, 1080, 1080, 1080, 1080, 1080, 540, 540, 17280, 540, 17280, 540, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 17280, 540, 1080, 1080, 540, 540, 1080, 540, 17280, 17280, 540, 17280, 17280, 540, 1080, 17280, 540, 540, 1080, 1080, 540, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 1080, 540, 17280, 17280, 17280, 1080, 540, 17280, 17280, 1080, 1080, 17280, 17280, 540, 17280, 17280, 1080, 540, 17280, 1080, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 1080, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 17280, 540, 540, 540, 1080, 540, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 540, 1080, 1080, 1080, 1080, 540, 1080, 17280, 17280, 540, 17280, 17280, 540, 17280, 1080, 17280, 1080, 1080, 1080, 540, 1080, 540, 540, 17280, 540, 1080, 540, 540, 1080, 17280, 1080, 17280, 540, 17280, 17280, 1080, 17280, 1080, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2419200 . Total input tokens: 539911765 . Total output tokens: 475200731
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 27.66692643472925,
    "estimated_duration": 3600.100322861355,
    "input_throughput": 5071.343952295498,
    "output_throughput": 4402.30037461941,
    "total_throughput": 9473.644326914908,
    "itl": 99.21588425320365,
    "ttft": 2131091.8776660357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.982932291813176,
    "arrivals": 805539,
    "finished_requests": 73543,
    "scheduler_time": 208.20687715134073
}
#Debug simulation 
Total elapsed time: 27.667067229747772. Arrivals time: 0.4230869123712182 Scheduler time: 27.063014508225024 Scheduler overhead time: 0.06363251013681293 Adapter cache time: 0.026948415208607912 Engine time: 0.06328450236469507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 57.50831434503198,
    "estimated_duration": 3600.0527333035716,
    "input_throughput": 5528.3523532547515,
    "output_throughput": 4817.7927616310435,
    "total_throughput": 10346.145114885794,
    "itl": 119.75234812728627,
    "ttft": 2082837.5060582443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 656,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.337740324959231,
    "arrivals": 794133,
    "finished_requests": 80388,
    "scheduler_time": 189.61266879553136
}
#Debug simulation 
Total elapsed time: 57.508483867160976. Arrivals time: 0.41523174475878477 Scheduler time: 56.92410552222282 Scheduler overhead time: 0.06250921078026295 Adapter cache time: 0.0201096273958683 Engine time: 0.06178415846079588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 49.78967772377655,
    "estimated_duration": 3600.076476043326,
    "input_throughput": 5378.65129500848,
    "output_throughput": 4681.104724342303,
    "total_throughput": 10059.756019350783,
    "itl": 112.13979946370725,
    "ttft": 2093834.4423598729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 844,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.1682094608433635,
    "arrivals": 794133,
    "finished_requests": 78226,
    "scheduler_time": 195.61858334351598
}
#Debug simulation 
Total elapsed time: 49.78983277082443. Arrivals time: 0.405647792853415 Scheduler time: 49.211742174346 Scheduler overhead time: 0.06306230742484331 Adapter cache time: 0.02176353195682168 Engine time: 0.06234517553821206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 39.97945233201608,
    "estimated_duration": 3600.0221073696744,
    "input_throughput": 5054.0425745590155,
    "output_throughput": 4411.097078401727,
    "total_throughput": 9465.139652960743,
    "itl": 99.83183505355152,
    "ttft": 2126576.8814220154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.40333005786869,
    "arrivals": 794133,
    "finished_requests": 73597,
    "scheduler_time": 207.68813117976512
}
#Debug simulation 
Total elapsed time: 39.97961248503998. Arrivals time: 0.3559487839229405 Scheduler time: 39.43950438499451 Scheduler overhead time: 0.06560468161478639 Adapter cache time: 0.026940223295241594 Engine time: 0.06426810380071402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 49.48066207394004,
    "estimated_duration": 3600.076656279635,
    "input_throughput": 5386.202531597935,
    "output_throughput": 4686.708815094029,
    "total_throughput": 10072.911346691964,
    "itl": 112.2662673254089,
    "ttft": 2096876.706052355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3969447531085395,
    "arrivals": 794133,
    "finished_requests": 78307,
    "scheduler_time": 195.53756603572236
}
#Debug simulation 
Total elapsed time: 49.480825275182724. Arrivals time: 0.39890001993626356 Scheduler time: 48.91043985495344 Scheduler overhead time: 0.0636266223154962 Adapter cache time: 0.019925879314541817 Engine time: 0.062463248148560524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 39.87360660918057,
    "estimated_duration": 3600.044258330487,
    "input_throughput": 5054.17675293748,
    "output_throughput": 4411.262434691473,
    "total_throughput": 9465.439187628952,
    "itl": 99.82913046454533,
    "ttft": 2126538.6055620946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.317376429573592,
    "arrivals": 794133,
    "finished_requests": 73600,
    "scheduler_time": 207.69424995446107
}
#Debug simulation 
Total elapsed time: 39.87376382295042. Arrivals time: 0.36122389789670706 Scheduler time: 39.32957080658525 Scheduler overhead time: 0.06526179797947407 Adapter cache time: 0.02681570639833808 Engine time: 0.06360167637467384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 51.234139691106975,
    "estimated_duration": 3600.070653358027,
    "input_throughput": 5351.743300379037,
    "output_throughput": 4662.333775128484,
    "total_throughput": 10014.077075507521,
    "itl": 111.77691276644343,
    "ttft": 2089528.4528503472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 843,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.381645352900933,
    "arrivals": 794133,
    "finished_requests": 77900,
    "scheduler_time": 196.5360177054475
}
#Debug simulation 
Total elapsed time: 51.23430140642449. Arrivals time: 0.4024000051431358 Scheduler time: 50.65684001054615 Scheduler overhead time: 0.06416141195222735 Adapter cache time: 0.022367284633219242 Engine time: 0.0630071465857327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 1080, 270, 17280, 17280, 270, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 270, 270, 1080, 270, 17280, 270, 17280, 1080, 17280, 1080, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 17280, 17280, 270, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 270, 17280, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 17280, 1080, 1080, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 270, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 1080, 270, 17280, 1080, 1080, 270, 17280, 17280, 17280, 270, 1080, 17280, 270, 17280, 1080, 1080, 17280, 17280, 270, 1080, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 17280, 17280, 1080, 270, 17280, 270, 17280, 1080, 1080, 1080, 1080, 1080, 270, 270, 17280, 270, 17280, 270, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 17280, 270, 1080, 1080, 270, 270, 1080, 270, 17280, 17280, 270, 17280, 17280, 270, 1080, 17280, 270, 270, 1080, 1080, 270, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 1080, 270, 17280, 17280, 17280, 1080, 270, 17280, 17280, 1080, 1080, 17280, 17280, 270, 17280, 17280, 1080, 270, 17280, 1080, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 1080, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 17280, 270, 270, 270, 1080, 270, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 270, 1080, 1080, 1080, 1080, 270, 1080, 17280, 17280, 270, 17280, 17280, 270, 17280, 1080, 17280, 1080, 1080, 1080, 270, 1080, 270, 270, 17280, 270, 1080, 270, 270, 1080, 17280, 1080, 17280, 270, 17280, 17280, 1080, 17280, 1080, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2384640 . Total input tokens: 532176210 . Total output tokens: 468373698
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 36.08193513099104,
    "estimated_duration": 3600.0264690362824,
    "input_throughput": 5050.6667538106785,
    "output_throughput": 4411.202566588671,
    "total_throughput": 9461.86932039935,
    "itl": 99.66275606648014,
    "ttft": 2126456.366677564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.33661516178396,
    "arrivals": 794133,
    "finished_requests": 73572,
    "scheduler_time": 207.6208608198041
}
#Debug simulation 
Total elapsed time: 36.08207265520468. Arrivals time: 0.36125148506835103 Scheduler time: 35.537120557855815 Scheduler overhead time: 0.06513052061200142 Adapter cache time: 0.02744512353092432 Engine time: 0.06403441214933991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 58.31290216324851,
    "estimated_duration": 3600.0197114182993,
    "input_throughput": 5489.801885616293,
    "output_throughput": 4785.446853348702,
    "total_throughput": 10275.248738964994,
    "itl": 118.47908438736528,
    "ttft": 2080397.361881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4979643779015626,
    "arrivals": 788332,
    "finished_requests": 80095,
    "scheduler_time": 191.04799131829063
}
#Debug simulation 
Total elapsed time: 58.31306700501591. Arrivals time: 0.4013121142052114 Scheduler time: 57.746692213695496 Scheduler overhead time: 0.06189937563613057 Adapter cache time: 0.018172815907746553 Engine time: 0.060380285140126944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 57.15928898612037,
    "estimated_duration": 3600.013637204757,
    "input_throughput": 5334.5078478400565,
    "output_throughput": 4657.957077357108,
    "total_throughput": 9992.464925197164,
    "itl": 111.04833569232267,
    "ttft": 2095771.1814826804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 527,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.854368339260114,
    "arrivals": 788332,
    "finished_requests": 77903,
    "scheduler_time": 196.89927755306587
}
#Debug simulation 
Total elapsed time: 57.15945331891999. Arrivals time: 0.410073796287179 Scheduler time: 56.576807817444205 Scheduler overhead time: 0.06496758665889502 Adapter cache time: 0.018862546887248755 Engine time: 0.06311892392113805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 35.64256713213399,
    "estimated_duration": 3600.070833527411,
    "input_throughput": 5066.58392110804,
    "output_throughput": 4416.651709161137,
    "total_throughput": 9483.235630269177,
    "itl": 99.40423505917015,
    "ttft": 2126695.9206549497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 814,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.109128386834658,
    "arrivals": 788332,
    "finished_requests": 73878,
    "scheduler_time": 207.72556273729128
}
#Debug simulation 
Total elapsed time: 35.64271331299096. Arrivals time: 0.3705785465426743 Scheduler time: 35.09152925759554 Scheduler overhead time: 0.06590020004659891 Adapter cache time: 0.022479930892586708 Engine time: 0.06478028278797865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 57.58213577372953,
    "estimated_duration": 3600.0290294462666,
    "input_throughput": 5338.667505955139,
    "output_throughput": 4663.814336681199,
    "total_throughput": 10002.481842636338,
    "itl": 111.37094873334263,
    "ttft": 2094796.731756765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.655286078467028,
    "arrivals": 788332,
    "finished_requests": 77959,
    "scheduler_time": 196.61534672552
}
#Debug simulation 
Total elapsed time: 57.582294658757746. Arrivals time: 0.4500849498435855 Scheduler time: 56.95866311574355 Scheduler overhead time: 0.06538027711212635 Adapter cache time: 0.018855201080441475 Engine time: 0.06334609305486083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 35.629770069848746,
    "estimated_duration": 3600.014335229163,
    "input_throughput": 5066.663435616267,
    "output_throughput": 4416.721023692216,
    "total_throughput": 9483.384459308481,
    "itl": 99.40274189167349,
    "ttft": 2126677.3505615615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 814,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.052999631972074,
    "arrivals": 788332,
    "finished_requests": 73878,
    "scheduler_time": 207.72533490336338
}
#Debug simulation 
Total elapsed time: 35.62991765700281. Arrivals time: 0.3535975660197437 Scheduler time: 35.09614579938352 Scheduler overhead time: 0.06567978439852595 Adapter cache time: 0.022604525554925203 Engine time: 0.06464119208976626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 46.69799107220024,
    "estimated_duration": 3600.0675313457,
    "input_throughput": 5358.096711255194,
    "output_throughput": 4682.562716732893,
    "total_throughput": 10040.659427988086,
    "itl": 111.82541413886076,
    "ttft": 2100231.619296619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.453701228848643,
    "arrivals": 788332,
    "finished_requests": 78198,
    "scheduler_time": 195.7688789888988
}
#Debug simulation 
Total elapsed time: 46.69815469998866. Arrivals time: 0.3863546960055828 Scheduler time: 46.14367782976478 Scheduler overhead time: 0.06302369898185134 Adapter cache time: 0.018455255310982466 Engine time: 0.06125540239736438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 1080, 135, 17280, 17280, 135, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 135, 135, 1080, 135, 17280, 135, 17280, 1080, 17280, 1080, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 17280, 17280, 135, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 135, 17280, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 17280, 1080, 1080, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 135, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 1080, 135, 17280, 1080, 1080, 135, 17280, 17280, 17280, 135, 1080, 17280, 135, 17280, 1080, 1080, 17280, 17280, 135, 1080, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 17280, 17280, 1080, 135, 17280, 135, 17280, 1080, 1080, 1080, 1080, 1080, 135, 135, 17280, 135, 17280, 135, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 17280, 135, 1080, 1080, 135, 135, 1080, 135, 17280, 17280, 135, 17280, 17280, 135, 1080, 17280, 135, 135, 1080, 1080, 135, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 1080, 135, 17280, 17280, 17280, 1080, 135, 17280, 17280, 1080, 1080, 17280, 17280, 135, 17280, 17280, 1080, 135, 17280, 1080, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 1080, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 17280, 135, 135, 135, 1080, 135, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 135, 1080, 1080, 1080, 1080, 135, 1080, 17280, 17280, 135, 17280, 17280, 135, 17280, 1080, 17280, 1080, 1080, 1080, 135, 1080, 135, 135, 17280, 135, 1080, 135, 135, 1080, 17280, 1080, 17280, 135, 17280, 17280, 1080, 17280, 1080, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2367360 . Total input tokens: 528280478 . Total output tokens: 464973909
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 29.5492264139466,
    "estimated_duration": 3600.105172229986,
    "input_throughput": 5063.047085568746,
    "output_throughput": 4421.146393937404,
    "total_throughput": 9484.193479506152,
    "itl": 99.68820125242738,
    "ttft": 2127670.235306841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 902,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.632274832166749,
    "arrivals": 788332,
    "finished_requests": 73849,
    "scheduler_time": 207.4562656455078
}
#Debug simulation 
Total elapsed time: 29.549335835967213. Arrivals time: 0.35793092055246234 Scheduler time: 29.01348749920726 Scheduler overhead time: 0.06442587170749903 Adapter cache time: 0.02307517360895872 Engine time: 0.06349763879552484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 52.74186244001612,
    "estimated_duration": 3600.1229449295097,
    "input_throughput": 5529.300611257253,
    "output_throughput": 4807.458596483815,
    "total_throughput": 10336.759207741068,
    "itl": 119.13816548461668,
    "ttft": 2074906.1705921362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 571,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.775685557243469,
    "arrivals": 785377,
    "finished_requests": 80381,
    "scheduler_time": 189.98516378303646
}
#Debug simulation 
Total elapsed time: 52.74203468300402. Arrivals time: 0.4082289580255747 Scheduler time: 52.16560110170394 Scheduler overhead time: 0.06297595147043467 Adapter cache time: 0.0182567210868001 Engine time: 0.06202780921012163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 45.7435693978332,
    "estimated_duration": 3600.1186251037284,
    "input_throughput": 5369.628063142785,
    "output_throughput": 4675.615931826414,
    "total_throughput": 10045.2439949692,
    "itl": 111.53911156705612,
    "ttft": 2094572.4967696068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.265451345965271,
    "arrivals": 785377,
    "finished_requests": 78131,
    "scheduler_time": 196.05589839041485
}
#Debug simulation 
Total elapsed time: 45.74373449571431. Arrivals time: 0.3885291628539562 Scheduler time: 45.18800387624651 Scheduler overhead time: 0.06251589674502611 Adapter cache time: 0.01854135375469923 Engine time: 0.06076560216024518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 37.589284237939864,
    "estimated_duration": 3600.057306536183,
    "input_throughput": 5063.627728065107,
    "output_throughput": 4416.523306763143,
    "total_throughput": 9480.15103482825,
    "itl": 99.41085242901958,
    "ttft": 2119931.2523733885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 810,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.078532899734587,
    "arrivals": 785377,
    "finished_requests": 73809,
    "scheduler_time": 207.73606262663668
}
#Debug simulation 
Total elapsed time: 37.589401790872216. Arrivals time: 0.354617842938751 Scheduler time: 37.05506420414895 Scheduler overhead time: 0.06537777138873935 Adapter cache time: 0.022207691799849272 Engine time: 0.06470765173435211 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 45.960690485313535,
    "estimated_duration": 3600.0937216618972,
    "input_throughput": 5365.782808310507,
    "output_throughput": 4673.979429688938,
    "total_throughput": 10039.762237999445,
    "itl": 111.44638432743419,
    "ttft": 2094429.6467272649,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8706773163517876,
    "arrivals": 785377,
    "finished_requests": 78069,
    "scheduler_time": 196.13081635642615
}
#Debug simulation 
Total elapsed time: 45.96084394818172. Arrivals time: 0.3705418515019119 Scheduler time: 45.421965959016234 Scheduler overhead time: 0.06289798859506845 Adapter cache time: 0.018107487354427576 Engine time: 0.06174422753974795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 40.999927388038486,
    "estimated_duration": 3600.0666752756106,
    "input_throughput": 5063.498441623848,
    "output_throughput": 4419.214818787215,
    "total_throughput": 9482.713260411063,
    "itl": 99.58203607318003,
    "ttft": 2118984.343150685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 940,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.99845300185962,
    "arrivals": 785377,
    "finished_requests": 73754,
    "scheduler_time": 207.554275700421
}
#Debug simulation 
Total elapsed time: 41.000083005055785. Arrivals time: 0.3667359184473753 Scheduler time: 40.45159781770781 Scheduler overhead time: 0.06618418684229255 Adapter cache time: 0.023815684486180544 Engine time: 0.06435047695413232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 45.441326182801276,
    "estimated_duration": 3600.1101827850584,
    "input_throughput": 5378.965091845957,
    "output_throughput": 4678.86207498491,
    "total_throughput": 10057.827166830868,
    "itl": 111.73558350710655,
    "ttft": 2091955.7172789574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.817584722461161,
    "arrivals": 785377,
    "finished_requests": 78189,
    "scheduler_time": 195.8848596398221
}
#Debug simulation 
Total elapsed time: 45.441480955109. Arrivals time: 0.38953015906736255 Scheduler time: 44.88261901913211 Scheduler overhead time: 0.06300357542932034 Adapter cache time: 0.018702442292124033 Engine time: 0.061999869998544455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 1080, 66, 17280, 17280, 66, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 66, 66, 1080, 66, 17280, 66, 17280, 1080, 17280, 1080, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 17280, 17280, 66, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 66, 17280, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 17280, 1080, 1080, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 66, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 1080, 66, 17280, 1080, 1080, 66, 17280, 17280, 17280, 66, 1080, 17280, 66, 17280, 1080, 1080, 17280, 17280, 66, 1080, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 17280, 17280, 1080, 66, 17280, 66, 17280, 1080, 1080, 1080, 1080, 1080, 66, 66, 17280, 66, 17280, 66, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 17280, 66, 1080, 1080, 66, 66, 1080, 66, 17280, 17280, 66, 17280, 17280, 66, 1080, 17280, 66, 66, 1080, 1080, 66, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 1080, 66, 17280, 17280, 17280, 1080, 66, 17280, 17280, 1080, 1080, 17280, 17280, 66, 17280, 17280, 1080, 66, 17280, 1080, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 1080, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 17280, 66, 66, 66, 1080, 66, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 66, 1080, 1080, 1080, 1080, 66, 1080, 17280, 17280, 66, 17280, 17280, 66, 17280, 1080, 17280, 1080, 1080, 1080, 66, 1080, 66, 66, 17280, 66, 1080, 66, 66, 1080, 17280, 1080, 17280, 66, 17280, 17280, 1080, 17280, 1080, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2358528 . Total input tokens: 526322899 . Total output tokens: 463260640
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.06752214813605,
    "estimated_duration": 3600.0923885358798,
    "input_throughput": 5074.700598844901,
    "output_throughput": 4420.697105074138,
    "total_throughput": 9495.397703919038,
    "itl": 99.70484012827609,
    "ttft": 2123496.0987024484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 999,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.358021352831291,
    "arrivals": 785377,
    "finished_requests": 73900,
    "scheduler_time": 207.3956010521529
}
#Debug simulation 
Total elapsed time: 31.067671800032258. Arrivals time: 0.3408194473013282 Scheduler time: 30.549740272108465 Scheduler overhead time: 0.06334457406774163 Adapter cache time: 0.023389770183712244 Engine time: 0.06332996673882008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 48.94191862689331,
    "estimated_duration": 3600.039770526935,
    "input_throughput": 5535.568290981161,
    "output_throughput": 4817.115394661791,
    "total_throughput": 10352.683685642953,
    "itl": 119.2771508465418,
    "ttft": 2080432.457590625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 538,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.557476059189114,
    "arrivals": 784050,
    "finished_requests": 80909,
    "scheduler_time": 189.5847071377905
}
#Debug simulation 
Total elapsed time: 48.94207514682785. Arrivals time: 0.40324109699577093 Scheduler time: 48.376069008372724 Scheduler overhead time: 0.06051721377298236 Adapter cache time: 0.01783014228567481 Engine time: 0.05995219387114048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 48.234551570843905,
    "estimated_duration": 3600.093260612518,
    "input_throughput": 5384.668284038229,
    "output_throughput": 4680.038204658452,
    "total_throughput": 10064.70648869668,
    "itl": 111.70869197677065,
    "ttft": 2093492.5204525124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 732,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.353258405420937,
    "arrivals": 784050,
    "finished_requests": 78664,
    "scheduler_time": 195.6973995628821
}
#Debug simulation 
Total elapsed time: 48.23472469719127. Arrivals time: 0.3790175491012633 Scheduler time: 47.68430193141103 Scheduler overhead time: 0.0631291470490396 Adapter cache time: 0.020426690112799406 Engine time: 0.06236414844170213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 35.157434141263366,
    "estimated_duration": 3600.049393954629,
    "input_throughput": 5086.184381455397,
    "output_throughput": 4421.98210578236,
    "total_throughput": 9508.166487237757,
    "itl": 99.3587722605915,
    "ttft": 2122426.260106473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.141261555198608,
    "arrivals": 784050,
    "finished_requests": 74265,
    "scheduler_time": 207.59989745817953
}
#Debug simulation 
Total elapsed time: 35.15760520612821. Arrivals time: 0.3726815599948168 Scheduler time: 34.61084250640124 Scheduler overhead time: 0.0639356430619955 Adapter cache time: 0.019765664357692003 Engine time: 0.06322736153379083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 49.77495142491534,
    "estimated_duration": 3600.03962585188,
    "input_throughput": 5384.684063140804,
    "output_throughput": 4679.599046361481,
    "total_throughput": 10064.283109502285,
    "itl": 111.63916839344638,
    "ttft": 2092653.957953147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 767,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.264345919345498,
    "arrivals": 784050,
    "finished_requests": 78592,
    "scheduler_time": 195.74763451541122
}
#Debug simulation 
Total elapsed time: 49.77511100890115. Arrivals time: 0.39183380687609315 Scheduler time: 49.2113567837514 Scheduler overhead time: 0.06339565571397543 Adapter cache time: 0.02069472149014473 Engine time: 0.061999311204999685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 35.27998583810404,
    "estimated_duration": 3600.0050403341556,
    "input_throughput": 5086.247045448692,
    "output_throughput": 4422.036586516098,
    "total_throughput": 9508.28363196479,
    "itl": 99.35765224829618,
    "ttft": 2122405.090062552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.097145596579677,
    "arrivals": 784050,
    "finished_requests": 74265,
    "scheduler_time": 207.5996597963247
}
#Debug simulation 
Total elapsed time: 35.28015175880864. Arrivals time: 0.36892332835122943 Scheduler time: 34.735968806780875 Scheduler overhead time: 0.06375296087935567 Adapter cache time: 0.01977929286658764 Engine time: 0.06479785591363907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 50.24887795513496,
    "estimated_duration": 3600.083725119537,
    "input_throughput": 5372.9000425837985,
    "output_throughput": 4684.3541116366805,
    "total_throughput": 10057.254154220478,
    "itl": 111.8796806314692,
    "ttft": 2086582.5260795008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.851779914833582,
    "arrivals": 784050,
    "finished_requests": 78645,
    "scheduler_time": 195.54323362044295
}
#Debug simulation 
Total elapsed time: 50.24904640996829. Arrivals time: 0.4029667912982404 Scheduler time: 49.67297795135528 Scheduler overhead time: 0.06399611430242658 Adapter cache time: 0.02079583751037717 Engine time: 0.06239658081904054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 1080, 33, 17280, 17280, 33, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 33, 33, 1080, 33, 17280, 33, 17280, 1080, 17280, 1080, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 17280, 17280, 33, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 33, 17280, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 17280, 1080, 1080, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 17280, 17280, 33, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 1080, 33, 17280, 1080, 1080, 33, 17280, 17280, 17280, 33, 1080, 17280, 33, 17280, 1080, 1080, 17280, 17280, 33, 1080, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 17280, 17280, 1080, 33, 17280, 33, 17280, 1080, 1080, 1080, 1080, 1080, 33, 33, 17280, 33, 17280, 33, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 17280, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 17280, 33, 1080, 1080, 33, 33, 1080, 33, 17280, 17280, 33, 17280, 17280, 33, 1080, 17280, 33, 33, 1080, 1080, 33, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 1080, 33, 17280, 17280, 17280, 1080, 33, 17280, 17280, 1080, 1080, 17280, 17280, 33, 17280, 17280, 1080, 33, 17280, 1080, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 1080, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 17280, 33, 33, 33, 1080, 33, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 33, 1080, 1080, 1080, 1080, 33, 1080, 17280, 17280, 33, 17280, 17280, 33, 17280, 1080, 17280, 1080, 1080, 1080, 33, 1080, 33, 33, 17280, 33, 1080, 33, 33, 1080, 17280, 1080, 17280, 33, 17280, 17280, 1080, 17280, 1080, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2354304 . Total input tokens: 525381591 . Total output tokens: 462430944
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 34.98352139769122,
    "estimated_duration": 3600.0644131454724,
    "input_throughput": 5086.303159781848,
    "output_throughput": 4421.996712578465,
    "total_throughput": 9508.299872360312,
    "itl": 99.35599186615448,
    "ttft": 2122369.0894146166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.045987653955846,
    "arrivals": 784050,
    "finished_requests": 74268,
    "scheduler_time": 207.60579567507304
}
#Debug simulation 
Total elapsed time: 34.98369113402441. Arrivals time: 0.370816795155406 Scheduler time: 34.43890559207648 Scheduler overhead time: 0.0637275050394237 Adapter cache time: 0.019913374911993742 Engine time: 0.0633235415443778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 63.08201221982017,
    "estimated_duration": 3600.11246192301,
    "input_throughput": 5573.3447808134315,
    "output_throughput": 4810.39139281487,
    "total_throughput": 10383.736173628302,
    "itl": 119.28265234431933,
    "ttft": 2062870.6459400854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 727,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.807221366227692,
    "arrivals": 771112,
    "finished_requests": 80984,
    "scheduler_time": 189.75524640790167
}
#Debug simulation 
Total elapsed time: 63.08217623690143. Arrivals time: 0.4370437706820667 Scheduler time: 62.473403623793274 Scheduler overhead time: 0.06436037505045533 Adapter cache time: 0.020655157044529915 Engine time: 0.06208207132294774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 54.137053614947945,
    "estimated_duration": 3600.0560998382994,
    "input_throughput": 5426.612102205154,
    "output_throughput": 4678.447649956485,
    "total_throughput": 10105.059752161638,
    "itl": 111.84909792139639,
    "ttft": 2083587.3436385405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 618,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.514433740330866,
    "arrivals": 771112,
    "finished_requests": 78860,
    "scheduler_time": 195.82387076836338
}
#Debug simulation 
Total elapsed time: 54.13721320684999. Arrivals time: 0.40805880818516016 Scheduler time: 53.55738210165873 Scheduler overhead time: 0.06396987847983837 Adapter cache time: 0.01941551361232996 Engine time: 0.06260470952838659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 39.61617778381333,
    "estimated_duration": 3600.048912634463,
    "input_throughput": 5120.982922009513,
    "output_throughput": 4421.642701613003,
    "total_throughput": 9542.625623622516,
    "itl": 99.60940092766914,
    "ttft": 2109283.5614975314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.295650963624964,
    "arrivals": 771112,
    "finished_requests": 74436,
    "scheduler_time": 207.43858210774076
}
#Debug simulation 
Total elapsed time: 39.61634789267555. Arrivals time: 0.4225347167812288 Scheduler time: 39.01410897821188 Scheduler overhead time: 0.06591522321105003 Adapter cache time: 0.022066721227020025 Engine time: 0.0644371435046196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 57.38672482594848,
    "estimated_duration": 3600.018437773151,
    "input_throughput": 5405.63175894108,
    "output_throughput": 4669.1658086055595,
    "total_throughput": 10074.79756754664,
    "itl": 111.45448787595475,
    "ttft": 2081815.5131419199,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.377792990286828,
    "arrivals": 771112,
    "finished_requests": 78641,
    "scheduler_time": 196.30642199252185
}
#Debug simulation 
Total elapsed time: 57.38688466185704. Arrivals time: 0.4109604209661484 Scheduler time: 56.80370829999447 Scheduler overhead time: 0.06398298451676965 Adapter cache time: 0.020099208690226078 Engine time: 0.06253608874976635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_384_slots_64_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 39.497794228605926,
    "estimated_duration": 3600.100855763947,
    "input_throughput": 5121.11902934112,
    "output_throughput": 4421.694457396544,
    "total_throughput": 9542.813486737665,
    "itl": 99.60819987153454,
    "ttft": 2109258.2823287346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.239315091585765,
    "arrivals": 771112,
    "finished_requests": 74438,
    "scheduler_time": 207.4447335853947
}
#Debug simulation 
Total elapsed time: 39.497926926705986. Arrivals time: 0.36155899707227945 Scheduler time: 38.95619817916304 Scheduler overhead time: 0.06595270149409771 Adapter cache time: 0.022262861020863056 Engine time: 0.06504530413076282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_384_slots_64_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_384_slots_64_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 56.14812209503725,
    "estimated_duration": 3600.023541052099,
    "input_throughput": 5400.347186151538,
    "output_throughput": 4661.045909465392,
    "total_throughput": 10061.39309561693,
    "itl": 111.20740774746419,
    "ttft": 2081637.219507801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.843120406223443,
    "arrivals": 771112,
    "finished_requests": 78555,
    "scheduler_time": 196.75114018056203
}
#Debug simulation 
Total elapsed time: 56.14827416371554. Arrivals time: 0.38980450201779604 Scheduler time: 55.585061946418136 Scheduler overhead time: 0.06476716743782163 Adapter cache time: 0.01955071371048689 Engine time: 0.06312572583556175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_384_slots_64_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_384_slots_64_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 540, 270, 17280, 17280, 270, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 270, 17280, 270, 17280, 540, 540, 540, 270, 270, 540, 270, 17280, 270, 17280, 540, 17280, 540, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 17280, 17280, 270, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 270, 17280, 540, 540, 270, 270, 270, 540, 270, 540, 270, 17280, 270, 540, 17280, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 17280, 17280, 540, 270, 270, 17280, 540, 540, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 270, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 17280, 270, 270, 17280, 270, 17280, 540, 270, 17280, 540, 540, 270, 17280, 17280, 17280, 270, 540, 17280, 270, 17280, 540, 540, 17280, 17280, 270, 540, 17280, 270, 540, 540, 270, 540, 270, 540, 17280, 17280, 540, 270, 17280, 270, 17280, 540, 540, 540, 540, 540, 270, 270, 17280, 270, 17280, 270, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 270, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 270, 270, 270, 270, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270, 540, 17280, 17280, 540, 540, 270, 17280, 540, 17280, 270, 540, 540, 270, 270, 540, 270, 17280, 17280, 270, 17280, 17280, 270, 540, 17280, 270, 270, 540, 540, 270, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 540, 270, 17280, 17280, 17280, 540, 270, 17280, 17280, 540, 540, 17280, 17280, 270, 17280, 17280, 540, 270, 17280, 540, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 540, 17280, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 17280, 270, 270, 270, 540, 270, 540, 540, 270, 17280, 540, 17280, 270, 17280, 270, 540, 540, 540, 540, 270, 540, 17280, 17280, 270, 17280, 17280, 270, 17280, 540, 17280, 540, 540, 540, 270, 540, 270, 270, 17280, 270, 540, 270, 270, 540, 17280, 540, 17280, 270, 17280, 17280, 540, 17280, 540, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2315520 . Total input tokens: 516741114 . Total output tokens: 454867705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 37.92704682005569,
    "estimated_duration": 3600.104045511221,
    "input_throughput": 5133.837179801695,
    "output_throughput": 4425.153217408684,
    "total_throughput": 9558.990397210378,
    "itl": 99.6487331366019,
    "ttft": 2111382.9175128588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 900,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.643567325696376,
    "arrivals": 771112,
    "finished_requests": 74580,
    "scheduler_time": 207.3216060438943
}
#Debug simulation 
Total elapsed time: 37.927214220166206. Arrivals time: 0.364482250995934 Scheduler time: 37.383095535915345 Scheduler overhead time: 0.06552754668518901 Adapter cache time: 0.02322645578533411 Engine time: 0.0637075467966497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 54.30581606505439,
    "estimated_duration": 3600.0719138768823,
    "input_throughput": 5502.4364717947265,
    "output_throughput": 4810.603625234213,
    "total_throughput": 10313.04009702894,
    "itl": 119.63871092349065,
    "ttft": 2078023.3315078514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 851,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.627160086189511,
    "arrivals": 765327,
    "finished_requests": 80123,
    "scheduler_time": 189.65005418347144
}
#Debug simulation 
Total elapsed time: 54.30597761273384. Arrivals time: 0.40021518245339394 Scheduler time: 53.73797519225627 Scheduler overhead time: 0.0616690362803638 Adapter cache time: 0.021564822643995285 Engine time: 0.059923757798969746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 57.99276142101735,
    "estimated_duration": 3600.0272905183383,
    "input_throughput": 5301.7250869928575,
    "output_throughput": 4649.566419700655,
    "total_throughput": 9951.291506693513,
    "itl": 111.38181010780107,
    "ttft": 2090169.3634507842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 630,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.611864106217406,
    "arrivals": 765327,
    "finished_requests": 77236,
    "scheduler_time": 197.20838881192168
}
#Debug simulation 
Total elapsed time: 57.99291983200237. Arrivals time: 0.7133026709780097 Scheduler time: 57.105905044358224 Scheduler overhead time: 0.06460432196035981 Adapter cache time: 0.019998993258923292 Engine time: 0.06320876674726605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 37.467290439177305,
    "estimated_duration": 3600.090896440551,
    "input_throughput": 5044.016532458644,
    "output_throughput": 4421.7047452159695,
    "total_throughput": 9465.721277674615,
    "itl": 99.74920328638457,
    "ttft": 2123638.9197827918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.0401581727294555,
    "arrivals": 765327,
    "finished_requests": 73414,
    "scheduler_time": 207.6532433152097
}
#Debug simulation 
Total elapsed time: 37.46745924791321. Arrivals time: 0.3557132473215461 Scheduler time: 36.933449413627386 Scheduler overhead time: 0.06605203077197075 Adapter cache time: 0.02031019702553749 Engine time: 0.06479283142834902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 47.29307070141658,
    "estimated_duration": 3600.0794481517046,
    "input_throughput": 5345.643138481382,
    "output_throughput": 4677.9439849990595,
    "total_throughput": 10023.587123480442,
    "itl": 112.0932247285261,
    "ttft": 2095796.1085645217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 881,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.030983093823297,
    "arrivals": 765327,
    "finished_requests": 77802,
    "scheduler_time": 195.73382399419071
}
#Debug simulation 
Total elapsed time: 47.29323842516169. Arrivals time: 0.38677104003727436 Scheduler time: 46.7333114836365 Scheduler overhead time: 0.0634332993067801 Adapter cache time: 0.021943374071270227 Engine time: 0.062204862013459206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 37.14051708392799,
    "estimated_duration": 3600.0421922641526,
    "input_throughput": 5044.08477184525,
    "output_throughput": 4421.7645654837315,
    "total_throughput": 9465.84933732898,
    "itl": 99.74794568329193,
    "ttft": 2123619.358355862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.9916927534016144,
    "arrivals": 765327,
    "finished_requests": 73414,
    "scheduler_time": 207.65300455813892
}
#Debug simulation 
Total elapsed time: 37.14068110007793. Arrivals time: 0.3548882193863392 Scheduler time: 36.6078344816342 Scheduler overhead time: 0.06623853323981166 Adapter cache time: 0.01976722851395607 Engine time: 0.06457041203975677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 59.7045121002011,
    "estimated_duration": 3600.0469080406788,
    "input_throughput": 5337.701005251143,
    "output_throughput": 4677.947379626117,
    "total_throughput": 10015.64838487726,
    "itl": 112.08038083877078,
    "ttft": 2090067.3072748396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 878,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.605082585820901,
    "arrivals": 765327,
    "finished_requests": 77834,
    "scheduler_time": 195.80381643572528
}
#Debug simulation 
Total elapsed time: 59.70467545604333. Arrivals time: 0.7258210093714297 Scheduler time: 58.80195453669876 Scheduler overhead time: 0.06380277452990413 Adapter cache time: 0.022241235710680485 Engine time: 0.06510194716975093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 540, 135, 17280, 17280, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 135, 17280, 135, 17280, 540, 540, 540, 135, 135, 540, 135, 17280, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 135, 17280, 540, 540, 135, 135, 135, 540, 135, 540, 135, 17280, 135, 540, 17280, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 17280, 17280, 540, 135, 135, 17280, 540, 540, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 135, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 17280, 135, 135, 17280, 135, 17280, 540, 135, 17280, 540, 540, 135, 17280, 17280, 17280, 135, 540, 17280, 135, 17280, 540, 540, 17280, 17280, 135, 540, 17280, 135, 540, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 17280, 135, 17280, 540, 540, 540, 540, 540, 135, 135, 17280, 135, 17280, 135, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 135, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 135, 135, 135, 135, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135, 540, 17280, 17280, 540, 540, 135, 17280, 540, 17280, 135, 540, 540, 135, 135, 540, 135, 17280, 17280, 135, 17280, 17280, 135, 540, 17280, 135, 135, 540, 540, 135, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 540, 135, 17280, 17280, 17280, 540, 135, 17280, 17280, 540, 540, 17280, 17280, 135, 17280, 17280, 540, 135, 17280, 540, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 540, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 17280, 135, 135, 135, 540, 135, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 540, 540, 540, 540, 135, 540, 17280, 17280, 135, 17280, 17280, 135, 17280, 540, 17280, 540, 540, 540, 135, 540, 135, 135, 17280, 135, 540, 135, 135, 540, 17280, 540, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2298240 . Total input tokens: 512866905 . Total output tokens: 451475026
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 34.670281384605914,
    "estimated_duration": 3600.0344633934374,
    "input_throughput": 5032.066827194759,
    "output_throughput": 4420.815178807549,
    "total_throughput": 9452.882006002308,
    "itl": 99.9451959772228,
    "ttft": 2125135.4246049896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1013,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.456173225846158,
    "arrivals": 765327,
    "finished_requests": 73299,
    "scheduler_time": 207.4158418719669
}
#Debug simulation 
Total elapsed time: 34.67039843369275. Arrivals time: 0.34823526814579964 Scheduler time: 34.142258369363844 Scheduler overhead time: 0.06468740152195096 Adapter cache time: 0.023915891535580158 Engine time: 0.06406484637409449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 58.01363284420222,
    "estimated_duration": 3600.035683660875,
    "input_throughput": 5564.713730733991,
    "output_throughput": 4815.652544413254,
    "total_throughput": 10380.366275147244,
    "itl": 119.84450252566336,
    "ttft": 2061613.2871012702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 801,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.296539634592003,
    "arrivals": 762451,
    "finished_requests": 80958,
    "scheduler_time": 189.53062325770992
}
#Debug simulation 
Total elapsed time: 58.0138033288531. Arrivals time: 0.38906848849728703 Scheduler time: 57.4567956328392 Scheduler overhead time: 0.06210872856900096 Adapter cache time: 0.020288214087486267 Engine time: 0.06063389265909791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 42.77769216708839,
    "estimated_duration": 3600.109048037212,
    "input_throughput": 5395.788222190322,
    "output_throughput": 4672.520964100009,
    "total_throughput": 10068.309186290331,
    "itl": 111.73696951286209,
    "ttft": 2083198.7794723136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 470,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4377324486617042,
    "arrivals": 762451,
    "finished_requests": 78588,
    "scheduler_time": 196.28579864053955
}
#Debug simulation 
Total elapsed time: 42.777858663350344. Arrivals time: 0.43523706402629614 Scheduler time: 42.17695796396583 Scheduler overhead time: 0.06183465430513024 Adapter cache time: 0.017451461870223284 Engine time: 0.061210628133267164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 33.26564973592758,
    "estimated_duration": 3600.0843697522096,
    "input_throughput": 5104.493148660526,
    "output_throughput": 4423.869099794512,
    "total_throughput": 9528.362248455038,
    "itl": 99.85073706918475,
    "ttft": 2114850.1797069763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 773,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.80705908722247,
    "arrivals": 762451,
    "finished_requests": 74319,
    "scheduler_time": 207.44710562846763
}
#Debug simulation 
Total elapsed time: 33.26581719471142. Arrivals time: 0.6842142366804183 Scheduler time: 32.40640819538385 Scheduler overhead time: 0.06416785717010498 Adapter cache time: 0.021057477686554193 Engine time: 0.06295605516061187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 50.647335027344525,
    "estimated_duration": 3600.024024723419,
    "input_throughput": 5384.730453705601,
    "output_throughput": 4664.9391461464575,
    "total_throughput": 10049.669599852059,
    "itl": 111.77684122532824,
    "ttft": 2080749.9346181338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.326089153820644,
    "arrivals": 762451,
    "finished_requests": 78362,
    "scheduler_time": 196.65585718639431
}
#Debug simulation 
Total elapsed time: 50.647494815289974. Arrivals time: 0.3862676862627268 Scheduler time: 50.09222627803683 Scheduler overhead time: 0.06356755923479795 Adapter cache time: 0.01771817309781909 Engine time: 0.06210620794445276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 33.00112898694351,
    "estimated_duration": 3600.0313086311266,
    "input_throughput": 5104.568384153166,
    "output_throughput": 4423.934303520212,
    "total_throughput": 9528.50268767338,
    "itl": 99.84940367560438,
    "ttft": 2114835.8476587306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 773,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.754244207185721,
    "arrivals": 762451,
    "finished_requests": 74319,
    "scheduler_time": 207.44685938742137
}
#Debug simulation 
Total elapsed time: 33.00129616307095. Arrivals time: 0.36345552559942007 Scheduler time: 32.462072843685746 Scheduler overhead time: 0.06390520837157965 Adapter cache time: 0.020964428316801786 Engine time: 0.06361877918243408 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 50.2194735519588,
    "estimated_duration": 3600.065471099203,
    "input_throughput": 5403.308122077198,
    "output_throughput": 4675.128865048414,
    "total_throughput": 10078.436987125613,
    "itl": 112.02057681384017,
    "ttft": 2081731.1882106105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3196388890966624,
    "arrivals": 762451,
    "finished_requests": 78614,
    "scheduler_time": 196.1951025902093
}
#Debug simulation 
Total elapsed time: 50.219630209729075. Arrivals time: 0.39420506404712796 Scheduler time: 49.65642280736938 Scheduler overhead time: 0.06359874410554767 Adapter cache time: 0.018097340129315853 Engine time: 0.06161756720393896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 540, 66, 17280, 17280, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 66, 17280, 66, 17280, 540, 540, 540, 66, 66, 540, 66, 17280, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 66, 17280, 540, 540, 66, 66, 66, 540, 66, 540, 66, 17280, 66, 540, 17280, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 17280, 17280, 540, 66, 66, 17280, 540, 540, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 66, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 17280, 66, 66, 17280, 66, 17280, 540, 66, 17280, 540, 540, 66, 17280, 17280, 17280, 66, 540, 17280, 66, 17280, 540, 540, 17280, 17280, 66, 540, 17280, 66, 540, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 17280, 66, 17280, 540, 540, 540, 540, 540, 66, 66, 17280, 66, 17280, 66, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 66, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 66, 66, 66, 66, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66, 540, 17280, 17280, 540, 540, 66, 17280, 540, 17280, 66, 540, 540, 66, 66, 540, 66, 17280, 17280, 66, 17280, 17280, 66, 540, 17280, 66, 66, 540, 540, 66, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 540, 66, 17280, 17280, 17280, 540, 66, 17280, 17280, 540, 540, 17280, 17280, 66, 17280, 17280, 540, 66, 17280, 540, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 540, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 17280, 66, 66, 66, 540, 66, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 540, 540, 540, 540, 66, 540, 17280, 17280, 66, 17280, 17280, 66, 17280, 540, 17280, 540, 540, 540, 66, 540, 66, 66, 17280, 66, 540, 66, 66, 540, 17280, 540, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2289408 . Total input tokens: 510946244 . Total output tokens: 449735579
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 32.305085971951485,
    "estimated_duration": 3600.056699854526,
    "input_throughput": 5107.01484250038,
    "output_throughput": 4424.5517023783705,
    "total_throughput": 9531.56654487875,
    "itl": 99.9155584107769,
    "ttft": 2113956.69381132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 955,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.044824442956617,
    "arrivals": 762451,
    "finished_requests": 74381,
    "scheduler_time": 207.28031362952822
}
#Debug simulation 
Total elapsed time: 32.30523669766262. Arrivals time: 0.3401966355741024 Scheduler time: 31.788383154198527 Scheduler overhead time: 0.063948062248528 Adapter cache time: 0.02249247208237648 Engine time: 0.06327973352745175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 63.87266306532547,
    "estimated_duration": 3600.1021646541258,
    "input_throughput": 5518.835602797073,
    "output_throughput": 4815.613337369162,
    "total_throughput": 10334.448940166236,
    "itl": 119.51368569106245,
    "ttft": 2060802.7899885715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.205492144320228,
    "arrivals": 761080,
    "finished_requests": 80704,
    "scheduler_time": 189.5017734465633
}
#Debug simulation 
Total elapsed time: 63.872816735412925. Arrivals time: 0.4054005085490644 Scheduler time: 63.29780430253595 Scheduler overhead time: 0.06382893491536379 Adapter cache time: 0.018750417046248913 Engine time: 0.061977306846529245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 41.995010145008564,
    "estimated_duration": 3600.0291317570022,
    "input_throughput": 5229.097129781409,
    "output_throughput": 4577.332403962414,
    "total_throughput": 9806.429533743823,
    "itl": 106.6636187407708,
    "ttft": 2086508.1702457143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8410433134529787,
    "arrivals": 761080,
    "finished_requests": 76439,
    "scheduler_time": 200.63251350790614
}
#Debug simulation 
Total elapsed time: 41.995175004936755. Arrivals time: 0.4263495490886271 Scheduler time: 41.3975043008104 Scheduler overhead time: 0.06439780350774527 Adapter cache time: 0.01739490684121847 Engine time: 0.06324635352939367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 26.184364879038185,
    "estimated_duration": 3600.0059557254417,
    "input_throughput": 5059.950517867768,
    "output_throughput": 4416.7649152668955,
    "total_throughput": 9476.715433134665,
    "itl": 99.42375667644156,
    "ttft": 2120151.8306154707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 886,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.6461767034791635,
    "arrivals": 761080,
    "finished_requests": 73878,
    "scheduler_time": 207.6167794064332
}
#Debug simulation 
Total elapsed time: 26.184520654845983. Arrivals time: 0.34298239229246974 Scheduler time: 25.66848700074479 Scheduler overhead time: 0.06229481380432844 Adapter cache time: 0.022002148907631636 Engine time: 0.06181358452886343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 43.69122271705419,
    "estimated_duration": 3600.082881549642,
    "input_throughput": 5364.752044732524,
    "output_throughput": 4680.846956708498,
    "total_throughput": 10045.599001441022,
    "itl": 111.65632119547703,
    "ttft": 2086882.0775405515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8867787841055486,
    "arrivals": 761080,
    "finished_requests": 78289,
    "scheduler_time": 195.78725884944637
}
#Debug simulation 
Total elapsed time: 43.691389847081155. Arrivals time: 0.3854663730598986 Scheduler time: 43.13796944869682 Scheduler overhead time: 0.0614950992166996 Adapter cache time: 0.018711742479354143 Engine time: 0.06170353712514043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 35.11134613491595,
    "estimated_duration": 3600.0091705604627,
    "input_throughput": 5061.601550632484,
    "output_throughput": 4424.568451174305,
    "total_throughput": 9486.170001806788,
    "itl": 99.57725755072774,
    "ttft": 2117781.786538641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.771776214237357,
    "arrivals": 761080,
    "finished_requests": 73961,
    "scheduler_time": 207.47339774566316
}
#Debug simulation 
Total elapsed time: 35.11149395117536. Arrivals time: 0.3310364596545696 Scheduler time: 34.60608309274539 Scheduler overhead time: 0.06387750152498484 Adapter cache time: 0.019285633228719234 Engine time: 0.06383905094116926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 40.03032052889466,
    "estimated_duration": 3600.1117347781546,
    "input_throughput": 5362.423008570769,
    "output_throughput": 4686.189830449695,
    "total_throughput": 10048.612839020463,
    "itl": 112.00827392857286,
    "ttft": 2087056.6008701117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.93249529939143,
    "arrivals": 761080,
    "finished_requests": 78395,
    "scheduler_time": 195.4713567823934
}
#Debug simulation 
Total elapsed time: 40.030487870797515. Arrivals time: 0.4301732904277742 Scheduler time: 39.43611704790965 Scheduler overhead time: 0.060597209725528955 Adapter cache time: 0.018505835440009832 Engine time: 0.059680407866835594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 540, 33, 17280, 17280, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 17280, 17280, 540, 540, 33, 17280, 33, 17280, 540, 540, 540, 33, 33, 540, 33, 17280, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 33, 17280, 540, 540, 33, 33, 33, 540, 33, 540, 33, 17280, 33, 540, 17280, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 17280, 17280, 540, 33, 33, 17280, 540, 540, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 540, 17280, 17280, 33, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 17280, 33, 33, 17280, 33, 17280, 540, 33, 17280, 540, 540, 33, 17280, 17280, 17280, 33, 540, 17280, 33, 17280, 540, 540, 17280, 17280, 33, 540, 17280, 33, 540, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 17280, 33, 17280, 540, 540, 540, 540, 540, 33, 33, 17280, 33, 17280, 33, 540, 17280, 540, 17280, 540, 17280, 540, 17280, 33, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 33, 33, 33, 33, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33, 540, 17280, 17280, 540, 540, 33, 17280, 540, 17280, 33, 540, 540, 33, 33, 540, 33, 17280, 17280, 33, 17280, 17280, 33, 540, 17280, 33, 33, 540, 540, 33, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 540, 33, 17280, 17280, 17280, 540, 33, 17280, 17280, 540, 540, 17280, 17280, 33, 17280, 17280, 540, 33, 17280, 540, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 540, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 17280, 33, 33, 33, 540, 33, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 540, 540, 540, 540, 33, 540, 17280, 17280, 33, 17280, 17280, 33, 17280, 540, 17280, 540, 540, 540, 33, 540, 33, 33, 17280, 33, 540, 33, 33, 540, 17280, 540, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2285184 . Total input tokens: 510005577 . Total output tokens: 448916184
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 30.278224186971784,
    "estimated_duration": 3600.093682843728,
    "input_throughput": 5049.706646978144,
    "output_throughput": 4419.035836710078,
    "total_throughput": 9468.742483688222,
    "itl": 99.43062093902387,
    "ttft": 2119837.2389302594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.76831063728782,
    "arrivals": 761080,
    "finished_requests": 73845,
    "scheduler_time": 207.60621354291476
}
#Debug simulation 
Total elapsed time: 30.278381431940943. Arrivals time: 0.34503607684746385 Scheduler time: 29.758529384620488 Scheduler overhead time: 0.06368825118988752 Adapter cache time: 0.021230557933449745 Engine time: 0.0628107963129878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 51.61514606978744,
    "estimated_duration": 3600.133066947664,
    "input_throughput": 5554.558575512427,
    "output_throughput": 4822.100371617284,
    "total_throughput": 10376.65894712971,
    "itl": 120.03564661787833,
    "ttft": 2069936.1836511607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9806702372339235,
    "arrivals": 753814,
    "finished_requests": 81096,
    "scheduler_time": 189.26182091451028
}
#Debug simulation 
Total elapsed time: 51.615302903112024. Arrivals time: 0.3946229200810194 Scheduler time: 51.05929294042289 Scheduler overhead time: 0.06025138823315501 Adapter cache time: 0.018397788517177105 Engine time: 0.058485162910073996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 49.84821503004059,
    "estimated_duration": 3600.078791142448,
    "input_throughput": 5392.962522867123,
    "output_throughput": 4687.339077555288,
    "total_throughput": 10080.301600422412,
    "itl": 112.3404465572493,
    "ttft": 2083263.0556303652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.388417395614093,
    "arrivals": 753814,
    "finished_requests": 78795,
    "scheduler_time": 195.4458803451859
}
#Debug simulation 
Total elapsed time: 49.84837782103568. Arrivals time: 0.37436235696077347 Scheduler time: 49.30589719908312 Scheduler overhead time: 0.06262002047151327 Adapter cache time: 0.018996352795511484 Engine time: 0.061367401387542486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 25.328211633954197,
    "estimated_duration": 3600.0701151620915,
    "input_throughput": 5087.44091479323,
    "output_throughput": 4428.413472519879,
    "total_throughput": 9515.85438731311,
    "itl": 100.13910645219336,
    "ttft": 2116944.6931043733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1016,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.649928469401808,
    "arrivals": 753814,
    "finished_requests": 74347,
    "scheduler_time": 206.97556896515326
}
#Debug simulation 
Total elapsed time: 25.32833191473037. Arrivals time: 0.3393330918624997 Scheduler time: 24.81590986903757 Scheduler overhead time: 0.061915479600429535 Adapter cache time: 0.0234028953127563 Engine time: 0.06130741164088249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 49.776412618812174,
    "estimated_duration": 3600.0143480848046,
    "input_throughput": 5389.26740953727,
    "output_throughput": 4684.982994296302,
    "total_throughput": 10074.250403833574,
    "itl": 112.33137102980386,
    "ttft": 2083396.9300712624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.193764173178927,
    "arrivals": 753814,
    "finished_requests": 78762,
    "scheduler_time": 195.56898890855533
}
#Debug simulation 
Total elapsed time: 49.77655524900183. Arrivals time: 0.37464932538568974 Scheduler time: 49.23500568047166 Scheduler overhead time: 0.06228328729048371 Adapter cache time: 0.01848828187212348 Engine time: 0.060762687120586634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 25.316425753757358,
    "estimated_duration": 3600.1065460733953,
    "input_throughput": 5087.424154147967,
    "output_throughput": 4428.441157495391,
    "total_throughput": 9515.865311643358,
    "itl": 100.13729140285818,
    "ttft": 2116917.4775388497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1016,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.5782659262931205,
    "arrivals": 753814,
    "finished_requests": 74348,
    "scheduler_time": 206.981676605154
}
#Debug simulation 
Total elapsed time: 25.316519011743367. Arrivals time: 0.3235842464491725 Scheduler time: 24.819195023272187 Scheduler overhead time: 0.06239250209182501 Adapter cache time: 0.023601911962032318 Engine time: 0.06134124333038926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 42.541022937744856,
    "estimated_duration": 3600.0373176173184,
    "input_throughput": 5372.591807687476,
    "output_throughput": 4677.500401897221,
    "total_throughput": 10050.092209584696,
    "itl": 111.95614473387485,
    "ttft": 2084274.1976476267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5749957267194823,
    "arrivals": 753814,
    "finished_requests": 78481,
    "scheduler_time": 196.0280342224838
}
#Debug simulation 
Total elapsed time: 42.54118734272197. Arrivals time: 0.37850513914600015 Scheduler time: 41.9980461737141 Scheduler overhead time: 0.06114010279998183 Adapter cache time: 0.018282237462699413 Engine time: 0.06011056574061513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 270, 135, 17280, 17280, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 135, 17280, 135, 17280, 270, 270, 270, 135, 135, 270, 135, 17280, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 135, 17280, 270, 270, 135, 135, 135, 270, 135, 270, 135, 17280, 135, 270, 17280, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 17280, 17280, 270, 135, 135, 17280, 270, 270, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 135, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 17280, 135, 135, 17280, 135, 17280, 270, 135, 17280, 270, 270, 135, 17280, 17280, 17280, 135, 270, 17280, 135, 17280, 270, 270, 17280, 17280, 135, 270, 17280, 135, 270, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 17280, 135, 17280, 270, 270, 270, 270, 270, 135, 135, 17280, 135, 17280, 135, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 135, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 135, 135, 135, 135, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135, 270, 17280, 17280, 270, 270, 135, 17280, 270, 17280, 135, 270, 270, 135, 135, 270, 135, 17280, 17280, 135, 17280, 17280, 135, 270, 17280, 135, 135, 270, 270, 135, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 270, 135, 17280, 17280, 17280, 270, 135, 17280, 17280, 270, 270, 17280, 17280, 135, 17280, 17280, 270, 135, 17280, 270, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 270, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 17280, 135, 135, 135, 270, 135, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 270, 270, 270, 270, 135, 270, 17280, 17280, 135, 17280, 17280, 135, 17280, 270, 17280, 270, 270, 270, 135, 270, 135, 135, 17280, 135, 270, 135, 135, 270, 17280, 270, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2263680 . Total input tokens: 505216006 . Total output tokens: 444651293
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 25.29969913791865,
    "estimated_duration": 3600.0348150181226,
    "input_throughput": 5087.525521585213,
    "output_throughput": 4428.529394630242,
    "total_throughput": 9516.054916215455,
    "itl": 100.13537912883746,
    "ttft": 2116893.062371795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1016,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.506810500361048,
    "arrivals": 753814,
    "finished_requests": 74348,
    "scheduler_time": 206.9814009758139
}
#Debug simulation 
Total elapsed time: 25.299812464043498. Arrivals time: 0.3318706681020558 Scheduler time: 24.79295454826206 Scheduler overhead time: 0.06258434243500233 Adapter cache time: 0.02376695442944765 Engine time: 0.06190435076132417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 46.33684923686087,
    "estimated_duration": 3600.005202967432,
    "input_throughput": 5530.634784524261,
    "output_throughput": 4822.7271965298505,
    "total_throughput": 10353.361981054111,
    "itl": 119.81526338418023,
    "ttft": 2067167.3328047795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.914546146914422,
    "arrivals": 750842,
    "finished_requests": 80750,
    "scheduler_time": 189.2143903869442
}
#Debug simulation 
Total elapsed time: 46.33699703216553. Arrivals time: 0.432728074491024 Scheduler time: 45.743103043176234 Scheduler overhead time: 0.059571466874331236 Adapter cache time: 0.01802506437525153 Engine time: 0.059683638624846935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 56.44187851017341,
    "estimated_duration": 3600.0020549688675,
    "input_throughput": 5346.046670566813,
    "output_throughput": 4659.116507127952,
    "total_throughput": 10005.163177694765,
    "itl": 111.61614434473437,
    "ttft": 2073099.0484706862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.997317188777966,
    "arrivals": 750842,
    "finished_requests": 77999,
    "scheduler_time": 196.74804942411225
}
#Debug simulation 
Total elapsed time: 56.442042312119156. Arrivals time: 0.39699457166716456 Scheduler time: 55.87417094549164 Scheduler overhead time: 0.06334042502567172 Adapter cache time: 0.01864418527111411 Engine time: 0.06284831603989005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 21.204038522206247,
    "estimated_duration": 3600.0175343862225,
    "input_throughput": 5060.353686056679,
    "output_throughput": 4419.710139748728,
    "total_throughput": 9480.063825805408,
    "itl": 99.76641335377033,
    "ttft": 2115587.1777185528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 989,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.4387097138911695,
    "arrivals": 750842,
    "finished_requests": 73804,
    "scheduler_time": 207.34172932523543
}
#Debug simulation 
Total elapsed time: 21.204151642043144. Arrivals time: 0.3275744691491127 Scheduler time: 20.705474485643208 Scheduler overhead time: 0.06071531120687723 Adapter cache time: 0.022839949000626802 Engine time: 0.06113481242209673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 49.16793737234548,
    "estimated_duration": 3600.0189374311453,
    "input_throughput": 5374.323673365412,
    "output_throughput": 4687.118121673139,
    "total_throughput": 10061.44179503855,
    "itl": 112.09385928889881,
    "ttft": 2081092.5168147932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1618445684760745,
    "arrivals": 750842,
    "finished_requests": 78430,
    "scheduler_time": 195.51696231412063
}
#Debug simulation 
Total elapsed time: 49.16811437625438. Arrivals time: 0.4365864717401564 Scheduler time: 48.562743661459535 Scheduler overhead time: 0.06296998215839267 Adapter cache time: 0.019081084057688713 Engine time: 0.06129978643730283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 20.69206085195765,
    "estimated_duration": 3600.090104015774,
    "input_throughput": 5070.532812397636,
    "output_throughput": 4420.851295429206,
    "total_throughput": 9491.384107826842,
    "itl": 99.71479972320175,
    "ttft": 2117449.769247562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1007,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.505063293613513,
    "arrivals": 750842,
    "finished_requests": 73906,
    "scheduler_time": 207.36802773391545
}
#Debug simulation 
Total elapsed time: 20.692165364976972. Arrivals time: 0.33549372805282474 Scheduler time: 20.18597022118047 Scheduler overhead time: 0.060711891390383244 Adapter cache time: 0.02316326880827546 Engine time: 0.060486841481179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 58.49481863202527,
    "estimated_duration": 3600.010453921382,
    "input_throughput": 5321.194547958482,
    "output_throughput": 4637.679866127579,
    "total_throughput": 9958.874414086062,
    "itl": 110.9878327388934,
    "ttft": 2070178.2716048674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2174961540475344,
    "arrivals": 750842,
    "finished_requests": 77682,
    "scheduler_time": 197.81133042053108
}
#Debug simulation 
Total elapsed time: 58.494985721074045. Arrivals time: 0.39894624846056104 Scheduler time: 57.92360712401569 Scheduler overhead time: 0.06500927731394768 Adapter cache time: 0.0180974961258471 Engine time: 0.06333111738786101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 270, 66, 17280, 17280, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 66, 17280, 66, 17280, 270, 270, 270, 66, 66, 270, 66, 17280, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 66, 17280, 270, 270, 66, 66, 66, 270, 66, 270, 66, 17280, 66, 270, 17280, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 17280, 17280, 270, 66, 66, 17280, 270, 270, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 66, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 17280, 66, 66, 17280, 66, 17280, 270, 66, 17280, 270, 270, 66, 17280, 17280, 17280, 66, 270, 17280, 66, 17280, 270, 270, 17280, 17280, 66, 270, 17280, 66, 270, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 17280, 66, 17280, 270, 270, 270, 270, 270, 66, 66, 17280, 66, 17280, 66, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 66, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 66, 66, 66, 66, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66, 270, 17280, 17280, 270, 270, 66, 17280, 270, 17280, 66, 270, 270, 66, 66, 270, 66, 17280, 17280, 66, 17280, 17280, 66, 270, 17280, 66, 66, 270, 270, 66, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 270, 66, 17280, 17280, 17280, 270, 66, 17280, 17280, 270, 270, 17280, 17280, 66, 17280, 17280, 270, 66, 17280, 270, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 270, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 17280, 66, 66, 66, 270, 66, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 270, 270, 270, 270, 66, 270, 17280, 17280, 66, 17280, 17280, 66, 17280, 270, 17280, 270, 270, 270, 66, 270, 66, 66, 17280, 66, 270, 66, 66, 270, 17280, 270, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2254848 . Total input tokens: 503229701 . Total output tokens: 442900951
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 23.47608573595062,
    "estimated_duration": 3600.110950548952,
    "input_throughput": 5067.867699249097,
    "output_throughput": 4418.760482249669,
    "total_throughput": 9486.628181498767,
    "itl": 99.72493743749439,
    "ttft": 2115460.504577948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1093,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.078181275520492,
    "arrivals": 750842,
    "finished_requests": 73815,
    "scheduler_time": 207.36940162910682
}
#Debug simulation 
Total elapsed time: 23.476226346101612. Arrivals time: 0.3722300548106432 Scheduler time: 22.92809300031513 Scheduler overhead time: 0.06245705345645547 Adapter cache time: 0.024229138623923063 Engine time: 0.06163630448281765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 59.91140164108947,
    "estimated_duration": 3600.0217671827418,
    "input_throughput": 5461.865030718268,
    "output_throughput": 4796.280166248094,
    "total_throughput": 10258.145196966363,
    "itl": 119.57208881180675,
    "ttft": 2061127.170620584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 738,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.879957865579144,
    "arrivals": 749452,
    "finished_requests": 80174,
    "scheduler_time": 190.44750344517652
}
#Debug simulation 
Total elapsed time: 59.911560151260346. Arrivals time: 0.43656494840979576 Scheduler time: 59.30743882386014 Scheduler overhead time: 0.06234109168872237 Adapter cache time: 0.020168847870081663 Engine time: 0.060636235401034355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 54.2316989377141,
    "estimated_duration": 3600.1155546509654,
    "input_throughput": 5347.685291692402,
    "output_throughput": 4691.780234159065,
    "total_throughput": 10039.465525851467,
    "itl": 112.34182632160866,
    "ttft": 2083480.278100395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 478,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.492968479106207,
    "arrivals": 749452,
    "finished_requests": 78485,
    "scheduler_time": 195.3064259971322
}
#Debug simulation 
Total elapsed time: 54.2318576509133. Arrivals time: 0.42633146652951837 Scheduler time: 53.636302303057164 Scheduler overhead time: 0.06387277692556381 Adapter cache time: 0.01720070280134678 Engine time: 0.06247820891439915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 27.892954866867512,
    "estimated_duration": 3600.0701733200563,
    "input_throughput": 5035.812950079475,
    "output_throughput": 4421.273817927033,
    "total_throughput": 9457.086768006508,
    "itl": 99.68292153460393,
    "ttft": 2120387.1014017393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 645,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.853635595939156,
    "arrivals": 749452,
    "finished_requests": 73878,
    "scheduler_time": 207.52687410687167
}
#Debug simulation 
Total elapsed time: 27.89307339116931. Arrivals time: 0.38262308621779084 Scheduler time: 27.33991287369281 Scheduler overhead time: 0.06290042866021395 Adapter cache time: 0.018606548197567463 Engine time: 0.062156904488801956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 56.1560334097594,
    "estimated_duration": 3600.0680490247228,
    "input_throughput": 5318.804461262144,
    "output_throughput": 4664.941265360139,
    "total_throughput": 9983.745726622285,
    "itl": 111.95591549627804,
    "ttft": 2076980.083515882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0646222483785777,
    "arrivals": 749452,
    "finished_requests": 78048,
    "scheduler_time": 196.5786452520924
}
#Debug simulation 
Total elapsed time: 56.15618834877387. Arrivals time: 0.4244212042540312 Scheduler time: 55.561819532420486 Scheduler overhead time: 0.06421097926795483 Adapter cache time: 0.017572405282408 Engine time: 0.06249336898326874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 27.80882882140577,
    "estimated_duration": 3600.025856754388,
    "input_throughput": 5035.874941282921,
    "output_throughput": 4421.328244111534,
    "total_throughput": 9457.203185394455,
    "itl": 99.6818053971286,
    "ttft": 2120369.5007681344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 645,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.809519637320225,
    "arrivals": 749452,
    "finished_requests": 73878,
    "scheduler_time": 207.52667349982212
}
#Debug simulation 
Total elapsed time: 27.808986386284232. Arrivals time: 0.37275897338986397 Scheduler time: 27.265662234276533 Scheduler overhead time: 0.06275914143770933 Adapter cache time: 0.018690080381929874 Engine time: 0.062194396276026964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 58.28762363176793,
    "estimated_duration": 3600.0317368481433,
    "input_throughput": 5299.364670796718,
    "output_throughput": 4653.662029843013,
    "total_throughput": 9953.026700639732,
    "itl": 111.53453868516925,
    "ttft": 2074049.8015439226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7961573719698816,
    "arrivals": 749452,
    "finished_requests": 77799,
    "scheduler_time": 197.0994459045037
}
#Debug simulation 
Total elapsed time: 58.28777747601271. Arrivals time: 0.43228725949302316 Scheduler time: 57.68473243992776 Scheduler overhead time: 0.06512878648936749 Adapter cache time: 0.016631622333079576 Engine time: 0.06306291557848454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_384_slots_64_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 270, 33, 17280, 17280, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 17280, 17280, 270, 270, 33, 17280, 33, 17280, 270, 270, 270, 33, 33, 270, 33, 17280, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 270, 33, 33, 33, 270, 33, 270, 33, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 17280, 17280, 270, 33, 33, 17280, 270, 270, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 17280, 17280, 33, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 17280, 33, 33, 17280, 33, 17280, 270, 33, 17280, 270, 270, 33, 17280, 17280, 17280, 33, 270, 17280, 33, 17280, 270, 270, 17280, 17280, 33, 270, 17280, 33, 270, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 17280, 33, 17280, 270, 270, 270, 270, 270, 33, 33, 17280, 33, 17280, 33, 270, 17280, 270, 17280, 270, 17280, 270, 17280, 33, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 33, 33, 33, 33, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33, 270, 17280, 17280, 270, 270, 33, 17280, 270, 17280, 33, 270, 270, 33, 33, 270, 33, 17280, 17280, 33, 17280, 17280, 33, 270, 17280, 33, 33, 270, 270, 33, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 270, 33, 17280, 17280, 17280, 270, 33, 17280, 17280, 270, 270, 17280, 17280, 33, 17280, 17280, 270, 33, 17280, 270, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 270, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 17280, 33, 33, 33, 270, 33, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 270, 270, 270, 270, 33, 270, 17280, 17280, 33, 17280, 17280, 33, 17280, 270, 17280, 270, 270, 270, 33, 270, 33, 33, 17280, 33, 270, 33, 33, 270, 17280, 270, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2250624 . Total input tokens: 502301574 . Total output tokens: 442081840
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 27.887428023852408,
    "estimated_duration": 3600.089894783322,
    "input_throughput": 5035.843695533929,
    "output_throughput": 4421.391538879342,
    "total_throughput": 9457.23523441327,
    "itl": 99.6809389308299,
    "ttft": 2120402.510577158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 645,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.763125389758532,
    "arrivals": 749452,
    "finished_requests": 73880,
    "scheduler_time": 207.53285261058303
}
#Debug simulation 
Total elapsed time: 27.887541998177767. Arrivals time: 0.3884672154672444 Scheduler time: 27.326893082354218 Scheduler overhead time: 0.0627801651135087 Adapter cache time: 0.018640434835106134 Engine time: 0.06388283893465996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 61.441808918025345,
    "estimated_duration": 3600.1009180288233,
    "input_throughput": 5277.1170677076825,
    "output_throughput": 4609.937159508017,
    "total_throughput": 9887.0542272157,
    "itl": 112.24362092845452,
    "ttft": 2048941.2754219098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 464,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0681577908248028,
    "arrivals": 745115,
    "finished_requests": 76809,
    "scheduler_time": 199.25686836036562
}
#Debug simulation 
Total elapsed time: 61.44197710696608. Arrivals time: 0.4388189297169447 Scheduler time: 60.828770825173706 Scheduler overhead time: 0.06585607770830393 Adapter cache time: 0.01812807796522975 Engine time: 0.06410274794325233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 57.2742879842408,
    "estimated_duration": 3600.107928019991,
    "input_throughput": 5348.547428296177,
    "output_throughput": 4674.145702419219,
    "total_throughput": 10022.693130715395,
    "itl": 112.04759401654569,
    "ttft": 2078701.2597521225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.209658131594775,
    "arrivals": 745115,
    "finished_requests": 77859,
    "scheduler_time": 196.12081504203493
}
#Debug simulation 
Total elapsed time: 57.274451185017824. Arrivals time: 0.42747796000912786 Scheduler time: 56.67374021327123 Scheduler overhead time: 0.06484714616090059 Adapter cache time: 0.019003471825271845 Engine time: 0.06326542515307665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 32.889407355803996,
    "estimated_duration": 3600.064988612812,
    "input_throughput": 5080.396342246995,
    "output_throughput": 4425.912601688775,
    "total_throughput": 9506.30894393577,
    "itl": 99.90803817474486,
    "ttft": 2118387.5949505456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5293518131971675,
    "arrivals": 745115,
    "finished_requests": 73797,
    "scheduler_time": 207.34938698186374
}
#Debug simulation 
Total elapsed time: 32.889516768977046. Arrivals time: 0.37818405544385314 Scheduler time: 32.3362001623027 Scheduler overhead time: 0.0645995088852942 Adapter cache time: 0.01942342519760132 Engine time: 0.06407057540491223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 58.87962603289634,
    "estimated_duration": 3600.1020121791767,
    "input_throughput": 5347.325974340164,
    "output_throughput": 4675.859445941221,
    "total_throughput": 10023.185420281385,
    "itl": 111.99819078734981,
    "ttft": 2076274.1116604907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.172950336262578,
    "arrivals": 745115,
    "finished_requests": 77871,
    "scheduler_time": 195.98007662790445
}
#Debug simulation 
Total elapsed time: 58.87979333195835. Arrivals time: 0.38015902088955045 Scheduler time: 58.32724138908088 Scheduler overhead time: 0.0643817656673491 Adapter cache time: 0.01952699013054371 Engine time: 0.06280632643029094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 32.90576195390895,
    "estimated_duration": 3600.02540233812,
    "input_throughput": 5080.452206843122,
    "output_throughput": 4425.961269509813,
    "total_throughput": 9506.413476352935,
    "itl": 99.9069834921884,
    "ttft": 2118371.204247488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.489999549640375,
    "arrivals": 745115,
    "finished_requests": 73797,
    "scheduler_time": 207.34915297072862
}
#Debug simulation 
Total elapsed time: 32.90591658977792. Arrivals time: 0.37808312149718404 Scheduler time: 32.35194682981819 Scheduler overhead time: 0.06503240577876568 Adapter cache time: 0.019109945744276047 Engine time: 0.06456230487674475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 58.71013693185523,
    "estimated_duration": 3600.0971929198445,
    "input_throughput": 5337.3622906040855,
    "output_throughput": 4664.576843376876,
    "total_throughput": 10001.93913398096,
    "itl": 111.63995842191001,
    "ttft": 2076379.7350750214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6707545408280398,
    "arrivals": 745115,
    "finished_requests": 77695,
    "scheduler_time": 196.59424379483184
}
#Debug simulation 
Total elapsed time: 58.710299415979534. Arrivals time: 0.4266225122846663 Scheduler time: 58.111027140636 Scheduler overhead time: 0.06430979818105698 Adapter cache time: 0.01905485149472952 Engine time: 0.06339150667190552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_384_slots_64_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 135, 66, 17280, 17280, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 66, 17280, 66, 17280, 135, 135, 135, 66, 66, 135, 66, 17280, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 135, 66, 66, 66, 135, 66, 135, 66, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 17280, 17280, 135, 66, 66, 17280, 135, 135, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 66, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 17280, 66, 66, 17280, 66, 17280, 135, 66, 17280, 135, 135, 66, 17280, 17280, 17280, 66, 135, 17280, 66, 17280, 135, 135, 17280, 17280, 66, 135, 17280, 66, 135, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 17280, 66, 17280, 135, 135, 135, 135, 135, 66, 66, 17280, 66, 17280, 66, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 66, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 66, 66, 66, 66, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66, 135, 17280, 17280, 135, 135, 66, 17280, 135, 17280, 66, 135, 135, 66, 66, 135, 66, 17280, 17280, 66, 17280, 17280, 66, 135, 17280, 66, 66, 135, 135, 66, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 135, 66, 17280, 17280, 17280, 135, 66, 17280, 17280, 135, 135, 17280, 17280, 66, 17280, 17280, 135, 66, 17280, 135, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 135, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 17280, 66, 66, 66, 135, 66, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 135, 135, 135, 135, 66, 135, 17280, 17280, 66, 17280, 17280, 66, 17280, 135, 17280, 135, 135, 135, 66, 135, 66, 66, 17280, 66, 135, 66, 66, 135, 17280, 135, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2237568 . Total input tokens: 499381644 . Total output tokens: 439481698
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 32.92790182726458,
    "estimated_duration": 3600.0904154487066,
    "input_throughput": 5080.360460258165,
    "output_throughput": 4425.881342209034,
    "total_throughput": 9506.241802467199,
    "itl": 99.90583457706259,
    "ttft": 2118353.1057265867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.446504942551288,
    "arrivals": 745115,
    "finished_requests": 73797,
    "scheduler_time": 207.35524974561915
}
#Debug simulation 
Total elapsed time: 32.92804410820827. Arrivals time: 0.3831764035858214 Scheduler time: 32.36963561642915 Scheduler overhead time: 0.06481945281848311 Adapter cache time: 0.019124857150018215 Engine time: 0.06415601214393973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_384_slots_64_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 47.00460661994293,
    "estimated_duration": 3600.1280926631425,
    "input_throughput": 5544.41800020355,
    "output_throughput": 4809.483039030331,
    "total_throughput": 10353.90103923388,
    "itl": 119.91205722891642,
    "ttft": 2066002.4813477735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.041708154697002,
    "arrivals": 743719,
    "finished_requests": 80612,
    "scheduler_time": 190.02173392449777
}
#Debug simulation 
Total elapsed time: 47.0047298730351. Arrivals time: 0.35627799667418003 Scheduler time: 46.490732374135405 Scheduler overhead time: 0.059451133012771606 Adapter cache time: 0.01601324789226055 Engine time: 0.058178402949124575 
