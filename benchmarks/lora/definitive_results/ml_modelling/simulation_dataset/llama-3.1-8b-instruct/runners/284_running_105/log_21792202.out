INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 48.08528371481225,
    "estimated_duration": 3600.052632388581,
    "input_throughput": 5326.117687140073,
    "output_throughput": 4677.418837853936,
    "total_throughput": 10003.53652499401,
    "itl": 111.60380652014607,
    "ttft": 1965171.3196036185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.120283238426788,
    "arrivals": 394936,
    "finished_requests": 77807,
    "scheduler_time": 193.0347475011081
}
#Debug simulation 
Total elapsed time: 48.085518040694296. Arrivals time: 0.37483168533071876 Scheduler time: 47.545237419661134 Scheduler overhead time: 0.061251787934452295 Adapter cache time: 0.017747214529663324 Engine time: 0.060930703766644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 28.29648415837437,
    "estimated_duration": 3600.091289680667,
    "input_throughput": 5035.750079995355,
    "output_throughput": 4426.030819183304,
    "total_throughput": 9461.780899178659,
    "itl": 99.89129283822386,
    "ttft": 2003933.438543726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.624935457212848,
    "arrivals": 394936,
    "finished_requests": 73597,
    "scheduler_time": 203.88961844422454
}
#Debug simulation 
Total elapsed time: 28.296697640325874. Arrivals time: 0.3325803275220096 Scheduler time: 27.789175393991172 Scheduler overhead time: 0.0625449800863862 Adapter cache time: 0.0249022850766778 Engine time: 0.06095551745966077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 54.23237469419837,
    "estimated_duration": 3600.078560013743,
    "input_throughput": 5344.750310095052,
    "output_throughput": 4680.327031512245,
    "total_throughput": 10025.077341607297,
    "itl": 111.73181760609056,
    "ttft": 1959398.8490742468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8684580583311563,
    "arrivals": 394936,
    "finished_requests": 77969,
    "scheduler_time": 192.87899478840885
}
#Debug simulation 
Total elapsed time: 54.23255756404251. Arrivals time: 0.363634561188519 Scheduler time: 53.70114687690511 Scheduler overhead time: 0.06253863777965307 Adapter cache time: 0.018222587648779154 Engine time: 0.06185352290049195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 28.393762469291687,
    "estimated_duration": 3600.04811016329,
    "input_throughput": 5037.423791310789,
    "output_throughput": 4426.023350914962,
    "total_throughput": 9463.44714222575,
    "itl": 99.81295632488748,
    "ttft": 2003757.0886164864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 886,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.597401146511569,
    "arrivals": 394936,
    "finished_requests": 73613,
    "scheduler_time": 204.0534114990379
}
#Debug simulation 
Total elapsed time: 28.393903181422502. Arrivals time: 0.3188012270256877 Scheduler time: 27.902409201022238 Scheduler overhead time: 0.062288157641887665 Adapter cache time: 0.022301902063190937 Engine time: 0.06148806540295482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 47.109792267903686,
    "estimated_duration": 3600.1150904235406,
    "input_throughput": 5310.456615916356,
    "output_throughput": 4677.031310690428,
    "total_throughput": 9987.487926606784,
    "itl": 111.65509203778433,
    "ttft": 1953667.2037203668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.002718429737706,
    "arrivals": 394936,
    "finished_requests": 77692,
    "scheduler_time": 193.0260094235003
}
#Debug simulation 
Total elapsed time: 47.109983487054706. Arrivals time: 0.36937881726771593 Scheduler time: 46.572637015488 Scheduler overhead time: 0.06225607916712761 Adapter cache time: 0.018662008922547102 Engine time: 0.061761034186929464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_384_slots_64_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 37.15480892499909,
    "estimated_duration": 3600.0906144384867,
    "input_throughput": 5046.906577054007,
    "output_throughput": 4425.117227913099,
    "total_throughput": 9472.023804967106,
    "itl": 99.74319221841728,
    "ttft": 1999402.5283082777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 854,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.295110543593788,
    "arrivals": 394936,
    "finished_requests": 73723,
    "scheduler_time": 204.09685902170432
}
#Debug simulation 
Total elapsed time: 37.15497072506696. Arrivals time: 0.34134478494524956 Scheduler time: 36.636207554489374 Scheduler overhead time: 0.06447104178369045 Adapter cache time: 0.022112880367785692 Engine time: 0.06384160602465272 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 48.59152328921482,
    "estimated_duration": 3600.039729671984,
    "input_throughput": 5563.705265504107,
    "output_throughput": 4818.304880646549,
    "total_throughput": 10382.010146150655,
    "itl": 119.34619374871434,
    "ttft": 1932275.184187668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.729398694019818,
    "arrivals": 393551,
    "finished_requests": 80936,
    "scheduler_time": 186.61084485924675
}
#Debug simulation 
Total elapsed time: 48.591706052888185. Arrivals time: 0.3819501004181802 Scheduler time: 48.05143231200054 Scheduler overhead time: 0.05912040965631604 Adapter cache time: 0.01718102628365159 Engine time: 0.05802809586748481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 60.00222047697753,
    "estimated_duration": 3600.048000267702,
    "input_throughput": 5408.679828311202,
    "output_throughput": 4684.163377473312,
    "total_throughput": 10092.843205784513,
    "itl": 111.70815971217307,
    "ttft": 1931336.6915862947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.883605742398656,
    "arrivals": 393551,
    "finished_requests": 78792,
    "scheduler_time": 192.64699983645593
}
#Debug simulation 
Total elapsed time: 60.00240200990811. Arrivals time: 0.3756985794752836 Scheduler time: 59.45568978320807 Scheduler overhead time: 0.0643581235781312 Adapter cache time: 0.0185975544154644 Engine time: 0.06236918969079852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 35.21408986579627,
    "estimated_duration": 3600.04430675933,
    "input_throughput": 5099.794179068521,
    "output_throughput": 4422.156685713335,
    "total_throughput": 9521.950864781857,
    "itl": 99.33268301618492,
    "ttft": 1993868.0825170788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 814,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.1091283868346595,
    "arrivals": 393551,
    "finished_requests": 74275,
    "scheduler_time": 204.26058725764997
}
#Debug simulation 
Total elapsed time: 35.21424998994917. Arrivals time: 0.3433737801387906 Scheduler time: 34.69779556430876 Scheduler overhead time: 0.06350662047043443 Adapter cache time: 0.020040420349687338 Engine time: 0.06288527976721525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 58.95393154071644,
    "estimated_duration": 3600.06139039367,
    "input_throughput": 5410.86856240246,
    "output_throughput": 4684.3945619927035,
    "total_throughput": 10095.263124395164,
    "itl": 111.70976163293294,
    "ttft": 1931955.8746342957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.689226373084814,
    "arrivals": 393551,
    "finished_requests": 78794,
    "scheduler_time": 192.55385677454183
}
#Debug simulation 
Total elapsed time: 58.95412032492459. Arrivals time: 0.3663184503093362 Scheduler time: 58.415693916380405 Scheduler overhead time: 0.06403422215953469 Adapter cache time: 0.019433835987001657 Engine time: 0.06307083321735263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 32.82547650905326,
    "estimated_duration": 3600.052912873909,
    "input_throughput": 5094.062905136616,
    "output_throughput": 4418.573666824642,
    "total_throughput": 9512.636571961257,
    "itl": 99.23983596697003,
    "ttft": 1994840.9512201594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 757,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.621505682454469,
    "arrivals": 393551,
    "finished_requests": 74247,
    "scheduler_time": 204.3806608474105
}
#Debug simulation 
Total elapsed time: 32.8256639810279. Arrivals time: 0.3408909351564944 Scheduler time: 32.31212280411273 Scheduler overhead time: 0.06318961596116424 Adapter cache time: 0.02012393856421113 Engine time: 0.06251937290653586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 56.73824479896575,
    "estimated_duration": 3600.0595259448323,
    "input_throughput": 5388.837562320155,
    "output_throughput": 4683.505336088817,
    "total_throughput": 10072.342898408971,
    "itl": 111.58687027544435,
    "ttft": 1932196.4358295028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.855888248104584,
    "arrivals": 393551,
    "finished_requests": 78544,
    "scheduler_time": 192.6976843146342
}
#Debug simulation 
Total elapsed time: 56.73843218199909. Arrivals time: 0.36925564147531986 Scheduler time: 56.19986231951043 Scheduler overhead time: 0.06341927126049995 Adapter cache time: 0.01818714151158929 Engine time: 0.0623745946213603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 540, 33, 8640, 8640, 33, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 33, 8640, 33, 8640, 540, 540, 540, 33, 33, 540, 33, 8640, 33, 8640, 540, 8640, 540, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 8640, 8640, 33, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 540, 33, 33, 33, 540, 33, 540, 33, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 8640, 8640, 540, 33, 33, 8640, 540, 540, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 33, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 8640, 33, 33, 8640, 33, 8640, 540, 33, 8640, 540, 540, 33, 8640, 8640, 8640, 33, 540, 8640, 33, 8640, 540, 540, 8640, 8640, 33, 540, 8640, 33, 540, 540, 33, 540, 33, 540, 8640, 8640, 540, 33, 8640, 33, 8640, 540, 540, 540, 540, 540, 33, 33, 8640, 33, 8640, 33, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 33, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 33, 33, 33, 33, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33, 540, 8640, 8640, 540, 540, 33, 8640, 540, 8640, 33, 540, 540, 33, 33, 540, 33, 8640, 8640, 33, 8640, 8640, 33, 540, 8640, 33, 33, 540, 540, 33, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 540, 33, 8640, 8640, 8640, 540, 33, 8640, 8640, 540, 540, 8640, 8640, 33, 8640, 8640, 540, 33, 8640, 540, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 540, 8640, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 8640, 33, 33, 33, 540, 33, 540, 540, 33, 8640, 540, 8640, 33, 8640, 33, 540, 540, 540, 540, 33, 540, 8640, 8640, 33, 8640, 8640, 33, 8640, 540, 8640, 540, 540, 540, 33, 540, 33, 33, 8640, 33, 540, 33, 33, 540, 8640, 540, 8640, 33, 8640, 8640, 540, 8640, 540, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1179264 . Total input tokens: 262892489 . Total output tokens: 231624415
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 29.546096378937364,
    "estimated_duration": 3600.024136227204,
    "input_throughput": 5102.096904063863,
    "output_throughput": 4426.17949131282,
    "total_throughput": 9528.276395376683,
    "itl": 99.47640599117854,
    "ttft": 1996115.4618867745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 797,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.863098621759589,
    "arrivals": 393551,
    "finished_requests": 74344,
    "scheduler_time": 204.12529254536355
}
#Debug simulation 
Total elapsed time: 29.54626170406118. Arrivals time: 0.32558920234441757 Scheduler time: 29.046617407817394 Scheduler overhead time: 0.06370989140123129 Adapter cache time: 0.020855252165347338 Engine time: 0.06270167278125882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 60.96087631303817,
    "estimated_duration": 3600.133496811294,
    "input_throughput": 5466.149246251546,
    "output_throughput": 4768.783161848372,
    "total_throughput": 10234.93240809992,
    "itl": 118.00225616759647,
    "ttft": 1919169.2320047254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6302125585405656,
    "arrivals": 386177,
    "finished_requests": 79766,
    "scheduler_time": 188.87130170103075
}
#Debug simulation 
Total elapsed time: 60.96111574815586. Arrivals time: 0.37788259238004684 Scheduler time: 60.418627148494124 Scheduler overhead time: 0.061664409935474396 Adapter cache time: 0.01765332417562604 Engine time: 0.06067938543856144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 55.388080133125186,
    "estimated_duration": 3600.0504165034113,
    "input_throughput": 5353.822521941266,
    "output_throughput": 4674.66564436211,
    "total_throughput": 10028.488166303376,
    "itl": 111.44130498659703,
    "ttft": 1944293.9042013013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 550,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0247982774395545,
    "arrivals": 386177,
    "finished_requests": 78139,
    "scheduler_time": 193.03198204154862
}
#Debug simulation 
Total elapsed time: 55.38825472909957. Arrivals time: 0.36811607936397195 Scheduler time: 54.8515695175156 Scheduler overhead time: 0.06319913826882839 Adapter cache time: 0.017685213591903448 Engine time: 0.06193376798182726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 36.120111339725554,
    "estimated_duration": 3600.038387743618,
    "input_throughput": 5056.039975009366,
    "output_throughput": 4422.554785583543,
    "total_throughput": 9478.594760592909,
    "itl": 99.5483843618489,
    "ttft": 1974103.428504773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 946,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.113653041953241,
    "arrivals": 386177,
    "finished_requests": 73754,
    "scheduler_time": 203.9870153897385
}
#Debug simulation 
Total elapsed time: 36.12034720880911. Arrivals time: 0.33311976213008165 Scheduler time: 35.608067580964416 Scheduler overhead time: 0.0651674959808588 Adapter cache time: 0.023349848110228777 Engine time: 0.06335659883916378 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 54.238207146991044,
    "estimated_duration": 3600.09990998844,
    "input_throughput": 5367.105492375639,
    "output_throughput": 4685.065531988016,
    "total_throughput": 10052.171024363655,
    "itl": 111.81078600398861,
    "ttft": 1944686.297088787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.152410352467554,
    "arrivals": 386177,
    "finished_requests": 78334,
    "scheduler_time": 192.66614253449913
}
#Debug simulation 
Total elapsed time: 54.23838673532009. Arrivals time: 0.36446715984493494 Scheduler time: 53.70640226546675 Scheduler overhead time: 0.06238872651010752 Adapter cache time: 0.01850818982347846 Engine time: 0.06132873706519604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 33.274145517963916,
    "estimated_duration": 3600.0940655743107,
    "input_throughput": 5060.285278155315,
    "output_throughput": 4419.357025178706,
    "total_throughput": 9479.642303334022,
    "itl": 99.31081316254911,
    "ttft": 1985369.9599041145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 874,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.5133821174595745,
    "arrivals": 386177,
    "finished_requests": 73855,
    "scheduler_time": 204.23154670336535
}
#Debug simulation 
Total elapsed time: 33.2743656039238. Arrivals time: 0.34262387454509735 Scheduler time: 32.75596459303051 Scheduler overhead time: 0.06372627383098006 Adapter cache time: 0.021875321865081787 Engine time: 0.06281854026019573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 55.11416624719277,
    "estimated_duration": 3600.082786103147,
    "input_throughput": 5363.90442868186,
    "output_throughput": 4679.856548031417,
    "total_throughput": 10043.760976713276,
    "itl": 111.72723310324828,
    "ttft": 1943138.9660111414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8495043271640137,
    "arrivals": 386177,
    "finished_requests": 78268,
    "scheduler_time": 192.81513681877442
}
#Debug simulation 
Total elapsed time: 55.114342337008566. Arrivals time: 0.37007872853428125 Scheduler time: 54.5759895183146 Scheduler overhead time: 0.06243978999555111 Adapter cache time: 0.018547697458416224 Engine time: 0.06176056759431958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 270, 135, 8640, 8640, 135, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 135, 8640, 135, 8640, 270, 270, 270, 135, 135, 270, 135, 8640, 135, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 8640, 8640, 135, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 270, 135, 135, 135, 270, 135, 270, 135, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 8640, 8640, 270, 135, 135, 8640, 270, 270, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 135, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 8640, 135, 135, 8640, 135, 8640, 270, 135, 8640, 270, 270, 135, 8640, 8640, 8640, 135, 270, 8640, 135, 8640, 270, 270, 8640, 8640, 135, 270, 8640, 135, 270, 270, 135, 270, 135, 270, 8640, 8640, 270, 135, 8640, 135, 8640, 270, 270, 270, 270, 270, 135, 135, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 135, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 135, 135, 135, 135, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135, 270, 8640, 8640, 270, 270, 135, 8640, 270, 8640, 135, 270, 270, 135, 135, 270, 135, 8640, 8640, 135, 8640, 8640, 135, 270, 8640, 135, 135, 270, 270, 135, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 270, 135, 8640, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 135, 8640, 8640, 270, 135, 8640, 270, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 270, 8640, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 8640, 135, 135, 135, 270, 135, 270, 270, 135, 8640, 270, 8640, 135, 8640, 135, 270, 270, 270, 270, 135, 270, 8640, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 270, 270, 270, 135, 270, 135, 135, 8640, 135, 270, 135, 135, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1157760 . Total input tokens: 258103549 . Total output tokens: 227361900
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 39.408226053696126,
    "estimated_duration": 3600.0579867214137,
    "input_throughput": 5058.201025418412,
    "output_throughput": 4414.030012464265,
    "total_throughput": 9472.231037882677,
    "itl": 99.39983146830569,
    "ttft": 1988833.6682904942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 871,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.430384488161676,
    "arrivals": 386177,
    "finished_requests": 73858,
    "scheduler_time": 204.46809618119235
}
#Debug simulation 
Total elapsed time: 39.40845819469541. Arrivals time: 0.3263078071177006 Scheduler time: 38.90630037430674 Scheduler overhead time: 0.06380805652588606 Adapter cache time: 0.022088197991251945 Engine time: 0.06304053170606494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 45.0665166862309,
    "estimated_duration": 3600.028999560241,
    "input_throughput": 5526.8327011894835,
    "output_throughput": 4823.384201105359,
    "total_throughput": 10350.216902294842,
    "itl": 119.69768415255868,
    "ttft": 1935299.3736237169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.993895055297824,
    "arrivals": 383371,
    "finished_requests": 80566,
    "scheduler_time": 186.26783964179216
}
#Debug simulation 
Total elapsed time: 45.066695573274046. Arrivals time: 0.3719788114540279 Scheduler time: 44.53778469609097 Scheduler overhead time: 0.05823439033702016 Adapter cache time: 0.017121232114732265 Engine time: 0.05764611577615142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 35.07732953829691,
    "estimated_duration": 3600.034599719403,
    "input_throughput": 5356.41018603071,
    "output_throughput": 4680.373072334721,
    "total_throughput": 10036.78325836543,
    "itl": 112.08523380549512,
    "ttft": 1955776.0762227138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.550611638678228,
    "arrivals": 383371,
    "finished_requests": 78108,
    "scheduler_time": 192.24604414463067
}
#Debug simulation 
Total elapsed time: 35.07756507815793. Arrivals time: 0.3486778442747891 Scheduler time: 34.56458911206573 Scheduler overhead time: 0.05827780580148101 Adapter cache time: 0.024112232960760593 Engine time: 0.05740880034863949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 36.33391461195424,
    "estimated_duration": 3600.092214631312,
    "input_throughput": 5074.43473968649,
    "output_throughput": 4423.753629219463,
    "total_throughput": 9498.188368905952,
    "itl": 99.5577245662296,
    "ttft": 1986458.125824255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 821,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.162243287661136,
    "arrivals": 383371,
    "finished_requests": 73854,
    "scheduler_time": 203.98610916526638
}
#Debug simulation 
Total elapsed time: 36.334084330592304. Arrivals time: 0.3262994051910937 Scheduler time: 35.832531731110066 Scheduler overhead time: 0.06404886860400438 Adapter cache time: 0.021046908106654882 Engine time: 0.06331770028918982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 45.443353961221874,
    "estimated_duration": 3600.111856140964,
    "input_throughput": 5361.425081022325,
    "output_throughput": 4682.306182027685,
    "total_throughput": 10043.731263050011,
    "itl": 111.97742083972416,
    "ttft": 1944208.0413127046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 830,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.687356253201133,
    "arrivals": 383371,
    "finished_requests": 78250,
    "scheduler_time": 192.46266149473794
}
#Debug simulation 
Total elapsed time: 45.44362536026165. Arrivals time: 0.36800041049718857 Scheduler time: 44.906892663333565 Scheduler overhead time: 0.06179337156936526 Adapter cache time: 0.020759362261742353 Engine time: 0.06066484982147813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 29.82314595207572,
    "estimated_duration": 3600.0492192895044,
    "input_throughput": 5050.849833544388,
    "output_throughput": 4407.632238742448,
    "total_throughput": 9458.482072286837,
    "itl": 98.91034980396472,
    "ttft": 1995105.2468690255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 801,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.955001151887723,
    "arrivals": 383371,
    "finished_requests": 73585,
    "scheduler_time": 204.7600056610221
}
#Debug simulation 
Total elapsed time: 29.82330954587087. Arrivals time: 0.3220359357073903 Scheduler time: 29.32909103948623 Scheduler overhead time: 0.0628048456273973 Adapter cache time: 0.02033203560858965 Engine time: 0.062279372941702604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 49.5898799020797,
    "estimated_duration": 3600.015829306113,
    "input_throughput": 5362.757253131619,
    "output_throughput": 4683.463017786173,
    "total_throughput": 10046.220270917793,
    "itl": 112.02258690432956,
    "ttft": 1938159.2668114423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 766,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.890083440477005,
    "arrivals": 383371,
    "finished_requests": 78245,
    "scheduler_time": 192.430385214295
}
#Debug simulation 
Total elapsed time: 49.5901432428509. Arrivals time: 0.3617805689573288 Scheduler time: 49.06029267096892 Scheduler overhead time: 0.0618669711984694 Adapter cache time: 0.01981245493516326 Engine time: 0.06123892590403557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_384_slots_64_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 270, 66, 8640, 8640, 66, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 66, 8640, 66, 8640, 270, 270, 270, 66, 66, 270, 66, 8640, 66, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 8640, 8640, 66, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 270, 66, 66, 66, 270, 66, 270, 66, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 8640, 8640, 270, 66, 66, 8640, 270, 270, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 66, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 8640, 66, 66, 8640, 66, 8640, 270, 66, 8640, 270, 270, 66, 8640, 8640, 8640, 66, 270, 8640, 66, 8640, 270, 270, 8640, 8640, 66, 270, 8640, 66, 270, 270, 66, 270, 66, 270, 8640, 8640, 270, 66, 8640, 66, 8640, 270, 270, 270, 270, 270, 66, 66, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 66, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 66, 66, 66, 66, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66, 270, 8640, 8640, 270, 270, 66, 8640, 270, 8640, 66, 270, 270, 66, 66, 270, 66, 8640, 8640, 66, 8640, 8640, 66, 270, 8640, 66, 66, 270, 270, 66, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 270, 66, 8640, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 66, 8640, 8640, 270, 66, 8640, 270, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 270, 8640, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 8640, 66, 66, 66, 270, 66, 270, 270, 66, 8640, 270, 8640, 66, 8640, 66, 270, 270, 270, 270, 66, 270, 8640, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 270, 270, 270, 66, 270, 66, 66, 8640, 66, 270, 66, 66, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1148928 . Total input tokens: 256116975 . Total output tokens: 225599643
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 25.435700551141053,
    "estimated_duration": 3600.0610236143875,
    "input_throughput": 5063.582778299199,
    "output_throughput": 4418.958983097506,
    "total_throughput": 9482.541761396706,
    "itl": 99.52072619007551,
    "ttft": 1994618.990924648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.162817945573492,
    "arrivals": 383371,
    "finished_requests": 73784,
    "scheduler_time": 203.984447867298
}
#Debug simulation 
Total elapsed time: 25.435874627903104. Arrivals time: 0.3145460640080273 Scheduler time: 24.9483584696427 Scheduler overhead time: 0.06124646496027708 Adapter cache time: 0.023866975214332342 Engine time: 0.06119877565652132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 47.02426603529602,
    "estimated_duration": 3600.0356846032637,
    "input_throughput": 5535.06541205187,
    "output_throughput": 4819.779724464643,
    "total_throughput": 10354.845136516513,
    "itl": 119.44308240691178,
    "ttft": 1920647.225309947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.026957100457574,
    "arrivals": 381963,
    "finished_requests": 80751,
    "scheduler_time": 186.25957216305542
}
#Debug simulation 
Total elapsed time: 47.0245209322311. Arrivals time: 0.37689921725541353 Scheduler time: 46.48654994554818 Scheduler overhead time: 0.05980090983211994 Adapter cache time: 0.017887592315673828 Engine time: 0.059287145268172026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 44.039931770879775,
    "estimated_duration": 3600.073699778649,
    "input_throughput": 5387.8274773076455,
    "output_throughput": 4685.1688066933375,
    "total_throughput": 10072.996284000983,
    "itl": 111.55600688939599,
    "ttft": 1944867.9359885408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.270730376737198,
    "arrivals": 381963,
    "finished_requests": 78470,
    "scheduler_time": 192.473787028335
}
#Debug simulation 
Total elapsed time: 44.040112542919815. Arrivals time: 0.3665054151788354 Scheduler time: 43.50938926637173 Scheduler overhead time: 0.06056286580860615 Adapter cache time: 0.017886755988001823 Engine time: 0.06074539851397276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 35.135192510671914,
    "estimated_duration": 3600.065951591874,
    "input_throughput": 5078.388631162672,
    "output_throughput": 4416.652976307071,
    "total_throughput": 9495.041607469744,
    "itl": 99.14440632341622,
    "ttft": 1982828.5849972656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.9465290995687505,
    "arrivals": 381963,
    "finished_requests": 73898,
    "scheduler_time": 204.18427941100194
}
#Debug simulation 
Total elapsed time: 35.13543985877186. Arrivals time: 0.33202636195346713 Scheduler time: 34.62961279647425 Scheduler overhead time: 0.06320000626146793 Adapter cache time: 0.020848816260695457 Engine time: 0.06282387813553214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 44.84486953075975,
    "estimated_duration": 3600.017049116198,
    "input_throughput": 5396.171666678398,
    "output_throughput": 4683.418653291993,
    "total_throughput": 10079.59031997039,
    "itl": 111.62269238336228,
    "ttft": 1938237.2866923388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 587,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.03055867067072,
    "arrivals": 381963,
    "finished_requests": 78492,
    "scheduler_time": 192.42977935905844
}
#Debug simulation 
Total elapsed time: 44.84505630284548. Arrivals time: 0.37420493131503463 Scheduler time: 44.30534955486655 Scheduler overhead time: 0.06157034542411566 Adapter cache time: 0.017633191775530577 Engine time: 0.0609755371697247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 29.513871774077415,
    "estimated_duration": 3600.0515069568255,
    "input_throughput": 5073.968793141233,
    "output_throughput": 4413.69796218045,
    "total_throughput": 9487.666755321683,
    "itl": 99.09589129604224,
    "ttft": 1986980.2033227414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 821,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.136303565353183,
    "arrivals": 381963,
    "finished_requests": 73822,
    "scheduler_time": 204.2622275525168
}
#Debug simulation 
Total elapsed time: 29.514097100123763. Arrivals time: 0.33537439117208123 Scheduler time: 29.004297088366002 Scheduler overhead time: 0.06322475336492062 Adapter cache time: 0.021413872484117746 Engine time: 0.06277199042961001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 53.826949931215495,
    "estimated_duration": 3600.1126577156165,
    "input_throughput": 5365.88819202612,
    "output_throughput": 4674.632601824926,
    "total_throughput": 10040.520793851047,
    "itl": 111.48712522086896,
    "ttft": 1936984.3516554828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7345937502337447,
    "arrivals": 381963,
    "finished_requests": 78276,
    "scheduler_time": 192.9503575730946
}
#Debug simulation 
Total elapsed time: 53.82712993910536. Arrivals time: 0.3657750440761447 Scheduler time: 53.29259185027331 Scheduler overhead time: 0.06258889706805348 Adapter cache time: 0.01868199510499835 Engine time: 0.062063559889793396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 270, 33, 8640, 8640, 33, 270, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 33, 8640, 33, 8640, 270, 270, 270, 33, 33, 270, 33, 8640, 33, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 8640, 8640, 33, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 270, 33, 33, 33, 270, 33, 270, 33, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 8640, 8640, 270, 33, 33, 8640, 270, 270, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 8640, 8640, 33, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 8640, 33, 33, 8640, 33, 8640, 270, 33, 8640, 270, 270, 33, 8640, 8640, 8640, 33, 270, 8640, 33, 8640, 270, 270, 8640, 8640, 33, 270, 8640, 33, 270, 270, 33, 270, 33, 270, 8640, 8640, 270, 33, 8640, 33, 8640, 270, 270, 270, 270, 270, 33, 33, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 33, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 33, 33, 33, 33, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33, 270, 8640, 8640, 270, 270, 33, 8640, 270, 8640, 33, 270, 270, 33, 33, 270, 33, 8640, 8640, 33, 8640, 8640, 33, 270, 8640, 33, 33, 270, 270, 33, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 270, 33, 8640, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 33, 8640, 8640, 270, 33, 8640, 270, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 270, 8640, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 8640, 33, 33, 33, 270, 33, 270, 270, 33, 8640, 270, 8640, 33, 8640, 33, 270, 270, 270, 270, 33, 270, 8640, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 270, 270, 270, 33, 270, 33, 33, 8640, 33, 270, 33, 33, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1144704 . Total input tokens: 255186972 . Total output tokens: 224781889
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 24.959533195011318,
    "estimated_duration": 3600.0952310056155,
    "input_throughput": 5079.596740248418,
    "output_throughput": 4416.146512757401,
    "total_throughput": 9495.74325300582,
    "itl": 99.23103593082193,
    "ttft": 1987717.5813664226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 995,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.346067487876882,
    "arrivals": 381963,
    "finished_requests": 73916,
    "scheduler_time": 204.08434510508624
}
#Debug simulation 
Total elapsed time: 24.95975830871612. Arrivals time: 0.32094590133056045 Scheduler time: 24.46754515217617 Scheduler overhead time: 0.06132730795070529 Adapter cache time: 0.022407096810638905 Engine time: 0.061058510560542345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 45.83669169107452,
    "estimated_duration": 3600.0146189873367,
    "input_throughput": 5521.515911396892,
    "output_throughput": 4810.766297630115,
    "total_throughput": 10332.282209027007,
    "itl": 119.15759017116676,
    "ttft": 1918230.3889749548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.033569509489524,
    "arrivals": 377711,
    "finished_requests": 80528,
    "scheduler_time": 186.7449191660217
}
#Debug simulation 
Total elapsed time: 45.83688503690064. Arrivals time: 0.36971191223710775 Scheduler time: 45.30734211066738 Scheduler overhead time: 0.058891347609460354 Adapter cache time: 0.01845758967101574 Engine time: 0.05848945304751396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 63.10949791921303,
    "estimated_duration": 3600.0976977727237,
    "input_throughput": 5321.07365082106,
    "output_throughput": 4641.416817754062,
    "total_throughput": 9962.490468575123,
    "itl": 110.83562723996192,
    "ttft": 1920322.968538334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.064490024056291,
    "arrivals": 377711,
    "finished_requests": 77604,
    "scheduler_time": 194.2760064467998
}
#Debug simulation 
Total elapsed time: 63.10975397191942. Arrivals time: 0.36792112048715353 Scheduler time: 62.569736503530294 Scheduler overhead time: 0.06389928702265024 Adapter cache time: 0.018789172172546387 Engine time: 0.06341809779405594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 50.35258399415761,
    "estimated_duration": 3600.0073713123343,
    "input_throughput": 5071.687393057823,
    "output_throughput": 4422.776221759746,
    "total_throughput": 9494.463614817569,
    "itl": 99.51710447986272,
    "ttft": 1962834.8277227306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 938,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.0285387782287,
    "arrivals": 377711,
    "finished_requests": 73953,
    "scheduler_time": 203.8073779353325
}
#Debug simulation 
Total elapsed time: 50.352767932228744. Arrivals time: 0.34653420373797417 Scheduler time: 49.82260711165145 Scheduler overhead time: 0.0667080138809979 Adapter cache time: 0.0230789827182889 Engine time: 0.06633712584152818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 41.442404836416245,
    "estimated_duration": 3600.11985716276,
    "input_throughput": 5380.70224563755,
    "output_throughput": 4683.455459532419,
    "total_throughput": 10064.15770516997,
    "itl": 111.64668240875402,
    "ttft": 1944048.5565074487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 612,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.190156694184982,
    "arrivals": 377711,
    "finished_requests": 78418,
    "scheduler_time": 192.60237196921304
}
#Debug simulation 
Total elapsed time: 41.44263465004042. Arrivals time: 0.35117012448608875 Scheduler time: 40.92918159207329 Scheduler overhead time: 0.05994659103453159 Adapter cache time: 0.01789131434634328 Engine time: 0.059386583510786295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 36.55151841463521,
    "estimated_duration": 3600.10781958858,
    "input_throughput": 5085.547688427925,
    "output_throughput": 4426.406874064431,
    "total_throughput": 9511.954562492356,
    "itl": 99.47307799749136,
    "ttft": 1982535.7015105046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.762264701048866,
    "arrivals": 377711,
    "finished_requests": 74056,
    "scheduler_time": 203.80042781891774
}
#Debug simulation 
Total elapsed time: 36.55169920902699. Arrivals time: 0.3307108087465167 Scheduler time: 36.04497770452872 Scheduler overhead time: 0.06426882650703192 Adapter cache time: 0.021142893936485052 Engine time: 0.06347855692729354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 55.75748607702553,
    "estimated_duration": 3600.0090332091254,
    "input_throughput": 5382.9884373165905,
    "output_throughput": 4690.863229569853,
    "total_throughput": 10073.851666886443,
    "itl": 111.81163049878958,
    "ttft": 1921071.7379132695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.036913622110126,
    "arrivals": 377711,
    "finished_requests": 78532,
    "scheduler_time": 192.32070827112557
}
#Debug simulation 
Total elapsed time: 55.75775086693466. Arrivals time: 0.36434289114549756 Scheduler time: 55.22363659366965 Scheduler overhead time: 0.06279621832072735 Adapter cache time: 0.019794195890426636 Engine time: 0.061864132061600685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_384_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 135, 66, 8640, 8640, 66, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 66, 8640, 66, 8640, 135, 135, 135, 66, 66, 135, 66, 8640, 66, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 8640, 8640, 66, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 135, 66, 66, 66, 135, 66, 135, 66, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 8640, 8640, 135, 66, 66, 8640, 135, 135, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 66, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 8640, 66, 66, 8640, 66, 8640, 135, 66, 8640, 135, 135, 66, 8640, 8640, 8640, 66, 135, 8640, 66, 8640, 135, 135, 8640, 8640, 66, 135, 8640, 66, 135, 135, 66, 135, 66, 135, 8640, 8640, 135, 66, 8640, 66, 8640, 135, 135, 135, 135, 135, 66, 66, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 66, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 66, 66, 66, 66, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66, 135, 8640, 8640, 135, 135, 66, 8640, 135, 8640, 66, 135, 135, 66, 66, 135, 66, 8640, 8640, 66, 8640, 8640, 66, 135, 8640, 66, 66, 135, 135, 66, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 135, 66, 8640, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 66, 8640, 8640, 135, 66, 8640, 135, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 135, 8640, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 8640, 66, 66, 66, 135, 66, 135, 135, 66, 8640, 135, 8640, 66, 8640, 66, 135, 135, 135, 135, 66, 135, 8640, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 135, 135, 135, 66, 135, 66, 66, 8640, 66, 135, 66, 66, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1131648 . Total input tokens: 252230559 . Total output tokens: 222259201
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 37.229297869838774,
    "estimated_duration": 3600.034105611477,
    "input_throughput": 5076.169409482869,
    "output_throughput": 4415.656222595682,
    "total_throughput": 9491.825632078551,
    "itl": 99.01940470610967,
    "ttft": 1981761.797845402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 664,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.890110219232769,
    "arrivals": 377711,
    "finished_requests": 73872,
    "scheduler_time": 204.3385518048678
}
#Debug simulation 
Total elapsed time: 37.22943641897291. Arrivals time: 0.3344302913174033 Scheduler time: 36.7198201273568 Scheduler overhead time: 0.06442832574248314 Adapter cache time: 0.019942542538046837 Engine time: 0.06371756922453642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 135, 33, 8640, 8640, 33, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 33, 8640, 33, 8640, 135, 135, 135, 33, 33, 135, 33, 8640, 33, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 8640, 8640, 33, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 135, 33, 33, 33, 135, 33, 135, 33, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 8640, 8640, 135, 33, 33, 8640, 135, 135, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 8640, 33, 33, 8640, 33, 8640, 135, 33, 8640, 135, 135, 33, 8640, 8640, 8640, 33, 135, 8640, 33, 8640, 135, 135, 8640, 8640, 33, 135, 8640, 33, 135, 135, 33, 135, 33, 135, 8640, 8640, 135, 33, 8640, 33, 8640, 135, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 33, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 33, 33, 33, 33, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33, 135, 8640, 8640, 135, 135, 33, 8640, 135, 8640, 33, 135, 135, 33, 33, 135, 33, 8640, 8640, 33, 8640, 8640, 33, 135, 8640, 33, 33, 135, 135, 33, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 135, 33, 8640, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 33, 8640, 8640, 135, 33, 8640, 135, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 135, 8640, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 8640, 33, 33, 33, 135, 33, 135, 135, 33, 8640, 135, 8640, 33, 8640, 33, 135, 135, 135, 135, 33, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 135, 135, 135, 33, 135, 33, 33, 8640, 33, 135, 33, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1127424 . Total input tokens: 251283552 . Total output tokens: 221449305
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 60.8372699720785,
    "estimated_duration": 3600.023979634543,
    "input_throughput": 5503.343619953119,
    "output_throughput": 4798.124706311958,
    "total_throughput": 10301.468326265078,
    "itl": 119.11292989781624,
    "ttft": 1912915.4805413738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.742623512083718,
    "arrivals": 376270,
    "finished_requests": 80396,
    "scheduler_time": 187.48146564523674
}
#Debug simulation 
Total elapsed time: 60.83753851009533. Arrivals time: 0.37265783827751875 Scheduler time: 60.30203481996432 Scheduler overhead time: 0.060833854135125875 Adapter cache time: 0.017684522084891796 Engine time: 0.05970907909795642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 135, 33, 8640, 8640, 33, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 33, 8640, 33, 8640, 135, 135, 135, 33, 33, 135, 33, 8640, 33, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 8640, 8640, 33, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 135, 33, 33, 33, 135, 33, 135, 33, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 8640, 8640, 135, 33, 33, 8640, 135, 135, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 8640, 33, 33, 8640, 33, 8640, 135, 33, 8640, 135, 135, 33, 8640, 8640, 8640, 33, 135, 8640, 33, 8640, 135, 135, 8640, 8640, 33, 135, 8640, 33, 135, 135, 33, 135, 33, 135, 8640, 8640, 135, 33, 8640, 33, 8640, 135, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 33, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 33, 33, 33, 33, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33, 135, 8640, 8640, 135, 135, 33, 8640, 135, 8640, 33, 135, 135, 33, 33, 135, 33, 8640, 8640, 33, 8640, 8640, 33, 135, 8640, 33, 33, 135, 135, 33, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 135, 33, 8640, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 33, 8640, 8640, 135, 33, 8640, 135, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 135, 8640, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 8640, 33, 33, 33, 135, 33, 135, 135, 33, 8640, 135, 8640, 33, 8640, 33, 135, 135, 135, 135, 33, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 135, 135, 135, 33, 135, 33, 33, 8640, 33, 135, 33, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1127424 . Total input tokens: 251283552 . Total output tokens: 221449305
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 43.237196835223585,
    "estimated_duration": 3600.0120409218666,
    "input_throughput": 5341.883244111455,
    "output_throughput": 4654.570542966602,
    "total_throughput": 9996.453787078057,
    "itl": 111.2900469470478,
    "ttft": 1947658.7035494815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9887140098167637,
    "arrivals": 376270,
    "finished_requests": 77901,
    "scheduler_time": 193.78908427347514
}
#Debug simulation 
Total elapsed time: 43.23736893804744. Arrivals time: 0.3445075764320791 Scheduler time: 42.732199505437165 Scheduler overhead time: 0.05961589142680168 Adapter cache time: 0.017237440682947636 Engine time: 0.05880273878574371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 135, 33, 8640, 8640, 33, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 33, 8640, 33, 8640, 135, 135, 135, 33, 33, 135, 33, 8640, 33, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 8640, 8640, 33, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 135, 33, 33, 33, 135, 33, 135, 33, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 8640, 8640, 135, 33, 33, 8640, 135, 135, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 8640, 33, 33, 8640, 33, 8640, 135, 33, 8640, 135, 135, 33, 8640, 8640, 8640, 33, 135, 8640, 33, 8640, 135, 135, 8640, 8640, 33, 135, 8640, 33, 135, 135, 33, 135, 33, 135, 8640, 8640, 135, 33, 8640, 33, 8640, 135, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 33, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 33, 33, 33, 33, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33, 135, 8640, 8640, 135, 135, 33, 8640, 135, 8640, 33, 135, 135, 33, 33, 135, 33, 8640, 8640, 33, 8640, 8640, 33, 135, 8640, 33, 33, 135, 135, 33, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 135, 33, 8640, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 33, 8640, 8640, 135, 33, 8640, 135, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 135, 8640, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 8640, 33, 33, 33, 135, 33, 135, 135, 33, 8640, 135, 8640, 33, 8640, 33, 135, 135, 135, 135, 33, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 135, 135, 135, 33, 135, 33, 33, 8640, 33, 135, 33, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1127424 . Total input tokens: 251283552 . Total output tokens: 221449305
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 32.240525770932436,
    "estimated_duration": 3600.045895983642,
    "input_throughput": 5084.147127240726,
    "output_throughput": 4426.52495563418,
    "total_throughput": 9510.672082874906,
    "itl": 99.60166119317454,
    "ttft": 1978877.8969443196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.710614165337783,
    "arrivals": 376270,
    "finished_requests": 74081,
    "scheduler_time": 203.79591680473308
}
#Debug simulation 
Total elapsed time: 32.24076003395021. Arrivals time: 0.33240382513031363 Scheduler time: 31.73323459131643 Scheduler overhead time: 0.06399968545883894 Adapter cache time: 0.020454879384487867 Engine time: 0.06344055524095893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 135, 33, 8640, 8640, 33, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 33, 8640, 33, 8640, 135, 135, 135, 33, 33, 135, 33, 8640, 33, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 8640, 8640, 33, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 135, 33, 33, 33, 135, 33, 135, 33, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 8640, 8640, 135, 33, 33, 8640, 135, 135, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 8640, 33, 33, 8640, 33, 8640, 135, 33, 8640, 135, 135, 33, 8640, 8640, 8640, 33, 135, 8640, 33, 8640, 135, 135, 8640, 8640, 33, 135, 8640, 33, 135, 135, 33, 135, 33, 135, 8640, 8640, 135, 33, 8640, 33, 8640, 135, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 33, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 33, 33, 33, 33, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33, 135, 8640, 8640, 135, 135, 33, 8640, 135, 8640, 33, 135, 135, 33, 33, 135, 33, 8640, 8640, 33, 8640, 8640, 33, 135, 8640, 33, 33, 135, 135, 33, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 135, 33, 8640, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 33, 8640, 8640, 135, 33, 8640, 135, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 135, 8640, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 8640, 33, 33, 33, 135, 33, 135, 135, 33, 8640, 135, 8640, 33, 8640, 33, 135, 135, 135, 135, 33, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 135, 135, 135, 33, 135, 33, 33, 8640, 33, 135, 33, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1127424 . Total input tokens: 251283552 . Total output tokens: 221449305
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 44.373346865642816,
    "estimated_duration": 3600.043795699657,
    "input_throughput": 5323.816899920562,
    "output_throughput": 4640.884930332624,
    "total_throughput": 9964.701830253187,
    "itl": 110.61311794550551,
    "ttft": 1948102.0820366903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.420743077760558,
    "arrivals": 376270,
    "finished_requests": 77670,
    "scheduler_time": 194.46671040262262
}
#Debug simulation 
Total elapsed time: 44.37351387972012. Arrivals time: 0.33508932450786233 Scheduler time: 43.87714074458927 Scheduler overhead time: 0.06032706284895539 Adapter cache time: 0.016353107057511806 Engine time: 0.05948291625827551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 135, 33, 8640, 8640, 33, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 33, 8640, 33, 8640, 135, 135, 135, 33, 33, 135, 33, 8640, 33, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 8640, 8640, 33, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 135, 33, 33, 33, 135, 33, 135, 33, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 8640, 8640, 135, 33, 33, 8640, 135, 135, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 8640, 33, 33, 8640, 33, 8640, 135, 33, 8640, 135, 135, 33, 8640, 8640, 8640, 33, 135, 8640, 33, 8640, 135, 135, 8640, 8640, 33, 135, 8640, 33, 135, 135, 33, 135, 33, 135, 8640, 8640, 135, 33, 8640, 33, 8640, 135, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 33, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 33, 33, 33, 33, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33, 135, 8640, 8640, 135, 135, 33, 8640, 135, 8640, 33, 135, 135, 33, 33, 135, 33, 8640, 8640, 33, 8640, 8640, 33, 135, 8640, 33, 33, 135, 135, 33, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 135, 33, 8640, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 33, 8640, 8640, 135, 33, 8640, 135, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 135, 8640, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 8640, 33, 33, 33, 135, 33, 135, 135, 33, 8640, 135, 8640, 33, 8640, 33, 135, 135, 135, 135, 33, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 135, 135, 135, 33, 135, 33, 33, 8640, 33, 135, 33, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1127424 . Total input tokens: 251283552 . Total output tokens: 221449305
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 23.768561612349004,
    "estimated_duration": 3600.041651604089,
    "input_throughput": 5068.461080684924,
    "output_throughput": 4424.637974091907,
    "total_throughput": 9493.09905477683,
    "itl": 99.67613188206147,
    "ttft": 1982864.46504359,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 902,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.718799378010465,
    "arrivals": 376270,
    "finished_requests": 73985,
    "scheduler_time": 203.73907773571437
}
#Debug simulation 
Total elapsed time: 23.768780699931085. Arrivals time: 0.32733645709231496 Scheduler time: 23.271181056275964 Scheduler overhead time: 0.061356320045888424 Adapter cache time: 0.021725789178162813 Engine time: 0.06082274531945586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 135, 33, 8640, 8640, 33, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 33, 8640, 33, 8640, 135, 135, 135, 33, 33, 135, 33, 8640, 33, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 8640, 8640, 33, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 135, 33, 33, 33, 135, 33, 135, 33, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 8640, 8640, 135, 33, 33, 8640, 135, 135, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 8640, 33, 33, 8640, 33, 8640, 135, 33, 8640, 135, 135, 33, 8640, 8640, 8640, 33, 135, 8640, 33, 8640, 135, 135, 8640, 8640, 33, 135, 8640, 33, 135, 135, 33, 135, 33, 135, 8640, 8640, 135, 33, 8640, 33, 8640, 135, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 33, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 33, 33, 33, 33, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33, 135, 8640, 8640, 135, 135, 33, 8640, 135, 8640, 33, 135, 135, 33, 33, 135, 33, 8640, 8640, 33, 8640, 8640, 33, 135, 8640, 33, 33, 135, 135, 33, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 135, 33, 8640, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 33, 8640, 8640, 135, 33, 8640, 135, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 135, 8640, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 8640, 33, 33, 33, 135, 33, 135, 135, 33, 8640, 135, 8640, 33, 8640, 33, 135, 135, 135, 135, 33, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 135, 135, 135, 33, 135, 33, 33, 8640, 33, 135, 33, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1127424 . Total input tokens: 251283552 . Total output tokens: 221449305
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 44.50672680605203,
    "estimated_duration": 3600.12056181551,
    "input_throughput": 5323.656713967059,
    "output_throughput": 4642.857846841342,
    "total_throughput": 9966.5145608084,
    "itl": 110.74965980128029,
    "ttft": 1947996.6195272559,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.108969498057836,
    "arrivals": 376270,
    "finished_requests": 77640,
    "scheduler_time": 194.4235357222389
}
#Debug simulation 
Total elapsed time: 44.506885821931064. Arrivals time: 0.3285891804844141 Scheduler time: 44.018063586205244 Scheduler overhead time: 0.05976593680679798 Adapter cache time: 0.016116897109895945 Engine time: 0.059459945652633905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 135, 33, 8640, 8640, 33, 135, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 33, 8640, 33, 8640, 135, 135, 135, 33, 33, 135, 33, 8640, 33, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 8640, 8640, 33, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 135, 33, 33, 33, 135, 33, 135, 33, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 8640, 8640, 135, 33, 33, 8640, 135, 135, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 8640, 8640, 33, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 8640, 33, 33, 8640, 33, 8640, 135, 33, 8640, 135, 135, 33, 8640, 8640, 8640, 33, 135, 8640, 33, 8640, 135, 135, 8640, 8640, 33, 135, 8640, 33, 135, 135, 33, 135, 33, 135, 8640, 8640, 135, 33, 8640, 33, 8640, 135, 135, 135, 135, 135, 33, 33, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 33, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 33, 33, 33, 33, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33, 135, 8640, 8640, 135, 135, 33, 8640, 135, 8640, 33, 135, 135, 33, 33, 135, 33, 8640, 8640, 33, 8640, 8640, 33, 135, 8640, 33, 33, 135, 135, 33, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 135, 33, 8640, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 33, 8640, 8640, 135, 33, 8640, 135, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 135, 8640, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 8640, 33, 33, 33, 135, 33, 135, 135, 33, 8640, 135, 8640, 33, 8640, 33, 135, 135, 135, 135, 33, 135, 8640, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 135, 135, 135, 33, 135, 33, 33, 8640, 33, 135, 33, 33, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1127424 . Total input tokens: 251283552 . Total output tokens: 221449305
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 24.919056221842766,
    "estimated_duration": 3600.0122815193995,
    "input_throughput": 5068.707707935543,
    "output_throughput": 4421.900192318612,
    "total_throughput": 9490.607900254156,
    "itl": 99.48296627412137,
    "ttft": 1986032.9627591113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.757096600998218,
    "arrivals": 376270,
    "finished_requests": 73934,
    "scheduler_time": 203.91474322217408
}
#Debug simulation 
Total elapsed time: 24.91927376994863. Arrivals time: 0.3265657634474337 Scheduler time: 24.421957226935774 Scheduler overhead time: 0.062039399053901434 Adapter cache time: 0.021355424541980028 Engine time: 0.0607087523676455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 66, 33, 8640, 8640, 33, 66, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 33, 8640, 33, 8640, 66, 66, 66, 33, 33, 66, 33, 8640, 33, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 8640, 8640, 33, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 66, 33, 33, 33, 66, 33, 66, 33, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 8640, 8640, 66, 33, 33, 8640, 66, 66, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 8640, 33, 33, 8640, 33, 8640, 66, 33, 8640, 66, 66, 33, 8640, 8640, 8640, 33, 66, 8640, 33, 8640, 66, 66, 8640, 8640, 33, 66, 8640, 33, 66, 66, 33, 66, 33, 66, 8640, 8640, 66, 33, 8640, 33, 8640, 66, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 33, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 33, 33, 33, 33, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33, 66, 8640, 8640, 66, 66, 33, 8640, 66, 8640, 33, 66, 66, 33, 33, 66, 33, 8640, 8640, 33, 8640, 8640, 33, 66, 8640, 33, 33, 66, 66, 33, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 66, 33, 8640, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 33, 8640, 8640, 66, 33, 8640, 66, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 66, 8640, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 8640, 33, 33, 33, 66, 33, 66, 66, 33, 8640, 66, 8640, 33, 8640, 33, 66, 66, 66, 66, 33, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 66, 66, 66, 33, 66, 33, 33, 8640, 33, 66, 33, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1118592 . Total input tokens: 249316425 . Total output tokens: 219720730
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 58.054881792049855,
    "estimated_duration": 3600.0514678108675,
    "input_throughput": 5488.672363902461,
    "output_throughput": 4789.669579497768,
    "total_throughput": 10278.34194340023,
    "itl": 118.78649423274699,
    "ttft": 1905122.300881878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8947089198185716,
    "arrivals": 373284,
    "finished_requests": 79966,
    "scheduler_time": 187.53255267709926
}
#Debug simulation 
Total elapsed time: 58.05505596101284. Arrivals time: 0.36838478641584516 Scheduler time: 57.525126731488854 Scheduler overhead time: 0.06031702645123005 Adapter cache time: 0.01760498760268092 Engine time: 0.05940054962411523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 66, 33, 8640, 8640, 33, 66, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 33, 8640, 33, 8640, 66, 66, 66, 33, 33, 66, 33, 8640, 33, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 8640, 8640, 33, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 66, 33, 33, 33, 66, 33, 66, 33, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 8640, 8640, 66, 33, 33, 8640, 66, 66, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 8640, 33, 33, 8640, 33, 8640, 66, 33, 8640, 66, 66, 33, 8640, 8640, 8640, 33, 66, 8640, 33, 8640, 66, 66, 8640, 8640, 33, 66, 8640, 33, 66, 66, 33, 66, 33, 66, 8640, 8640, 66, 33, 8640, 33, 8640, 66, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 33, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 33, 33, 33, 33, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33, 66, 8640, 8640, 66, 66, 33, 8640, 66, 8640, 33, 66, 66, 33, 33, 66, 33, 8640, 8640, 33, 8640, 8640, 33, 66, 8640, 33, 33, 66, 66, 33, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 66, 33, 8640, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 33, 8640, 8640, 66, 33, 8640, 66, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 66, 8640, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 8640, 33, 33, 33, 66, 33, 66, 66, 33, 8640, 66, 8640, 33, 8640, 33, 66, 66, 66, 66, 33, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 66, 66, 66, 33, 66, 33, 33, 8640, 33, 66, 33, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1118592 . Total input tokens: 249316425 . Total output tokens: 219720730
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 56.10143756493926,
    "estimated_duration": 3600.115307820009,
    "input_throughput": 5355.387078330748,
    "output_throughput": 4674.584717729653,
    "total_throughput": 10029.971796060401,
    "itl": 111.60040920135305,
    "ttft": 1923871.279979079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 587,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.288767771706922,
    "arrivals": 373284,
    "finished_requests": 78034,
    "scheduler_time": 192.60630943279
}
#Debug simulation 
Total elapsed time: 56.10162030579522. Arrivals time: 0.3578407373279333 Scheduler time: 55.57425457937643 Scheduler overhead time: 0.06278240960091352 Adapter cache time: 0.017849644646048546 Engine time: 0.06333758123219013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 66, 33, 8640, 8640, 33, 66, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 33, 8640, 33, 8640, 66, 66, 66, 33, 33, 66, 33, 8640, 33, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 8640, 8640, 33, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 66, 33, 33, 33, 66, 33, 66, 33, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 8640, 8640, 66, 33, 33, 8640, 66, 66, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 8640, 33, 33, 8640, 33, 8640, 66, 33, 8640, 66, 66, 33, 8640, 8640, 8640, 33, 66, 8640, 33, 8640, 66, 66, 8640, 8640, 33, 66, 8640, 33, 66, 66, 33, 66, 33, 66, 8640, 8640, 66, 33, 8640, 33, 8640, 66, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 33, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 33, 33, 33, 33, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33, 66, 8640, 8640, 66, 66, 33, 8640, 66, 8640, 33, 66, 66, 33, 33, 66, 33, 8640, 8640, 33, 8640, 8640, 33, 66, 8640, 33, 33, 66, 66, 33, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 66, 33, 8640, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 33, 8640, 8640, 66, 33, 8640, 66, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 66, 8640, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 8640, 33, 33, 33, 66, 33, 66, 66, 33, 8640, 66, 8640, 33, 8640, 33, 66, 66, 66, 66, 33, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 66, 66, 66, 33, 66, 33, 33, 8640, 33, 66, 33, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1118592 . Total input tokens: 249316425 . Total output tokens: 219720730
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 33.033331252168864,
    "estimated_duration": 3600.012936163686,
    "input_throughput": 5063.344861039466,
    "output_throughput": 4423.521049057678,
    "total_throughput": 9486.865910097145,
    "itl": 99.45325939168755,
    "ttft": 1971259.1236564824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.466919991201757,
    "arrivals": 373284,
    "finished_requests": 73720,
    "scheduler_time": 203.71421798585487
}
#Debug simulation 
Total elapsed time: 33.033501401077956. Arrivals time: 0.32074278919026256 Scheduler time: 32.53755845827982 Scheduler overhead time: 0.06559139769524336 Adapter cache time: 0.018042261246591806 Engine time: 0.06435709679499269 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 66, 33, 8640, 8640, 33, 66, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 33, 8640, 33, 8640, 66, 66, 66, 33, 33, 66, 33, 8640, 33, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 8640, 8640, 33, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 66, 33, 33, 33, 66, 33, 66, 33, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 8640, 8640, 66, 33, 33, 8640, 66, 66, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 8640, 33, 33, 8640, 33, 8640, 66, 33, 8640, 66, 66, 33, 8640, 8640, 8640, 33, 66, 8640, 33, 8640, 66, 66, 8640, 8640, 33, 66, 8640, 33, 66, 66, 33, 66, 33, 66, 8640, 8640, 66, 33, 8640, 33, 8640, 66, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 33, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 33, 33, 33, 33, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33, 66, 8640, 8640, 66, 66, 33, 8640, 66, 8640, 33, 66, 66, 33, 33, 66, 33, 8640, 8640, 33, 8640, 8640, 33, 66, 8640, 33, 33, 66, 66, 33, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 66, 33, 8640, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 33, 8640, 8640, 66, 33, 8640, 66, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 66, 8640, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 8640, 33, 33, 33, 66, 33, 66, 66, 33, 8640, 66, 8640, 33, 8640, 33, 66, 66, 66, 66, 33, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 66, 66, 66, 33, 66, 33, 33, 8640, 33, 66, 33, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1118592 . Total input tokens: 249316425 . Total output tokens: 219720730
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 51.01972386101261,
    "estimated_duration": 3600.100117851406,
    "input_throughput": 5367.956269931059,
    "output_throughput": 4683.939737227053,
    "total_throughput": 10051.896007158113,
    "itl": 111.75386652195598,
    "ttft": 1932880.305626975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0474911754718,
    "arrivals": 373284,
    "finished_requests": 78180,
    "scheduler_time": 192.18477226549717
}
#Debug simulation 
Total elapsed time: 51.019913794938475. Arrivals time: 0.35655336920171976 Scheduler time: 50.499555898830295 Scheduler overhead time: 0.06086327042430639 Adapter cache time: 0.01732644345611334 Engine time: 0.060310402885079384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 66, 33, 8640, 8640, 33, 66, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 33, 8640, 33, 8640, 66, 66, 66, 33, 33, 66, 33, 8640, 33, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 8640, 8640, 33, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 66, 33, 33, 33, 66, 33, 66, 33, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 8640, 8640, 66, 33, 33, 8640, 66, 66, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 8640, 33, 33, 8640, 33, 8640, 66, 33, 8640, 66, 66, 33, 8640, 8640, 8640, 33, 66, 8640, 33, 8640, 66, 66, 8640, 8640, 33, 66, 8640, 33, 66, 66, 33, 66, 33, 66, 8640, 8640, 66, 33, 8640, 33, 8640, 66, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 33, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 33, 33, 33, 33, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33, 66, 8640, 8640, 66, 66, 33, 8640, 66, 8640, 33, 66, 66, 33, 33, 66, 33, 8640, 8640, 33, 8640, 8640, 33, 66, 8640, 33, 33, 66, 66, 33, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 66, 33, 8640, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 33, 8640, 8640, 66, 33, 8640, 66, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 66, 8640, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 8640, 33, 33, 33, 66, 33, 66, 66, 33, 8640, 66, 8640, 33, 8640, 33, 66, 66, 66, 66, 33, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 66, 66, 66, 33, 66, 33, 33, 8640, 33, 66, 33, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1118592 . Total input tokens: 249316425 . Total output tokens: 219720730
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 32.308412566781044,
    "estimated_duration": 3600.0819476304937,
    "input_throughput": 5064.083613985336,
    "output_throughput": 4423.834021467846,
    "total_throughput": 9487.917635453183,
    "itl": 99.41531543611794,
    "ttft": 1972751.8982460783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.854245754368636,
    "arrivals": 373284,
    "finished_requests": 73765,
    "scheduler_time": 203.70579630057838
}
#Debug simulation 
Total elapsed time: 32.30854873685166. Arrivals time: 0.333178429864347 Scheduler time: 31.80241110548377 Scheduler overhead time: 0.06412824196740985 Adapter cache time: 0.018931949511170387 Engine time: 0.06288064830005169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 66, 33, 8640, 8640, 33, 66, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 33, 8640, 33, 8640, 66, 66, 66, 33, 33, 66, 33, 8640, 33, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 8640, 8640, 33, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 66, 33, 33, 33, 66, 33, 66, 33, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 8640, 8640, 66, 33, 33, 8640, 66, 66, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 8640, 33, 33, 8640, 33, 8640, 66, 33, 8640, 66, 66, 33, 8640, 8640, 8640, 33, 66, 8640, 33, 8640, 66, 66, 8640, 8640, 33, 66, 8640, 33, 66, 66, 33, 66, 33, 66, 8640, 8640, 66, 33, 8640, 33, 8640, 66, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 33, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 33, 33, 33, 33, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33, 66, 8640, 8640, 66, 66, 33, 8640, 66, 8640, 33, 66, 66, 33, 33, 66, 33, 8640, 8640, 33, 8640, 8640, 33, 66, 8640, 33, 33, 66, 66, 33, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 66, 33, 8640, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 33, 8640, 8640, 66, 33, 8640, 66, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 66, 8640, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 8640, 33, 33, 33, 66, 33, 66, 66, 33, 8640, 66, 8640, 33, 8640, 33, 66, 66, 66, 66, 33, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 66, 66, 66, 33, 66, 33, 33, 8640, 33, 66, 33, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1118592 . Total input tokens: 249316425 . Total output tokens: 219720730
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 57.3305941298604,
    "estimated_duration": 3600.102981340448,
    "input_throughput": 5332.597456101639,
    "output_throughput": 4657.156499939156,
    "total_throughput": 9989.753956040795,
    "itl": 111.32474406680358,
    "ttft": 1920336.8365073418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7984329596394497,
    "arrivals": 373284,
    "finished_requests": 77713,
    "scheduler_time": 193.49352712754396
}
#Debug simulation 
Total elapsed time: 57.33076450088993. Arrivals time: 0.3669855585321784 Scheduler time: 56.795656838454306 Scheduler overhead time: 0.06226663617417216 Adapter cache time: 0.01842975802719593 Engine time: 0.06197511497884989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 66, 33, 8640, 8640, 33, 66, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 33, 8640, 33, 8640, 66, 66, 66, 33, 33, 66, 33, 8640, 33, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 8640, 8640, 33, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 66, 33, 33, 33, 66, 33, 66, 33, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 8640, 8640, 66, 33, 33, 8640, 66, 66, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 8640, 8640, 33, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 8640, 33, 33, 8640, 33, 8640, 66, 33, 8640, 66, 66, 33, 8640, 8640, 8640, 33, 66, 8640, 33, 8640, 66, 66, 8640, 8640, 33, 66, 8640, 33, 66, 66, 33, 66, 33, 66, 8640, 8640, 66, 33, 8640, 33, 8640, 66, 66, 66, 66, 66, 33, 33, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 33, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 33, 33, 33, 33, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33, 66, 8640, 8640, 66, 66, 33, 8640, 66, 8640, 33, 66, 66, 33, 33, 66, 33, 8640, 8640, 33, 8640, 8640, 33, 66, 8640, 33, 33, 66, 66, 33, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 66, 33, 8640, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 33, 8640, 8640, 66, 33, 8640, 66, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 66, 8640, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 8640, 33, 33, 33, 66, 33, 66, 66, 33, 8640, 66, 8640, 33, 8640, 33, 66, 66, 66, 66, 33, 66, 8640, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 66, 66, 66, 33, 66, 33, 33, 8640, 33, 66, 33, 33, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1118592 . Total input tokens: 249316425 . Total output tokens: 219720730
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 42.80426763789728,
    "estimated_duration": 3600.007980655958,
    "input_throughput": 5078.9320741084675,
    "output_throughput": 4422.694639998797,
    "total_throughput": 9501.626714107266,
    "itl": 99.41587568040948,
    "ttft": 1955653.3293938779,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.643949867691878,
    "arrivals": 373284,
    "finished_requests": 73838,
    "scheduler_time": 203.71079408165753
}
#Debug simulation 
Total elapsed time: 42.804454850032926. Arrivals time: 0.3370264144614339 Scheduler time: 42.291235169861466 Scheduler overhead time: 0.06524361995980144 Adapter cache time: 0.01928342366591096 Engine time: 0.06445148913189769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 4320, 1080, 540, 4320, 4320, 540, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 540, 540, 1080, 540, 4320, 540, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 4320, 4320, 540, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 4320, 1080, 1080, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 1080, 540, 4320, 1080, 1080, 540, 4320, 4320, 4320, 540, 1080, 4320, 540, 4320, 1080, 1080, 4320, 4320, 540, 1080, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 4320, 540, 1080, 1080, 540, 540, 1080, 540, 4320, 4320, 540, 4320, 4320, 540, 1080, 4320, 540, 540, 1080, 1080, 540, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 540, 540, 1080, 540, 4320, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 540, 4320, 4320, 1080, 540, 4320, 1080, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 4320, 540, 1080, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 4320, 540, 540, 540, 1080, 540, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 1080, 1080, 1080, 540, 1080, 540, 540, 4320, 540, 1080, 540, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 4320, 540, 540, 540]
Prompts retrieved: 760320 . Total input tokens: 169427239 . Total output tokens: 149312575
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 48.01497101178393,
    "estimated_duration": 3600.084984269738,
    "input_throughput": 5479.34315056214,
    "output_throughput": 4792.7135263169175,
    "total_throughput": 10272.056676879058,
    "itl": 118.44815611363973,
    "ttft": 1790145.5225567059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 906,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.990842582946769,
    "arrivals": 253064,
    "finished_requests": 80075,
    "scheduler_time": 182.78141319235792
}
#Debug simulation 
Total elapsed time: 48.01513233361766. Arrivals time: 0.377955490257591 Scheduler time: 47.471589056774974 Scheduler overhead time: 0.06027355045080185 Adapter cache time: 0.02184943202883005 Engine time: 0.05904841935262084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 4320, 1080, 540, 4320, 4320, 540, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 540, 540, 1080, 540, 4320, 540, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 4320, 4320, 540, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 4320, 1080, 1080, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 1080, 540, 4320, 1080, 1080, 540, 4320, 4320, 4320, 540, 1080, 4320, 540, 4320, 1080, 1080, 4320, 4320, 540, 1080, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 4320, 540, 1080, 1080, 540, 540, 1080, 540, 4320, 4320, 540, 4320, 4320, 540, 1080, 4320, 540, 540, 1080, 1080, 540, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 540, 540, 1080, 540, 4320, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 540, 4320, 4320, 1080, 540, 4320, 1080, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 4320, 540, 1080, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 4320, 540, 540, 540, 1080, 540, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 1080, 1080, 1080, 540, 1080, 540, 540, 4320, 540, 1080, 540, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 4320, 540, 540, 540]
Prompts retrieved: 760320 . Total input tokens: 169427239 . Total output tokens: 149312575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 54.29295115126297,
    "estimated_duration": 3600.000106561249,
    "input_throughput": 5329.650397796055,
    "output_throughput": 4668.604861807677,
    "total_throughput": 9998.255259603731,
    "itl": 111.22962861945983,
    "ttft": 1805287.893268612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 799,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.8170748537452965,
    "arrivals": 253064,
    "finished_requests": 77876,
    "scheduler_time": 188.31674130396019
}
#Debug simulation 
Total elapsed time: 54.29314002208412. Arrivals time: 0.37876302609220147 Scheduler time: 53.73847839748487 Scheduler overhead time: 0.06471767649054527 Adapter cache time: 0.021401673089712858 Engine time: 0.0637880964204669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 4320, 1080, 540, 4320, 4320, 540, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 540, 540, 1080, 540, 4320, 540, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 4320, 4320, 540, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 4320, 1080, 1080, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 1080, 540, 4320, 1080, 1080, 540, 4320, 4320, 4320, 540, 1080, 4320, 540, 4320, 1080, 1080, 4320, 4320, 540, 1080, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 4320, 540, 1080, 1080, 540, 540, 1080, 540, 4320, 4320, 540, 4320, 4320, 540, 1080, 4320, 540, 540, 1080, 1080, 540, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 540, 540, 1080, 540, 4320, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 540, 4320, 4320, 1080, 540, 4320, 1080, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 4320, 540, 1080, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 4320, 540, 540, 540, 1080, 540, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 1080, 1080, 1080, 540, 1080, 540, 540, 4320, 540, 1080, 540, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 4320, 540, 540, 540]
Prompts retrieved: 760320 . Total input tokens: 169427239 . Total output tokens: 149312575
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 36.07803383190185,
    "estimated_duration": 3600.089241759752,
    "input_throughput": 5039.6484035855365,
    "output_throughput": 4409.801517097905,
    "total_throughput": 9449.449920683443,
    "itl": 99.07491995750611,
    "ttft": 1857847.6579249022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.899358923914814,
    "arrivals": 253064,
    "finished_requests": 73625,
    "scheduler_time": 199.4290073096994
}
#Debug simulation 
Total elapsed time: 36.07821509381756. Arrivals time: 0.34309145994484425 Scheduler time: 35.55218481179327 Scheduler overhead time: 0.0650759618729353 Adapter cache time: 0.026422360446304083 Engine time: 0.06405492639169097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 4320, 1080, 540, 4320, 4320, 540, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 540, 540, 1080, 540, 4320, 540, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 4320, 4320, 540, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 4320, 1080, 1080, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 1080, 540, 4320, 1080, 1080, 540, 4320, 4320, 4320, 540, 1080, 4320, 540, 4320, 1080, 1080, 4320, 4320, 540, 1080, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 4320, 540, 1080, 1080, 540, 540, 1080, 540, 4320, 4320, 540, 4320, 4320, 540, 1080, 4320, 540, 540, 1080, 1080, 540, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 540, 540, 1080, 540, 4320, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 540, 4320, 4320, 1080, 540, 4320, 1080, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 4320, 540, 1080, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 4320, 540, 540, 540, 1080, 540, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 1080, 1080, 1080, 540, 1080, 540, 540, 4320, 540, 1080, 540, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 4320, 540, 540, 540]
Prompts retrieved: 760320 . Total input tokens: 169427239 . Total output tokens: 149312575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 53.45864021591842,
    "estimated_duration": 3600.02109028569,
    "input_throughput": 5325.488245536517,
    "output_throughput": 4664.910726583349,
    "total_throughput": 9990.398972119867,
    "itl": 111.14351776200988,
    "ttft": 1789774.5631898148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 852,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.841684723626814,
    "arrivals": 253064,
    "finished_requests": 77782,
    "scheduler_time": 188.51906139917918
}
#Debug simulation 
Total elapsed time: 53.45881838304922. Arrivals time: 0.37593997176736593 Scheduler time: 52.907444012351334 Scheduler overhead time: 0.06409468408674002 Adapter cache time: 0.021681356243789196 Engine time: 0.06371407210826874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 4320, 1080, 540, 4320, 4320, 540, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 540, 540, 1080, 540, 4320, 540, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 4320, 4320, 540, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 4320, 1080, 1080, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 1080, 540, 4320, 1080, 1080, 540, 4320, 4320, 4320, 540, 1080, 4320, 540, 4320, 1080, 1080, 4320, 4320, 540, 1080, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 4320, 540, 1080, 1080, 540, 540, 1080, 540, 4320, 4320, 540, 4320, 4320, 540, 1080, 4320, 540, 540, 1080, 1080, 540, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 540, 540, 1080, 540, 4320, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 540, 4320, 4320, 1080, 540, 4320, 1080, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 4320, 540, 1080, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 4320, 540, 540, 540, 1080, 540, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 1080, 1080, 1080, 540, 1080, 540, 540, 4320, 540, 1080, 540, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 4320, 540, 540, 540]
Prompts retrieved: 760320 . Total input tokens: 169427239 . Total output tokens: 149312575
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 38.88947768090293,
    "estimated_duration": 3600.077555521681,
    "input_throughput": 5042.7349744606545,
    "output_throughput": 4416.339580133317,
    "total_throughput": 9459.074554593972,
    "itl": 99.19293720276508,
    "ttft": 1861850.5009768568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 987,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.338053041584808,
    "arrivals": 253064,
    "finished_requests": 73737,
    "scheduler_time": 199.35724304737707
}
#Debug simulation 
Total elapsed time: 38.889661634806544. Arrivals time: 0.3455552733503282 Scheduler time: 38.358721802942455 Scheduler overhead time: 0.06882628938183188 Adapter cache time: 0.024141809903085232 Engine time: 0.06493034865707159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 4320, 1080, 540, 4320, 4320, 540, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 540, 540, 1080, 540, 4320, 540, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 4320, 4320, 540, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 4320, 1080, 1080, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 1080, 540, 4320, 1080, 1080, 540, 4320, 4320, 4320, 540, 1080, 4320, 540, 4320, 1080, 1080, 4320, 4320, 540, 1080, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 4320, 540, 1080, 1080, 540, 540, 1080, 540, 4320, 4320, 540, 4320, 4320, 540, 1080, 4320, 540, 540, 1080, 1080, 540, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 540, 540, 1080, 540, 4320, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 540, 4320, 4320, 1080, 540, 4320, 1080, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 4320, 540, 1080, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 4320, 540, 540, 540, 1080, 540, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 1080, 1080, 1080, 540, 1080, 540, 540, 4320, 540, 1080, 540, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 4320, 540, 540, 540]
Prompts retrieved: 760320 . Total input tokens: 169427239 . Total output tokens: 149312575
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 50.59411368891597,
    "estimated_duration": 3600.0882676185774,
    "input_throughput": 5351.979331536036,
    "output_throughput": 4672.420437937486,
    "total_throughput": 10024.399769473523,
    "itl": 111.2566747431912,
    "ttft": 1798898.5667823527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 824,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.260350855030094,
    "arrivals": 253064,
    "finished_requests": 78141,
    "scheduler_time": 188.1791991165359
}
#Debug simulation 
Total elapsed time: 50.59429971687496. Arrivals time: 0.37405770225450397 Scheduler time: 50.04862591344863 Scheduler overhead time: 0.06318500777706504 Adapter cache time: 0.021168118808418512 Engine time: 0.06166484113782644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 4320, 1080, 540, 4320, 4320, 540, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 540, 540, 1080, 540, 4320, 540, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 4320, 4320, 540, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 4320, 1080, 1080, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 540, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 1080, 540, 4320, 1080, 1080, 540, 4320, 4320, 4320, 540, 1080, 4320, 540, 4320, 1080, 1080, 4320, 4320, 540, 1080, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 1080, 1080, 1080, 540, 540, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 4320, 540, 1080, 1080, 540, 540, 1080, 540, 4320, 4320, 540, 4320, 4320, 540, 1080, 4320, 540, 540, 1080, 1080, 540, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 540, 540, 1080, 540, 4320, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 540, 4320, 4320, 1080, 540, 4320, 1080, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 4320, 540, 1080, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 4320, 540, 540, 540, 1080, 540, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 540, 1080, 1080, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 1080, 1080, 1080, 540, 1080, 540, 540, 4320, 540, 1080, 540, 540, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 4320, 540, 540, 540]
Prompts retrieved: 760320 . Total input tokens: 169427239 . Total output tokens: 149312575
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 39.79993028007448,
    "estimated_duration": 3600.0975273479853,
    "input_throughput": 5052.734783382362,
    "output_throughput": 4416.011199483067,
    "total_throughput": 9468.745982865428,
    "itl": 99.16818927692871,
    "ttft": 1850708.0899002827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1010,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.432566357255002,
    "arrivals": 253064,
    "finished_requests": 73785,
    "scheduler_time": 199.2953675498913
}
#Debug simulation 
Total elapsed time: 39.800120775122195. Arrivals time: 0.35654196375980973 Scheduler time: 39.25891442364082 Scheduler overhead time: 0.06686196383088827 Adapter cache time: 0.02437041886150837 Engine time: 0.06567567680031061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 1080, 270, 4320, 4320, 270, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 270, 270, 1080, 270, 4320, 270, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 4320, 4320, 270, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 4320, 1080, 1080, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 1080, 270, 4320, 1080, 1080, 270, 4320, 4320, 4320, 270, 1080, 4320, 270, 4320, 1080, 1080, 4320, 4320, 270, 1080, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 4320, 270, 1080, 1080, 270, 270, 1080, 270, 4320, 4320, 270, 4320, 4320, 270, 1080, 4320, 270, 270, 1080, 1080, 270, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 1080, 270, 4320, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 270, 4320, 4320, 1080, 270, 4320, 1080, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 1080, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 4320, 270, 270, 270, 1080, 270, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 1080, 1080, 1080, 270, 1080, 270, 270, 4320, 270, 1080, 270, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 725760 . Total input tokens: 161762081 . Total output tokens: 142588191
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 56.63610920496285,
    "estimated_duration": 3600.0551395793304,
    "input_throughput": 5493.292806151536,
    "output_throughput": 4791.978270092785,
    "total_throughput": 10285.271076244322,
    "itl": 117.99913958734565,
    "ttft": 1764796.8129325078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 721,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.767546912035991,
    "arrivals": 241330,
    "finished_requests": 79933,
    "scheduler_time": 182.2669885141866
}
#Debug simulation 
Total elapsed time: 56.63628314482048. Arrivals time: 0.3851506132632494 Scheduler time: 56.08245744742453 Scheduler overhead time: 0.06202141009271145 Adapter cache time: 0.01974358130246401 Engine time: 0.06173456134274602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 1080, 270, 4320, 4320, 270, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 270, 270, 1080, 270, 4320, 270, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 4320, 4320, 270, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 4320, 1080, 1080, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 1080, 270, 4320, 1080, 1080, 270, 4320, 4320, 4320, 270, 1080, 4320, 270, 4320, 1080, 1080, 4320, 4320, 270, 1080, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 4320, 270, 1080, 1080, 270, 270, 1080, 270, 4320, 4320, 270, 4320, 4320, 270, 1080, 4320, 270, 270, 1080, 1080, 270, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 1080, 270, 4320, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 270, 4320, 4320, 1080, 270, 4320, 1080, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 1080, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 4320, 270, 270, 270, 1080, 270, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 1080, 1080, 1080, 270, 1080, 270, 270, 4320, 270, 1080, 270, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 725760 . Total input tokens: 161762081 . Total output tokens: 142588191
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 51.98968568397686,
    "estimated_duration": 3600.064145655197,
    "input_throughput": 5341.1792740433175,
    "output_throughput": 4665.743253567227,
    "total_throughput": 10006.922527610544,
    "itl": 110.8946965619896,
    "ttft": 1789664.614733863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 738,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.395726593984299,
    "arrivals": 241330,
    "finished_requests": 77798,
    "scheduler_time": 187.8932832906869
}
#Debug simulation 
Total elapsed time: 51.98986255377531. Arrivals time: 0.36924841487780213 Scheduler time: 51.448767992667854 Scheduler overhead time: 0.06326161604374647 Adapter cache time: 0.02035182947292924 Engine time: 0.06265432666987181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 1080, 270, 4320, 4320, 270, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 270, 270, 1080, 270, 4320, 270, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 4320, 4320, 270, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 4320, 1080, 1080, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 1080, 270, 4320, 1080, 1080, 270, 4320, 4320, 4320, 270, 1080, 4320, 270, 4320, 1080, 1080, 4320, 4320, 270, 1080, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 4320, 270, 1080, 1080, 270, 270, 1080, 270, 4320, 4320, 270, 4320, 4320, 270, 1080, 4320, 270, 270, 1080, 1080, 270, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 1080, 270, 4320, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 270, 4320, 4320, 1080, 270, 4320, 1080, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 1080, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 4320, 270, 270, 270, 1080, 270, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 1080, 1080, 1080, 270, 1080, 270, 270, 4320, 270, 1080, 270, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 725760 . Total input tokens: 161762081 . Total output tokens: 142588191
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 29.6481074700132,
    "estimated_duration": 3600.0676195826377,
    "input_throughput": 5052.500097792253,
    "output_throughput": 4418.080903114797,
    "total_throughput": 9470.58100090705,
    "itl": 99.32951667265033,
    "ttft": 1843191.7669575014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.526952854064234,
    "arrivals": 241330,
    "finished_requests": 73541,
    "scheduler_time": 198.36712289464975
}
#Debug simulation 
Total elapsed time: 29.64824271807447. Arrivals time: 0.3318166877143085 Scheduler time: 29.133410732727498 Scheduler overhead time: 0.06446059886366129 Adapter cache time: 0.028426144272089005 Engine time: 0.06325071724131703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 1080, 270, 4320, 4320, 270, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 270, 270, 1080, 270, 4320, 270, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 4320, 4320, 270, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 4320, 1080, 1080, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 1080, 270, 4320, 1080, 1080, 270, 4320, 4320, 4320, 270, 1080, 4320, 270, 4320, 1080, 1080, 4320, 4320, 270, 1080, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 4320, 270, 1080, 1080, 270, 270, 1080, 270, 4320, 4320, 270, 4320, 4320, 270, 1080, 4320, 270, 270, 1080, 1080, 270, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 1080, 270, 4320, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 270, 4320, 4320, 1080, 270, 4320, 1080, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 1080, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 4320, 270, 270, 270, 1080, 270, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 1080, 1080, 1080, 270, 1080, 270, 270, 4320, 270, 1080, 270, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 725760 . Total input tokens: 161762081 . Total output tokens: 142588191
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 45.80468286527321,
    "estimated_duration": 3600.118109337509,
    "input_throughput": 5339.481210392111,
    "output_throughput": 4667.791580619107,
    "total_throughput": 10007.272791011217,
    "itl": 110.81452096177672,
    "ttft": 1797634.2878013463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 712,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.871583638414733,
    "arrivals": 241330,
    "finished_requests": 77830,
    "scheduler_time": 187.83678745554244
}
#Debug simulation 
Total elapsed time: 45.80486406525597. Arrivals time: 0.38621473079547286 Scheduler time: 45.24647208908573 Scheduler overhead time: 0.06337886536493897 Adapter cache time: 0.020272533874958754 Engine time: 0.06291657593101263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 1080, 270, 4320, 4320, 270, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 270, 270, 1080, 270, 4320, 270, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 4320, 4320, 270, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 4320, 1080, 1080, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 1080, 270, 4320, 1080, 1080, 270, 4320, 4320, 4320, 270, 1080, 4320, 270, 4320, 1080, 1080, 4320, 4320, 270, 1080, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 4320, 270, 1080, 1080, 270, 270, 1080, 270, 4320, 4320, 270, 4320, 4320, 270, 1080, 4320, 270, 270, 1080, 1080, 270, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 1080, 270, 4320, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 270, 4320, 4320, 1080, 270, 4320, 1080, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 1080, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 4320, 270, 270, 270, 1080, 270, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 1080, 1080, 1080, 270, 1080, 270, 270, 4320, 270, 1080, 270, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 725760 . Total input tokens: 161762081 . Total output tokens: 142588191
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 29.80167956231162,
    "estimated_duration": 3600.0091879324104,
    "input_throughput": 5043.594905497473,
    "output_throughput": 4415.527341787008,
    "total_throughput": 9459.12224728448,
    "itl": 99.28780847196214,
    "ttft": 1842144.0454616577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.178758901427425,
    "arrivals": 241330,
    "finished_requests": 73503,
    "scheduler_time": 198.39472754673085
}
#Debug simulation 
Total elapsed time: 29.801856958307326. Arrivals time: 0.34349308954551816 Scheduler time: 29.27587650110945 Scheduler overhead time: 0.06387925893068314 Adapter cache time: 0.028565223328769207 Engine time: 0.06308930553495884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 1080, 270, 4320, 4320, 270, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 270, 270, 1080, 270, 4320, 270, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 4320, 4320, 270, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 4320, 1080, 1080, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 1080, 270, 4320, 1080, 1080, 270, 4320, 4320, 4320, 270, 1080, 4320, 270, 4320, 1080, 1080, 4320, 4320, 270, 1080, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 4320, 270, 1080, 1080, 270, 270, 1080, 270, 4320, 4320, 270, 4320, 4320, 270, 1080, 4320, 270, 270, 1080, 1080, 270, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 1080, 270, 4320, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 270, 4320, 4320, 1080, 270, 4320, 1080, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 1080, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 4320, 270, 270, 270, 1080, 270, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 1080, 1080, 1080, 270, 1080, 270, 270, 4320, 270, 1080, 270, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 725760 . Total input tokens: 161762081 . Total output tokens: 142588191
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 53.10989637719467,
    "estimated_duration": 3600.0225990306985,
    "input_throughput": 5327.265169158584,
    "output_throughput": 4655.876328252202,
    "total_throughput": 9983.141497410787,
    "itl": 110.40607558266241,
    "ttft": 1780927.1369949987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 721,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.602806998151332,
    "arrivals": 241330,
    "finished_requests": 77519,
    "scheduler_time": 188.3954711442271
}
#Debug simulation 
Total elapsed time: 53.110083798877895. Arrivals time: 0.3853875361382961 Scheduler time: 52.55033366801217 Scheduler overhead time: 0.0646282834932208 Adapter cache time: 0.020193513948470354 Engine time: 0.06368654407560825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 1080, 270, 4320, 4320, 270, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 270, 270, 1080, 270, 4320, 270, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 4320, 4320, 270, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 4320, 1080, 1080, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 270, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 1080, 270, 4320, 1080, 1080, 270, 4320, 4320, 4320, 270, 1080, 4320, 270, 4320, 1080, 1080, 4320, 4320, 270, 1080, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 1080, 1080, 1080, 270, 270, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 4320, 270, 1080, 1080, 270, 270, 1080, 270, 4320, 4320, 270, 4320, 4320, 270, 1080, 4320, 270, 270, 1080, 1080, 270, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 1080, 270, 4320, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 270, 4320, 4320, 1080, 270, 4320, 1080, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 1080, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 4320, 270, 270, 270, 1080, 270, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 270, 1080, 1080, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 1080, 1080, 1080, 270, 1080, 270, 270, 4320, 270, 1080, 270, 270, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 725760 . Total input tokens: 161762081 . Total output tokens: 142588191
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.311301036272198,
    "estimated_duration": 3600.0477927774077,
    "input_throughput": 5041.105297660172,
    "output_throughput": 4416.953028207528,
    "total_throughput": 9458.058325867702,
    "itl": 99.3131210650555,
    "ttft": 1841116.909327912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.44191497078161,
    "arrivals": 241330,
    "finished_requests": 73481,
    "scheduler_time": 198.38888078574217
}
#Debug simulation 
Total elapsed time: 31.31147234607488. Arrivals time: 0.34718108968809247 Scheduler time: 30.782260796055198 Scheduler overhead time: 0.06409580493345857 Adapter cache time: 0.028089832980185747 Engine time: 0.06279305089265108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 1080, 135, 4320, 4320, 135, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 135, 135, 1080, 135, 4320, 135, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 4320, 4320, 135, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 4320, 1080, 1080, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 1080, 135, 4320, 1080, 1080, 135, 4320, 4320, 4320, 135, 1080, 4320, 135, 4320, 1080, 1080, 4320, 4320, 135, 1080, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 4320, 135, 1080, 1080, 135, 135, 1080, 135, 4320, 4320, 135, 4320, 4320, 135, 1080, 4320, 135, 135, 1080, 1080, 135, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 1080, 135, 4320, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 135, 4320, 4320, 1080, 135, 4320, 1080, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 1080, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 4320, 135, 135, 135, 1080, 135, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 1080, 1080, 1080, 135, 1080, 135, 135, 4320, 135, 1080, 135, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 708480 . Total input tokens: 157889463 . Total output tokens: 139202281
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 52.32763035688549,
    "estimated_duration": 3600.0699179620324,
    "input_throughput": 5482.847680684454,
    "output_throughput": 4799.411787474685,
    "total_throughput": 10282.259468159138,
    "itl": 118.46291307827191,
    "ttft": 1753270.2971756354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 759,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.018818455250097,
    "arrivals": 235580,
    "finished_requests": 79807,
    "scheduler_time": 181.84630132236728
}
#Debug simulation 
Total elapsed time: 52.32782221818343. Arrivals time: 0.4047142695635557 Scheduler time: 51.75568116828799 Scheduler overhead time: 0.06298460206016898 Adapter cache time: 0.020366038661450148 Engine time: 0.05932066170498729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 1080, 135, 4320, 4320, 135, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 135, 135, 1080, 135, 4320, 135, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 4320, 4320, 135, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 4320, 1080, 1080, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 1080, 135, 4320, 1080, 1080, 135, 4320, 4320, 4320, 135, 1080, 4320, 135, 4320, 1080, 1080, 4320, 4320, 135, 1080, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 4320, 135, 1080, 1080, 135, 135, 1080, 135, 4320, 4320, 135, 4320, 4320, 135, 1080, 4320, 135, 135, 1080, 1080, 135, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 1080, 135, 4320, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 135, 4320, 4320, 1080, 135, 4320, 1080, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 1080, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 4320, 135, 135, 135, 1080, 135, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 1080, 1080, 1080, 135, 1080, 135, 135, 4320, 135, 1080, 135, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 708480 . Total input tokens: 157889463 . Total output tokens: 139202281
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 50.45159929944202,
    "estimated_duration": 3600.119730132259,
    "input_throughput": 5332.459317761283,
    "output_throughput": 4670.986317275126,
    "total_throughput": 10003.445635036409,
    "itl": 111.12347036191895,
    "ttft": 1774672.3506244798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.772387407161302,
    "arrivals": 235580,
    "finished_requests": 77691,
    "scheduler_time": 187.4136157829272
}
#Debug simulation 
Total elapsed time: 50.45178105123341. Arrivals time: 0.37880936032161117 Scheduler time: 49.90298369433731 Scheduler overhead time: 0.06247213575989008 Adapter cache time: 0.02065776800736785 Engine time: 0.06133578345179558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 1080, 135, 4320, 4320, 135, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 135, 135, 1080, 135, 4320, 135, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 4320, 4320, 135, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 4320, 1080, 1080, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 1080, 135, 4320, 1080, 1080, 135, 4320, 4320, 4320, 135, 1080, 4320, 135, 4320, 1080, 1080, 4320, 4320, 135, 1080, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 4320, 135, 1080, 1080, 135, 135, 1080, 135, 4320, 4320, 135, 4320, 4320, 135, 1080, 4320, 135, 135, 1080, 1080, 135, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 1080, 135, 4320, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 135, 4320, 4320, 1080, 135, 4320, 1080, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 1080, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 4320, 135, 135, 135, 1080, 135, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 1080, 1080, 1080, 135, 1080, 135, 135, 4320, 135, 1080, 135, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 708480 . Total input tokens: 157889463 . Total output tokens: 139202281
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 33.83679173607379,
    "estimated_duration": 3600.069980634041,
    "input_throughput": 5049.32990130334,
    "output_throughput": 4420.468792441985,
    "total_throughput": 9469.798693745326,
    "itl": 99.26938820214139,
    "ttft": 1829235.2344438413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.548699378594698,
    "arrivals": 235580,
    "finished_requests": 73524,
    "scheduler_time": 198.01796313220697
}
#Debug simulation 
Total elapsed time: 33.836954235099256. Arrivals time: 0.34325475338846445 Scheduler time: 33.31394911278039 Scheduler overhead time: 0.06336695468053222 Adapter cache time: 0.02680170675739646 Engine time: 0.06256719818338752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 1080, 135, 4320, 4320, 135, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 135, 135, 1080, 135, 4320, 135, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 4320, 4320, 135, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 4320, 1080, 1080, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 1080, 135, 4320, 1080, 1080, 135, 4320, 4320, 4320, 135, 1080, 4320, 135, 4320, 1080, 1080, 4320, 4320, 135, 1080, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 4320, 135, 1080, 1080, 135, 135, 1080, 135, 4320, 4320, 135, 4320, 4320, 135, 1080, 4320, 135, 135, 1080, 1080, 135, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 1080, 135, 4320, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 135, 4320, 4320, 1080, 135, 4320, 1080, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 1080, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 4320, 135, 135, 135, 1080, 135, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 1080, 1080, 1080, 135, 1080, 135, 135, 4320, 135, 1080, 135, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 708480 . Total input tokens: 157889463 . Total output tokens: 139202281
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 50.06202075211331,
    "estimated_duration": 3600.0303577988725,
    "input_throughput": 5328.837841170165,
    "output_throughput": 4664.856773671193,
    "total_throughput": 9993.694614841359,
    "itl": 110.93198451039487,
    "ttft": 1774907.324426132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 791,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.402289591212748,
    "arrivals": 235580,
    "finished_requests": 77593,
    "scheduler_time": 187.66705227692688
}
#Debug simulation 
Total elapsed time: 50.06221563369036. Arrivals time: 0.37334285397082567 Scheduler time: 49.51601286837831 Scheduler overhead time: 0.06435927236452699 Adapter cache time: 0.02082259627059102 Engine time: 0.062033855356276035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 1080, 135, 4320, 4320, 135, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 135, 135, 1080, 135, 4320, 135, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 4320, 4320, 135, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 4320, 1080, 1080, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 1080, 135, 4320, 1080, 1080, 135, 4320, 4320, 4320, 135, 1080, 4320, 135, 4320, 1080, 1080, 4320, 4320, 135, 1080, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 4320, 135, 1080, 1080, 135, 135, 1080, 135, 4320, 4320, 135, 4320, 4320, 135, 1080, 4320, 135, 135, 1080, 1080, 135, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 1080, 135, 4320, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 135, 4320, 4320, 1080, 135, 4320, 1080, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 1080, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 4320, 135, 135, 135, 1080, 135, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 1080, 1080, 1080, 135, 1080, 135, 135, 4320, 135, 1080, 135, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 708480 . Total input tokens: 157889463 . Total output tokens: 139202281
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 39.59635288035497,
    "estimated_duration": 3600.0605408468996,
    "input_throughput": 5040.5535668383945,
    "output_throughput": 4415.99601440594,
    "total_throughput": 9456.549581244335,
    "itl": 99.09016428860568,
    "ttft": 1822720.8432922438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1080,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.034756915848718,
    "arrivals": 235580,
    "finished_requests": 73356,
    "scheduler_time": 198.28717620189474
}
#Debug simulation 
Total elapsed time: 39.596550174057484. Arrivals time: 0.33398196613416076 Scheduler time: 39.079020753037184 Scheduler overhead time: 0.06626555370166898 Adapter cache time: 0.02500151051208377 Engine time: 0.06462617591023445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 1080, 135, 4320, 4320, 135, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 135, 135, 1080, 135, 4320, 135, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 4320, 4320, 135, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 4320, 1080, 1080, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 1080, 135, 4320, 1080, 1080, 135, 4320, 4320, 4320, 135, 1080, 4320, 135, 4320, 1080, 1080, 4320, 4320, 135, 1080, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 4320, 135, 1080, 1080, 135, 135, 1080, 135, 4320, 4320, 135, 4320, 4320, 135, 1080, 4320, 135, 135, 1080, 1080, 135, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 1080, 135, 4320, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 135, 4320, 4320, 1080, 135, 4320, 1080, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 1080, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 4320, 135, 135, 135, 1080, 135, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 1080, 1080, 1080, 135, 1080, 135, 135, 4320, 135, 1080, 135, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 708480 . Total input tokens: 157889463 . Total output tokens: 139202281
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 49.27323819510639,
    "estimated_duration": 3600.0155055885057,
    "input_throughput": 5326.129004232037,
    "output_throughput": 4669.934330533311,
    "total_throughput": 9996.06333476535,
    "itl": 111.0122355046899,
    "ttft": 1776488.639230116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.973074412704421,
    "arrivals": 235580,
    "finished_requests": 77566,
    "scheduler_time": 187.55856163475994
}
#Debug simulation 
Total elapsed time: 49.273424172308296. Arrivals time: 0.3578213551081717 Scheduler time: 48.74646182637662 Scheduler overhead time: 0.061598313972353935 Adapter cache time: 0.020380449946969748 Engine time: 0.06153343664482236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 1080, 135, 4320, 4320, 135, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 135, 135, 1080, 135, 4320, 135, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 4320, 4320, 135, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 4320, 1080, 1080, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 135, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 1080, 135, 4320, 1080, 1080, 135, 4320, 4320, 4320, 135, 1080, 4320, 135, 4320, 1080, 1080, 4320, 4320, 135, 1080, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 1080, 1080, 1080, 135, 135, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 4320, 135, 1080, 1080, 135, 135, 1080, 135, 4320, 4320, 135, 4320, 4320, 135, 1080, 4320, 135, 135, 1080, 1080, 135, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 4320, 1080, 1080, 1080, 4320, 135, 1080, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 1080, 135, 4320, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 135, 4320, 4320, 1080, 135, 4320, 1080, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 1080, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 4320, 135, 135, 135, 1080, 135, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 135, 1080, 1080, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 1080, 1080, 1080, 135, 1080, 135, 135, 4320, 135, 1080, 135, 135, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 708480 . Total input tokens: 157889463 . Total output tokens: 139202281
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 33.69843250932172,
    "estimated_duration": 3600.0863388416683,
    "input_throughput": 5050.379987789408,
    "output_throughput": 4418.729303341751,
    "total_throughput": 9469.109291131159,
    "itl": 99.12494938120247,
    "ttft": 1825108.3261456988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1094,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.04850800126792,
    "arrivals": 235580,
    "finished_requests": 73448,
    "scheduler_time": 198.25491231589217
}
#Debug simulation 
Total elapsed time: 33.69859940325841. Arrivals time: 0.325376370921731 Scheduler time: 33.19346702191979 Scheduler overhead time: 0.0641884976066649 Adapter cache time: 0.025098594836890697 Engine time: 0.06344021623954177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 4320, 1080, 66, 4320, 4320, 66, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 66, 66, 1080, 66, 4320, 66, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 4320, 4320, 66, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 4320, 1080, 1080, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 1080, 66, 4320, 1080, 1080, 66, 4320, 4320, 4320, 66, 1080, 4320, 66, 4320, 1080, 1080, 4320, 4320, 66, 1080, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 4320, 66, 1080, 1080, 66, 66, 1080, 66, 4320, 4320, 66, 4320, 4320, 66, 1080, 4320, 66, 66, 1080, 1080, 66, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 66, 66, 1080, 66, 4320, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 66, 4320, 4320, 1080, 66, 4320, 1080, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 4320, 66, 1080, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 4320, 66, 66, 66, 1080, 66, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 1080, 1080, 1080, 66, 1080, 66, 66, 4320, 66, 1080, 66, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 4320, 66, 66, 66]
Prompts retrieved: 699648 . Total input tokens: 155958110 . Total output tokens: 137481854
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 53.22296896064654,
    "estimated_duration": 3600.034176776489,
    "input_throughput": 5477.57966499558,
    "output_throughput": 4787.623992901356,
    "total_throughput": 10265.203657896936,
    "itl": 118.12928072929789,
    "ttft": 1745349.3832541993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 767,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.071717727505698,
    "arrivals": 232701,
    "finished_requests": 80037,
    "scheduler_time": 181.84201004820824
}
#Debug simulation 
Total elapsed time: 53.22315961075947. Arrivals time: 0.3705623261630535 Scheduler time: 52.68672005319968 Scheduler overhead time: 0.06114105274900794 Adapter cache time: 0.019982983823865652 Engine time: 0.05991582898423076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 4320, 1080, 66, 4320, 4320, 66, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 66, 66, 1080, 66, 4320, 66, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 4320, 4320, 66, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 4320, 1080, 1080, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 1080, 66, 4320, 1080, 1080, 66, 4320, 4320, 4320, 66, 1080, 4320, 66, 4320, 1080, 1080, 4320, 4320, 66, 1080, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 4320, 66, 1080, 1080, 66, 66, 1080, 66, 4320, 4320, 66, 4320, 4320, 66, 1080, 4320, 66, 66, 1080, 1080, 66, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 66, 66, 1080, 66, 4320, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 66, 4320, 4320, 1080, 66, 4320, 1080, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 4320, 66, 1080, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 4320, 66, 66, 66, 1080, 66, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 1080, 1080, 1080, 66, 1080, 66, 66, 4320, 66, 1080, 66, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 4320, 66, 66, 66]
Prompts retrieved: 699648 . Total input tokens: 155958110 . Total output tokens: 137481854
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 40.44600157672539,
    "estimated_duration": 3600.0177104087406,
    "input_throughput": 5335.17847550236,
    "output_throughput": 4676.648104069594,
    "total_throughput": 10011.826579571954,
    "itl": 111.47010306998806,
    "ttft": 1776047.6208597342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 965,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.0600508983573045,
    "arrivals": 232701,
    "finished_requests": 78049,
    "scheduler_time": 186.4531043383829
}
#Debug simulation 
Total elapsed time: 40.446201275102794. Arrivals time: 0.35070861503481865 Scheduler time: 39.926648697350174 Scheduler overhead time: 0.060723199509084225 Adapter cache time: 0.022612723987549543 Engine time: 0.06023509381338954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 4320, 1080, 66, 4320, 4320, 66, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 66, 66, 1080, 66, 4320, 66, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 4320, 4320, 66, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 4320, 1080, 1080, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 1080, 66, 4320, 1080, 1080, 66, 4320, 4320, 4320, 66, 1080, 4320, 66, 4320, 1080, 1080, 4320, 4320, 66, 1080, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 4320, 66, 1080, 1080, 66, 66, 1080, 66, 4320, 4320, 66, 4320, 4320, 66, 1080, 4320, 66, 66, 1080, 1080, 66, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 66, 66, 1080, 66, 4320, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 66, 4320, 4320, 1080, 66, 4320, 1080, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 4320, 66, 1080, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 4320, 66, 66, 66, 1080, 66, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 1080, 1080, 1080, 66, 1080, 66, 66, 4320, 66, 1080, 66, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 4320, 66, 66, 66]
Prompts retrieved: 699648 . Total input tokens: 155958110 . Total output tokens: 137481854
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 29.926112751010805,
    "estimated_duration": 3600.077967193566,
    "input_throughput": 5040.065844502977,
    "output_throughput": 4422.998375341535,
    "total_throughput": 9463.064219844511,
    "itl": 99.42680875505467,
    "ttft": 1822084.020900382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.275048801088666,
    "arrivals": 232701,
    "finished_requests": 73787,
    "scheduler_time": 197.37745702121333
}
#Debug simulation 
Total elapsed time: 29.92623780015856. Arrivals time: 0.31540311546996236 Scheduler time: 29.433905530255288 Scheduler overhead time: 0.06262996699661016 Adapter cache time: 0.025953921489417553 Engine time: 0.06171678937971592 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 4320, 1080, 66, 4320, 4320, 66, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 66, 66, 1080, 66, 4320, 66, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 4320, 4320, 66, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 4320, 1080, 1080, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 1080, 66, 4320, 1080, 1080, 66, 4320, 4320, 4320, 66, 1080, 4320, 66, 4320, 1080, 1080, 4320, 4320, 66, 1080, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 4320, 66, 1080, 1080, 66, 66, 1080, 66, 4320, 4320, 66, 4320, 4320, 66, 1080, 4320, 66, 66, 1080, 1080, 66, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 66, 66, 1080, 66, 4320, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 66, 4320, 4320, 1080, 66, 4320, 1080, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 4320, 66, 1080, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 4320, 66, 66, 66, 1080, 66, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 1080, 1080, 1080, 66, 1080, 66, 66, 4320, 66, 1080, 66, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 4320, 66, 66, 66]
Prompts retrieved: 699648 . Total input tokens: 155958110 . Total output tokens: 137481854
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 49.41051010880619,
    "estimated_duration": 3600.0577664974066,
    "input_throughput": 5342.10539035634,
    "output_throughput": 4673.938889700347,
    "total_throughput": 10016.044280056687,
    "itl": 111.34173115852875,
    "ttft": 1755193.1217005455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 931,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.379331781291392,
    "arrivals": 232701,
    "finished_requests": 78066,
    "scheduler_time": 186.66794315583965
}
#Debug simulation 
Total elapsed time: 49.41069273278117. Arrivals time: 0.3692294196225703 Scheduler time: 48.86998844286427 Scheduler overhead time: 0.06244495278224349 Adapter cache time: 0.0220555174164474 Engine time: 0.06139389984309673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 4320, 1080, 66, 4320, 4320, 66, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 66, 66, 1080, 66, 4320, 66, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 4320, 4320, 66, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 4320, 1080, 1080, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 1080, 66, 4320, 1080, 1080, 66, 4320, 4320, 4320, 66, 1080, 4320, 66, 4320, 1080, 1080, 4320, 4320, 66, 1080, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 4320, 66, 1080, 1080, 66, 66, 1080, 66, 4320, 4320, 66, 4320, 4320, 66, 1080, 4320, 66, 66, 1080, 1080, 66, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 66, 66, 1080, 66, 4320, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 66, 4320, 4320, 1080, 66, 4320, 1080, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 4320, 66, 1080, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 4320, 66, 66, 66, 1080, 66, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 1080, 1080, 1080, 66, 1080, 66, 66, 4320, 66, 1080, 66, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 4320, 66, 66, 66]
Prompts retrieved: 699648 . Total input tokens: 155958110 . Total output tokens: 137481854
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 33.38058369792998,
    "estimated_duration": 3600.0714691314593,
    "input_throughput": 5055.952959842572,
    "output_throughput": 4428.597914431521,
    "total_throughput": 9484.550874274093,
    "itl": 99.58562988903378,
    "ttft": 1817096.6402302815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1054,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.861854776497948,
    "arrivals": 232701,
    "finished_requests": 73954,
    "scheduler_time": 197.2382022015552
}
#Debug simulation 
Total elapsed time: 33.38077407795936. Arrivals time: 0.33383561903610826 Scheduler time: 32.868813472334296 Scheduler overhead time: 0.06387761188670993 Adapter cache time: 0.02398307016119361 Engine time: 0.06327701825648546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 4320, 1080, 66, 4320, 4320, 66, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 66, 66, 1080, 66, 4320, 66, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 4320, 4320, 66, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 4320, 1080, 1080, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 1080, 66, 4320, 1080, 1080, 66, 4320, 4320, 4320, 66, 1080, 4320, 66, 4320, 1080, 1080, 4320, 4320, 66, 1080, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 4320, 66, 1080, 1080, 66, 66, 1080, 66, 4320, 4320, 66, 4320, 4320, 66, 1080, 4320, 66, 66, 1080, 1080, 66, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 66, 66, 1080, 66, 4320, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 66, 4320, 4320, 1080, 66, 4320, 1080, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 4320, 66, 1080, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 4320, 66, 66, 66, 1080, 66, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 1080, 1080, 1080, 66, 1080, 66, 66, 4320, 66, 1080, 66, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 4320, 66, 66, 66]
Prompts retrieved: 699648 . Total input tokens: 155958110 . Total output tokens: 137481854
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 53.62759509542957,
    "estimated_duration": 3600.013225219621,
    "input_throughput": 5337.499558443365,
    "output_throughput": 4668.9189590337155,
    "total_throughput": 10006.418517477081,
    "itl": 111.0641241411698,
    "ttft": 1758113.947341362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.202895566564959,
    "arrivals": 232701,
    "finished_requests": 77960,
    "scheduler_time": 187.0652853962614
}
#Debug simulation 
Total elapsed time: 53.62777750939131. Arrivals time: 0.3767494522035122 Scheduler time: 53.07845071516931 Scheduler overhead time: 0.06380060035735369 Adapter cache time: 0.02113732835277915 Engine time: 0.06215425115078688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 4320, 1080, 66, 4320, 4320, 66, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 66, 66, 1080, 66, 4320, 66, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 4320, 4320, 66, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 4320, 1080, 1080, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 66, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 1080, 66, 4320, 1080, 1080, 66, 4320, 4320, 4320, 66, 1080, 4320, 66, 4320, 1080, 1080, 4320, 4320, 66, 1080, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 1080, 1080, 1080, 66, 66, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 4320, 66, 1080, 1080, 66, 66, 1080, 66, 4320, 4320, 66, 4320, 4320, 66, 1080, 4320, 66, 66, 1080, 1080, 66, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 4320, 1080, 1080, 1080, 4320, 66, 1080, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 66, 66, 1080, 66, 4320, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 66, 4320, 4320, 1080, 66, 4320, 1080, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 4320, 66, 1080, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 4320, 66, 66, 66, 1080, 66, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 66, 1080, 1080, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 1080, 1080, 1080, 66, 1080, 66, 66, 4320, 66, 1080, 66, 66, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 4320, 66, 66, 66]
Prompts retrieved: 699648 . Total input tokens: 155958110 . Total output tokens: 137481854
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 25.290766783989966,
    "estimated_duration": 3600.061876419728,
    "input_throughput": 5059.989973871266,
    "output_throughput": 4425.603377641071,
    "total_throughput": 9485.593351512338,
    "itl": 99.5491426471179,
    "ttft": 1820496.9849392609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.743518525808991,
    "arrivals": 232701,
    "finished_requests": 73984,
    "scheduler_time": 197.17814818686998
}
#Debug simulation 
Total elapsed time: 25.29095343593508. Arrivals time: 0.3353749648667872 Scheduler time: 24.777137991040945 Scheduler overhead time: 0.062609126791358 Adapter cache time: 0.02767112571746111 Engine time: 0.06145891081541777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 4320, 1080, 33, 4320, 4320, 33, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 33, 33, 1080, 33, 4320, 33, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 4320, 4320, 33, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 4320, 1080, 1080, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 1080, 33, 4320, 1080, 1080, 33, 4320, 4320, 4320, 33, 1080, 4320, 33, 4320, 1080, 1080, 4320, 4320, 33, 1080, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 4320, 33, 1080, 1080, 33, 33, 1080, 33, 4320, 4320, 33, 4320, 4320, 33, 1080, 4320, 33, 33, 1080, 1080, 33, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 33, 33, 1080, 33, 4320, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 33, 4320, 4320, 1080, 33, 4320, 1080, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 33, 4320, 33, 1080, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 4320, 33, 33, 33, 1080, 33, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 1080, 1080, 1080, 33, 1080, 33, 33, 4320, 33, 1080, 33, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 4320, 33, 33, 33]
Prompts retrieved: 695424 . Total input tokens: 155001906 . Total output tokens: 136655995
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 42.58854354918003,
    "estimated_duration": 3600.060704868903,
    "input_throughput": 5461.4712394734415,
    "output_throughput": 4797.662710698362,
    "total_throughput": 10259.133950171805,
    "itl": 118.45995381800486,
    "ttft": 1742895.2877661402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 894,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.911493674563367,
    "arrivals": 231176,
    "finished_requests": 80012,
    "scheduler_time": 181.03613560067768
}
#Debug simulation 
Total elapsed time: 42.588736976031214. Arrivals time: 0.376673246268183 Scheduler time: 42.048906376585364 Scheduler overhead time: 0.05966279376298189 Adapter cache time: 0.020965329371392727 Engine time: 0.058324553072452545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 4320, 1080, 33, 4320, 4320, 33, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 33, 33, 1080, 33, 4320, 33, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 4320, 4320, 33, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 4320, 1080, 1080, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 1080, 33, 4320, 1080, 1080, 33, 4320, 4320, 4320, 33, 1080, 4320, 33, 4320, 1080, 1080, 4320, 4320, 33, 1080, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 4320, 33, 1080, 1080, 33, 33, 1080, 33, 4320, 4320, 33, 4320, 4320, 33, 1080, 4320, 33, 33, 1080, 1080, 33, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 33, 33, 1080, 33, 4320, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 33, 4320, 4320, 1080, 33, 4320, 1080, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 33, 4320, 33, 1080, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 4320, 33, 33, 33, 1080, 33, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 1080, 1080, 1080, 33, 1080, 33, 33, 4320, 33, 1080, 33, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 4320, 33, 33, 33]
Prompts retrieved: 695424 . Total input tokens: 155001906 . Total output tokens: 136655995
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 46.14990011136979,
    "estimated_duration": 3600.1023752641754,
    "input_throughput": 5174.096472362992,
    "output_throughput": 4544.202718346179,
    "total_throughput": 9718.299190709171,
    "itl": 104.69378677053048,
    "ttft": 1775892.558353107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 677,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.935508146970539,
    "arrivals": 231176,
    "finished_requests": 75735,
    "scheduler_time": 192.77653575398793
}
#Debug simulation 
Total elapsed time: 46.15009971894324. Arrivals time: 0.3676525498740375 Scheduler time: 45.60618489142507 Scheduler overhead time: 0.06536861788481474 Adapter cache time: 0.019999248441308737 Engine time: 0.06448169890791178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 4320, 1080, 33, 4320, 4320, 33, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 33, 33, 1080, 33, 4320, 33, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 4320, 4320, 33, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 4320, 1080, 1080, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 1080, 33, 4320, 1080, 1080, 33, 4320, 4320, 4320, 33, 1080, 4320, 33, 4320, 1080, 1080, 4320, 4320, 33, 1080, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 4320, 33, 1080, 1080, 33, 33, 1080, 33, 4320, 4320, 33, 4320, 4320, 33, 1080, 4320, 33, 33, 1080, 1080, 33, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 33, 33, 1080, 33, 4320, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 33, 4320, 4320, 1080, 33, 4320, 1080, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 33, 4320, 33, 1080, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 4320, 33, 33, 33, 1080, 33, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 1080, 1080, 1080, 33, 1080, 33, 33, 4320, 33, 1080, 33, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 4320, 33, 33, 33]
Prompts retrieved: 695424 . Total input tokens: 155001906 . Total output tokens: 136655995
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 35.044581120833755,
    "estimated_duration": 3600.0719632287687,
    "input_throughput": 5018.364128420606,
    "output_throughput": 4420.437747507533,
    "total_throughput": 9438.801875928139,
    "itl": 99.04183122888274,
    "ttft": 1816505.0902294815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 877,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.588871300667545,
    "arrivals": 231176,
    "finished_requests": 73698,
    "scheduler_time": 197.72347504437633
}
#Debug simulation 
Total elapsed time: 35.04471520613879. Arrivals time: 0.34684231597930193 Scheduler time: 34.51717142527923 Scheduler overhead time: 0.06591707188636065 Adapter cache time: 0.02207927918061614 Engine time: 0.06516486685723066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 4320, 1080, 33, 4320, 4320, 33, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 33, 33, 1080, 33, 4320, 33, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 4320, 4320, 33, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 4320, 1080, 1080, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 1080, 33, 4320, 1080, 1080, 33, 4320, 4320, 4320, 33, 1080, 4320, 33, 4320, 1080, 1080, 4320, 4320, 33, 1080, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 4320, 33, 1080, 1080, 33, 33, 1080, 33, 4320, 4320, 33, 4320, 4320, 33, 1080, 4320, 33, 33, 1080, 1080, 33, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 33, 33, 1080, 33, 4320, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 33, 4320, 4320, 1080, 33, 4320, 1080, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 33, 4320, 33, 1080, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 4320, 33, 33, 33, 1080, 33, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 1080, 1080, 1080, 33, 1080, 33, 33, 4320, 33, 1080, 33, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 4320, 33, 33, 33]
Prompts retrieved: 695424 . Total input tokens: 155001906 . Total output tokens: 136655995
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 46.66961946012452,
    "estimated_duration": 3600.0706834147663,
    "input_throughput": 5333.083066521563,
    "output_throughput": 4674.092671991391,
    "total_throughput": 10007.175738512955,
    "itl": 111.20960297380874,
    "ttft": 1768882.7085805621,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 814,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.579660634258754,
    "arrivals": 231176,
    "finished_requests": 78004,
    "scheduler_time": 186.53657874235887
}
#Debug simulation 
Total elapsed time: 46.66979710943997. Arrivals time: 0.3769255531951785 Scheduler time: 46.12237480143085 Scheduler overhead time: 0.06232925225049257 Adapter cache time: 0.020547525491565466 Engine time: 0.062175719533115625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 4320, 1080, 33, 4320, 4320, 33, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 33, 33, 1080, 33, 4320, 33, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 4320, 4320, 33, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 4320, 1080, 1080, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 1080, 33, 4320, 1080, 1080, 33, 4320, 4320, 4320, 33, 1080, 4320, 33, 4320, 1080, 1080, 4320, 4320, 33, 1080, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 4320, 33, 1080, 1080, 33, 33, 1080, 33, 4320, 4320, 33, 4320, 4320, 33, 1080, 4320, 33, 33, 1080, 1080, 33, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 33, 33, 1080, 33, 4320, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 33, 4320, 4320, 1080, 33, 4320, 1080, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 33, 4320, 33, 1080, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 4320, 33, 33, 33, 1080, 33, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 1080, 1080, 1080, 33, 1080, 33, 33, 4320, 33, 1080, 33, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 4320, 33, 33, 33]
Prompts retrieved: 695424 . Total input tokens: 155001906 . Total output tokens: 136655995
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 40.511961217969656,
    "estimated_duration": 3600.0250003873184,
    "input_throughput": 5019.3781982224855,
    "output_throughput": 4414.372677492582,
    "total_throughput": 9433.750875715066,
    "itl": 98.84674701786732,
    "ttft": 1812501.3259306878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 798,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.948119981871013,
    "arrivals": 231176,
    "finished_requests": 73493,
    "scheduler_time": 198.02202590776082
}
#Debug simulation 
Total elapsed time: 40.51214849669486. Arrivals time: 0.3568641832098365 Scheduler time: 39.97477534133941 Scheduler overhead time: 0.06669799564406276 Adapter cache time: 0.021384257823228836 Engine time: 0.06507659889757633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 4320, 1080, 33, 4320, 4320, 33, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 33, 33, 1080, 33, 4320, 33, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 4320, 4320, 33, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 4320, 1080, 1080, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 1080, 33, 4320, 1080, 1080, 33, 4320, 4320, 4320, 33, 1080, 4320, 33, 4320, 1080, 1080, 4320, 4320, 33, 1080, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 4320, 33, 1080, 1080, 33, 33, 1080, 33, 4320, 4320, 33, 4320, 4320, 33, 1080, 4320, 33, 33, 1080, 1080, 33, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 33, 33, 1080, 33, 4320, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 33, 4320, 4320, 1080, 33, 4320, 1080, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 33, 4320, 33, 1080, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 4320, 33, 33, 33, 1080, 33, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 1080, 1080, 1080, 33, 1080, 33, 33, 4320, 33, 1080, 33, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 4320, 33, 33, 33]
Prompts retrieved: 695424 . Total input tokens: 155001906 . Total output tokens: 136655995
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 54.76002877717838,
    "estimated_duration": 3600.1015542726773,
    "input_throughput": 5330.209359573686,
    "output_throughput": 4675.077840519499,
    "total_throughput": 10005.287200093186,
    "itl": 111.1089158940135,
    "ttft": 1754045.8668834695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5517356306267684,
    "arrivals": 231176,
    "finished_requests": 77981,
    "scheduler_time": 186.71919983711174
}
#Debug simulation 
Total elapsed time: 54.76021907990798. Arrivals time: 0.3832436194643378 Scheduler time: 54.201841560192406 Scheduler overhead time: 0.06424850691109896 Adapter cache time: 0.020003913901746273 Engine time: 0.06504952209070325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_384_slots_64_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 4320, 1080, 33, 4320, 4320, 33, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 33, 33, 1080, 33, 4320, 33, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 4320, 4320, 33, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 4320, 1080, 1080, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 4320, 4320, 33, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 1080, 33, 4320, 1080, 1080, 33, 4320, 4320, 4320, 33, 1080, 4320, 33, 4320, 1080, 1080, 4320, 4320, 33, 1080, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 1080, 1080, 1080, 33, 33, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 4320, 33, 1080, 1080, 33, 33, 1080, 33, 4320, 4320, 33, 4320, 4320, 33, 1080, 4320, 33, 33, 1080, 1080, 33, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 4320, 1080, 1080, 1080, 4320, 33, 1080, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 33, 33, 1080, 33, 4320, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 33, 4320, 4320, 1080, 33, 4320, 1080, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 33, 4320, 33, 1080, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 4320, 33, 33, 33, 1080, 33, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 33, 1080, 1080, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 1080, 1080, 1080, 33, 1080, 33, 33, 4320, 33, 1080, 33, 33, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 4320, 33, 33, 33]
Prompts retrieved: 695424 . Total input tokens: 155001906 . Total output tokens: 136655995
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 41.397509186994284,
    "estimated_duration": 3600.048655434734,
    "input_throughput": 5013.636405372527,
    "output_throughput": 4407.338766393417,
    "total_throughput": 9420.975171765944,
    "itl": 98.47871064892189,
    "ttft": 1808042.537729432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 767,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.663070477079629,
    "arrivals": 231176,
    "finished_requests": 73369,
    "scheduler_time": 198.4184757426108
}
#Debug simulation 
Total elapsed time: 41.397699284832925. Arrivals time: 0.3569468413479626 Scheduler time: 40.86030498286709 Scheduler overhead time: 0.06625506980344653 Adapter cache time: 0.021040077321231365 Engine time: 0.06549033429473639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 540, 270, 4320, 4320, 270, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 270, 4320, 270, 4320, 540, 540, 540, 270, 270, 540, 270, 4320, 270, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 4320, 4320, 270, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 540, 270, 270, 270, 540, 270, 540, 270, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 4320, 4320, 540, 270, 270, 4320, 540, 540, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 4320, 270, 270, 4320, 270, 4320, 540, 270, 4320, 540, 540, 270, 4320, 4320, 4320, 270, 540, 4320, 270, 4320, 540, 540, 4320, 4320, 270, 540, 4320, 270, 540, 540, 270, 540, 270, 540, 4320, 4320, 540, 270, 4320, 270, 4320, 540, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 270, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 270, 270, 270, 270, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270, 540, 4320, 4320, 540, 540, 270, 4320, 540, 4320, 270, 540, 540, 270, 270, 540, 270, 4320, 4320, 270, 4320, 4320, 270, 540, 4320, 270, 270, 540, 540, 270, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 540, 270, 4320, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 270, 4320, 4320, 540, 270, 4320, 540, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 540, 4320, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 4320, 270, 270, 270, 540, 270, 540, 540, 270, 4320, 540, 4320, 270, 4320, 270, 540, 540, 540, 540, 270, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 540, 540, 540, 270, 540, 270, 270, 4320, 270, 540, 270, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 656640 . Total input tokens: 146301359 . Total output tokens: 129047843
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 58.55154295312241,
    "estimated_duration": 3600.0003788132376,
    "input_throughput": 5505.126087384823,
    "output_throughput": 4782.066719024951,
    "total_throughput": 10287.192806409774,
    "itl": 117.34692886043949,
    "ttft": 1723395.1136812891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0005074643297736,
    "arrivals": 218283,
    "finished_requests": 80097,
    "scheduler_time": 181.36082640138653
}
#Debug simulation 
Total elapsed time: 58.551721927244216. Arrivals time: 0.3945080735720694 Scheduler time: 57.99061236763373 Scheduler overhead time: 0.062093641608953476 Adapter cache time: 0.01847330527380109 Engine time: 0.061132075265049934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 540, 270, 4320, 4320, 270, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 270, 4320, 270, 4320, 540, 540, 540, 270, 270, 540, 270, 4320, 270, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 4320, 4320, 270, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 540, 270, 270, 270, 540, 270, 540, 270, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 4320, 4320, 540, 270, 270, 4320, 540, 540, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 4320, 270, 270, 4320, 270, 4320, 540, 270, 4320, 540, 540, 270, 4320, 4320, 4320, 270, 540, 4320, 270, 4320, 540, 540, 4320, 4320, 270, 540, 4320, 270, 540, 540, 270, 540, 270, 540, 4320, 4320, 540, 270, 4320, 270, 4320, 540, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 270, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 270, 270, 270, 270, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270, 540, 4320, 4320, 540, 540, 270, 4320, 540, 4320, 270, 540, 540, 270, 270, 540, 270, 4320, 4320, 270, 4320, 4320, 270, 540, 4320, 270, 270, 540, 540, 270, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 540, 270, 4320, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 270, 4320, 4320, 540, 270, 4320, 540, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 540, 4320, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 4320, 270, 270, 270, 540, 270, 540, 540, 270, 4320, 540, 4320, 270, 4320, 270, 540, 540, 540, 540, 270, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 540, 540, 540, 270, 540, 270, 270, 4320, 270, 540, 270, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 656640 . Total input tokens: 146301359 . Total output tokens: 129047843
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 52.57354040397331,
    "estimated_duration": 3600.03056343936,
    "input_throughput": 5389.521188248885,
    "output_throughput": 4682.3160811989965,
    "total_throughput": 10071.837269447882,
    "itl": 111.04037110389807,
    "ttft": 1722524.9108591604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 659,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.816432907120332,
    "arrivals": 218283,
    "finished_requests": 78440,
    "scheduler_time": 185.6414561655377
}
#Debug simulation 
Total elapsed time: 52.57373832212761. Arrivals time: 0.38105552550405264 Scheduler time: 52.01883481768891 Scheduler overhead time: 0.06497328169643879 Adapter cache time: 0.01944839209318161 Engine time: 0.06327562965452671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 540, 270, 4320, 4320, 270, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 270, 4320, 270, 4320, 540, 540, 540, 270, 270, 540, 270, 4320, 270, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 4320, 4320, 270, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 540, 270, 270, 270, 540, 270, 540, 270, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 4320, 4320, 540, 270, 270, 4320, 540, 540, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 4320, 270, 270, 4320, 270, 4320, 540, 270, 4320, 540, 540, 270, 4320, 4320, 4320, 270, 540, 4320, 270, 4320, 540, 540, 4320, 4320, 270, 540, 4320, 270, 540, 540, 270, 540, 270, 540, 4320, 4320, 540, 270, 4320, 270, 4320, 540, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 270, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 270, 270, 270, 270, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270, 540, 4320, 4320, 540, 540, 270, 4320, 540, 4320, 270, 540, 540, 270, 270, 540, 270, 4320, 4320, 270, 4320, 4320, 270, 540, 4320, 270, 270, 540, 540, 270, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 540, 270, 4320, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 270, 4320, 4320, 540, 270, 4320, 540, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 540, 4320, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 4320, 270, 270, 270, 540, 270, 540, 540, 270, 4320, 540, 4320, 270, 4320, 270, 540, 540, 540, 540, 270, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 540, 540, 540, 270, 540, 270, 270, 4320, 270, 540, 270, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 656640 . Total input tokens: 146301359 . Total output tokens: 129047843
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 28.895405740011483,
    "estimated_duration": 3600.0020467819704,
    "input_throughput": 5081.304055466242,
    "output_throughput": 4416.627211146404,
    "total_throughput": 9497.931266612646,
    "itl": 99.07490809755247,
    "ttft": 1802127.5582309377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.820210510771558,
    "arrivals": 218283,
    "finished_requests": 73869,
    "scheduler_time": 196.42162628833347
}
#Debug simulation 
Total elapsed time: 28.895493730902672. Arrivals time: 0.33425473561510444 Scheduler time: 28.378163823857903 Scheduler overhead time: 0.06270362390205264 Adapter cache time: 0.031108368653804064 Engine time: 0.06231582351028919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 540, 270, 4320, 4320, 270, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 270, 4320, 270, 4320, 540, 540, 540, 270, 270, 540, 270, 4320, 270, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 4320, 4320, 270, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 540, 270, 270, 270, 540, 270, 540, 270, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 4320, 4320, 540, 270, 270, 4320, 540, 540, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 4320, 270, 270, 4320, 270, 4320, 540, 270, 4320, 540, 540, 270, 4320, 4320, 4320, 270, 540, 4320, 270, 4320, 540, 540, 4320, 4320, 270, 540, 4320, 270, 540, 540, 270, 540, 270, 540, 4320, 4320, 540, 270, 4320, 270, 4320, 540, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 270, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 270, 270, 270, 270, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270, 540, 4320, 4320, 540, 540, 270, 4320, 540, 4320, 270, 540, 540, 270, 270, 540, 270, 4320, 4320, 270, 4320, 4320, 270, 540, 4320, 270, 270, 540, 540, 270, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 540, 270, 4320, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 270, 4320, 4320, 540, 270, 4320, 540, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 540, 4320, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 4320, 270, 270, 270, 540, 270, 540, 540, 270, 4320, 540, 4320, 270, 4320, 270, 540, 540, 540, 540, 270, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 540, 540, 540, 270, 540, 270, 270, 4320, 270, 540, 270, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 656640 . Total input tokens: 146301359 . Total output tokens: 129047843
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 50.04559214413166,
    "estimated_duration": 3600.015794535878,
    "input_throughput": 5366.6458989774455,
    "output_throughput": 4668.373684778984,
    "total_throughput": 10035.01958375643,
    "itl": 110.58669585420816,
    "ttft": 1743826.3581343782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.202093499018805,
    "arrivals": 218283,
    "finished_requests": 78082,
    "scheduler_time": 186.24721568891064
}
#Debug simulation 
Total elapsed time: 50.04577683703974. Arrivals time: 0.3761103446595371 Scheduler time: 49.49988704267889 Scheduler overhead time: 0.06331607513129711 Adapter cache time: 0.01855320716276765 Engine time: 0.06219235435128212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_384_slots_64_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 540, 270, 4320, 4320, 270, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 270, 4320, 270, 4320, 540, 540, 540, 270, 270, 540, 270, 4320, 270, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 4320, 4320, 270, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 540, 270, 270, 270, 540, 270, 540, 270, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 4320, 4320, 540, 270, 270, 4320, 540, 540, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 4320, 270, 270, 4320, 270, 4320, 540, 270, 4320, 540, 540, 270, 4320, 4320, 4320, 270, 540, 4320, 270, 4320, 540, 540, 4320, 4320, 270, 540, 4320, 270, 540, 540, 270, 540, 270, 540, 4320, 4320, 540, 270, 4320, 270, 4320, 540, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 270, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 270, 270, 270, 270, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270, 540, 4320, 4320, 540, 540, 270, 4320, 540, 4320, 270, 540, 540, 270, 270, 540, 270, 4320, 4320, 270, 4320, 4320, 270, 540, 4320, 270, 270, 540, 540, 270, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 540, 270, 4320, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 270, 4320, 4320, 540, 270, 4320, 540, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 540, 4320, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 4320, 270, 270, 270, 540, 270, 540, 540, 270, 4320, 540, 4320, 270, 4320, 270, 540, 540, 540, 540, 270, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 540, 540, 540, 270, 540, 270, 270, 4320, 270, 540, 270, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 656640 . Total input tokens: 146301359 . Total output tokens: 129047843
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 29.612978163175285,
    "estimated_duration": 3600.089082307033,
    "input_throughput": 5076.59576809364,
    "output_throughput": 4418.75871299434,
    "total_throughput": 9495.35448108798,
    "itl": 99.04995884229932,
    "ttft": 1795043.3025524176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.10270442797795,
    "arrivals": 218283,
    "finished_requests": 73867,
    "scheduler_time": 196.4771465552099
}
#Debug simulation 
Total elapsed time: 29.61317369993776. Arrivals time: 0.3477881201542914 Scheduler time: 29.082130289636552 Scheduler overhead time: 0.06430120021104813 Adapter cache time: 0.028477763757109642 Engine time: 0.06342320516705513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_384_slots_64_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_384_slots_64_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 540, 270, 4320, 4320, 270, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 270, 4320, 270, 4320, 540, 540, 540, 270, 270, 540, 270, 4320, 270, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 4320, 4320, 270, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 540, 270, 270, 270, 540, 270, 540, 270, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 4320, 4320, 540, 270, 270, 4320, 540, 540, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 4320, 270, 270, 4320, 270, 4320, 540, 270, 4320, 540, 540, 270, 4320, 4320, 4320, 270, 540, 4320, 270, 4320, 540, 540, 4320, 4320, 270, 540, 4320, 270, 540, 540, 270, 540, 270, 540, 4320, 4320, 540, 270, 4320, 270, 4320, 540, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 270, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 270, 270, 270, 270, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270, 540, 4320, 4320, 540, 540, 270, 4320, 540, 4320, 270, 540, 540, 270, 270, 540, 270, 4320, 4320, 270, 4320, 4320, 270, 540, 4320, 270, 270, 540, 540, 270, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 540, 270, 4320, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 270, 4320, 4320, 540, 270, 4320, 540, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 540, 4320, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 4320, 270, 270, 270, 540, 270, 540, 540, 270, 4320, 540, 4320, 270, 4320, 270, 540, 540, 540, 540, 270, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 540, 540, 540, 270, 540, 270, 270, 4320, 270, 540, 270, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 656640 . Total input tokens: 146301359 . Total output tokens: 129047843
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 53.53484160033986,
    "estimated_duration": 3600.051385310899,
    "input_throughput": 5371.263332212154,
    "output_throughput": 4675.94297922674,
    "total_throughput": 10047.206311438895,
    "itl": 110.63889896321274,
    "ttft": 1729467.1633686346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 581,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7090580664714627,
    "arrivals": 218283,
    "finished_requests": 78148,
    "scheduler_time": 186.12152438723348
}
#Debug simulation 
Total elapsed time: 53.535038514994085. Arrivals time: 0.38474952848628163 Scheduler time: 52.97716543637216 Scheduler overhead time: 0.06454756297171116 Adapter cache time: 0.018653516191989183 Engine time: 0.06369834998622537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_384_slots_64_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_384_slots_64_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 4320, 540, 270, 4320, 4320, 270, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 270, 4320, 270, 4320, 540, 540, 540, 270, 270, 540, 270, 4320, 270, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 4320, 4320, 270, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 540, 270, 270, 270, 540, 270, 540, 270, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 4320, 4320, 540, 270, 270, 4320, 540, 540, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 270, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 4320, 270, 270, 4320, 270, 4320, 540, 270, 4320, 540, 540, 270, 4320, 4320, 4320, 270, 540, 4320, 270, 4320, 540, 540, 4320, 4320, 270, 540, 4320, 270, 540, 540, 270, 540, 270, 540, 4320, 4320, 540, 270, 4320, 270, 4320, 540, 540, 540, 540, 540, 270, 270, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 270, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 270, 270, 270, 270, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270, 540, 4320, 4320, 540, 540, 270, 4320, 540, 4320, 270, 540, 540, 270, 270, 540, 270, 4320, 4320, 270, 4320, 4320, 270, 540, 4320, 270, 270, 540, 540, 270, 270, 4320, 4320, 4320, 4320, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 270, 270, 540, 270, 4320, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 270, 4320, 4320, 540, 270, 4320, 540, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 4320, 270, 540, 4320, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 4320, 270, 270, 270, 540, 270, 540, 540, 270, 4320, 540, 4320, 270, 4320, 270, 540, 540, 540, 540, 270, 540, 4320, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 540, 540, 540, 270, 540, 270, 270, 4320, 270, 540, 270, 270, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 4320, 270, 270, 270]
Prompts retrieved: 656640 . Total input tokens: 146301359 . Total output tokens: 129047843
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 29.52640589606017,
    "estimated_duration": 3600.058895369652,
    "input_throughput": 5084.434597318033,
    "output_throughput": 4419.621862426859,
    "total_throughput": 9504.056459744892,
    "itl": 99.0369534275594,
    "ttft": 1799853.3190189633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.304057806674349,
    "arrivals": 218283,
    "finished_requests": 73930,
    "scheduler_time": 196.5360858069384
}
#Debug simulation 
Total elapsed time: 29.526600283104926. Arrivals time: 0.34501767065376043 Scheduler time: 29.00176605442539 Scheduler overhead time: 0.06347529962658882 Adapter cache time: 0.02714194869622588 Engine time: 0.06246180692687631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 540, 135, 4320, 4320, 135, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 135, 4320, 135, 4320, 540, 540, 540, 135, 135, 540, 135, 4320, 135, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 4320, 4320, 135, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 540, 135, 135, 135, 540, 135, 540, 135, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 4320, 4320, 540, 135, 135, 4320, 540, 540, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 4320, 135, 135, 4320, 135, 4320, 540, 135, 4320, 540, 540, 135, 4320, 4320, 4320, 135, 540, 4320, 135, 4320, 540, 540, 4320, 4320, 135, 540, 4320, 135, 540, 540, 135, 540, 135, 540, 4320, 4320, 540, 135, 4320, 135, 4320, 540, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 135, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 135, 135, 135, 135, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135, 540, 4320, 4320, 540, 540, 135, 4320, 540, 4320, 135, 540, 540, 135, 135, 540, 135, 4320, 4320, 135, 4320, 4320, 135, 540, 4320, 135, 135, 540, 540, 135, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 540, 135, 4320, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 135, 4320, 4320, 540, 135, 4320, 540, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 540, 4320, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 4320, 135, 135, 135, 540, 135, 540, 540, 135, 4320, 540, 4320, 135, 4320, 135, 540, 540, 540, 540, 135, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 540, 540, 540, 135, 540, 135, 135, 4320, 135, 540, 135, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 639360 . Total input tokens: 142453126 . Total output tokens: 125634299
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 57.577296674251556,
    "estimated_duration": 3600.1111870675913,
    "input_throughput": 5486.364440896134,
    "output_throughput": 4792.873081804638,
    "total_throughput": 10279.23752270077,
    "itl": 117.6516696553741,
    "ttft": 1712954.8261341946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 550,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6368249675725157,
    "arrivals": 212473,
    "finished_requests": 79848,
    "scheduler_time": 180.82900744287292
}
#Debug simulation 
Total elapsed time: 57.57748732017353. Arrivals time: 0.3833143301308155 Scheduler time: 57.02823875891045 Scheduler overhead time: 0.06223635654896498 Adapter cache time: 0.017727666068822145 Engine time: 0.061066304333508015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 540, 135, 4320, 4320, 135, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 135, 4320, 135, 4320, 540, 540, 540, 135, 135, 540, 135, 4320, 135, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 4320, 4320, 135, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 540, 135, 135, 135, 540, 135, 540, 135, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 4320, 4320, 540, 135, 135, 4320, 540, 540, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 4320, 135, 135, 4320, 135, 4320, 540, 135, 4320, 540, 540, 135, 4320, 4320, 4320, 135, 540, 4320, 135, 4320, 540, 540, 4320, 4320, 135, 540, 4320, 135, 540, 540, 135, 540, 135, 540, 4320, 4320, 540, 135, 4320, 135, 4320, 540, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 135, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 135, 135, 135, 135, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135, 540, 4320, 4320, 540, 540, 135, 4320, 540, 4320, 135, 540, 540, 135, 135, 540, 135, 4320, 4320, 135, 4320, 4320, 135, 540, 4320, 135, 135, 540, 540, 135, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 540, 135, 4320, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 135, 4320, 4320, 540, 135, 4320, 540, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 540, 4320, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 4320, 135, 135, 135, 540, 135, 540, 540, 135, 4320, 540, 4320, 135, 4320, 135, 540, 540, 540, 540, 135, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 540, 540, 540, 135, 540, 135, 135, 4320, 135, 540, 135, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 639360 . Total input tokens: 142453126 . Total output tokens: 125634299
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 53.322061019949615,
    "estimated_duration": 3600.069518622855,
    "input_throughput": 5344.432906217894,
    "output_throughput": 4669.966486211417,
    "total_throughput": 10014.39939242931,
    "itl": 110.71616280801146,
    "ttft": 1736397.0231802566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.094190370738511,
    "arrivals": 212473,
    "finished_requests": 77742,
    "scheduler_time": 186.0058681491374
}
#Debug simulation 
Total elapsed time: 53.322257891763. Arrivals time: 0.3792659384198487 Scheduler time: 52.77263021375984 Scheduler overhead time: 0.06294038519263268 Adapter cache time: 0.018297388218343258 Engine time: 0.06332391267642379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 540, 135, 4320, 4320, 135, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 135, 4320, 135, 4320, 540, 540, 540, 135, 135, 540, 135, 4320, 135, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 4320, 4320, 135, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 540, 135, 135, 135, 540, 135, 540, 135, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 4320, 4320, 540, 135, 135, 4320, 540, 540, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 4320, 135, 135, 4320, 135, 4320, 540, 135, 4320, 540, 540, 135, 4320, 4320, 4320, 135, 540, 4320, 135, 4320, 540, 540, 4320, 4320, 135, 540, 4320, 135, 540, 540, 135, 540, 135, 540, 4320, 4320, 540, 135, 4320, 135, 4320, 540, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 135, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 135, 135, 135, 135, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135, 540, 4320, 4320, 540, 540, 135, 4320, 540, 4320, 135, 540, 540, 135, 135, 540, 135, 4320, 4320, 135, 4320, 4320, 135, 540, 4320, 135, 135, 540, 540, 135, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 540, 135, 4320, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 135, 4320, 4320, 540, 135, 4320, 540, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 540, 4320, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 4320, 135, 135, 135, 540, 135, 540, 540, 135, 4320, 540, 4320, 135, 4320, 135, 540, 540, 540, 540, 135, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 540, 540, 540, 135, 540, 135, 135, 4320, 135, 540, 135, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 639360 . Total input tokens: 142453126 . Total output tokens: 125634299
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 36.5648263162002,
    "estimated_duration": 3600.068631914378,
    "input_throughput": 5060.878517283035,
    "output_throughput": 4418.815480065089,
    "total_throughput": 9479.693997348124,
    "itl": 99.02127174673659,
    "ttft": 1786804.2237802986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.463881948338813,
    "arrivals": 212473,
    "finished_requests": 73558,
    "scheduler_time": 196.40161566462157
}
#Debug simulation 
Total elapsed time: 36.565016279928386. Arrivals time: 0.35283896140754223 Scheduler time: 36.028446739539504 Scheduler overhead time: 0.06614780984818935 Adapter cache time: 0.02591997478157282 Engine time: 0.06437336467206478 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 540, 135, 4320, 4320, 135, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 135, 4320, 135, 4320, 540, 540, 540, 135, 135, 540, 135, 4320, 135, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 4320, 4320, 135, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 540, 135, 135, 135, 540, 135, 540, 135, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 4320, 4320, 540, 135, 135, 4320, 540, 540, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 4320, 135, 135, 4320, 135, 4320, 540, 135, 4320, 540, 540, 135, 4320, 4320, 4320, 135, 540, 4320, 135, 4320, 540, 540, 4320, 4320, 135, 540, 4320, 135, 540, 540, 135, 540, 135, 540, 4320, 4320, 540, 135, 4320, 135, 4320, 540, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 135, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 135, 135, 135, 135, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135, 540, 4320, 4320, 540, 540, 135, 4320, 540, 4320, 135, 540, 540, 135, 135, 540, 135, 4320, 4320, 135, 4320, 4320, 135, 540, 4320, 135, 135, 540, 540, 135, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 540, 135, 4320, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 135, 4320, 4320, 540, 135, 4320, 540, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 540, 4320, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 4320, 135, 135, 135, 540, 135, 540, 540, 135, 4320, 540, 4320, 135, 4320, 135, 540, 540, 540, 540, 135, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 540, 540, 540, 135, 540, 135, 135, 4320, 135, 540, 135, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 639360 . Total input tokens: 142453126 . Total output tokens: 125634299
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 55.437372520100325,
    "estimated_duration": 3600.1128481997644,
    "input_throughput": 5342.033655866348,
    "output_throughput": 4673.655162896819,
    "total_throughput": 10015.688818763167,
    "itl": 110.7719685490545,
    "ttft": 1734877.8269864079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.760205255462782,
    "arrivals": 212473,
    "finished_requests": 77744,
    "scheduler_time": 185.9482198739746
}
#Debug simulation 
Total elapsed time: 55.43756625475362. Arrivals time: 0.37858656933531165 Scheduler time: 54.8868427076377 Scheduler overhead time: 0.0644692974165082 Adapter cache time: 0.01798795349895954 Engine time: 0.06390998139977455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 540, 135, 4320, 4320, 135, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 135, 4320, 135, 4320, 540, 540, 540, 135, 135, 540, 135, 4320, 135, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 4320, 4320, 135, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 540, 135, 135, 135, 540, 135, 540, 135, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 4320, 4320, 540, 135, 135, 4320, 540, 540, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 4320, 135, 135, 4320, 135, 4320, 540, 135, 4320, 540, 540, 135, 4320, 4320, 4320, 135, 540, 4320, 135, 4320, 540, 540, 4320, 4320, 135, 540, 4320, 135, 540, 540, 135, 540, 135, 540, 4320, 4320, 540, 135, 4320, 135, 4320, 540, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 135, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 135, 135, 135, 135, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135, 540, 4320, 4320, 540, 540, 135, 4320, 540, 4320, 135, 540, 540, 135, 135, 540, 135, 4320, 4320, 135, 4320, 4320, 135, 540, 4320, 135, 135, 540, 540, 135, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 540, 135, 4320, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 135, 4320, 4320, 540, 135, 4320, 540, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 540, 4320, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 4320, 135, 135, 135, 540, 135, 540, 540, 135, 4320, 540, 4320, 135, 4320, 135, 540, 540, 540, 540, 135, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 540, 540, 540, 135, 540, 135, 135, 4320, 135, 540, 135, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 639360 . Total input tokens: 142453126 . Total output tokens: 125634299
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 41.84100827900693,
    "estimated_duration": 3600.028757344851,
    "input_throughput": 5062.575392714888,
    "output_throughput": 4424.474378851251,
    "total_throughput": 9487.049771566139,
    "itl": 98.98442722892467,
    "ttft": 1792727.2444790755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 711,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.301736602438645,
    "arrivals": 212473,
    "finished_requests": 73598,
    "scheduler_time": 196.53880184710184
}
#Debug simulation 
Total elapsed time: 41.84120641089976. Arrivals time: 0.358319288585335 Scheduler time: 41.30371027858928 Scheduler overhead time: 0.06578476773574948 Adapter cache time: 0.020427939482033253 Engine time: 0.06534715974703431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 540, 135, 4320, 4320, 135, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 135, 4320, 135, 4320, 540, 540, 540, 135, 135, 540, 135, 4320, 135, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 4320, 4320, 135, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 540, 135, 135, 135, 540, 135, 540, 135, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 4320, 4320, 540, 135, 135, 4320, 540, 540, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 4320, 135, 135, 4320, 135, 4320, 540, 135, 4320, 540, 540, 135, 4320, 4320, 4320, 135, 540, 4320, 135, 4320, 540, 540, 4320, 4320, 135, 540, 4320, 135, 540, 540, 135, 540, 135, 540, 4320, 4320, 540, 135, 4320, 135, 4320, 540, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 135, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 135, 135, 135, 135, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135, 540, 4320, 4320, 540, 540, 135, 4320, 540, 4320, 135, 540, 540, 135, 135, 540, 135, 4320, 4320, 135, 4320, 4320, 135, 540, 4320, 135, 135, 540, 540, 135, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 540, 135, 4320, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 135, 4320, 4320, 540, 135, 4320, 540, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 540, 4320, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 4320, 135, 135, 135, 540, 135, 540, 540, 135, 4320, 540, 4320, 135, 4320, 135, 540, 540, 540, 540, 135, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 540, 540, 540, 135, 540, 135, 135, 4320, 135, 540, 135, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 639360 . Total input tokens: 142453126 . Total output tokens: 125634299
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 53.56867404282093,
    "estimated_duration": 3600.061244646315,
    "input_throughput": 5336.456158508332,
    "output_throughput": 4671.188587418636,
    "total_throughput": 10007.64474592697,
    "itl": 110.65778226132426,
    "ttft": 1736435.7279470414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.440933386967502,
    "arrivals": 212473,
    "finished_requests": 77756,
    "scheduler_time": 186.04556707538683
}
#Debug simulation 
Total elapsed time: 53.568864486180246. Arrivals time: 0.37035624496638775 Scheduler time: 53.02922682603821 Scheduler overhead time: 0.06389169674366713 Adapter cache time: 0.01726172398775816 Engine time: 0.06262422446161509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_384_slots_64_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 4320, 540, 135, 4320, 4320, 135, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 135, 4320, 135, 4320, 540, 540, 540, 135, 135, 540, 135, 4320, 135, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 4320, 4320, 135, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 540, 135, 135, 135, 540, 135, 540, 135, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 4320, 4320, 540, 135, 135, 4320, 540, 540, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 135, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 4320, 135, 135, 4320, 135, 4320, 540, 135, 4320, 540, 540, 135, 4320, 4320, 4320, 135, 540, 4320, 135, 4320, 540, 540, 4320, 4320, 135, 540, 4320, 135, 540, 540, 135, 540, 135, 540, 4320, 4320, 540, 135, 4320, 135, 4320, 540, 540, 540, 540, 540, 135, 135, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 135, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 135, 135, 135, 135, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135, 540, 4320, 4320, 540, 540, 135, 4320, 540, 4320, 135, 540, 540, 135, 135, 540, 135, 4320, 4320, 135, 4320, 4320, 135, 540, 4320, 135, 135, 540, 540, 135, 135, 4320, 4320, 4320, 4320, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 135, 135, 540, 135, 4320, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 135, 4320, 4320, 540, 135, 4320, 540, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 4320, 135, 540, 4320, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 4320, 135, 135, 135, 540, 135, 540, 540, 135, 4320, 540, 4320, 135, 4320, 135, 540, 540, 540, 540, 135, 540, 4320, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 540, 540, 540, 135, 540, 135, 135, 4320, 135, 540, 135, 135, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 4320, 135, 135, 135]
Prompts retrieved: 639360 . Total input tokens: 142453126 . Total output tokens: 125634299
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 43.652528249658644,
    "estimated_duration": 3600.0729127512545,
    "input_throughput": 5069.02594538115,
    "output_throughput": 4421.364340600452,
    "total_throughput": 9490.390285981603,
    "itl": 98.8691963801719,
    "ttft": 1780078.103825603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 716,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.2722655355558095,
    "arrivals": 212473,
    "finished_requests": 73574,
    "scheduler_time": 196.6583933955729
}
#Debug simulation 
Total elapsed time: 43.65270671993494. Arrivals time: 0.34718432929366827 Scheduler time: 43.12408611131832 Scheduler overhead time: 0.06720313988626003 Adapter cache time: 0.020558437332510948 Engine time: 0.06588371749967337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_384_slots_64_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_384_slots_64_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 4320, 540, 66, 4320, 4320, 66, 540, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 66, 4320, 66, 4320, 540, 540, 540, 66, 66, 540, 66, 4320, 66, 4320, 540, 4320, 540, 66, 540, 4320, 540, 4320, 66, 4320, 4320, 4320, 4320, 66, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 540, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 540, 66, 66, 66, 540, 66, 540, 66, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 4320, 4320, 4320, 540, 66, 540, 4320, 4320, 540, 66, 66, 4320, 540, 540, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 4320, 4320, 66, 540, 540, 540, 540, 66, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 4320, 66, 66, 4320, 66, 4320, 540, 66, 4320, 540, 540, 66, 4320, 4320, 4320, 66, 540, 4320, 66, 4320, 540, 540, 4320, 4320, 66, 540, 4320, 66, 540, 540, 66, 540, 66, 540, 4320, 4320, 540, 66, 4320, 66, 4320, 540, 540, 540, 540, 540, 66, 66, 4320, 66, 4320, 66, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 66, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 66, 66, 66, 66, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66, 540, 4320, 4320, 540, 540, 66, 4320, 540, 4320, 66, 540, 540, 66, 66, 540, 66, 4320, 4320, 66, 4320, 4320, 66, 540, 4320, 66, 66, 540, 540, 66, 66, 4320, 4320, 4320, 4320, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 4320, 540, 540, 540, 4320, 66, 540, 540, 4320, 4320, 66, 4320, 4320, 66, 4320, 66, 66, 540, 66, 4320, 4320, 4320, 540, 66, 4320, 4320, 540, 540, 4320, 4320, 66, 4320, 4320, 540, 66, 4320, 540, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 4320, 66, 540, 4320, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 4320, 66, 66, 66, 540, 66, 540, 540, 66, 4320, 540, 4320, 66, 4320, 66, 540, 540, 540, 540, 66, 540, 4320, 4320, 66, 4320, 4320, 66, 4320, 540, 4320, 540, 540, 540, 66, 540, 66, 66, 4320, 66, 540, 66, 66, 540, 4320, 540, 4320, 66, 4320, 4320, 540, 4320, 540, 66, 4320, 66, 4320, 4320, 66, 66, 66]
Prompts retrieved: 630528 . Total input tokens: 140507344 . Total output tokens: 123871760
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 57.263729121070355,
    "estimated_duration": 3600.091657142679,
    "input_throughput": 5516.436216449619,
    "output_throughput": 4794.499874955802,
    "total_throughput": 10310.936091405421,
    "itl": 117.80528190807928,
    "ttft": 1702851.4808400455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.139368054000727,
    "arrivals": 209565,
    "finished_requests": 80014,
    "scheduler_time": 180.08034581560162
}
#Debug simulation 
Total elapsed time: 57.26391214411706. Arrivals time: 0.374703090172261 Scheduler time: 56.724314730614424 Scheduler overhead time: 0.061220680363476276 Adapter cache time: 0.018534475471824408 Engine time: 0.0606360980309546 
